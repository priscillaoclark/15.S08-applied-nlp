{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1110,
     "status": "ok",
     "timestamp": 1727919469024,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "zVye29lvxy9R",
    "outputId": "786a5418-8f23-4fd7-d9bb-63c37ecbd299"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "# _\"But the power of instruction is seldom of much efficacy, except in those happy dispositions where it is almost superfluous.\"_\n",
    "# Edward Gibbon\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsUVOy9yxy9S"
   },
   "source": [
    "\n",
    "# Recall, our definition of ML:\n",
    "##  \"Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed\"\n",
    "     - Arthur Samuel, 1959\n",
    "### Not actually his words, though the definition is fine\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# And our workflow for ML in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PlOiAc7xy9T"
   },
   "source": [
    "![](ml_workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD20gKKDxy9T"
   },
   "source": [
    "# The most important modeling parts\n",
    " - ## Finding a model that captures our understanding of reality\n",
    " - ## Finding a data representation (features) that let the model capture the bahavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "# Feature Engineering\n",
    "\n",
    "## __Feature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.__\n",
    "# $ \\\\ $\n",
    "- ## We know something about the structure of the problem\n",
    "- ## We create combinations of the data (features) that embed this knowledge\n",
    "- ## These features improve the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD20gKKDxy9T"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Imagine we have some 2-D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4836,
     "status": "ok",
     "timestamp": 1727919478221,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "kgCq0dXjxy9T"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def plot_raw_data(dataset):\n",
    "    X, y = dataset\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=y,\n",
    "        cmap=ListedColormap([\"#FF0000\", \"#0000FF\"]),\n",
    "        edgecolors=\"k\",\n",
    "        s=100,\n",
    "    )\n",
    "    h = 0.02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    plt.title(\"Classification Data\", fontsize=16)\n",
    "\n",
    "\n",
    "def plot_classifiers(dataset, classifiers, names, plot_orig=True):\n",
    "    i = 1\n",
    "    h = 0.02  # step size in the mesh\n",
    "    X, y = dataset\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    plt.figure(figsize=(16, 7))\n",
    "    num_plots = len(classifiers)\n",
    "    if plot_orig:\n",
    "        num_plots += 1\n",
    "    ax = plt.subplot(\n",
    "        1,\n",
    "        num_plots,\n",
    "        i,\n",
    "    )\n",
    "    if plot_orig:\n",
    "        ax.set_title(\"Input data\", fontsize=16)\n",
    "        ax.scatter(\n",
    "            X_train[:, 0],\n",
    "            X_train[:, 1],\n",
    "            c=y_train,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            s=100,\n",
    "        )\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(1, num_plots, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name, fontsize=16)\n",
    "        ax.text(\n",
    "            xx.max() - 0.3,\n",
    "            yy.min() + 0.3,\n",
    "            (\"accuracy=%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1727919479439,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "FPZKc_yRxy9U",
    "outputId": "6d0e9305-6b72-4e76-c356-3d8a9f038d1f"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=512,\n",
    "    class_sep=1.8,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    random_state=1,\n",
    "    n_clusters_per_class=1,\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "plot_raw_data(linearly_separable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LcgDYXKxy9U"
   },
   "source": [
    "## How should we fit this data to distinguish between <span style=\"color:red\">*red*</span> and <span style=\"color:blue\">*blue*</span>?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 2142,
     "status": "ok",
     "timestamp": 1727919486948,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "IxPWlxJ3xy9U",
    "outputId": "18aead0e-78dc-449a-95fd-8934761729db"
   },
   "outputs": [],
   "source": [
    "names = [\"logistic regression\"]\n",
    "classifiers = [LogisticRegression()]\n",
    "\n",
    "plot_classifiers(linearly_separable, classifiers, names, plot_orig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf2fgXt1xy9V"
   },
   "source": [
    "## Consider another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1727919505610,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "P1N3-3GVxy9V",
    "outputId": "dd49c1a2-3c0f-4269-9830-7acab432decc"
   },
   "outputs": [],
   "source": [
    "ds = make_circles(n_samples=512, noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "X, y = ds\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plot_raw_data((X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeYUJ7LQxy9W"
   },
   "source": [
    "# Questions:\n",
    " - ## Is this simpler than the data above?\n",
    " - ## How would we distinguish between <span style=\"color:red\">*red*</span> and <span style=\"color:blue\">*blue*</span>?\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1727919512354,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "ERybKW9kxy9W",
    "outputId": "178026b4-898b-42cd-c500-e2df14bce767"
   },
   "outputs": [],
   "source": [
    "names = [\"logistic regression\", ]\n",
    "classifiers = [LogisticRegression(),]\n",
    "\n",
    "plot_classifiers(ds, classifiers, names, plot_orig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH53ByI8xy9W"
   },
   "source": [
    "## We have a problem:\n",
    " - ## The data are, in some way, simpler than above (really only a function of $r$)\n",
    " - ## Our simplest model can't capture the behavior\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XES7_Tixy9W"
   },
   "source": [
    "## Consider another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1727919518752,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "WjdRJ2ulxy9W",
    "outputId": "87c13839-c796-4336-fed1-6c0bcf19a9df"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "cntrs = np.array([[0, 5], [0, -5], [0, 0]])\n",
    "x, y = make_blobs(n_samples=512, cluster_std=[1.0, 1.0, 1.0], centers=cntrs, random_state=1234)\n",
    "y[y == 0] = 1\n",
    "y = y - 1\n",
    "\n",
    "plot_raw_data((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1727919522581,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "eW-CJjGUxy9X",
    "outputId": "fd3b4a05-69b5-4222-c513-277cff1c01b3"
   },
   "outputs": [],
   "source": [
    "names = [\"logistic regression\", ]\n",
    "classifiers = [LogisticRegression(),]\n",
    "\n",
    "plot_classifiers((x,y), classifiers, names, plot_orig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkRSzV8txy9X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_97eiyAxy9X"
   },
   "source": [
    "## What can we do to make our model better fit our data?\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "\n",
    "## In this case a good ML practitioner would\n",
    " - ## engineer features that make the data easier to model\n",
    " - ## use a model that can more easily find this better representation by itself\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## What do these look like for our case\n",
    " - ## What can we do to our data?\n",
    " - ## What other models might we choose?\n",
    "\n",
    "# $ \\\\ $\n",
    "\n",
    "## In this case\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "## Three blobs\n",
    " - ## Polynomial function of $x_1$ and $x_2$ to capture curvey behavior\n",
    " - ## Or a better model would be a decision tree, which can make multiple horizontal decision boundaries.\n",
    "\n",
    "# $ \\\\ $\n",
    "## Circle Data\n",
    " - ## Polynomial function of $x_1$ and $x_2$ to capture curvey behavior\n",
    "   - ## In this case it's perhaps more obvious\n",
    " - ## Need a model that allows for curved boundaries (hard)\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Let's try to engineer features for the circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "executionInfo": {
     "elapsed": 2564,
     "status": "ok",
     "timestamp": 1727919530689,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "_P3b7VF3xy9X",
    "outputId": "4684c51d-8d45-4c0e-f13f-5af1250917b1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ds = make_circles(n_samples=512, noise=0.2, factor=0.5, random_state=1)\n",
    "X, y = ds\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "poly_transformer = PolynomialFeatures(degree=26)\n",
    "\n",
    "\n",
    "pline = make_pipeline(poly_transformer, LogisticRegression())\n",
    "classifiers = [LogisticRegression(), pline]\n",
    "names = [\"Logistic Regression\", \"Transformed Logistic Regression\"]\n",
    "plot_classifiers(ds, classifiers, names, plot_orig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmyvmwFDxy9X"
   },
   "source": [
    "## What are the downsides of this approach?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    " - ## Do we have enough data to fit a 26th degree polynomial?\n",
    " - ## Did we overfit (look at the decision boundary)\n",
    " - ## Polynomials are in principle able to capture anything, but they might be inefficient!\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Maybe there are better transformations to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1727919545485,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "Fplic-kdxy9X",
    "outputId": "dc305312-e0d6-4989-94c3-10220d47a7bb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "def dist_from_origin(x):\n",
    "    return (x[:, 0] ** 2 + x[:, 1] ** 2)[:, np.newaxis]\n",
    "\n",
    "\n",
    "dist_transformer = FunctionTransformer(dist_from_origin)\n",
    "\n",
    "\n",
    "pline = make_pipeline(dist_transformer, LogisticRegression())\n",
    "classifiers = [LogisticRegression(), pline]\n",
    "names = [\"Logistic Regression\", \"Transformed Logistic Regression\"]\n",
    "plot_classifiers(ds, classifiers, names, plot_orig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdohRYi3xy9Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu0VUqLKxy9Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t76p59vmxy9Y"
   },
   "source": [
    "# There is a fundamental and necessary compatibility between a model and the representation of the data\n",
    " - ## When the two are incompatible we fail to capture even simple behavior\n",
    "\n",
    "\n",
    "$ \\\\ $\n",
    "$ \\\\ $\n",
    "# All ML methods find \"better\" representations of data for a given problem\n",
    " - ## E.g. Logistic regression finding linear combinations of features that are better at predicting a given outcome\n",
    " - ## E.g. a decision tree finds a set of y/n questions that are better at prediction outcomes\n",
    "\n",
    "$ \\\\ $\n",
    "$ \\\\ $\n",
    "# But we typically have to \"help\" the models by doing feature engineering\n",
    " - ## E.g. transforming the bag of words into TFIDF\n",
    " - ## E.g. standardizing data to have 0 mean and unit variance\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# And sometimes that can be hard\n",
    " - ## BOW / TFIDF: `The quick brown fox jumps over the lazy dog`\n",
    " - ## BOW / TFIDF: `The quick brown dog jumps over the lazy fox`\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Different data representations are amenable to different modeling techniques\n",
    " - ## E.g. Tree-based methods are invariant to scaling features\n",
    " - ## Linear regression does not do well with correlation between covariates\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "![ml-evolution](ml-evolution-1.jpg)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "![ml-evolution](ml-evolution-2.jpg)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec5bFzM5xy9Y"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# As our problems get more complex, it gets harder to feature engineer\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## There are cases where the boundary is clear / intuitive. Wouldn't it be nice if we could build models that capture arbitrary behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 2374,
     "status": "ok",
     "timestamp": 1727919557597,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "z5tintz4xy9Y",
    "outputId": "b890cb55-d5d7-4728-adb7-4c4fcda09453"
   },
   "outputs": [],
   "source": [
    "ds = make_moons(n_samples=512, noise=0.2, random_state=1)\n",
    "X, y = ds\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "names = [\"logistic regression\", \"MLP\"]\n",
    "classifiers = [LogisticRegression(), MLPClassifier(alpha=0.1, max_iter=1000)]\n",
    "plot_classifiers((X, y), classifiers, names, plot_orig=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "QG8QYymqxy9Y"
   },
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "for device in physical_devices:\n",
    "    print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run this if you are testing your GPU."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable logging to see device placement\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# List all available physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available Physical Devices:\")\n",
    "for device in physical_devices:\n",
    "    print(f\" - {device}\")\n",
    "\n",
    "# Specifically check for GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"\\nGPU Devices Detected:\")\n",
    "    for gpu in gpus:\n",
    "        print(f\" - {gpu}\")\n",
    "    # Optionally, set TensorFlow to use the first GPU\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"TensorFlow is set to use the following GPU: {logical_gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"\\nNo GPU devices detected. TensorFlow will use the CPU.\")\n",
    "\n",
    "# Perform a test computation to see where it's executed\n",
    "print(\"\\nRunning a test computation to verify device usage...\")\n",
    "\n",
    "# Define a simple computation\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(\"\\nResult of the computation:\")\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mLLOBEtxy9Y"
   },
   "source": [
    "\n",
    "# Deep Learning / Neural Networks\n",
    " - ## Use a layered / stacked approach to machine learning the best representation\n",
    " - ## Each layer learns the best way to represent the data from the previous one\n",
    " - ## All of the layers are learned at once / together for the specific problem\n",
    "\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2OPLAyrxy9Y"
   },
   "source": [
    "\n",
    "![deep-learning](deep-learning.png)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# The ML evolution continues\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "[here](https://docs.google.com/presentation/d/14K1-_4wWnFwP2XMr_dI_m2artH1wJh9DO5euJzQ-uY4/edit)\n",
    "![ml-evolution](ml-evolution.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TE5sMzbxy9Y"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## What would a \"layered\" approach look like to solve the circles data?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## It could be\n",
    " - ## Layer 1: take $x_1$ and $x_2 $to some power $p$ (ie compute $r^{d}$ for some $d$)\n",
    " - ## Layer 2: use logistic regression to fit the data\n",
    " - ## Find the combination of $d$ and the logistic coefficients that give the best results\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## In reality, it will be hard to be this prescriptive\n",
    " - ## This is a very contrived example for our contrived data.\n",
    " - ## We'll need more general ways of layering representations\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7W0h3Ytxy9Z"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# How do we do deep learning?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "# Recall, our favorite tools\n",
    " - ## Keras\n",
    " - ## Scikit-learn\n",
    "\n",
    "# $ \\\\ $\n",
    "## Why the hell did I need to learn keras?\n",
    "# $ \\\\ $\n",
    "![ml-workflow](SKLearn_Keras.png)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## The basic keras building block is a `Layer`\n",
    "# $ \\\\ $\n",
    " - ## and they're great for stacking together to make deep models\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# We call these models of stacked numerical operations Neural Networks\n",
    " - ## AKA artificial neural netorks\n",
    " - ## AKA deep nets\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Neural networks are\n",
    " - ## Extremely powerful\n",
    " - ## Capable of learning arbitrary (well-behaved) functions\n",
    " - ## Roughly based on how our neurons (brain cells) work\n",
    "    - ### this is fairly irrelevant for understanding them\n",
    " - ## The method used for the vast majority of modern machine learning\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kJ_V4Q-xy9Z"
   },
   "source": [
    "# Building Artificial Neural Networks\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Recall:\n",
    " - ## As problems and data get more complex, feature engineering gets progressively harder\n",
    " - ## We we build feature engineering into our model directly by stacking together operations\n",
    " - ## We don't want to (and can't) be perscriptive with our operations\n",
    " - ## $x_{1}^{n} + x_{2}^{n} \\rightarrow $ logistic regression with coefficients $\\theta$ doesn't work\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## We need our building blocks to be\n",
    " - ## Powerful (able to learn lots of different functions)\n",
    " - ## Fast (we're going to do lots of operations)\n",
    " - ## Naturally able to operate on vectors / matrices of numbers\n",
    " - ## Able to change the `shape` of data to find the right next representation\n",
    "   - ## e.g. $ \\left( x_1, x_2 \\right)  \\rightarrow x_{1}^{n} + x_{2}^{n}$ is $\\mathbb{R}^2 \\rightarrow \\mathbb{R}^1$\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU-AYE5Rxy9Z"
   },
   "source": [
    "\n",
    "## We'll use matrix multiplication!\n",
    " - ## Fast: people have spent a lot of time optimizing matmul\n",
    " - ## Naturally able to operate on vectors and matrices $\\overrightarrow{y} = A\\overrightarrow{x}$\n",
    " - ## Can change the shape of data: $A_{m \\times n} B_{n \\times p} = C_{m \\times p}$\n",
    "\n",
    "# $ \\\\ $\n",
    "## Problem: matrix multiplication doesn't stack\n",
    " - ## $A_{m \\times n} B_{n \\times p} C_{p \\times q}... Z_{u \\times v} \\equiv O_{m \\times v}$\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Basic building block is a combination\n",
    " - ## Matrix multiplication and some NONLINEAR function\n",
    "\n",
    "# $ \\\\ $\n",
    "\n",
    "## E.g.\n",
    " - ## matrix multiplication ($\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}$)\n",
    " - ## sigmoid $\\sigma (x)$ ($\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{m}$)\n",
    "\n",
    "# $ \\\\ $\n",
    "## In this example, how many parameters would the building block have?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    " - ## $m \\times n$ matrix has $mn$ parameters\n",
    " - ## The activation function may have an unknown number of parameters (e.g. maybe $n$)\n",
    "\n",
    "# $ \\\\ $\n",
    "## NB\n",
    " - ## Sometimes the matrix multiplication is called a `kernel`\n",
    " - ## Sometimes the nonlinear function is called an `activation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIFJq6Swxy9Z"
   },
   "source": [
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## In fact, we've already seen a neural network\n",
    " - ## Logistic regression is just a single iteration of matrix multiplication followed by a nonlinear function\n",
    "\n",
    "# $ \\\\ $\n",
    "## We have already seen the most common neural network building block\n",
    " - ## the keras `Dense` layer is just a dense matrix (all elements non-zero) and a nonlinear function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3814,
     "status": "ok",
     "timestamp": 1727919598081,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "a4YORCSXxy9Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "Dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKpnem7Dxy9Z"
   },
   "source": [
    "## `Dense` implements the operation:\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `output = activation(dot(input, kernel) + bias)`\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where `activation` is the element-wise activation function\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; passed as the `activation` argument, `kernel` is a weights matrix\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; created by the layer, and `bias` is a bias vector created by the layer\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (only applicable if `use_bias` is `True`). These are all attributes of `Dense`.\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqjj6MJExy9a"
   },
   "source": [
    "## In practice\n",
    " - ## We'll have only a handful of different matrix operations and nonlinear function\n",
    " - ## We'll need to mix and match and stack them to build a model that solves our problem\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Who has seen a picture like this?\n",
    "![neural network](neural-net.jpg)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "# $ \\\\ $\n",
    "# Optimizing Neural Networks\n",
    "# $ \\\\ $\n",
    "![](ml_workflow.jpg)\n",
    "# $ \\\\ $\n",
    "## Recall: matrix operation + nonlinear function\n",
    " - ## A layer will have $\\mathcal{O}(mn)$ paremeters\n",
    " - ## A model can have dozens of layers\n",
    " - ## We have a lot of parameters to tweak\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Recall how keras will solve this:\n",
    " - ## Numerical optimization with SGD\n",
    " - ## We used a cousin of SGD called ADAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1PtT06exy9a",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# $ \\\\ $\n",
    "## Stochastic Gradient Descent\n",
    " - ## Estimate gradient of loss function wrt to each parameter $\\frac{\\partial J}{\\partial \\theta}$\n",
    " - ## Update parameters in direction of steepest decrease in loss $\\propto \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "<img src=\"sgd.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "# $ \\\\ $\n",
    "## Measuring $\\frac{\\partial J}{\\partial \\theta}$\n",
    " - ## We have millions/billions of parameters\n",
    " - ## Too slow to do $\\left( \\frac{\\partial J}{\\partial \\theta_{i}}\\right)_{\\theta_{j\\neq i}} = \\frac{J(\\theta + \\Delta \\theta_i) - J(\\theta)}{\\Delta \\theta_i}$\n",
    " - ## Clever trick: choose nonlinear functions with easy derivatives so that we know $\\frac{\\partial J}{\\partial \\theta}$ once we've calculated $J(\\theta)$\n",
    " - ## Linear functions already have nice derivatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSB9fLT1xy9a"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "![nice derivatives](nice-derivatives.jpg)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "#\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}} \\rightarrow \\frac{d\\sigma}{dx}=\\sigma(x)(1-\\sigma(x)) $$\n",
    "# $ \\\\ $\n",
    "#\n",
    "$$\\mathrm{ReLU}'(x)=\n",
    "\\begin{cases}\n",
    "  0, & \\text{if}\\ x < 0 \\\\\n",
    "  1, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## In both cases knowing $\\mathrm{Act}(x)$ tell you $\\frac{d\\mathrm{Act}}{dx}$\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Procedure for SGD\n",
    " - ## Take a small `batch` of data (because it's too costly to use all of it)\n",
    " - ## Calculate $J(\\theta)$ on the batch of data\n",
    " - ## Use the rules from the nonlinear function to calculate $\\frac{\\partial J}{\\partial \\theta}$\n",
    " - ## Update parameters in direction of steepest decrease of loss $\\theta \\Rightarrow \\theta + \\alpha \\frac{\\partial J}{\\partial \\theta}$\n",
    " - ## Repeat on new data until some stopping condition is met (and hope you're at the bottom of the loss surface)\n",
    "\n",
    "<img src=\"sgd.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## NB: in $\\theta \\Rightarrow \\theta + \\alpha \\frac{\\partial J}{\\partial \\theta}$ , $\\alpha$ is often called the `learning rate`: it is the rate at which we should learn to update our parameters\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPhbjZIOxy9a"
   },
   "source": [
    "### An aside on the mathematics of machine learning in general and deep learning/neural nets in particular\n",
    "- #### We will not have time in this course to review all the subtleties of neural nets, including initialization problems, vanishing gradients, etc.\n",
    "- #### If you are interested, Andrew Ng's course at Stamford is an excellent introduction to these problems. https://see.stanford.edu/Course/CS229\n",
    "- #### also https://youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&feature=shared\n",
    "- #### While deep learning currently dominates the ML-sphere, there are many other ML techniques that are useful in other areas of finance, including return prediction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk_QHHe8xy9a"
   },
   "source": [
    "## Deep learning in practice:\n",
    " - ## We will largely be able to assume that SGD/ADAM will work\n",
    " - ## We will see that we'll have to spend time making sure we don't overfit\n",
    "\n",
    "## Our primary job in designing models is to stitch together the right matrix operations and nonlinear functions to capture the behavior we want!\n",
    "- ### There are some very clever ideas and matrix manipulations that underpin LLMs\n",
    "- ### We will start with the basics and work up from there!\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "## Neural networks for deep learning\n",
    " - ## Made of up layers (mathematical operations)\n",
    " - ## Good choice for building deep models because they can be stacked to learn sequential representations\n",
    " - ## In practice, are the only way deep learing is done\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTT7NFQoxy9a"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwzpFNcdxy9a"
   },
   "source": [
    "# As problem complexity and data increase, the previous paradigm of feature engineering becomes difficult\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Why is deep learning is so popular right now?\n",
    " - ## Because it works\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "![yoda](snarky-yoda.jpg)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Why is deep learning is so effective right now?\n",
    " - ## Data is larger and more available\n",
    " - ## Compute power is larger and more available.\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# arXiv preprint publications by topic\n",
    "![arxiv publications](arxiv-stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q-Y0NIbxy9a"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Recap:\n",
    "## Deep Learning\n",
    " - ## A layered approach to ML\n",
    " - ## Learn the representation together with the model simultanesouly\n",
    "   - ## Particularly important for text, where finding the right representation is really hard\n",
    " - ## Eliminates the hardest part of ML, feature engineering\n",
    " - ## Can be done effectively with neural networks because we can stack them together\n",
    " - ## Keras is a good tool for it because it allows us to layer together operations\n",
    " - ## Our job in model selection and design is to find the right matrix operations / nonlinear functions that capture the behavior we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYOggq7Oxy9b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpiP_Trdxy9b"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cF_u_bPxy9b"
   },
   "source": [
    "# Let's return to a classification problem: sentiment analysis\n",
    "## How should we tackle it?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Steps\n",
    " - ## Load the data\n",
    " - ## turn words into numbers using the bag of words approach (`CountVectorizer`)\n",
    " - ## Construct a simple model to solve the problem\n",
    " - ## fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207,
     "status": "ok",
     "timestamp": 1727919623535,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "xkxGicgCxy9b",
    "outputId": "4cfa04a3-bdcf-4854-cacc-a6ea51bb0a9f"
   },
   "outputs": [],
   "source": [
    "# restart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1727919628435,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "y4Dw-cEnxy9b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "error",
     "timestamp": 1727919632141,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "VYenzLcnxy9b",
    "outputId": "ecad6246-3dde-4174-ba02-84c60748d365"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "error",
     "timestamp": 1727919641800,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "6UU3a5gOxy9b",
    "outputId": "fe1864c4-1815-4cbb-cacc-a150e9249e71"
   },
   "outputs": [],
   "source": [
    "# Load the IMDB data and print the number of train and test documents\n",
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('../data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlsYhZY1xy9c",
    "outputId": "9fa9d230-4f3a-416d-90fe-9be2e44a125c"
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(lowercase=True, max_features=50000)\n",
    "vec.fit(train_docs)\n",
    "term_document_matrix_train = vec.transform(train_docs)\n",
    "term_document_matrix_test = vec.transform(test_docs)\n",
    "print(term_document_matrix_train.shape, term_document_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bTBdsBYxy9c"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Recall: our job is to build the right set of matrix operations / nonlinear functions that can learn the behavior we want\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## What layer / operation should we use?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## We have a 50k-dimensional input, and we want to learn a 1-d output\n",
    "# $ \\\\ $\n",
    "## We use a `Dense` layer because all input dimensions are connected to the output\n",
    "# $ \\\\ $\n",
    "## We want our output to be a probability between 0 and 1 so we can use a sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1727919727601,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "IU0c0VOqxy9c"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "#text_input = Input(shape=(50000,1))\n",
    "output = Dense(1, activation='sigmoid')(text_input)\n",
    "model = Model(text_input, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4lyJvnnxy9c"
   },
   "source": [
    "## This should look familiar!\n",
    " - ## This is just logistic regression!\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Irg8PVEqxy9c"
   },
   "source": [
    "## How many parameters does this model have?\n",
    "# $  \\\\ $\n",
    "# $  \\\\ $\n",
    "## How many parameters in the weights matrix? Bias?\n",
    "# $  \\\\ $\n",
    "# $  \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgbHiQ2sxy9c",
    "outputId": "a777b25b-9e3f-4073-ed52-241879a36850"
   },
   "outputs": [],
   "source": [
    "term_document_matrix_train.shape[1] * 1 + 1 == model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mylLZARgxy9c",
    "outputId": "5c609e20-3260-41e1-81f5-48ad4446bf4b"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qxv1I6-xy9c"
   },
   "outputs": [],
   "source": [
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxoIxMhmxy9d",
    "outputId": "89f13790-c751-4e61-ea88-980fc77a4cc9"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kD1F3OLoxy9d",
    "outputId": "069736ee-8f48-4bc1-eceb-98fa171a83ec"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "y_predict = model.predict(term_document_matrix_test).squeeze() > 0.5\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print('Confusion Matrix:\\n', pd.DataFrame(cm), '\\n')\n",
    "print(f'f1 score = {f1_score( y_test, y_predict):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nbbfBGPxy9d"
   },
   "source": [
    "## What is logistic regression?\n",
    " - ## Shallowest possible neural network: a single transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1727919739313,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "WKuU5xXJxy9d",
    "outputId": "62d09ff9-2172-4141-be42-fd2bb5f5533f"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1727919739521,
     "user": {
      "displayName": "Andrew Zachary",
      "userId": "08217951406983746510"
     },
     "user_tz": 240
    },
    "id": "HX0N5l97xy9d",
    "outputId": "164c6da5-ae82-4528-d1e3-5ba73d347eb3"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "def plot_model_in_notebook(model):\n",
    "    ''' \n",
    "    Change the location of the dot program to the correct location for your computer\n",
    "    '''\n",
    "    return Image(model_to_dot(model, show_shapes=True, show_layer_names=True).create(prog='/usr/local/bin/dot', format='png'))\n",
    "\n",
    "\n",
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKyeb3-6xy9e"
   },
   "source": [
    "## Hypothesis: By allowing for a deeper network (more layers) we can learn an intermediate representation that is more useful.\n",
    " - ## I.e. the feature engineering will be done for us!\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## We can make deeper networks by stacking similar objects together\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHQgxJbRxy9e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxUnRX6cxy9e"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(16, activation=\"relu\")(text_input)  # <- THIS\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OVnYes4xy9e"
   },
   "source": [
    "# How many parameters does our new model have?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-DHocMJxy9e",
    "outputId": "29cacb27-b33c-4199-e747-82ce3bf25394"
   },
   "outputs": [],
   "source": [
    "num_hidden = 16\n",
    "layer_1_params = term_document_matrix_train.shape[1] * num_hidden + num_hidden\n",
    "layer_2_params = num_hidden * 1 + 1\n",
    "total_params = layer_1_params + layer_2_params\n",
    "total_params == model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af5pQJlLxy9e",
    "outputId": "09f94af8-3147-48f3-85d9-49590fd1caff"
   },
   "outputs": [],
   "source": [
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llzEDpiUxy9e",
    "outputId": "2ef60b0e-ce35-4a34-97d8-42edddf7acaf"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1N0knMLxy9e",
    "outputId": "077efef8-e0b2-4df9-d61b-26c4582bfe0b"
   },
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21UNgm3Zxy9e"
   },
   "source": [
    "## This should make you uncomfortable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLutqknuxy9e",
    "outputId": "19333a7d-039e-49d5-ddb9-884cf1784f5e"
   },
   "outputs": [],
   "source": [
    "total_params / term_document_matrix_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDhjNva-xy9e",
    "outputId": "39da4504-3cf8-4759-a29b-f00fd0af923e"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "111llJDBxy9e",
    "outputId": "458c957c-c78d-4f75-da56-61af7d8d06c7"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(figsize=(10,8), secondary_y=[\"acc\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyVJggnpxy9e",
    "outputId": "af24c7ca-d8ac-4538-c2ca-305d8b0a050d"
   },
   "outputs": [],
   "source": [
    "\n",
    "y_predict = model.predict(term_document_matrix_test).squeeze() > 0.5\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print('Confusion Matrix:\\n', pd.DataFrame(cm), '\\n')\n",
    "print(f'f1 score = {f1_score( y_test, y_predict):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hie2Q6J7xy9f"
   },
   "source": [
    "# What did we do?\n",
    " - ## Added a \"hidden layer\"\n",
    " - ## Increased the complexity of the representation of the data\n",
    " - ## Increased the number of free parameters by >10x\n",
    " - ## Got a modest performance increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3UHdoTixy9f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFVIZkNNxy9f",
    "outputId": "11d60554-e4b6-4886-e461-f352ba931394"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(16, activation=\"relu\")(text_input)  # <- THIS\n",
    "hidden_state = Dense(16, activation=\"relu\")(hidden_state)  # <- THIS\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ1PYxO4xy9f",
    "outputId": "fe903187-4e01-41bc-8ae4-7a59807ed08b"
   },
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnRbgxcSxy9f",
    "outputId": "5680c611-e117-4cb5-bd75-f425722eec6b"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUUbHYEtxy9f",
    "outputId": "5ce2b728-63d9-4028-8087-831ade5bd870"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(figsize=(10,8), secondary_y=[\"acc\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBWT3lCixy9f"
   },
   "source": [
    "## For now, we will see diminishing returns adding more and more layers\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwuFb4F3xy9f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSsGYI1uxy9f"
   },
   "source": [
    "# The rest of deep learning for NLP is just\n",
    "## $ \\\\ $\n",
    "## 1. Finding better / more complex matrix operations to stack together (for more complex problems)\n",
    " - ## e.g. more than just `Dense` layers\n",
    "\n",
    "## 2. Finding better representations for text for our networks\n",
    " - ## E.g. not just BOW\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# We will also cover the mechanics of doing deep learning for NLP\n",
    " - ## preventing overfitting\n",
    " - ## Finding ways to handle lots of data\n",
    " - ## Making training happen faster\n",
    " - ## Making training more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPPrJX45xy9f"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY5ec0iwxy9f"
   },
   "source": [
    "# Preventing Overfitting\n",
    "## Different models have different ways of doing it\n",
    " - ## Logistic regression: coefficient regularization\n",
    " - ## Decision trees: limit depth, minimum samples in a leaf, etc\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Neural networks are very prone to overfitting\n",
    " - ## $N_{params} >> N_{observations}$\n",
    "\n",
    "# $ \\\\ $\n",
    "# How can we prevent overfitting in a neural network?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Preventing Overfitting\n",
    " - ## Reduce the network size (number of parameters)\n",
    " - ## Add constraints like `$L_2$` regularization\n",
    " - ## Add mathematical operation (layers) to combat overfitting\n",
    " - ## Design networks that are smart and have implicit regularization in them\n",
    " - ## Be clever wtih our optimizer\n",
    "   - ## ie don't let `ADAM` run away\n",
    " - ## Be vigilant with cross validation\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Neural network: Reduce the number of parameters\n",
    " - ## There is nothing canonical about the size of the hidden layer\n",
    " - ## It has a large impact on the number of parameters for large input sizes\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEuAavmExy9g",
    "outputId": "f828ffe7-9a5c-4c77-fb19-f0f60868f533"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(8, activation=\"relu\")(text_input)  # <- THIS\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLjyHsOWxy9g",
    "outputId": "fdd532b8-4544-4720-b2fa-164520ad7fb5"
   },
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fD_ym5Vxy9g",
    "outputId": "90009450-df7f-4410-8fe6-b473c7661b1e"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(256, activation=\"relu\")(text_input)  # <- THIS\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaa7XBXDxy9g",
    "outputId": "5764f96f-4f90-414c-e970-eb1e70848b01"
   },
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaA14b1qxy9g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHMkSamHxy9g"
   },
   "source": [
    "## More \"traditional\" regularization:\n",
    " - ## Just like we can add $L_{1}$ or $L_{2}$ norm to logistic regression, we can add it to layers in a network\n",
    " - ## $L_2$ is often called `weight decay` because it causes weights to shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5k2KiVAxy9g",
    "outputId": "aec73977-adee-49f0-e6f9-4ff27f025262"
   },
   "outputs": [],
   "source": [
    "Dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdDU4hmhxy9g"
   },
   "source": [
    "## What can we regularize in a `Dense` layer?\n",
    " - ## The weight matrix (`input_dim` x `output_dim`)\n",
    " - ## The bias matrix (`output_dim`)\n",
    " - ## The activations (`output_dim`)\n",
    "\n",
    "# $ \\\\ $\n",
    "## These all do different things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_b-DZGfxy9g"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2, l1, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJuR2b8jxy9h",
    "outputId": "f649a75e-948b-453f-ee23-0b40e615fbd9"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(\n",
    "    16,\n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-3),\n",
    "    bias_regularizer=l2(1e-3),\n",
    "    activity_regularizer=l2(1e-3),# <- THIS\n",
    ")(\n",
    "    text_input\n",
    ")\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNc0xovIxy9i"
   },
   "source": [
    "## Under the hood, keras will modify the loss function to add the regularization costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrVlcZGZxy9i",
    "outputId": "f4468ed7-30a2-4e0a-be62-d56799aeb53c"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFuKRRzYxy9i",
    "outputId": "846fdaba-f02f-412f-8b8c-630fa25b2af2"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(figsize=(10,8), secondary_y=[\"acc\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7BONsBzxy9i"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHYCL_v6xy9i"
   },
   "source": [
    "# Notice: we are now much less overfit\n",
    " - ## Maybe too much regularization because val_performance is lower\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## NB: $L_1$ regularization does NOT give us sparse weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO7Nf8x6xy9i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwZrsAFpxy9i"
   },
   "source": [
    "# Layers dedicated to preventing overfitting: Dropout\n",
    "## A story about bank tellers\n",
    "# $ \\\\ $\n",
    "![hinton](./hinton.jpg)\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Idea:\n",
    " - ## Randomly select some of the neurons and set their outputs to zero\n",
    " - ## After lots of training, only the ones that really matter will survive\n",
    "\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Npurbr9xy9j",
    "outputId": "85e5c8f7-ebf9-407a-b973-55fa8b69143f"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrye0v0fxy9j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlymW0PBxy9j",
    "outputId": "5d695b0e-8e16-4573-a395-c601f4ceceb1"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "dropout_layer = Dropout(0.5, input_shape=(2,))\n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCMfVzthxy9j",
    "outputId": "0523eb65-16ba-47c2-ce9e-585757b3c387"
   },
   "outputs": [],
   "source": [
    "\n",
    "outputs = dropout_layer(K.variable(data), training=True)\n",
    "outputs = K.eval(outputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkQBa9EGxy9j"
   },
   "source": [
    "## Notice: the values got bigger!! Why?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Notice: `training=True`. Why?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_q1BNayYxy9j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iov4x_Hqxy9j"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1F_NH4bxy9j",
    "outputId": "00ec3401-7fef-4eb0-93de-9d922e42a381"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(\n",
    "    32,\n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-4),\n",
    ")(text_input)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "hidden_state = Dense(\n",
    "    16,\n",
    "    activation=\"relu\",\n",
    ")(hidden_state)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5Uf6B_uxy9k",
    "outputId": "9bfb2faa-cca4-4bdc-9528-cb1783c1fbaa"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPygvubHxy9k",
    "outputId": "3add8c0e-3433-4cd0-ebda-c5f934b67cc6"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(figsize=(10,8), secondary_y=[\"acc\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3f8PZYUxy9k",
    "outputId": "e7b79338-7d8e-4732-cc97-b4127f099021"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history)[\"val_acc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR7VO7Avxy9k"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Using Numerical Optimization Responsibly:\n",
    " - ## For how long should we learn?\n",
    " - ## How quickly should we learn?\n",
    "\n",
    "# $ \\\\ $\n",
    "## Note: We never chose `epochs` with any care! How should we choose it?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Idea: keep learning until the validation loss stops decreasing\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# We can actually do better!\n",
    "## What does it mean if the validation loss is stuck?\n",
    "\n",
    "\n",
    "<img src=\"lr-small.png\" alt=\"small LR\" style=\"width:800px;\"/>\n",
    "\n",
    "# $ \\\\ $\n",
    "## What happens when the LR is too big?\n",
    "# $ \\\\ $\n",
    "\n",
    "<img src=\"lr-big.png\" alt=\"big LR\" style=\"width:800px;\"/>\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "\n",
    "## We may close to a local minimum.\n",
    "# Idea: If validation loss stops decreasing\n",
    " - ## Reduce the learning rate (try to learn more slowly)\n",
    " - ## Stop if the the validation loss still doesn't decrease\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Question: why not just set the learning rate to be small?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztBKpuOkxy9k",
    "outputId": "49e72a3d-2134-474b-c39b-f7dcdd73bea8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# EarlyStopping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVjB95FTxy9k",
    "outputId": "433c799e-ce96-41ed-cde3-08d2bbc4fd19"
   },
   "outputs": [],
   "source": [
    "ReduceLROnPlateau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1yLi_Ykxy9k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnjUIiaNxy9k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS5fRDRFxy9l",
    "outputId": "a85840fd-1ed9-406b-8c9f-73542d4f7083"
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(\n",
    "    32,\n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-4),\n",
    ")(text_input)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "hidden_state = Dense(\n",
    "    16,\n",
    "    activation=\"relu\",\n",
    ")(hidden_state)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib3kPpNxxy9l"
   },
   "outputs": [],
   "source": [
    "lr = ReduceLROnPlateau(patience=1, verbose=True)\n",
    "es = EarlyStopping(patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ac8GQ6xxy9l",
    "outputId": "e9254f1f-53e8-41d5-8370-86af2f176413"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=30,  # <- we can set this to be high!\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    "    callbacks=[lr, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pNHBzjxxy9l",
    "outputId": "96dea7d1-89e6-48a8-e441-20f4c9d4d302"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).plot(figsize=(10,8), secondary_y=[\"acc\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1GGkxdSxy9l"
   },
   "source": [
    "## Notice, the learning rate is in the history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKL8Q0Ucxy9l",
    "outputId": "c90c038c-b100-4e43-fd1d-02ecac2e9c7e"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history).drop(\"learning_rate\", axis=1).plot(\n",
    "    figsize=(10, 8), secondary_y=[\"acc\", \"val_acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFsjLvtQxy9l"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# A word about GPUs\n",
    " - ## Most of deep learning is matrix operations\n",
    " - ## GPUs were designed to do matrix math quickly\n",
    " - ## They are an essential part of deep learning because they help with iteration speed.\n",
    " - ## Won't be required for this course\n",
    "\n",
    "## GPUs in practice\n",
    " - ## Cloud providers (amazon/google etc) charge about  $\\,\\$$1/hr for a reasonable GPU machine\n",
    " - ## You can build your own for about $1000, which pays if you do this full time.\n",
    " - ## GPUs can be a pain to set up\n",
    " - ## Once set up `keras` will use it by default?\n",
    "     - ### Apple machines with Apple silicon (M1-4) require a special version of tensorflow: `tensorflow-metal`\n",
    "     - https://developer.apple.com/metal/tensorflow-plugin/\n",
    "\n",
    "# $ \\\\ $\n",
    "### To see if your (non-Apple silicon!) computer can see your gpu, from the command prompt\n",
    "```shell\n",
    ">> nvidia-smi\n",
    "```\n",
    "### See [here](https://medium.com/analytics-vidhya/explained-output-of-nvidia-smi-utility-fc4fbee3b124) for a great explanation of the output\n",
    "# $ \\\\ $\n",
    "### To see if keras can see your gpu\n",
    "```python\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "```\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "### If you do have Apple silicon, this little bit of code will help you verify that tensorflow is using your GPU\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable logging to see device placement\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# List all available physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available Physical Devices:\")\n",
    "for device in physical_devices:\n",
    "    print(f\" - {device}\")\n",
    "\n",
    "# Specifically check for GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"\\nGPU Devices Detected:\")\n",
    "    for gpu in gpus:\n",
    "        print(f\" - {gpu}\")\n",
    "    # Optionally, set TensorFlow to use the first GPU\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"TensorFlow is set to use the following GPU: {logical_gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"\\nNo GPU devices detected. TensorFlow will use the CPU.\")\n",
    "\n",
    "# Perform a test computation to see where it's executed\n",
    "print(\"\\nRunning a test computation to verify device usage...\")\n",
    "\n",
    "# Define a simple computation\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(\"\\nResult of the computation:\")\n",
    "print(c)\n",
    "```\n",
    "\n",
    "#### If you're feeling really ambitious, you can try this bit of test code, too:\n",
    "- ##### Beware! This takes quite some time to run\n",
    "```python\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=100,)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=100,)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr09bjDVxy9l"
   },
   "source": [
    "# $\\\\ $\n",
    "# $\\\\ $\n",
    "# $\\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qNyzpSDxy9l"
   },
   "source": [
    "# The Power and Flexibility of Deep Learning\n",
    "## Recall:\n",
    " - ## deep learning allows for sequential building of feature representation\n",
    " - ## `keras` allows for somewhat arbitrary matrix operations\n",
    "\n",
    "\n",
    "## The combination is very powerful!\n",
    "## Our job in designing models is to make operations that will capture the behavior that we want\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# We can\n",
    " - ## Make complex networks\n",
    " - ## Find new ways to solve problems\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Tesla's self-driving cars\n",
    " - ## approximately 10 different optical cameras\n",
    " - ## approximately 30 different tasks:\n",
    "   - ## lane detection\n",
    "   - ## traffic light detection\n",
    "   - ## estimation of speed of car in front\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Seems pretty hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-M2xD7Zxy9l",
    "outputId": "9f2a0416-f3e7-4e15-9807-fd33a1c4e168"
   },
   "outputs": [],
   "source": [
    "# ok to restart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQHUn0wfxy9l"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EarnYF0Wxy9l",
    "outputId": "e243528a-8bd3-4b58-b47f-b8f21f028581"
   },
   "outputs": [],
   "source": [
    "num_cameras = 5\n",
    "num_tasks = 3\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "camera_inputs = [\n",
    "    Input(shape=(512,), name=\"cam_{}\".format(i)) for i in range(num_cameras)\n",
    "]\n",
    "\n",
    "streams = []\n",
    "for i in range(num_cameras):\n",
    "    blocks = [\n",
    "        Dense(32, activation=\"relu\", name=\"cam{}_b{}\".format(i, b)) for b in range(3)\n",
    "    ]\n",
    "    stream = blocks[0](camera_inputs[i])\n",
    "    for b in blocks[1:]:\n",
    "        stream = b(stream)\n",
    "    streams.append(stream)\n",
    "\n",
    "all_streams = Concatenate(name=\"comb\")(streams)\n",
    "all_streams = Dense(32, activation=\"relu\")(all_streams)\n",
    "all_streams = Dense(16, activation=\"relu\")(all_streams)\n",
    "\n",
    "outputs = []\n",
    "for t in range(num_tasks):\n",
    "    out = Dense(1, activation=\"sigmoid\")(all_streams)\n",
    "    outputs.append(out)\n",
    "\n",
    "model = Model(camera_inputs, outputs)\n",
    "model.compile(\"adam\", loss=[\"binary_crossentropy\"] * num_tasks)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anGsfluHxy9l"
   },
   "source": [
    "# $ \\\\ $\n",
    "## This is tough to parse. Let's plot the model topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9ZlWxIcxy9l",
    "outputId": "ff27f455-3e9d-47c3-9090-c57d27707cec"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "def plot_model_in_notebook(model):\n",
    "    return Image(model_to_dot(model, show_shapes=True).create(prog='dot', format='png'))\n",
    "\n",
    "\n",
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HbGJDmyxy9l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxTy0gNLxy9l"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# We can also be creative with how we compose problems\n",
    "# $ \\\\ $\n",
    "# Let's look at a famous data set of classifying handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.unique(y_test).shape[0]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[12, :, :])\n",
    "print(y_train[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncolumns = 8, 8\n",
    "f, ax = plt.subplots(nrows, ncolumns, figsize=(16,16))\n",
    "digit_inds = np.random.choice(np.arange(x_train.shape[0]), nrows * ncolumns)\n",
    "i = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncolumns):\n",
    "        ax[row, col].imshow(x_train[digit_inds[i]])\n",
    "        ax[row, col].set_xticks([])\n",
    "        ax[row, col].set_yticks([])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would we build a feedforward network for this problem?\n",
    "# $ \\\\ $\n",
    "## Let's process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def preprocess_training_data(data):\n",
    "    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "    data = data.astype(\"float32\") / 255.0\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "\n",
    "x_train = preprocess_training_data(x_train)\n",
    "x_test = preprocess_training_data(x_test)\n",
    "print(x_train.shape)\n",
    "\n",
    "y_train_ohe = preprocess_targets(y_train, num_classes)\n",
    "y_test_ohe = preprocess_targets(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "cutoff = int(x_train.shape[0] * train_frac)\n",
    "x_train, x_val = x_train[:cutoff], x_train[cutoff:]\n",
    "y_train, y_val = y_train[:cutoff], y_train[cutoff:]\n",
    "y_train_ohe, y_val_ohe = y_train_ohe[:cutoff], y_train_ohe[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "digit_input = Input(shape=(x_train.shape[1],))\n",
    "hidden = Dense(256, activation=\"relu\")(digit_input)\n",
    "output = Dense(num_classes, activation=\"softmax\")(hidden)\n",
    "model = Model(digit_input, output)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train, y_train_ohe,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val_ohe),\n",
    "    epochs=10,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# How well did we do with logistic regression (ie single-layer network)?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# How else can we approach this problem?\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Our problem is inherently 2D\n",
    "- ## Why would we use a 1D technique?\n",
    "\n",
    "# $ \\\\ $\n",
    "# Convolutional Neural Nets\n",
    "# $ \\\\ $\n",
    "## This is an \"aside,\" as NLP isn't 2D, but it does contain sequential data, and we can apply related ideas to that problem\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Mathematically, the convolution of two functions is the overlap of one function $g$ as it is shifted over another function $f$\n",
    "$$ (f \\ast g)(t):=\\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) d \\tau$$\n",
    "# $ \\\\ $\n",
    "## Convolutional neural nets are a discrete (finite) version of this integral\n",
    "### Worth becoming familiar with them, so here's a simple introduction \n",
    "- ### [intro to cnn](https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "# EXAMPLE\n",
    "# $ \\\\ $\n",
    "## Let's start with the pixel representation of the digit `2`\n",
    "<img src=\"two_as_pixels.png\" alt=\"two-pixels\" style=\"width:800px;\"/>\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now apply a \"convolution,\" that is, a small matrix/kernel applied to successive patches of this bigger matrix\n",
    "# $ \\\\ $\n",
    "<img src=\"cnn_demo.png\" alt=\"cnn-demo\" style=\"width:800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Now we do a \"smoothing operation\", taking the max among blocks of the convoluted matrix. This is called `MaxPooling` and we will use it later for NLP\n",
    "### Note: the numbers are wrong, but that's okay for now\n",
    "# $ \\\\ $\n",
    "<img src=\"max_pooling.png\" alt=\"max-polling\" style=\"width:800px;\"/>\n",
    "\n",
    "# $ \\\\ $\n",
    "## Finally, we add a fully-connected `Dense` layer and put it to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "Conv2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data to include the channel dimension\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Create the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Convolutional Layer 2\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense Layer 1\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Another look at EMBEDDINGS\n",
    "# $ \\\\ $\n",
    "## Embeddings are at the heart of NLP in general and LLMs in particular\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Why do we need them?\n",
    "# $ \\\\ $\n",
    "## How many words are there in the English language?\n",
    "# $ \\\\ $\n",
    "### As an aside, how many do you need to know to\n",
    "- ### Read a newspaper?\n",
    "- ### Read a complex technical paper?\n",
    "- ### Be considered \"well-educated?\"\n",
    "# $ \\\\ $\n",
    "\n",
    "## Let's revisit embeddings and see what we can find\n",
    "# $ \\\\ $\n",
    "## We start with the usual IMDB movie sentiment database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You should download the dataset from this URL\n",
    "### I've commented out the next 3 cells for now. Uncomment them for your use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "#                                   untar=True, cache_dir='.',\n",
    "#                                   cache_subdir='')\n",
    "\n",
    "# dataset_dir = os.path.join(os.path.dirname(dataset), '../data/aclImdb')\n",
    "# os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = os.path.join(dataset_dir, 'train')\n",
    "# os.listdir(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into test/train and batch appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '../data/aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i][:300], '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit of Keras magic to (maybe) make things run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an embedding layer with a 1,000 word \"vocabulary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\n",
    "result.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 40000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=64\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  # Dense(16, activation='relu'),\n",
    "  Dense(16, activation='relu',\n",
    "    kernel_regularizer=l2(1e-3)),\n",
    "  Dropout(0.2),\n",
    "  Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Now let's use the GloVe (Global Vectors for Word Representation) word embeddings, specifically the GloVe.6B dataset\n",
    "- ## Developed in 2014 by the Stamford Natural Language Processing Group\n",
    "- ## High-quality, widely used\n",
    "- ## Pretrained on a massive dataset of 6B words\n",
    "- ## Easy to integrate into this task!\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment these lines for your own analysis - it takes 15-20 minutes to download\n",
    "# !wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = \"glove.6B.200d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocab) + 2\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "word_index = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    trainable=False,\n",
    ")\n",
    "embedding_layer.build((1,))\n",
    "embedding_layer.set_weights([embedding_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  embedding_layer,\n",
    "  Conv1D(128, 9, activation=\"relu\"), # Keras will learn 128 different \"kernels\", each of dimenson \n",
    "  MaxPooling1D(9),\n",
    "  Conv1D(128, 9, activation=\"relu\"),\n",
    "  GlobalMaxPooling1D(),\n",
    "  Dense(16, activation='relu',\n",
    "    kernel_regularizer=l2(1e-3)),\n",
    "  Dropout(0.2),\n",
    "  Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "lr = ReduceLROnPlateau(patience=1, verbose=True)\n",
    "es = EarlyStopping(patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback, lr, es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = model.layers[2]\n",
    "filters = conv_layer.weights[0].numpy()\n",
    "filters.shape\n",
    "# filters[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 10))\n",
    "for i in range(filters.shape[2]):  # Iterate over each filter\n",
    "    plt.subplot(8, 16, i + 1)  # Adjust grid size as needed\n",
    "    plt.plot(filters[:, :15, i].squeeze())  # Plot the filter as a line graph\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased', lang='english')\n",
    "hypotheses = [\"A brown fox jumps over a dog\"]\n",
    "references = [\"A quick brown dog jumps over the lazy fox\"]\n",
    "P, R, F1 = scorer.score(hypotheses, references)\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5zM7-3Ixy9o"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Deep Learning in Practice: not always so easy\n",
    " - ## There is no silver bullet\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "## Why?\n",
    " - ## Neural networks fail silently\n",
    " - ## Neural networks are black box and therefore hard to debug\n",
    " - ## Backpropagation of gradients and a good optimizer doesn't guarantee anything\n",
    "   - ## This should not be surprising given $\\frac{N_{\\rm params}}{N_{\\rm observations}} \\gg 1$\n",
    " - ## Problems are generally not framed as nicely as IMDB sentiment classification\n",
    "   - ## Think about how to frame the self-driving car problem\n",
    "   - ## What is a \"good\" article summary?\n",
    "   - ## What does it mean to surface the \"right\" document for a given query?\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# There are things you can do to make it a little more robust\n",
    " - ## [Andrej Karpathy's Recipe](http://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "# $ \\\\ $\n",
    "# The Recipe:\n",
    "## 1. Become one with the data\n",
    " - ## The more time you spend looking at the data the better you'll do.\n",
    " - ## Find corrupt data, duplicates\n",
    "   - ## Real example: lots of training examples that say \"please enable javascript to view this page\"\n",
    "   - ## If 1% of your training data have some error mode, you'll need to look at ~1% of the data to find it\n",
    " - ## Label data yourself to get a sense for difficult the task is and where you fail\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## 2. Set up an end to end training / evaluation pipeline and set benchmarks\n",
    " - ## We need code we are confident will accurately evaluate a model\n",
    " - ## Make dumb and less dumb baselines\n",
    "   - ## constant guess\n",
    "   - ## linear model\n",
    "   - ## all zeros\n",
    " - ## Step (1) will have given us better intuition for what our benchmarks should be\n",
    " - ## This step reduces the surface area of neural network modeling efforts and reduces errors\n",
    " - ## Tips and tricks:\n",
    "   - ## Fix a random seed (helps with reproducibility)\n",
    "   - ## Don't try anything fancy (e.g. bagging a bunch of classifiers)\n",
    "   - ## Add significant digits to evaluation code: Does a loss of .3002481 mean anything?\n",
    "   - ## Verify the initial loss\n",
    "   - ## Monitor human-interpretable metrics and compare them to human performance\n",
    "   - ## Overfit on small amount of data: the network should be able to memorize it\n",
    "   - ## visualize your data right before it goes to the network (i.e. from the training data generator)\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## 3. Overfit\n",
    " - ## Choose a model that is large enough to overfit the training data\n",
    "   - ## training loss < validation loss\n",
    " - ## If you can't overfit something is wrong!\n",
    " - ## Tips and tricks\n",
    "   - ## Picking a model: Don't be a hero, steal from other people.\n",
    "   - ## The adam optimizer is a good choice\n",
    "   - ## Add ONE piece of complexity at once. Don't simultaneous make every layer 2x larger and 2x the number of layers\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## 4. Regularize\n",
    " - ## Once we can overfit, we can sacrifice some training loss for val loss.\n",
    " - ## Tips and tricks\n",
    "   - ## Get more data. If possible, this is the easiest way to regularize\n",
    "   - ## Augment your data: if you can't get more data, make up some fake data\n",
    "   - ## Reduce the input dimension (e.g. vocab size)\n",
    "   - ## Decrease model size\n",
    "   - ## Weight decay\n",
    "   - ## Dropout\n",
    "   - ## Early stopping\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## 5. Tune\n",
    " - ## Tune hyperparameters to optimize the network\n",
    " - ## Random search is better than grid search\n",
    " - ## There are fancy packages out there to help you\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## 6. Squeeze the juice (we won't do much of this)\n",
    " - ## Ensemble several models\n",
    " - ## Let it train for a long time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmjQihUtxy9o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNZP6nyQxy9o"
   },
   "source": [
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Recap\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Deep Learning:\n",
    " - ## A new way to think about ML where we sequentially learn feature representations\n",
    " - ## Logical extension of our way of thinking about ML\n",
    "\n",
    "Sequential Features             |  Evolution\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"deep-learning.png\" width=\"700\">  |  ![](./ml-evolution.jpg)\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Keras makes this easy since it allows for arbitrary matrix operations\n",
    " - ## We don't only need sequential networks\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## We had models with scary numbers of free parameters\n",
    "## Preventing overfitting is really important\n",
    " - ## 3-fold cross validation\n",
    " - ## Weight decay reduces variance\n",
    " - ## Dropout makes sure parameters really matter\n",
    " - ## Early stopping to prevent overtraining\n",
    " - ## Smarter network design\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "## The mechanics of DL\n",
    " - ## Hard in practice because it's a black box\n",
    " - ## There are good rules of thumb and recipes to make sure we do things right\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "## Deep learning works really well\n",
    " - ## SOTA on lots of problems\n",
    " - ## \"free juice\"\n",
    "\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# The rest of DL for NLP\n",
    " - ## Find more sophisticated matrix operations\n",
    " - ## Find better ways to represent text\n",
    "\n",
    "## We'll see that the line between these two will blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pViiJRP6xy9o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
