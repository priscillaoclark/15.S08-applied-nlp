[{"title": "Evolution of Regional Innovation with Spatial Knowledge Spillovers:\n  Convergence or Divergence?", "summary": "This paper extends endogenous economic growth models to incorporate knowledge\nexternality. We explores whether spatial knowledge spillovers among regions\nexist, whether spatial knowledge spillovers promote regional innovative\nactivities, and whether external knowledge spillovers affect the evolution of\nregional innovations in the long run. We empirically verify the theoretical\nresults through applying spatial statistics and econometric model in the\nanalysis of panel data of 31 regions in China. An accurate estimate of the\nrange of knowledge spillovers is achieved and the convergence of regional\nknowledge growth rate is found, with clear evidences that developing regions\nbenefit more from external knowledge spillovers than developed regions.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.06936v3", "link": "http://arxiv.org/abs/1801.06936v3"}, {"title": "Memory effect and multifractality of cross-correlations in financial\n  markets", "summary": "An average instantaneous cross-correlation function is introduced to quantify\nthe interaction of the financial market of a specific time. Based on the daily\ndata of the American and Chinese stock markets, memory effect of the average\ninstantaneous cross-correlations is investigated over different price return\ntime intervals. Long-range time-correlations are revealed, and are found to\npersist up to a month-order magnitude of the price return time interval.\nMultifractal nature is investigated by a multifractal detrended fluctuation\nanalysis.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1004.5547v1", "link": "http://dx.doi.org/10.1016/j.physa.2010.11.011"}, {"title": "The Blockchain: A Gentle Four Page Introduction", "summary": "Blockchain is a distributed database that keeps a chronologically-growing\nlist (chain) of records (blocks) secure from tampering and revision. While\ncomputerisation has changed the nature of a ledger from clay tables in the old\ndays to digital records in modern days, blockchain technology is the first true\ninnovation in record keeping that could potentially revolutionise the basic\nprinciples of information keeping. In this note, we provide a brief\nself-contained introduction to how the blockchain works.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1612.06244v1", "link": "http://arxiv.org/abs/1612.06244v1"}, {"title": "ADI finite difference schemes for the Heston-Hull-White PDE", "summary": "In this paper we investigate the effectiveness of Alternating Direction\nImplicit (ADI) time discretization schemes in the numerical solution of the\nthree-dimensional Heston-Hull-White partial differential equation, which is\nsemidiscretized by applying finite difference schemes on nonuniform spatial\ngrids. We consider the Heston-Hull-White model with arbitrary correlation\nfactors, with time-dependent mean-reversion levels, with short and long\nmaturities, for cases where the Feller condition is satisfied and for cases\nwhere it is not. In addition, both European-style call options and up-and-out\ncall options are considered. It is shown through extensive tests that ADI\nschemes, with a proper choice of their parameters, perform very well in all\nsituations - in terms of stability, accuracy and efficiency.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1111.4087v1", "link": "http://arxiv.org/abs/1111.4087v1"}, {"title": "Topology of the correlation networks among major currencies using\n  hierarchical structure methods", "summary": "We studied the topology of correlation networks among 34 major currencies\nusing the concept of a minimal spanning tree and hierarchical tree for the full\nyears of 2007-2008 when major economic turbulence occurred. We used the USD (US\nDollar) and the TL (Turkish Lira) as numeraires in which the USD was the major\ncurrency and the TL was the minor currency. We derived a hierarchical\norganization and constructed minimal spanning trees (MSTs) and hierarchical\ntrees (HTs) for the full years of 2007, 2008 and for the 2007-2008 periods. We\nperformed a technique to associate a value of reliability to the links of MSTs\nand HTs by using bootstrap replicas of data. We also used the average linkage\ncluster analysis for obtaining the hierarchical trees in the case of the TL as\nthe numeraire. These trees are useful tools for understanding and detecting the\nglobal structure, taxonomy and hierarchy in financial data. We illustrated how\nthe minimal spanning trees and their related hierarchical trees developed over\na period of time. From these trees we identified different clusters of\ncurrencies according to their proximity and economic ties. The clustered\nstructure of the currencies and the key currency in each cluster were obtained\nand we found that the clusters matched nicely with the geographical regions of\ncorresponding countries in the world such as Asia or Europe. As expected the\nkey currencies were generally those showing major economic activity.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1010.5653v1", "link": "http://dx.doi.org/10.1016/j.physa.2010.10.041"}, {"title": "How predictable is technological progress?", "summary": "Recently it has become clear that many technologies follow a generalized\nversion of Moore's law, i.e. costs tend to drop exponentially, at different\nrates that depend on the technology. Here we formulate Moore's law as a\ncorrelated geometric random walk with drift, and apply it to historical data on\n53 technologies. We derive a closed form expression approximating the\ndistribution of forecast errors as a function of time. Based on hind-casting\nexperiments we show that this works well, making it possible to collapse the\nforecast errors for many different technologies at different time horizons onto\nthe same universal distribution. This is valuable because it allows us to make\nforecasts for any given technology with a clear understanding of the quality of\nthe forecasts. As a practical demonstration we make distributional forecasts at\ndifferent time horizons for solar photovoltaic modules, and show how our method\ncan be used to estimate the probability that a given technology will outperform\nanother technology at a given point in the future.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1502.05274v4", "link": "http://dx.doi.org/10.1016/j.respol.2015.11.001"}, {"title": "Incorporating fat tails in financial models using entropic divergence\n  measures", "summary": "In the existing financial literature, entropy based ideas have been proposed\nin portfolio optimization, in model calibration for options pricing as well as\nin ascertaining a pricing measure in incomplete markets. The abstracted problem\ncorresponds to finding a probability measure that minimizes the relative\nentropy (also called $I$-divergence) with respect to a known measure while it\nsatisfies certain moment constraints on functions of underlying assets. In this\npaper, we show that under $I$-divergence, the optimal solution may not exist\nwhen the underlying assets have fat tailed distributions, ubiquitous in\nfinancial practice. We note that this drawback may be corrected if\n`polynomial-divergence' is used. This divergence can be seen to be equivalent\nto the well known (relative) Tsallis or (relative) Renyi entropy. We discuss\nexistence and uniqueness issues related to this new optimization problem as\nwell as the nature of the optimal solution under different objectives. We also\nidentify the optimal solution structure under $I$-divergence as well as\npolynomial-divergence when the associated constraints include those on marginal\ndistribution of functions of underlying assets. These results are applied to a\nsimple problem of model calibration to options prices as well as to portfolio\nmodeling in Markowitz framework, where we note that a reasonable view that a\nparticular portfolio of assets has heavy tailed losses may lead to fatter and\nmore reasonable tail distributions of all assets.", "category": ["q-fin.ST", "q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/1203.0643v1", "link": "http://arxiv.org/abs/1203.0643v1"}, {"title": "Intrinsic Prices Of Risk", "summary": "We review the nature of some well-known phenomena such as volatility smiles,\nconvexity adjustments and parallel derivative markets. We propose that the\nmarket is incomplete and postulate the existence of intrinsic risks in every\ncontingent claim as a basis for understanding these phenomena. In a continuous\ntime framework, we bring together the notion of intrinsic risk and the theory\nof change of measures to derive a probability measure, namely risk-subjective\nmeasure, for evaluating contingent claims. This paper is a modest attempt to\nprove that measure of intrinsic risk is a crucial ingredient for explaining\nthese phenomena, and in consequence proposes a new approach to pricing and\nhedging financial derivatives. By adapting theoretical knowledge to practical\napplications, we show that our approach is consistent and robust, compared with\nthe standard risk-neutral approach.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1403.0333v5", "link": "http://arxiv.org/abs/1403.0333v5"}, {"title": "Dynamic optimization and its relation to classical and quantum\n  constrained systems", "summary": "We study the structure of a simple dynamic optimization problem consisting of\none state and one control variable, from a physicist's point of view. By using\nan analogy to a physical model, we study this system in the classical and\nquantum frameworks. Classically, the dynamic optimization problem is equivalent\nto a classical mechanics constrained system, so we must use the Dirac method to\nanalyze it in a correct way. We find that there are two second-class\nconstraints in the model: one fix the momenta associated with the control\nvariables, and the other is a reminder of the optimal control law. The dynamic\nevolution of this constrained system is given by the Dirac's bracket of the\ncanonical variables with the Hamiltonian. This dynamic results to be identical\nto the unconstrained one given by the Pontryagin equations, which are the\ncorrect classical equations of motion for our physical optimization problem. In\nthe same Pontryagin scheme, by imposing a closed-loop $\\lambda$-strategy, the\noptimality condition for the action gives a consistency relation, which is\nassociated to the Hamilton-Jacobi-Bellman equation of the dynamic programming\nmethod. A similar result is achieved by quantizing the classical model. By\nsetting the wave function $\\Psi(x,t) = e^{iS(x,t)}$ in the quantum\nSchr\\\"odinger equation, a non-linear partial equation is obtained for the $S$\nfunction. For the right-hand side quantization, this is the\nHamilton-Jacobi-Bellman equation, when $S(x,t)$ is identified with the optimal\nvalue function. Thus, the Hamilton-Jacobi-Bellman equation in Bellman's maximum\nprinciple, can be interpreted as the quantum approach of the optimization\nproblem.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1607.01317v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.02.075"}, {"title": "Analytical solution of $k$th price auction", "summary": "We provide an exact analytical solution of the Nash equilibrium for the $k$th\nprice auction by using inverse of distribution functions. As applications, we\nidentify the unique symmetric equilibrium where the valuations have polynomial\ndistribution, fat tail distribution and exponential distributions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.04865v1", "link": "http://arxiv.org/abs/1911.04865v1"}, {"title": "Agent-Based Model Approach to Complex Phenomena in Real Economy", "summary": "An agent-based model for firms' dynamics is developed. The model consists of\nfirm agents with identical characteristic parameters and a bank agent. Dynamics\nof those agents is described by their balance sheets. Each firm tries to\nmaximize its expected profit with possible risks in market. Infinite growth of\na firm directed by the \"profit maximization\" principle is suppressed by a\nconcept of \"going concern\". Possibility of bankruptcy of firms is also\nintroduced by incorporating a retardation effect of information on firms'\ndecision. The firms, mutually interacting through the monopolistic bank, become\nheterogeneous in the course of temporal evolution. Statistical properties of\nfirms' dynamics obtained by simulations based on the model are discussed in\nlight of observations in the real economy.", "category": ["q-fin.GN", "q-fin.TR"], "id": "http://arxiv.org/abs/0901.1794v1", "link": "http://arxiv.org/abs/0901.1794v1"}, {"title": "Wealth distribution in presence of debts. A Fokker--Planck description", "summary": "We consider here a Fokker--Planck equation with variable coefficient of\ndiffusion which appears in the modeling of the wealth distribution in a\nmulti-agent society. At difference with previous studies, to describe a society\nin which agents can have debts, we allow the wealth variable to be negative. It\nis shown that, even starting with debts, if the initial mean wealth is assumed\npositive, the solution of the Fokker--Planck equation is such that debts are\nabsorbed in time, and a unique equilibrium density located in the positive part\nof the real axis will be reached.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1709.09858v1", "link": "http://arxiv.org/abs/1709.09858v1"}, {"title": "From Nobel Prize to Project Management: Getting Risks Right", "summary": "A major source of risk in project management is inaccurate forecasts of\nproject costs, demand, and other impacts. The paper presents a promising new\napproach to mitigating such risk, based on theories of decision making under\nuncertainty which won the 2002 Nobel prize in economics. First, the paper\ndocuments inaccuracy and risk in project management. Second, it explains\ninaccuracy in terms of optimism bias and strategic misrepresentation. Third,\nthe theoretical basis is presented for a promising new method called \"reference\nclass forecasting,\" which achieves accuracy by basing forecasts on actual\nperformance in a reference class of comparable projects and thereby bypassing\nboth optimism bias and strategic misrepresentation. Fourth, the paper presents\nthe first instance of practical reference class forecasting, which concerns\ncost forecasts for large transportation infrastructure projects. Finally,\npotentials for and barriers to reference class forecasting are assessed.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1302.3642v1", "link": "http://arxiv.org/abs/1302.3642v1"}, {"title": "Compromise, Don't Optimize: A Prior-Free Alternative to Perfect Bayesian\n  Equilibrium", "summary": "Perfect Bayesian equilibrium is the classic solution concept for games with\nincomplete information, where players optimize under given beliefs over states.\nWe introduce a new concept called perfect compromise equilibrium, where players\nfind compromise decisions that are good in all states. This solution concept is\ntractable even if states are high dimensional as it does not rely on priors,\nand it always exists. We demonstrate the power of our solution concept in\nprominent economic examples, including Cournot and Bertrand markets, Spence's\nsignaling, and bilateral trade with common value.", "category": [], "id": "http://arxiv.org/abs/2003.02539v1", "link": "http://arxiv.org/abs/2003.02539v1"}, {"title": "A Black--Scholes Model with Long Memory", "summary": "This note develops a stochastic model of asset volatility. The volatility\nobeys a continuous-time autoregressive equation. Conditions under which the\nprocess is asymptotically stationary and possesses long memory are\ncharacterised. Connections with the class of ARCH($\\infty$) processes are\nsketched.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1202.5574v1", "link": "http://arxiv.org/abs/1202.5574v1"}, {"title": "Max-factor individual risk models with application to credit portfolios", "summary": "Individual risk models need to capture possible correlations as failing to do\nso typically results in an underestimation of extreme quantiles of the\naggregate loss. Such dependence modelling is particularly important for\nmanaging credit risk, for instance, where joint defaults are a major cause of\nconcern. Often, the dependence between the individual loss occurrence\nindicators is driven by a small number of unobservable factors. Conditional\nloss probabilities are then expressed as monotone functions of linear\ncombinations of these hidden factors. However, combining the factors in a\nlinear way allows for some compensation between them. Such diversification\neffects are not always desirable and this is why the present work proposes a\nnew model replacing linear combinations with maxima. These max-factor models\ngive more insight into which of the factors is dominant.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1412.3230v1", "link": "http://arxiv.org/abs/1412.3230v1"}, {"title": "Global Inflation Dynamics: regularities & forecasts", "summary": "The analysis of dollar inflation performed by the authors through the\napproximation of empirical data for 1913-2012 with a power-law function with an\naccelerating log-periodic oscillation superimposed over it has made it possible\nto detect a quasi-singularity point around the 17th of December, 2012. It is\ndemonstrated that, if adequate measures are not taken, one may expect a surge\nof inflation around the end of this year that may also mark the start of\nstagflation as there are no sufficient grounds to expect the re-start of the\ndynamic growth of the world economy by that time. On the other hand, as the\nexperience of the 1970s and the 1980s indicates, the stagflation consequences\ncan only be eliminated with great difficulties and at a rather high cost,\nbecause the combination of low levels of economic growth and employment with\nhigh inflation leads to a sharp decline in consumption, aggravating the\neconomic depression. In order to mitigate the inflationary consequences of the\nexplosive growth of money (and, first of all, US dollar) supply it is necessary\nto take urgently the world monetary emission under control. This issue should\nbecome central at the forthcoming G8 and G20 summits.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1207.4069v1", "link": "http://arxiv.org/abs/1207.4069v1"}, {"title": "Representation of I(1) and I(2) autoregressive Hilbertian processes", "summary": "We extend the Granger-Johansen representation theorems for I(1) and I(2)\nvector autoregressive processes to accommodate processes that take values in an\narbitrary complex separable Hilbert space. This more general setting is of\ncentral relevance for statistical applications involving functional time\nseries. We first obtain a range of necessary and sufficient conditions for a\npole in the inverse of a holomorphic index-zero Fredholm operator pencil to be\nof first or second order. Those conditions form the basis for our development\nof I(1) and I(2) representations of autoregressive Hilbertian processes.\nCointegrating and attractor subspaces are characterized in terms of the\nbehavior of the autoregressive operator pencil in a neighborhood of one.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1701.08149v4", "link": "http://dx.doi.org/10.1017/S0266466619000276"}, {"title": "Applications of a constrained mechanics methodology in economics", "summary": "The paper presents instructive interdisciplinary applications of constrained\nmechanics calculus in economics on a level appropriate for the undergraduate\nphysics education. The aim of the paper is: 1. to meet the demand for\nillustrative examples suitable for presenting the background of the highly\nexpanding research field of econophysics even on the undergraduate level and 2.\nto enable the students to understand deeper the principles and methods\nroutinely used in mechanics by looking at the well known methodology from the\ndifferent perspective of economics. Two constrained dynamic economic problems\nare presented using the economic terminology in an intuitive way. First, the\nPhillips model of business cycle is presented as a system of forced\noscillations and the general problem of two interacting economies is solved by\nthe nonholonomic dynamics approach. Second, the Cass-Koopmans-Ramsey model of\neconomical growth is solved as a variational problem with a velocity dependent\nconstraint using the vakonomic approach. The specifics of the solution\ninterpretation in economics compared to mechanics is discussed in detail, a\ndiscussion of the nonholonomic and vakonomic approaches to constrained problems\nin mechanics and economics is provided and an economic interpretation of the\nLagrange multipliers (possibly surprising for the students of physics) is\ncarefully explained. The paper can be used by the undergraduate students of\nphysics interested in interdisciplinary physics applications to get in touch\nwith current scientific approach to economics based on a physical background or\nby university teachers as an attractive supplement to the classical mechanics\nlessons.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1106.3455v1", "link": "http://dx.doi.org/10.1088/0143-0807/32/6/001"}, {"title": "Investor Experiences and Financial Market Dynamics", "summary": "How do macro-financial shocks affect investor behavior and market dynamics?\nRecent evidence on experience effects suggests a long-lasting influence of\npersonally experienced outcomes on investor beliefs and investment, but also\nsignificant differences across older and younger generations. We formalize\nexperience-based learning in an OLG model, where different cross-cohort\nexperiences generate persistent heterogeneity in beliefs, portfolio choices,\nand trade. The model allows us to characterize a novel link between investor\ndemographics and the dependence of prices on past dividends, while also\ngenerating known features of asset prices, such as excess volatility and return\npredictability. The model produces new implications for the cross-section of\nasset holdings, trade volume, and investors' heterogenous responses to recent\nfinancial crises, which we show to be in line with the data.", "category": ["q-fin.EC", "q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/1612.09553v3", "link": "http://arxiv.org/abs/1612.09553v3"}, {"title": "How fast does the clock of Finance run? - A time-definition enforcing\n  scale invariance and quantifying overnights", "summary": "A symmetry-guided definition of time may enhance and simplify the analysis of\nhistorical series with recurrent patterns and seasonalities. By enforcing\nsimple-scaling and stationarity of the distributions of returns, we identify a\nsuccessful protocol of time definition in Finance. The essential structure of\nthe stochastic process underlying the series can thus be analyzed within a most\nparsimonious symmetry scheme in which multiscaling is reduced in the quest of a\ntime scale additive and independent of moment-order in the distribution of\nreturns. At the same time, duration of periods in which markets remain inactive\nare properly quantified by the novel clock, and the corresponding (e.g.,\novernight) returns are consistently taken into account for financial\napplications.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1612.07802v2", "link": "http://arxiv.org/abs/1612.07802v2"}, {"title": "A primer on reflexivity and price dynamics under systemic risk", "summary": "A simple quantitative example of a reflexive feedback process and the\nresulting price dynamics after an exogenous price shock to a financial network\nis presented. Furthermore, an outline of a theory that connects financial\nreflexivity, which stems from cross-ownership and delayed or incomplete\ninformation, and no-arbitrage pricing theory under systemic risk is provided.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1301.6415v1", "link": "http://arxiv.org/abs/1301.6415v1"}, {"title": "Asymptotic Theory of $L$-Statistics and Integrable Empirical Processes", "summary": "This paper develops asymptotic theory of integrals of empirical quantile\nfunctions with respect to random weight functions, which is an extension of\nclassical $L$-statistics. They appear when sample trimming or Winsorization is\napplied to asymptotically linear estimators. The key idea is to consider\nempirical processes in the spaces appropriate for integration. First, we\ncharacterize weak convergence of empirical distribution functions and random\nweight functions in the space of bounded integrable functions. Second, we\nestablish the delta method for empirical quantile functions as integrable\nfunctions. Third, we derive the delta method for $L$-statistics. Finally, we\nprove weak convergence of their bootstrap processes, showing validity of\nnonparametric bootstrap.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.07572v1", "link": "http://arxiv.org/abs/1910.07572v1"}, {"title": "A New Currency of the Future: The Novel Commodity Money with Attenuation\n  Coefficient Based on the Logistics Cost of Anchor", "summary": "In this paper, we reveal the attenuation mechanism of anchor of the commodity\nmoney from the perspective of logistics warehousing costs, and propose a novel\nDecayed Commodity Money (DCM) for the store of value across time and space.\nConsidering the logistics cost of commodity warehousing by the third financial\ninstitution such as London Metal Exchange, we can award the difference between\nthe original and the residual value of the anchor to the financial institution.\nThis type of currency has the characteristic of self-decaying value over time.\nTherefore DCM has the advantages of both the commodity money which has the\nfunction of preserving wealth and credit currency without the logistics cost.\nIn addition, DCM can also avoid the defects that precious metal money is\nhoarded by market and credit currency often leads to excessive liquidity. DCM\nis also different from virtual currency, such as bitcoin, which does not have a\ncorresponding commodity anchor. As a conclusion, DCM can provide a new way of\nstoring wealth for nations, corporations and individuals effectively.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1606.06948v1", "link": "http://arxiv.org/abs/1606.06948v1"}, {"title": "The Zumbach effect under rough Heston", "summary": "Previous literature has identified an effect, dubbed the Zumbach effect, that\nis nonzero empirically but conjectured to be zero in any conventional\nstochastic volatility model. Essentially this effect corresponds to the\nproperty that past squared returns forecast future volatilities better than\npast volatilities forecast future squared returns. We provide explicit\ncomputations of the Zumbach effect under rough Heston and show that they are\nconsistent with empirical estimates. In agreement with previous conjectures\nhowever, the Zumbach effect is found to be negligible in the classical Heston\nmodel.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1809.02098v1", "link": "http://arxiv.org/abs/1809.02098v1"}, {"title": "Behavioural investors in conic market models", "summary": "We treat a fairly broad class of financial models which includes markets with\nproportional transaction costs. We consider an investor with cumulative\nprospect theory preferences and a non-negativity constraint on portfolio\nwealth. The existence of an optimal strategy is shown in this context in a\nclass of generalized strategies.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1903.08156v1", "link": "http://arxiv.org/abs/1903.08156v1"}, {"title": "Designing agent-based market models", "summary": "In light of the growing interest in agent-based market models, we bring\ntogether several earlier works in which we considered the topic of\nself-consistent market modelling. Building upon the binary game structure of\nChallet and Zhang, we discuss generalizations of the strategy reward scheme\nsuch that the agents seek to maximize their wealth in a more direct way. We\nthen examine a disturbing feature whereby such reward schemes, while appearing\nmicroscopically acceptable, lead to unrealistic market dynamics (e.g.\ninstabilities). Finally, we discuss various mechanisms which are responsible\nfor re-stabilizing the market in reality. This discussion leads to a `toolbox'\nof processes from which, we believe, successful market models can be\nconstructed in the future.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0207523v1", "link": "http://arxiv.org/abs/cond-mat/0207523v1"}, {"title": "Mutual Conversion Between Preference Maps And Cook-Seiford Vectors", "summary": "In group decision making, the preference map and Cook-Seiford vector are two\nconcepts as ways of describing ties-permitted ordinal rankings. This paper\nshows that they are equivalent for representing ties-permitted ordinal\nrankings. Transformation formulas from one to the other are given and the\ninherent consistency of the mutual conversion is discussed. The proposed\nmethods are illustrated by some examples. Some possible future applications of\nthe proposed formulas are also pointed out.", "category": [], "id": "http://arxiv.org/abs/1812.03566v1", "link": "http://arxiv.org/abs/1812.03566v1"}, {"title": "Endogenous agglomeration in a many-region world", "summary": "We theoretically study a general family of economic geography models that\nfeatures endogenous agglomeration. In many-region settings, the spatial\nscale---global or local---of the dispersion force(s) in a model plays a key\nrole in determining the resulting endogenous spatial patterns and comparative\nstatics. A global dispersion force accrues from competition between different\nlocations and leads to the formation of multiple economic clusters, or cities.\nA local dispersion force is caused by crowding effects within each location and\ninduces the flattening of each city. By distinguishing local and global\ndispersion forces, we can reduce a wide variety of extant models into only\nthree prototypical classes that are qualitatively different in implications.\nOur framework adds consistent interpretations to the empirical literature and\nalso provides general predictions on treatment effects in structural economic\ngeography models.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.05113v1", "link": "http://arxiv.org/abs/1912.05113v1"}, {"title": "Semiparametric Estimation of First-Price Auction Models", "summary": "We propose a semiparametric method to estimate the density of private values\nin first-price auctions. Specifically, we model private values through a set of\nconditional moment restrictions and use a two-step procedure. In the first step\nwe recover a sample of pseudo private values using Local Polynomial Estimator.\nIn the second step we use a GMM procedure to estimate the parameter(s) of\ninterest. We show that the proposed semiparametric estimator is consistent, has\nan asymptotic normal distribution, and attains the parametric (\"root-n\") rate\nof convergence.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1407.7140v4", "link": "http://arxiv.org/abs/1407.7140v4"}, {"title": "Informed trading, limit order book and implementation shortfall:\n  equilibrium and asymptotics", "summary": "We propose a static equilibrium model for limit order book where\nprofit-maximizing investors receive an information signal regarding the\nliquidation value of the asset and execute via a competitive dealer with random\ninitial inventory, who trades against a competitive limit order book populated\nby liquidity suppliers. We show that an equilibrium exists for bounded signal\ndistributions, obtain closed form solutions for Bernoulli-type signals and\npropose a straightforward iterative algorithm to compute the equilibrium order\nbook for the general case. We obtain the exact analytic asymptotics for the\nmarket impact of large trades and show that the functional form depends on the\ntail distribution of the private signal of the insiders. In particular, the\nimpact follows a power law if the signal has fat tails while the law is\nlogarithmic in case of lighter tails. Moreover, the tail distribution of the\ntrade volume in equilibrium obeys a power law in our model. We find that the\nliquidity suppliers charge a minimum bid-ask spread that is independent of the\namount of `noise' trading but increasing in the degree of informational\nadvantage of insiders in equilibrium. The model also predicts that the order\nbook flattens as the amount of noise trading increases converging to a model\nwith proportional transactions costs.. Competition among the insiders leads to\naggressive trading causing the aggregate profit to vanish in the limiting case\n$N\\to\\infty$. The numerical results also show that the spread increases with\nthe number of insiders keeping the other parameters fixed. Finally, an\nequilibrium may not exist if the liquidation value is unbounded. We conjecture\nthat existence of equilibrium requires a sufficient amount of competition among\ninsiders if the signal distribution exhibit fat tails.", "category": ["q-fin.TR", "q-fin.MF"], "id": "http://arxiv.org/abs/2003.04425v1", "link": "http://arxiv.org/abs/2003.04425v1"}, {"title": "Functional central limit theorems for rough volatility", "summary": "We extend Donsker's approximation of Brownian motion to fractional Brownian\nmotion with Hurst exponent $H \\in (0,1)$ and to Volterra-like processes. Some\nof the most relevant consequences of our `rough Donsker (rDonsker) Theorem' are\nconvergence results for discrete approximations of a large class of rough\nmodels. This justifies the validity of simple and easy-to-implement Monte-Carlo\nmethods, for which we provide detailed numerical recipes. We test these against\nthe current benchmark Hybrid scheme \\cite{BLP15} and find remarkable agreement\n(for a large range of values of~$H$). This rDonsker Theorem further provides a\nweak convergence proof for the Hybrid scheme itself, and allows to construct\nbinomial trees for rough volatility models, the first available scheme (in the\nrough volatility context) for early exercise options such as American or\nBermudan.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1711.03078v3", "link": "http://arxiv.org/abs/1711.03078v3"}, {"title": "Incentivizing the Dynamic Workforce: Learning Contracts in the\n  Gig-Economy", "summary": "In principal-agent models, a principal offers a contract to an agent to\nperform a certain task. The agent exerts a level of effort that maximizes her\nutility. The principal is oblivious to the agent's chosen level of effort, and\nconditions her wage only on possible outcomes. In this work, we consider a\nmodel in which the principal is unaware of the agent's utility and action\nspace. She sequentially offers contracts to identical agents, and observes the\nresulting outcomes. We present an algorithm for learning the optimal contract\nunder mild assumptions. We bound the number of samples needed for the principal\nobtain a contract that is within $\\epsilon$ of her optimal net profit for every\n$\\epsilon>0$.", "category": [], "id": "http://arxiv.org/abs/1811.06736v1", "link": "http://arxiv.org/abs/1811.06736v1"}, {"title": "A Path Integral Way to Option Pricing", "summary": "An efficient computational algorithm to price financial derivatives is\npresented. It is based on a path integral formulation of the pricing problem.\nIt is shown how the path integral approach can be worked out in order to obtain\nfast and accurate predictions for the value of a large class of options,\nincluding those with path-dependent and early exercise features. As examples,\nthe application of the method to European and American options in the\nBlack-Scholes model is illustrated. A particularly simple and fast\nsemi-analytical approximation for the price of American options is derived. The\nresults of the algorithm are compared with those obtained with the standard\nprocedures known in the literature and found to be in good agreement.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/0202143v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)00796-3"}, {"title": "Erratum: Higher Order Elicitability and Osband's Principle", "summary": "This note corrects conditions in Proposition 3.4 and Theorem 5.2(ii) and\ncomments on imprecisions in Propositions 4.2 and 4.4 in Fissler and Ziegel\n(2016).", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1901.08826v1", "link": "http://arxiv.org/abs/1901.08826v1"}, {"title": "Growth and inequalities in a physicist's view", "summary": "It is still common wisdom amongst economists, politicians and lay people that\neconomic growth is a necessity of our social systems, at least to avoid\ndistributional conflicts. This paper challenges such belief moving from a\npurely physical theoretical perspective. It formally considers the constraints\nimposed by a finite environment on the prospect of continuous growth, including\nthe dynamics of costs. As costs grow faster than production it is easy to\ndeduce a final unavoidable global collapse. Then, analyzing and discussing the\nevolution of the unequal share of wealth under the premises of growth and\ncompetition, it is shown that the increase of inequalities is a necessary\nconsequence of the premises.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.00478v3", "link": "http://dx.doi.org/10.1007/s41247-020-00071-6"}, {"title": "Index Tracking with Cardinality Constraints: A Stochastic Neural\n  Networks Approach", "summary": "Partial (replication) index tracking is a popular passive investment\nstrategy. It aims to replicate the performance of a given index by constructing\na tracking portfolio which contains some constituents of the index. The\ntracking error optimisation is quadratic and NP-hard when taking the L0\nconstraint into account so it is usually solved by heuristic methods such as\nevolutionary algorithms. This paper introduces a simple, efficient and scalable\nconnectionist model as an alternative. We propose a novel reparametrisation\nmethod and then solve the optimisation problem with stochastic neural networks.\nThe proposed approach is examined with S&P 500 index data for more than 10\nyears and compared with widely used index tracking approaches such as forward\nand backward selection and the largest market capitalisation methods. The\nempirical results show our model achieves excellent performance. Compared with\nthe benchmarked models, our model has the lowest tracking error, across a range\nof portfolio sizes. Meanwhile it offers comparable performance to the others on\nsecondary criteria such as volatility, Sharpe ratio and maximum drawdown.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1911.05052v2", "link": "http://arxiv.org/abs/1911.05052v2"}, {"title": "Bipartite Producer-Consumer Networks and the Size Distribution of Firms", "summary": "A bipartite producer-consumer network is constructed to describe the\nindustrial structure. The edges from consumer to producer represent the choices\nof the consumer for the final products and the degree of producer can represent\nits market share. So the size distribution of firms can be characterized by\nproducer's degree distribution. The probability for a producer receiving a new\nconsumption is determined by its competency described by initial attractiveness\nand the self-reinforcing mechanism in the competition described by preferential\nattachment. The cases with constant total consumption and with growing market\nare studied. The following results are obtained: 1, Without market growth and a\nuniform initial attractiveness $a$, the final distribution of firm sizes is\nGamma distribution for $a>1$ and is exponential for $a=1$. If $a<1$, the\ndistribution is power in small size and exponential in upper tail; 2, For a\ngrowing market, the size distribution of firms obeys the power law. The\nexponent is affected by the market growth and the initial attractiveness of the\nfirms.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0507163v1", "link": "http://dx.doi.org/10.1016/j.physa.2005.08.006"}, {"title": "Investigating Causal Relationships in Stock Returns with Temporal Logic\n  Based Methods", "summary": "We describe a new framework for causal inference and its application to\nreturn time series. In this system, causal relationships are represented as\nlogical formulas, allowing us to test arbitrarily complex hypotheses in a\ncomputationally efficient way. We simulate return time series using a common\nfactor model, and show that on this data the method described significantly\noutperforms Granger causality (a primary approach to this type of problem).\nFinally we apply the method to real return data, showing that the method can\ndiscover novel relationships between stocks. The approach described is a\ngeneral one that will allow combination of price and volume data with\nqualitative information at varying time scales (from interest rate\nannouncements, to earnings reports to news stories) shedding light on some of\nthe previously invisible common causes of seemingly correlated price movements.", "category": ["q-fin.ST", "q-fin.PM"], "id": "http://arxiv.org/abs/1006.1791v1", "link": "http://arxiv.org/abs/1006.1791v1"}, {"title": "A Score-Driven Conditional Correlation Model for Noisy and Asynchronous\n  Data: an Application to High-Frequency Covariance Dynamics", "summary": "The analysis of the intraday dynamics of correlations among high-frequency\nreturns is challenging due to the presence of asynchronous trading and market\nmicrostructure noise. Both effects may lead to significant data reduction and\nmay severely underestimate correlations if traditional methods for\nlow-frequency data are employed. We propose to model intraday log-prices\nthrough a multivariate local-level model with score-driven covariance matrices\nand to treat asynchronicity as a missing value problem. The main advantages of\nthis approach are: (i) all available data are used when filtering correlations,\n(ii) market microstructure noise is taken into account, (iii) estimation is\nperformed through standard maximum likelihood methods. Our empirical analysis,\nperformed on 1-second NYSE data, shows that opening hours are dominated by\nidiosyncratic risk and that a market factor progressively emerges in the second\npart of the day. The method can be used as a nowcasting tool for high-frequency\ndata, allowing to study the real-time response of covariances to macro-news\nannouncements and to build intraday portfolios with very short optimization\nhorizons.", "category": ["q-fin.TR", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.04894v2", "link": "http://arxiv.org/abs/1803.04894v2"}, {"title": "Detecting long and short memory via spectral methods", "summary": "We study the properties of memory of a financial time series adopting two\ndifferent methods of analysis, the detrended fluctuation analysis (DFA) and the\nanalysis of the power spectrum (PSA). The methods are applied on three time\nseries: one of high-frequency returns, one of shuffled returns and one of\nabsolute values of returns. We prove that both DFA and PSA give results in line\nwith those obtained with standard econometrics measures of correlation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0610022v1", "link": "http://arxiv.org/abs/cond-mat/0610022v1"}, {"title": "An Emissions Trading System to reach NDC targets in the Chilean electric\n  sector", "summary": "In the context of the Paris Agreement, Chile has pledged to reduce Greenhouse\nGases (GHG) intensity by at least 30% below 2007 levels by 2030, and to phase\nout coal as a energy source by 2040, among other strategies. In pursue of these\ngoals, Chile has implemented a $5 per tonne of CO2 emission tax, first of its\nkind in Latin America. However, such a low price has proven to be insufficient.\nIn our work, we study an alternative approach for capping and pricing carbon\nemissions in the Chilean electric sector; the cap and trade paradigm. We model\nthe Chilean electric market (generators and emissions auctioneer) as a two\nstage capacity expansion equilibrium problem, where we allow future investment\nand trading of emission permits among generator agents. The model studies\ngeneration and future investments in the Chilean electric sector in two regimes\nof demand: deterministic and stochastic. We show that the current Chilean\nGreenhouse Gases (GHG) intensity pledge does not drive an important shift in\nthe future Chilean electric matrix. To encourage a shift to greener\ntechnologies, a more stringent carbon budget must be considered, resulting in a\ncarbon price approximately ten times higher than the present one. We also show\nthat achieving the emissions reduction goal does not necessarily results in\nfurther reductions of carbon generation, or phasing out coal in the longer\nterm. Finally, we demonstrate that under technology change costs reductions,\nhigher demand scenarios will relax the need for stringent carbon budgets to\nachieve new renewable energy investments and hence meet the Chilean pledges.\nThese results suggest that some aspects of the Chilean pledge require further\nanalysis, of the economic impact, particularly with the recent announcement of\nachieving carbon neutrality towards 2050.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.03843v1", "link": "http://arxiv.org/abs/2005.03843v1"}, {"title": "CO2 mitigation model for China's residential building sector", "summary": "This paper aims to investigate the factors that can mitigate carbon-dioxide\n(CO2) intensity and further assess CMRBS in China based on a household scale\nvia decomposition analysis. Here we show that: Three types of housing economic\nindicators and the final emission factor significantly contributed to the\ndecrease in CO2 intensity in the residential building sector. In addition, the\nCMRBS from 2001-2016 was 1816.99 MtCO2, and the average mitigation intensity\nduring this period was 266.12 kgCO2/household/year. Furthermore, the\nenergy-conservation and emission-mitigation strategy caused CMRBS to\neffectively increase and is the key to promoting a more significant emission\nmitigation in the future. Overall, this paper covers the CMRBS assessment gap\nin China, and the proposed assessment model can be regarded as a reference for\nother countries and cities for measuring the retrospective CO2 mitigation\neffect in residential buildings.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.01249v1", "link": "http://arxiv.org/abs/1909.01249v1"}, {"title": "Robust hedging of double touch barrier options", "summary": "We consider model-free pricing of digital options, which pay out if the\nunderlying asset has crossed both upper and lower barriers. We make only weak\nassumptions about the underlying process (typically continuity), but assume\nthat the initial prices of call options with the same maturity and all strikes\nare known. Under such circumstances, we are able to give upper and lower bounds\non the arbitrage-free prices of the relevant options, and further, using\ntechniques from the theory of Skorokhod embeddings, to show that these bounds\nare tight. Additionally, martingale inequalities are derived, which provide the\ntrading strategies with which we are able to realise any potential arbitrages.\nWe show that, depending of the risk aversion of the investor, the resulting\nhedging strategies can outperform significantly the standard delta/vega-hedging\nin presence of market frictions and/or model misspecification.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0808.4012v1", "link": "http://arxiv.org/abs/0808.4012v1"}, {"title": "High-order compact finite difference scheme for option pricing in\n  stochastic volatility jump models", "summary": "We derive a new high-order compact finite difference scheme for option\npricing in stochastic volatility jump models, e.g. in Bates model. In such\nmodels the option price is determined as the solution of a partial\nintegro-differential equation. The scheme is fourth order accurate in space and\nsecond order accurate in time. Numerical experiments for the European option\npricing problem are presented. We validate the stability of the scheme\nnumerically and compare its performance to standard finite difference and\nfinite element methods. The new scheme outperforms a standard discretisation\nbased on a second-order central finite difference approximation in all our\nexperiments. At the same time, it is very efficient, requiring only one initial\n$LU$-factorisation of a sparse matrix to perform the option price valuation.\nCompared to finite element approaches, it is very parsimonious in terms of\nmemory requirements and computational effort, since it achieves high-order\nconvergence without requiring additional unknowns, unlike finite element\nmethods with higher polynomial order basis functions. The new high-order\ncompact scheme can also be useful to upgrade existing implementations based on\nstandard finite differences in a straightforward manner to obtain a highly\nefficient option pricing code.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1704.05308v2", "link": "http://dx.doi.org/10.1016/j.cam.2019.01.043"}, {"title": "Relaxation dynamics of aftershocks after large volatility shocks in the\n  SSEC index", "summary": "The relaxation dynamics of aftershocks after large volatility shocks are\ninvestigated based on two high-frequency data sets of the Shanghai Stock\nExchange Composite (SSEC) index. Compared with previous relevant work, we have\ndefined main financial shocks based on large volatilities rather than large\ncrashes. We find that the occurrence rate of aftershocks with the magnitude\nexceeding a given threshold for both daily volatility (constructed using\n1-minute data) and minutely volatility (using intra-minute data) decays as a\npower law. The power-law relaxation exponent increases with the volatility\nthreshold and is significantly greater than 1. Taking financial volatility as\nthe counterpart of seismic activity, the power-law relaxation in financial\nvolatility deviates remarkably from the Omori law in Geophysics.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0709.1219v1", "link": "http://dx.doi.org/10.1016/j.physa.2008.05.019"}, {"title": "Could short selling make financial markets tumble?", "summary": "It is suggested to consider long term trends of financial markets as a growth\nphenomenon. The question that is asked is what conditions are needed for a long\nterm sustainable growth or contraction in a financial market? The paper discuss\nthe role of traditional market players of long only mutual funds versus hedge\nfunds which take both short and long positions. It will be argued that\nfinancial markets since their very origin and only till very recently, have\nbeen in a state of ``broken symmetry'' which favored long term growth instead\nof contraction. The reason for this ``broken symmetry'' into a long term ``bull\nphase'' is the historical almost complete dominance by long only players in\nfinancial markets. Dangers connected to short trading are illustrated by the\nappearence of long term bearish trends seen in analytical results and by\nsimulation results of an agent based market model. Recent short trade data of\nthe Nasdaq Composite index show an increase in the short activity prior to or\nat the same time as dips in the market, and reveal an steadily increase in the\nshort trading activity, reaching levels never seen before.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0308548v1", "link": "http://arxiv.org/abs/cond-mat/0308548v1"}, {"title": "Modelling share volume traded in financial markets", "summary": "A simple analytically solvable model exhibiting a 1/f spectrum in an\narbitrarily wide frequency range was recently proposed by Kaulakys and\nMeskauskas (KM). Signals consisting of a sequence of pulses show that inherent\norigin of the 1/f noise is Brownian fluctuations of the average intervent time\nbetween subsequent pulses of the pulse sequence. We generalize the KM model to\nreproduce the variety of self-affine time series exhibiting power spectral\ndensity S(f) scaled as power of their frequency f. Numerical calculations with\nthe generalized discrete model (GDM) reproduce power spectral density S(f)\nscaled as power of frequency 1/f^b for various values of b, including b =1/2\nfor applications in financial markets. The particular applications of the model\nproposed are related with financial time series of share volume traded.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0201514v1", "link": "http://arxiv.org/abs/cond-mat/0201514v1"}, {"title": "An Application of Correlation Clustering to Portfolio Diversification", "summary": "This paper presents a novel application of a clustering algorithm developed\nfor constructing a phylogenetic network to the correlation matrix for 126\nstocks listed on the Shanghai A Stock Market. We show that by visualizing the\ncorrelation matrix using a Neighbor-Net network and using the circular ordering\nproduced during the construction of the network we can reduce the risk of a\ndiversified portfolio compared with random or industry group based selection\nmethods in times of market increase.", "category": ["q-fin.ST", "q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1511.07945v1", "link": "http://arxiv.org/abs/1511.07945v1"}, {"title": "Impact of time illiquidity in a mixed market without full observation", "summary": "We study a problem of optimal investment/consumption over an infinite horizon\nin a market consisting of two possibly correlated assets: one liquid and one\nilliquid. The liquid asset is observed and can be traded continuously, while\nthe illiquid one can be traded only at discrete random times corresponding to\nthe jumps of a Poisson process with intensity $\\lambda$, is observed at the\ntrading dates, and is partially observed between two different trading dates.\nThe problem is a nonstandard mixed discrete/continuous optimal control problem\nwhich we face by the dynamic programming approach. When the utility has a\ngeneral form we prove that the value function is the unique viscosity solution\nof the HJB equation and, assuming sufficient regularity of the value function,\nwe give a verification theorem that describes the optimal investment strategies\nfor the illiquid asset. In the case of power utility, we prove the regularity\nof the value function needed to apply the verification theorem, providing the\ncomplete theoretical solution of the problem. This allows us to perform\nnumerical simulation, so to analyze the impact of time illiquidity in this\nmixed market and how this impact is affected by the degree of observation.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1211.1285v2", "link": "http://arxiv.org/abs/1211.1285v2"}, {"title": "Disentangling and quantifying market participant volatility\n  contributions", "summary": "Thanks to the access to labeled orders on the Cac40 index future provided by\nEuronext, we are able to quantify market participants contributions to the\nvolatility in the diffusive limit. To achieve this result we leverage the\nbranching properties of Hawkes point processes. We find that fast\nintermediaries (e.g., market maker type agents) have a smaller footprint on the\nvolatility than slower, directional agents. The branching structure of Hawkes\nprocesses allows us to examine also the degree of endogeneity of each agent\nbehavior. We find that high-frequency traders are more endogenously driven than\nother types of agents.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1807.07036v1", "link": "http://arxiv.org/abs/1807.07036v1"}, {"title": "X-model: further development and possible modifications", "summary": "Despite its critical importance, the famous X-model elaborated by Ziel and\nSteinert (2016) has neither bin been widely studied nor further developed. And\nyet, the possibilities to improve the model are as numerous as the fields it\ncan be applied to. The present paper takes advantage of a technique proposed by\nCoulon et al. (2014) to enhance the X-model. Instead of using the wholesale\nsupply and demand curves as inputs for the model, we rely on the transformed\nversions of these curves with a perfectly inelastic demand. As a result,\ncomputational requirements of our X-model reduce and its forecasting power\nincreases substantially. Moreover, our X-model becomes more robust towards\noutliers present in the initial auction curves data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.09206v1", "link": "http://arxiv.org/abs/1907.09206v1"}, {"title": "Optimal stopping under adverse nonlinear expectation and related games", "summary": "We study the existence of optimal actions in a zero-sum game\n$\\inf_{\\tau}\\sup_PE^P[X_{\\tau}]$ between a stopper and a controller choosing a\nprobability measure. This includes the optimal stopping problem\n$\\inf_{\\tau}\\mathcal{E}(X_{\\tau})$ for a class of sublinear expectations\n$\\mathcal{E}(\\cdot)$ such as the $G$-expectation. We show that the game has a\nvalue. Moreover, exploiting the theory of sublinear expectations, we define a\nnonlinear Snell envelope $Y$ and prove that the first hitting time\n$\\inf\\{t:Y_t=X_t\\}$ is an optimal stopping time. The existence of a saddle\npoint is shown under a compactness condition. Finally, the results are applied\nto the subhedging of American options under volatility uncertainty.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1212.2140v3", "link": "http://dx.doi.org/10.1214/14-AAP1054"}, {"title": "Investment and Consumption with Regime-Switching Discount Rates", "summary": "This paper considers the problem of consumption and investment in a financial\nmarket within a continuous time stochastic economy. The investor exhibits a\nchange in the discount rate. The investment opportunities are a stock and a\nriskless account. The market coefficients and discount factor switch according\nto a finite state Markov chain. The change in the discount rate leads to time\ninconsistencies of the investor's decisions. The randomness in our model is\ndriven by a Brownian motion and a Markov chain. Following Ekeland and Pirvu we\nintroduce and characterize the subgame perfect strategies. Numerical\nexperiments show the effect of time preference on subgame perfect strategies\nand the pre-commitment strategies.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1303.1248v1", "link": "http://arxiv.org/abs/1303.1248v1"}, {"title": "Combustion Models in Finance", "summary": "Combustion reaction kinetics models are used for the description of a special\nclass of bursty Financial Time Series. The small number of parameters they\ndepend upon enable financial analysts to predict the time as well as the\nmagnitude of the jump of the value of the portfolio. Several Financial Time\nSeries are analysed within this framework and applications are given.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0101042v1", "link": "http://arxiv.org/abs/physics/0101042v1"}, {"title": "Arbitrage hedging strategy and one more explanation of the volatility\n  smile", "summary": "We present an explicit hedging strategy, which enables to prove arbitrageness\nof market incorporating at least two assets depending on the same random\nfactor. The implied Black-Scholes volatility, computed taking into account the\nform of the graph of the option price, related to our strategy, demonstrates\nthe \"skewness\" inherent to the observational data.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1102.5525v1", "link": "http://arxiv.org/abs/1102.5525v1"}, {"title": "Interplay between dividend rate and business constraints for a financial\n  corporation", "summary": "We study a model of a corporation which has the possibility to choose various\nproduction/business policies with different expected profits and risks. In the\nmodel there are restrictions on the dividend distribution rates as well as\nrestrictions on the risk the company can undertake. The objective is to\nmaximize the expected present value of the total dividend distributions. We\noutline the corresponding Hamilton-Jacobi-Bellman equation, compute explicitly\nthe optimal return function and determine the optimal policy. As a consequence\nof these results, the way the dividend rate and business constraints affect the\noptimal policy is revealed. In particular, we show that under certain\nrelationships between the constraints and the exogenous parameters of the\nrandom processes that govern the returns, some business activities might be\nredundant, that is, under the optimal policy they will never be used in any\nscenario.", "category": ["math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/math/0503541v1", "link": "http://dx.doi.org/10.1214/105051604000000909"}, {"title": "Unveiling correlations between financial variables and topological\n  metrics of trading networks: Evidence from a stock and its warrant", "summary": "Traders adopt different trading strategies to maximize their returns in\nfinancial markets. These trading strategies not only results in specific\ntopological structures in trading networks, which connect the traders with the\npairwise buy-sell relationships, but also have potential impacts on market\ndynamics. Here, we present a detailed analysis on how the market behaviors are\ncorrelated with the structures of traders in trading networks based on audit\ntrail data for the Baosteel stock and its warrant at the transaction level from\n22 August 2005 to 23 August 2006. In our investigation, we divide each trade\nday into 48 time windows with a length of five minutes, construct a trading\nnetwork within each window, and obtain a time series of over 1,100 trading\nnetworks. We find that there are strongly simultaneous correlations between the\ntopological metrics (including network centralization, assortative index, and\naverage path length) of trading networks that characterize the patterns of\norder execution and the financial variables (including return, volatility,\nintertrade duration, and trading volume) for the stock and its warrant. Our\nanalysis may shed new lights on how the microscopic interactions between\nelements within complex system affect the system's performance.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1308.0925v1", "link": "http://dx.doi.org/10.1016/j.physa.2014.10.039"}, {"title": "Artificial Counselor System for Stock Investment", "summary": "This paper proposes a novel trading system which plays the role of an\nartificial counselor for stock investment. In this paper, the stock future\nprices (technical features) are predicted using Support Vector Regression.\nThereafter, the predicted prices are used to recommend which portions of the\nbudget an investor should invest in different existing stocks to have an\noptimum expected profit considering their level of risk tolerance. Two\ndifferent methods are used for suggesting best portions, which are Markowitz\nportfolio theory and fuzzy investment counselor. The first approach is an\noptimization-based method which considers merely technical features, while the\nsecond approach is based on Fuzzy Logic taking into account both technical and\nfundamental features of the stock market. The experimental results on New York\nStock Exchange (NYSE) show the effectiveness of the proposed system.", "category": ["q-fin.GN", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.00955v1", "link": "http://dx.doi.org/10.1609/aaai.v33i01.33019558"}, {"title": "Hysteresis and Duration Dependence of Financial Crises in the US:\n  Evidence from 1871-2016", "summary": "This study analyses the duration dependence of events that trigger volatility\npersistence in stock markets. Such events, in our context, are monthly spells\nof contiguous price decline or negative returns for the S&P500 stock market\nindex over the last 145 years. Factors known to affect the duration of these\nspells are the magnitude or intensity of the price decline, long-term interest\nrates and economic recessions, among others. The result of interest is the\nconditional probability of ending a spell of consecutive months over which\nstock market returns remain negative. In this study, we rely on continuous time\nsurvival models in order to investigate this question. Several specifications\nwere attempted, some of which under the proportional hazards assumption and\nothers under the accelerated failure time assumption. The best fit of the\nvarious models endeavored was obtained for the log-normal distribution. This\ndistribution yields a non-monotonic hazard function that increases up to a\nmaximum and then decreases. The peak is achieved 2-3 months after the spells\nonset with a hazard of around 0.9 or higher; this hazard then decays\nasymptotically to zero. Spells duration increase during recessions, when\ninterest rate rises and when price declines are more intense. The main\nconclusion is that short spells of negative returns appear to be mainly\nfrictional while long spells become structural and trigger hysteresis effects\nafter an initial period of adjustment. Although in line with our expectations,\nthese results may be of some importance for policy-makers.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1610.00259v1", "link": "http://arxiv.org/abs/1610.00259v1"}, {"title": "The role of consumer networks in firms' multi-characteristics\n  competition and market-share inequality", "summary": "We develop a location analysis spatial model of firms' competition in\nmulti-characteristics space, where consumers' opinions about the firms'\nproducts are distributed on multilayered networks. Firms do not compete on\nprice but only on location upon the products' multi-characteristics space, and\nthey aim to attract the maximum number of consumers. Boundedly rational\nconsumers have distinct ideal points/tastes over the possible available firm\nlocations but, crucially, they are affected by the opinions of their neighbors.\nProposing a dynamic agent-based analysis on firms' location choice we\ncharacterize multi-dimensional product differentiation competition as adaptive\nlearning by firms' managers and we argue that such a complex systems approach\nadvances the analysis in alternative ways, beyond game-theoretic calculations.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1601.05660v2", "link": "http://arxiv.org/abs/1601.05660v2"}, {"title": "Ergodicity-breaking reveals time optimal economic behavior in humans", "summary": "Ergodicity describes an equivalence between the expectation value and the\ntime average of observables. Applied to human behaviour, ergodic theory reveals\nhow individuals should tolerate risk in different environments. To optimise\nwealth over time, agents should adapt their utility function according to the\ndynamical setting they face. Linear utility is optimal for additive dynamics,\nwhereas logarithmic utility is optimal for multiplicative dynamics. Whether\nhumans approximate time optimal behavior across different dynamics is unknown.\nHere we compare the effects of additive versus multiplicative gamble dynamics\non risky choice. We show that utility functions are modulated by gamble\ndynamics in ways not explained by prevailing economic theory. Instead, as\npredicted by time optimality, risk aversion increases under multiplicative\ndynamics, distributing close to the values that maximise the time average\ngrowth of wealth. We suggest that our findings motivate a need for explicitly\ngrounding theories of decision-making on ergodic considerations.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1906.04652v3", "link": "http://arxiv.org/abs/1906.04652v3"}, {"title": "No-Arbitrage Prices of Cash Flows and Forward Contracts as Choquet\n  Representations", "summary": "In a market of deterministic cash flows, given as an additive, symmetric\nrelation of exchangeability on the finite signed Borel measures on the\nnon-negative real time axis, it is shown that the only arbitrage-free price\nfunctional that fulfills some additional mild requirements is the integral of\nthe unit zero-coupon bond prices with respect to the payment measures. For\nprobability measures, this is a Choquet representation, where the Dirac\nmeasures, as unit zero-coupon bonds, are the extreme points. Dropping one of\nthe requirements, the Lebesgue decomposition is used to construct\ncounterexamples, where the Choquet price formula does not hold despite of an\narbitrage-free market model. The concept is then extended to deterministic\nstreams of assets and currencies in general, yielding a valuation principle for\nforward markets. Under mild assumptions, it is shown that a foreign cash flow's\nworth in local currency is identical to the value of the cash flow in local\ncurrency for which the Radon-Nikodym derivative with respect to the foreign\ncash flow is the forward FX rate.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1506.01837v3", "link": "http://arxiv.org/abs/1506.01837v3"}, {"title": "Value at risk and the diversification dogma", "summary": "The so-called risk diversification principle is analyzed, showing that its\nconvenience depends on individual characteristics of the risks involved and the\ndependence relationship among them.\n  -----\n  Se analiza el principio de diversificaci\\'on de riesgos y se demuestra que no\nsiempre resulta mejor que no diversificar, pues esto depende de\ncaracter\\'isticas individuales de los riesgos involucrados, as\\'i como de la\nrelaci\\'on de dependencia entre los mismos.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1609.02774v1", "link": "http://arxiv.org/abs/1609.02774v1"}, {"title": "Investing and Stopping", "summary": "In this paper we solve the hedge fund manager's optimization problem in a\nmodel that allows for investors to enter and leave the fund over time depending\non its performance. The manager's payoff at the end of the year will then\ndepend not just on the terminal value of the fund level, but also on the lowest\nand the highest value reached over that time. We establish equivalence to an\noptimal stopping problem for Brownian motion; by approximating this problem\nwith the corresponding optimal stopping problem for a random walk we are led to\na simple and efficient numerical scheme to find the solution, which we then\nillustrate with some examples.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1403.0202v1", "link": "http://arxiv.org/abs/1403.0202v1"}, {"title": "Bayesian Optimization of Hyperparameters when the Marginal Likelihood is\n  Estimated by MCMC", "summary": "Bayesian models often involve a small set of hyperparameters determined by\nmaximizing the marginal likelihood. Bayesian optimization is a popular\niterative method where a Gaussian process posterior of the underlying function\nis sequentially updated by new function evaluations. An acquisition strategy\nuses this posterior distribution to decide where to place the next function\nevaluation. We propose a novel Bayesian optimization framework for situations\nwhere the user controls the computational effort, and therefore the precision\nof the function evaluations. This is a common situation in econometrics where\nthe marginal likelihood is often computed by Markov Chain Monte Carlo (MCMC)\nmethods, with the precision of the marginal likelihood estimate determined by\nthe number of MCMC draws. The proposed acquisition strategy gives the optimizer\nthe option to explore the function with cheap noisy evaluations and therefore\nfinds the optimum faster. Prior hyperparameter estimation in the steady-state\nBayesian vector autoregressive (BVAR) model on US macroeconomic time series\ndata is used for illustration. The proposed method is shown to find the optimum\nmuch quicker than traditional Bayesian optimization or grid search.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.10092v1", "link": "http://arxiv.org/abs/2004.10092v1"}, {"title": "Characterizing financial crisis by means of the three states random\n  field Ising model", "summary": "We propose a formula of time-series prediction by means of three states\nrandom field Ising model (RFIM). At the economic crisis due to disasters or\ninternational disputes, the stock price suddenly drops. The macroscopic\nphenomena should be explained from the corresponding microscopic view point\nbecause there are existing a huge number of active traders behind the crushes.\nHence, here we attempt to model the artificial financial market in which each\ntrader $i$ can choose his/her decision among `buying', `selling' or `staying\n(taking a wait-and-see attitude)', each of which corresponds to a realization\nof the three state Ising spin, namely, $S_{i}=+1$, -1 and $S_{i}=0$,\nrespectively. The decision making of traders is given by the Gibbs-Boltzmann\ndistribution with the energy function. The energy function contains three\ndistinct terms, namely, the ferromagnetic two-body interaction term (endogenous\ninformation), random field term as external information (exogenous news), and\nchemical potential term which controls the number of traders who are watching\nthe market calmly at the instance. We specify the details of the model system\nfrom the past financial market data to determine the conjugate hyper-parameters\nand draw each parameter flow as a function of time-step. Especially we will\nexamine to what extent one can characterize the crisis by means of a brand-new\norder parameter --- `turnover' --- which is defined as the number of active\ntraders who post their decisions $S_{i}=1,-1$, instead of $S_{i}=0$.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1309.5030v1", "link": "http://arxiv.org/abs/1309.5030v1"}, {"title": "Estimating Average Treatment Effects: Supplementary Analyses and\n  Remaining Challenges", "summary": "There is a large literature on semiparametric estimation of average treatment\neffects under unconfounded treatment assignment in settings with a fixed number\nof covariates. More recently attention has focused on settings with a large\nnumber of covariates. In this paper we extend lessons from the earlier\nliterature to this new setting. We propose that in addition to reporting point\nestimates and standard errors, researchers report results from a number of\nsupplementary analyses to assist in assessing the credibility of their\nestimates.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1702.01250v1", "link": "http://arxiv.org/abs/1702.01250v1"}, {"title": "Rational Inattention and Retirement Puzzles", "summary": "I present evidence incorporating costly thought solves three puzzles in the\nretirement literature. The first puzzle is, given incentives, the extent of\nbunching of labour market exits at legislated state pension ages (SPA) seems\nincompatible with rational expectations. Adding to the evidence for this\npuzzle, I include an empirical analysis focusing on whether liquidity\nconstraints can explain this bunching and find they cannot. The nature of this\npuzzle is clarified by exploring a life-cycle model with rational agents that\nmatches aggregate profiles. This model succeeds in matching aggregates by\noverestimating the impact of the SPA on poorer individuals whilst\nunderestimating its impact on wealthier people. The second puzzle is people are\noften mistaken about their own pension provisions. Concerning the second\npuzzle, I incorporate rational inattention to the SPA into the aforementioned\nlife-cycle model, allowing for mistaken beliefs. To the best of my knowledge,\nthis paper is the first not only to incorporate rational inattention into a\nlife-cycle model but also to assess a rationally inattentive model against\nnon-experimental individual choice data. This facilitates another important\ncontribution: discipling the cost of attention with subjective belief data.\nPreliminary results indicate rational inattention improves the aggregate fit\nand better matches the response of participation to the SPA across the wealth\ndistribution, hence offering a resolution to the first puzzle. The third puzzle\nis despite actuarially advantageous options to defer receipt of pension\nbenefits, take up is extremely low. An extension of the model generates an\nexplanation of this last puzzle: the actuarial calculations implying deferral\nis preferable ignore the utility cost of tracking your pension which can be\navoided by claiming. These puzzles are researched in the context of the reform\nto the UK female SPA.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1904.06520v1", "link": "http://arxiv.org/abs/1904.06520v1"}, {"title": "A Note on Applications of Stochastic Ordering to Control Problems in\n  Insurance and Finance", "summary": "We consider a controlled diffusion process $(X_t)_{t\\ge 0}$ where the\ncontroller is allowed to choose the drift $\\mu_t$ and the volatility $\\sigma_t$\nfrom a set $\\K(x) \\subset \\R\\times (0,\\infty)$ when $X_t=x$. By choosing the\nlargest $\\frac{\\mu}{\\sigma^2}$ at every point in time an extremal process is\nconstructed which is under suitable time changes stochastically larger than any\nother admissible process. This observation immediately leads to a very simple\nsolution of problems where ruin or hitting probabilities have to be minimized.\nUnder further conditions this extremal process also minimizes \"drawdown\"\nprobabilities.", "category": ["math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1210.3800v2", "link": "http://dx.doi.org/10.1080/17442508.2013.778861"}, {"title": "Uncertainty in the Hot Hand Fallacy: Detecting Streaky Alternatives to\n  Random Bernoulli Sequences", "summary": "We study a class of tests of the randomness of Bernoulli sequences and their\napplication to analyses of the human tendency to perceive streaks of\nconsecutive successes as overly representative of positive dependence-the hot\nhand fallacy. In particular, we study tests of randomness (i.e., that trials\nare i.i.d.) based on test statistics that compare the proportion of successes\nthat directly follow k consecutive successes with either the overall proportion\nof successes or the proportion of successes that directly follow k consecutive\nfailures. We derive the asymptotic distributions of these test statistics and\ntheir permutation distributions under randomness, under a set of general\nstationary processes, and under a Markov model of \"streakiness\", which allow us\nto evaluate their local asymptotic power. The results are applied to evaluate\ntests of randomness implemented on data from a basketball shooting experiment,\nwhose conclusions are disputed by Gilovich, Vallone, and Tversky (1985) and\nMiller and Sanjurjo (2018a). We establish that substantially larger data sets\nare required to derive an informative measurement of the deviation from\nrandomness in basketball shooting. Although multiple testing procedures reveal\nthat one shooter in the experiment exhibits a shooting pattern significantly\ninconsistent with randomness - supplying strong evidence that basketball\nshooting is not random for all shooters all of the time - we find that the\nevidence against randomness is limited to this shooter. Our results provide a\nmathematical and statistical foundation for the design and validation of\nexperiments that directly compare deviations from randomness with human beliefs\nabout deviations from randomness in Bernoulli sequences.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.01406v3", "link": "http://arxiv.org/abs/1908.01406v3"}, {"title": "Anticipated impacts of Brexit scenarios on UK food prices and\n  implications for policies on poverty and health: a structured expert\n  judgement approach", "summary": "Food insecurity is associated with increased risk for several health\nconditions and with poor chronic disease management. Key determinants for\nhousehold food insecurity are income and food costs. Whereas short-term\nhousehold incomes are likely to remain static, increased food prices would be a\nsignificant driver of food insecurity. To investigate food price drivers for\nhousehold food security and its health consequences in the UK under scenarios\nof Deal and No deal for Brexit . To estimate the 5\\% and 95\\% quantiles of the\nprojected price distributions. Structured expert judgement elicitation, a\nwell-established method for quantifying uncertainty, using experts. In July\n2018, each expert estimated the median, 5\\% and 95\\% quantiles of changes in\nprice for ten food categories under Brexit Deal and No-deal to June 2020\nassuming Brexit had taken place on 29th March 2019. These were aggregated based\non the accuracy and informativeness of the experts on calibration questions.\nTen specialists in food procurement, retail, agriculture, economics, statistics\nand household food security. Results: when combined in proportions used to\ncalculate Consumer Prices Index food basket costs, median food price change for\nBrexit with a Deal is expected to be +6.1\\% [90\\% credible interval:-3\\%,\n+17\\%] and with No deal +22.5\\% [+1\\%, +52\\%]. The number of households\nexperiencing food insecurity and its severity are likely to increase because of\nexpected sizeable increases in median food prices after Brexit. Higher\nincreases are more likely than lower rises and towards the upper limits, these\nwould entail severe impacts. Research showing a low food budget leads to\nincreasingly poor diet suggests that demand for health services in both the\nshort and longer term is likely to increase due to the effects of food\ninsecurity on the incidence and management of diet-sensitive conditions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1904.03053v3", "link": "http://arxiv.org/abs/1904.03053v3"}, {"title": "Legal Architecture and Design for Gulf Cooperation Council Economic\n  Integration", "summary": "The Cooperation Council for the Arab States of the Gulf (GCC) is generally\nregarded as a success story for economic integration in Arab countries. The\nidea of regional integration gained ground by signing the GCC Charter. It\nenvisioned a closer economic relationship between member states.Although\neconomic integration among GCC member states is an ambitious step in the right\ndirection, there are gaps and challenges ahead. The best way to address the\ngaps and challenges that exist in formulating integration processes in the GCC\nis to start with a clear set of rules and put the necessary mechanisms in\nplace. Integration attempts must also exhibit a high level of commitment in\norder to deflect dynamics of disintegration that have all too often frustrated\nmeaningful integration in Arab countries. If the GCC can address these issues,\nit could become an economic powerhouse within Arab countries and even Asia.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.08798v1", "link": "http://arxiv.org/abs/1909.08798v1"}, {"title": "An analysis of Cross-correlations in South African Market data", "summary": "We apply random matrix theory to compare correlation matrix estimators C\nobtained from emerging market data. The correlation matrices are constructed\nfrom 10 years of daily data for stocks listed on the Johannesburg Stock\nExchange (JSE) from January 1993 to December 2002. We test the spectral\nproperties of C against random matrix predictions and find some agreement\nbetween the distributions of eigenvalues, nearest neighbour spacings,\ndistributions of eigenvector components and the inverse participation ratios\nfor eigenvectors. We show that interpolating both missing data and illiquid\ntrading days with a zero-order hold increases agreement with RMT predictions.\nFor the more realistic estimation of correlations in an emerging market, we\nsuggest a pairwise measured-data correlation matrix. For the data set used,\nthis approach suggests greater temporal stability for the leading eigenvectors.\nAn interpretation of eigenvectors in terms of trading strategies is given in\nlieu of classification by economic sectors.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0402389v3", "link": "http://dx.doi.org/10.1016/j.physa.2006.10.030"}, {"title": "Econometrics For Decision Making: Building Foundations Sketched By\n  Haavelmo And Wald", "summary": "In the early 1940s, Haavelmo proposed a probabilistic structure for\neconometric modeling, aiming to make econometrics useful for public decision\nmaking. His fundamental contribution has become thoroughly embedded in\nsubsequent econometric research, yet it could not fully answer all the deep\nissues that the author raised. Notably, Haavelmo struggled to formalize the\nimplications for decision making of the fact that models can at most\napproximate actuality. In the same period, Wald initiated his own seminal\ndevelopment of statistical decision theory. Haavelmo favorably cited Wald, but\neconometrics subsequently did not embrace statistical decision theory. Instead,\nit focused on study of identification, estimation, and statistical inference.\nThis paper proposes statistical decision theory as a framework for evaluation\nof the performance of models in decision making. I particularly consider the\ncommon practice of as-if optimization: specification of a model, point\nestimation of its parameters, and use of the point estimate to make a decision\nthat would be optimal if the estimate were accurate. A central theme is that\none should evaluate as-if optimization or any other model-based decision rule\nby its performance across the state space, not the model space. I use\nprediction and treatment choice to illustrate. Statistical decision theory is\nconceptually simple, but application is often challenging. Advancement of\ncomputation is the primary task to continue building the foundations sketched\nby Haavelmo and Wald.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.08726v2", "link": "http://arxiv.org/abs/1912.08726v2"}, {"title": "Parameter estimation for a subcritical affine two factor model", "summary": "For an affine two factor model, we study the asymptotic properties of the\nmaximum likelihood and least squares estimators of some appearing parameters in\nthe so-called subcritical (ergodic) case based on continuous time observations.\nWe prove strong consistency and asymptotic normality of the estimators in\nquestion.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1302.3451v3", "link": "http://arxiv.org/abs/1302.3451v3"}, {"title": "Oil price shocks, road transport pollution emissions and residents'\n  health losses in China", "summary": "China's rapid economic growth resulted in serious air pollution, which caused\nsubstantial losses to economic development and residents' health. In\nparticular, the road transport sector has been blamed to be one of the major\nemitters. During the past decades, fluctuation in the international oil prices\nhas imposed significant impacts on the China's road transport sector.\nTherefore, we propose an assumption that China's provincial economies are\nindependent \"economic entities\". Based on this assumption, we investigate the\nChina's road transport fuel (i.e., gasoline and diesel) demand system by using\nthe panel data of all 31 Chinese provinces except Hong Kong, Macau and Taiwan.\nTo connect the fuel demand system and the air pollution emissions, we propose\nthe concept of pollution emissions elasticities to estimate the air pollution\nemissions from the road transport sector, and residents' health losses by a\nsimplified approach consisting of air pollution concentrations and health loss\nassessment models under different scenarios based on real-world oil price\nfluctuations. Our framework, to the best of our knowledge, is the first attempt\nto address the transmission mechanism between the fuel demand system in road\ntransport sector and residents' health losses in the transitional China.", "category": ["q-fin.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1512.01742v1", "link": "http://dx.doi.org/10.1016/j.trd.2015.10.019"}, {"title": "Sur la d\u00e9composabilit\u00e9 empirique des indicateurs de pauvret\u00e9", "summary": "We study the empirical decomposition of poverty indicators. This property is\nvery important and convenient in the context of the fight against poverty.\nIndeed, it makes it possible to put in place sectoral poverty reduction\npolicies on the basis of a relevant stratification laid down at the outset. The\nsimultaneous impacts of these policies, measured as reduction gains over the\npopulation as a whole, is then obtained by aggregating those obtained at each\nstratum by a relatively simple formula. It turns out that indicators as\nimportant as those of Sen and Shorrocks do not verify this property contrary to\nthe elements of the class of Foster - Greer and Thorbecke. Given the data from\nthe 1996 Senegalese Survey of Households (ESAM), we show that the lack of\ndecomposability of these indicators on the income variable for several types of\npopulation stratification is practically zero , of the order of one to two per\nthousand. This makes it possible to use the decomposition of the Sen and\nShorrocks indicators without any untoward consequences. An explanatory model of\nthese results is presented for future research.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1701.02649v1", "link": "http://arxiv.org/abs/1701.02649v1"}, {"title": "Application of Stochastic Mesh Method to Efficient Approximation of CVA", "summary": "In this paper, the author considers the numerical computation of CVA for\nlarge systems by Mote Carlo methods. He introduces two types of stochastic mesh\nmethods for the computations of CVA. In the first method, stochastic mesh\nmethod is used to obtain the future value of the derivative contracts. In the\nsecond method, stochastic mesh method is used only to judge whether future\nvalue of the derivative contracts is positive or not. He discusses the rate of\nconvergence to the real CVA value of these methods.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1510.04588v1", "link": "http://arxiv.org/abs/1510.04588v1"}, {"title": "Comprehending environmental and economic sustainability: Comparative\n  analysis of stability principles in the biosphere and free market economy", "summary": "Using the formalism of Lyapunov potential function it is shown that the\nstability principles for biomass in the ecosystem and for employment in\neconomics are mathematically similar. The ecosystem is found to have a stable\nand an unstable stationary state with high (forest) and low (grasslands)\nbiomass, respectively. In economics, there is a stable stationary state with\nhigh employment, which corresponds to mass production of conventional goods\nsold at low cost price, and an unstable stationary state with lower employment,\nwhich corresponds to production of novel goods appearing in the course of\ntechnological progress. An additional stable stationary state is described for\neconomics, the one corresponding to very low employment in production of life\nessentials such as energy and raw materials. In this state the civilization\ncurrently pays 10% of global GDP for energy produced by a negligible minority\nof the working population (currently ~0.2%) and sold at prices greatly\nexceeding the cost price by 40 times. It is shown that economic ownership over\nenergy sources is equivalent to equating measurable variables of different\ndimensions (stores and fluxes), which leads to effective violation of the laws\nof energy and matter conservation.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1011.5978v1", "link": "http://dx.doi.org/10.1111/j.1749-6632.2009.05400.x"}, {"title": "G-Learner and GIRL: Goal Based Wealth Management with Reinforcement\n  Learning", "summary": "We present a reinforcement learning approach to goal based wealth management\nproblems such as optimization of retirement plans or target dated funds. In\nsuch problems, an investor seeks to achieve a financial goal by making periodic\ninvestments in the portfolio while being employed, and periodically draws from\nthe account when in retirement, in addition to the ability to re-balance the\nportfolio by selling and buying different assets (e.g. stocks). Instead of\nrelying on a utility of consumption, we present G-Learner: a reinforcement\nlearning algorithm that operates with explicitly defined one-step rewards, does\nnot assume a data generation process, and is suitable for noisy data. Our\napproach is based on G-learning - a probabilistic extension of the Q-learning\nmethod of reinforcement learning.\n  In this paper, we demonstrate how G-learning, when applied to a quadratic\nreward and Gaussian reference policy, gives an entropy-regulated Linear\nQuadratic Regulator (LQR). This critical insight provides a novel and\ncomputationally tractable tool for wealth management tasks which scales to high\ndimensional portfolios. In addition to the solution of the direct problem of\nG-learning, we also present a new algorithm, GIRL, that extends our goal-based\nG-learning approach to the setting of Inverse Reinforcement Learning (IRL)\nwhere rewards collected by the agent are not observed, and should instead be\ninferred. We demonstrate that GIRL can successfully learn the reward parameters\nof a G-Learner agent and thus imitate its behavior. Finally, we discuss\npotential applications of the G-Learner and GIRL algorithms for wealth\nmanagement and robo-advising.", "category": ["q-fin.PM", "q-fin.CP"], "id": "http://arxiv.org/abs/2002.10990v1", "link": "http://arxiv.org/abs/2002.10990v1"}, {"title": "A simple model suggesting economically rational sample-size choice\n  drives irreproducibility", "summary": "Several systematic studies have suggested that a large fraction of published\nresearch is not reproducible. One probable reason for low reproducibility is\ninsufficient sample size, resulting in low power and low positive predictive\nvalue. It has been suggested that insufficient sample-size choice is driven by\na combination of scientific competition and 'positive publication bias'. Here\nwe formalize this intuition in a simple model, in which scientists choose\neconomically rational sample sizes, balancing the cost of experimentation with\nincome from publication. Specifically, assuming that a scientist's income\nderives only from 'positive' findings (positive publication bias) and that\nindividual samples cost a fixed amount, allows to leverage basic statistical\nformulas into an economic optimality prediction. We find that if effects have\ni) low base probability, ii) small effect size or iii) low grant income per\npublication, then the rational (economically optimal) sample size is small.\nFurthermore, for plausible distributions of these parameters we find a robust\nemergence of a bimodal distribution of obtained statistical power and low\noverall reproducibility rates, both matching empirical findings. Finally, we\nexplore conditional equivalence testing as a means to align economic incentives\nwith adequate sample sizes. Overall, the model describes a simple mechanism\nexplaining both the prevalence and the persistence of small sample sizes, and\nis well suited for empirical validation. It proposes economic rationality, or\neconomic pressures, as a principal driver of irreproducibility and suggests\nstrategies to change this.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.08702v4", "link": "http://arxiv.org/abs/1908.08702v4"}, {"title": "On the optimal dividend problem for a spectrally negative L\u00e9vy\n  process", "summary": "In this paper we consider the optimal dividend problem for an insurance\ncompany whose risk process evolves as a spectrally negative L\\'{e}vy process in\nthe absence of dividend payments. The classical dividend problem for an\ninsurance company consists in finding a dividend payment policy that maximizes\nthe total expected discounted dividends. Related is the problem where we impose\nthe restriction that ruin be prevented: the beneficiaries of the dividends must\nthen keep the insurance company solvent by bail-out loans. Drawing on the\nfluctuation theory of spectrally negative L\\'{e}vy processes we give an\nexplicit analytical description of the optimal strategy in the set of barrier\nstrategies and the corresponding value function, for either of the problems.\nSubsequently we investigate when the dividend policy that is optimal among all\nadmissible ones takes the form of a barrier strategy.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0702893v1", "link": "http://dx.doi.org/10.1214/105051606000000709"}, {"title": "Dynamic robust duality in utility maximization", "summary": "A celebrated financial application of convex duality theory gives an explicit\nrelation between the following two quantities:\n  (i) The optimal terminal wealth $X^*(T) : = X_{\\varphi^*}(T)$ of the problem\nto maximize the expected $U$-utility of the terminal wealth $X_{\\varphi}(T)$\ngenerated by admissible portfolios $\\varphi(t), 0 \\leq t \\leq T$ in a market\nwith the risky asset price process modeled as a semimartingale;\n  (ii) The optimal scenario $\\frac{dQ^*}{dP}$ of the dual problem to minimize\nthe expected $V$-value of $\\frac{dQ}{dP}$ over a family of equivalent local\nmartingale measures $Q$, where $V$ is the convex conjugate function of the\nconcave function $U$.\n  In this paper we consider markets modeled by It\\^o-L\\'evy processes. In the\nfirst part we use the maximum principle in stochastic control theory to extend\nthe above relation to a \\emph{dynamic} relation, valid for all $t \\in [0,T]$.\nWe prove in particular that the optimal adjoint process for the primal problem\ncoincides with the optimal density process, and that the optimal adjoint\nprocess for the dual problem coincides with the optimal wealth process, $0 \\leq\nt \\leq T$. In the terminal time case $t=T$ we recover the classical duality\nconnection above. We get moreover an explicit relation between the optimal\nportfolio $\\varphi^*$ and the optimal measure $Q^*$. We also obtain that the\nexistence of an optimal scenario is equivalent to the replicability of a\nrelated $T$-claim.\n  In the second part we present robust (model uncertainty) versions of the\noptimization problems in (i) and (ii), and we prove a similar dynamic relation\nbetween them. In particular, we show how to get from the solution of one of the\nproblems to the other. We illustrate the results with explicit examples.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1304.5040v3", "link": "http://arxiv.org/abs/1304.5040v3"}, {"title": "Stratonovich representation of semimartingale rank processes", "summary": "Suppose that $X_1, \\ldots , X_n$ are continuous semimartingales that are\nreversible and have nondegenerate crossings. Then the corresponding rank\nprocesses can be represented by generalized Stratonovich integrals, and this\nrepresentation can be used to decompose the relative log-return of portfolios\ngenerated by functions of ranked market weights.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1705.00336v1", "link": "http://arxiv.org/abs/1705.00336v1"}, {"title": "Modeling Credit Spreads Using Nonlinear Regression", "summary": "The term structure of credit spreads is studied with an aim to predict its\nfuture movements. A completely new approach to tackle this problem is\npresented, which utilizes nonlinear parametric models. The Brain-Cousens\nregression model with five parameters is chosen to describe the term structure\nof credit spreads. Further, we investigate the dependence of the parameter\nchanges over time and the determinants of credit spreads.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1401.6955v1", "link": "http://arxiv.org/abs/1401.6955v1"}, {"title": "From Disequilibrium Markets to Equilibrium", "summary": "The modeling of financial markets as disequilibrium models by ordinary\ndifferential equations has become a popular modeling tool. One famous example\nof such a model is the Beja-Goldman model(The Journal of Finance, 1980) which\nwe consider in this paper. We study the passage from disequilibrium dynamics to\nequilibrium. Mathematically, this limit corresponds to an asymptotic limit also\nknown as a Tikhonov-Fenichel reduction. Furthermore, we analyze the stability\nof the reduced equilibrium model and discuss the economic implications. We\nconduct several numerical examples to visualize and support our analysis.", "category": ["econ.GN", "q-fin.EC", "q-fin.MF", "q-fin.TR"], "id": "http://arxiv.org/abs/1912.09679v1", "link": "http://arxiv.org/abs/1912.09679v1"}, {"title": "The Impact of Birth Order on Behavior in Contact Team Sports: the\n  Evidence of Rugby Teams in Argentina", "summary": "Several studies have shown that birth order and the sex of siblings may have\nan influence on individual behavioral traits. In particular, it has been found\nthat second brothers (of older male siblings) tend to have more disciplinary\nproblems. If this is the case, this should also be shown in contact sports. To\nassess this hypothesis we use a data set from the South Rugby Union (URS) from\nBah\\'ia Blanca, Argentina, and information obtained by surveying more than four\nhundred players of that league. We find a statistically significant positive\nrelation between being a second-born male rugby player with an older male\nbrother and the number of yellow cards received.\n  \\textbf{Keywords:} Birth Order; Behavior; Contact Sports; Rugby.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.09421v1", "link": "http://arxiv.org/abs/2004.09421v1"}, {"title": "Carbon-dioxide emissions trading and hierarchical structure in worldwide\n  finance and commodities markets", "summary": "In a highly interdependent economic world, the nature of relationships\nbetween financial entities is becoming an increasingly important area of study.\nRecently, many studies have shown the usefulness of minimal spanning trees\n(MST) in extracting interactions between financial entities. Here, we propose a\nmodified MST network whose metric distance is defined in terms of\ncross-correlation coefficient absolute values, enabling the connections between\nanticorrelated entities to manifest properly. We investigate 69 daily time\nseries, comprising three types of financial assets: 28 stock market indicators,\n21 currency futures, and 20 commodity futures. We show that though the\nresulting MST network evolves over time, the financial assets of similar type\ntend to have connections which are stable over time. In addition, we find a\ncharacteristic time lag between the volatility time series of the stock market\nindicators and those of the EU CO2 emission allowance (EUA) and crude oil\nfutures (WTI). This time lag is given by the peak of the cross-correlation\nfunction of the volatility time series EUA (or WTI) with that of the stock\nmarket indicators, and is markedly different (>20 days) from 0, showing that\nthe volatility of stock market indicators today can predict the volatility of\nEU emissions allowances and of crude oil in the near future.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1205.1861v2", "link": "http://dx.doi.org/10.1103/PhysRevE.87.012814"}, {"title": "Distance to the line in the Heston model", "summary": "The main object of study in the paper is the distance from a point to a line\nin the Riemannian manifold associated with the Heston model. We reduce the\nproblem of computing such a distance to certain minimization problems for\nfunctions of one variable over finite intervals. One of the main ideas in this\npaper is to use a new system of coordinates in the Heston manifold and the\nlevel sets associated with this system. In the case of a vertical line, the\nformulas for the distance to the line are rather simple. For slanted lines, the\nformulas are more complicated, and a more subtle analysis of the level sets\nintersecting the given line is needed. We also find simple formulas for the\nHeston distance from a point to a level set. As a natural application, we use\nthe formulas obtained in the present paper to compute the small maturity limit\nof the implied volatility in the correlated Heston model.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1409.6027v1", "link": "http://arxiv.org/abs/1409.6027v1"}, {"title": "A multifactor regime-switching model for inter-trade durations in the\n  limit order market", "summary": "This paper studies inter-trade durations in the NASDAQ limit order market and\nfinds that inter-trade durations in ultra-high frequency have two modes. One\nmode is to the order of approximately 10^{-4} seconds, and the other is to the\norder of 1 second. This phenomenon and other empirical evidence suggest that\nthere are two regimes associated with the dynamics of inter-trade durations,\nand the regime switchings are driven by the changes of high-frequency traders\n(HFTs) between providing and taking liquidity. To find how the two modes depend\non information in the limit order book (LOB), we propose a two-state\nmultifactor regime-switching (MF-RSD) model for inter-trade durations, in which\nthe probabilities transition matrices are time-varying and depend on some\nlagged LOB factors. The MF-RSD model has good in-sample fitness and the\nsuperior out-of-sample performance, compared with some benchmark duration\nmodels. Our findings of the effects of LOB factors on the inter-trade durations\nhelp to understand more about the high-frequency market microstructure.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.00764v1", "link": "http://arxiv.org/abs/1912.00764v1"}, {"title": "Price competition with uncertain quality and cost", "summary": "Consumers in many markets are uncertain about firms' qualities and costs, so\nbuy based on both the price and the quality inferred from it. Optimal pricing\ndepends on consumer heterogeneity only when firms with higher quality have\nhigher costs, regardless of whether costs and qualities are private or public.\nIf better quality firms have lower costs, then good quality is sold cheaper\nthan bad under private costs and qualities, but not under public. However, if\nhigher quality is costlier, then price weakly increases in quality under both\ninformational environments.", "category": [], "id": "http://arxiv.org/abs/1903.03987v2", "link": "http://arxiv.org/abs/1903.03987v2"}, {"title": "Manipulating decision making of typical agents", "summary": "We investigate how the choice of decision makers can be varied under the\npresence of risk and uncertainty. Our analysis is based on the approach we have\npreviously applied to individual decision makers, which we now generalize to\nthe case of decision makers that are members of a society. The approach employs\nthe mathematical techniques that are common in quantum theory, justifying our\nnaming as Quantum Decision Theory. However, we do not assume that decision\nmakers are quantum objects. The techniques of quantum theory are needed only\nfor defining the prospect probabilities taking into account such hidden\nvariables as behavioral biases and other subconscious feelings. The approach\ndescribes an agent's choice as a probabilistic event occurring with a\nprobability that is the sum of a utility factor and of an attraction factor.\nThe attraction factor embodies subjective and unconscious dimensions in the\nmind of the decision maker. We show that the typical aggregate amplitude of the\nattraction factor is $1/4$, and it can be either positive or negative depending\non the relative attraction of the competing choices. The most efficient way of\nvarying the decision makers choice is realized by influencing the attraction\nfactor. This can be done in two ways. One method is to arrange in a special\nmanner the payoff weights, which induces the required changes of the values of\nattraction factors. We show that a slight variation of the payoff weights can\ninvert the sign of the attraction factors and reverse the decision preferences,\neven when the prospect utilities remain unchanged. The second method of\ninfluencing the decision makers choice is by providing information to decision\nmakers. The methods of influencing decision making are illustrated by several\nexperiments, whose outcomes are compared quantitatively with the predictions of\nour approach.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1409.0636v1", "link": "http://arxiv.org/abs/1409.0636v1"}, {"title": "On strong binomial approximation for stochastic processes and\n  applications for financial modelling", "summary": "This paper considers binomial approximation of continuous time stochastic\nprocesses. It is shown that, under some mild integrability conditions, a\nprocess can be approximated in mean square sense and in other strong metrics by\nbinomial processes, i.e., by processes with fixed size binary increments at\nsampling points. Moreover, this approximation can be causal, i.e., at every\ntime it requires only past historical values of the underlying process. In\naddition, possibility of approximation of solutions of stochastic differential\nequations by solutions of ordinary equations with binary noise is established.\nSome consequences for the financial modelling and options pricing models are\ndiscussed.", "category": ["q-fin.CP", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1311.0675v3", "link": "http://arxiv.org/abs/1311.0675v3"}, {"title": "On the quadratic variation of the model-free price paths with jumps", "summary": "We prove that the model-free typical (in the sense of Vovk) c\\`adl\\`ag price\npaths with mildly restricted downward jumps possess quadratic variation which\ndoes not depend on the specific sequence of partitions as long as these\npartitions are obtained from stopping times such that the oscillations of a\npath on the consecutive (half-open on the right) intervals of these partitions\ntend (in a specified sense) to 0. Finally, we also define quasi-explicit,\npartition independent quantities which tend to this quadratic variation.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1710.07894v3", "link": "http://arxiv.org/abs/1710.07894v3"}, {"title": "Evidence for the exponential distribution of income in the USA", "summary": "Using tax and census data, we demonstrate that the distribution of individual\nincome in the USA is exponential. Our calculated Lorenz curve without fitting\nparameters and Gini coefficient 1/2 agree well with the data. From the\nindividual income distribution, we derive the distribution function of income\nfor families with two earners and show that it also agrees well with the data.\nThe family data for the period 1947-1994 fit the Lorenz curve and Gini\ncoefficient 3/8=0.375 calculated for two-earners families.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0008305v2", "link": "http://dx.doi.org/10.1007/PL00011112"}, {"title": "Random, but not so much: A parameterization for the returns and\n  correlation matrix of financial time series", "summary": "A parameterization that is a modified version of a previous work is proposed\nfor the returns and correlation matrix of financial time series and its\nproperties are studied. This parameterization allows easy introduction of\nnon-stationarity and it shows several of the characteristics of the true,\nobserved realizations, such as fat tails, volatility clustering, and a spectrum\nof eigenvalues of the correlation matrix that can be understood as an extension\nof Random Matrix Theory results. The predicted behavior of this\nparameterization for the eigenvalues is compared with the eigenvalues of\nBrazilian assets and it is shown that those predictions fit the data better\nthan Random Matrix Theory.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0701025v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.108"}, {"title": "Compactification of Extensive Forms and Belief in the Opponents' Future\n  Rationality", "summary": "We introduce an operation, called compactification, to reduce an extensive\nform to a compact one where each decision node in the game tree can be assigned\nto more than one player. Motivated by Thompson (1952)'s interchange of decision\nnodes, we attempt to capture the notion of a faithful representation of the\nchronological order of the moves in a dynamic game which plays a vital role in\nfields like epistemic game theory. The compactification process preserves\nperfect recall and the unambiguity of the order among information sets. We\nspecify an algorithm, called leaves-to-root process, which compactifies at\nleast as many information sets as any other compactification process. The\ncompact extensive form provides an approach to avoid problems in dynamic game\ntheory due to the vague definition of the chronological order of the moves, for\nexample, belief in the opponents' future rationality (Perea (2014))'s\nsensitivity to the specific extensive form representation. We show that any\nstrategy which can rationally be chosen under common belief in future\nrationality in a minimal compact game if and only if it satisfies this property\nin every extensive form game which is related to it via some compactification\nprocess.", "category": [], "id": "http://arxiv.org/abs/1905.00355v2", "link": "http://arxiv.org/abs/1905.00355v2"}, {"title": "A variation of the Dragulescu-Yakovenko income model", "summary": "In the context of the Dragulescu-Yakovenko (2000) model, we show that\nempirical income distribution with truncated datasets, cannot be properly\nmodeled by the one-parameter exponential distribution. However, a truncated\nversion characterized by an exponential distribution with two parameters gives\nan accurate fit.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1406.5083v1", "link": "http://arxiv.org/abs/1406.5083v1"}, {"title": "Market Coupling as the Universal Algorithm to Assess Zonal Divisions", "summary": "Adopting a zonal structure of electricity market requires specification of\nzones' borders. In this paper we use social welfare as the measure to assess\nquality of various zonal divisions. The social welfare is calculated by Market\nCoupling algorithm. The analyzed divisions are found by the usage of extended\nLocational Marginal Prices (LMP) methodology presented in paper [1], which\ntakes into account variable weather conditions. The offered method of\nassessment of a proposed division of market into zones is however not limited\nto LMP approach but can evaluate the social welfare of divisions obtained by\nany methodology.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1405.0878v1", "link": "http://arxiv.org/abs/1405.0878v1"}, {"title": "The equivalence of two tax processes", "summary": "We introduce two models of taxation, the latent and natural tax processes,\nwhich have both been used to represent loss-carry-forward taxation on the\ncapital of an insurance company. In the natural tax process, the tax rate is a\nfunction of the current level of capital, whereas in the latent tax process,\nthe tax rate is a function of the capital that would have resulted if no tax\nhad been paid. Whereas up to now these two types of tax processes have been\ntreated separately, we show that, in fact, they are essentially equivalent.\nThis allows a unified treatment, translating results from one model to the\nother. Significantly, we solve the question of existence and uniqueness for the\nnatural tax process, which is defined via an integral equation. Our results\nclarify the existing literature on processes with tax.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1811.01664v3", "link": "http://arxiv.org/abs/1811.01664v3"}, {"title": "Universality in the stock exchange", "summary": "We analyze the constituents stocks of the Dow Jones Industrial Average\n(DJIA30) and the Standard & Poor's 100 index (S&P100) of the NYSE stock\nexchange market. Surprisingly, we discover the data collapse of the histograms\nof the DJIA30 price fluctuations and of the S&P100 price fluctuations to the\nuniversal non-parametric Bramwell-Holdsworth-Pinton (BHP) distribution. Since\nthe BHP probability density function appears in several other dissimilar\nphenomena, our result reveals an universal feature of the stock exchange\nmarket.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0810.2508v2", "link": "http://arxiv.org/abs/0810.2508v2"}, {"title": "A new and stable estimation method of country economic fitness and\n  product complexity", "summary": "We present a new metric estimating fitness of countries and complexity of\nproducts by exploiting a non-linear non-homogeneous map applied to the publicly\navailable information on the goods exported by a country. The non homogeneous\nterms guarantee both convergence and stability. After a suitable rescaling of\nthe relevant quantities, the non homogeneous terms are eventually set to zero\nso that this new metric is parameter free. This new map almost reproduces the\nresults of the original homogeneous metrics already defined in literature and\nallows for an approximate analytic solution in case of actual binarized\nmatrices based on the Revealed Comparative Advantage (RCA) indicator. This\nsolution is connected with a new quantity describing the neighborhood of nodes\nin bipartite graphs, representing in this work the relations between countries\nand exported products. Moreover, we define the new indicator of country\nnet-efficiency quantifying how a country efficiently invests in capabilities\nable to generate innovative complex high quality products. Eventually, we\ndemonstrate analytically the local convergence of the algorithm involved.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.10276v2", "link": "http://dx.doi.org/10.3390/e20100783"}, {"title": "Statistical Analysis of the Stock Index of the Budapest Stock Exchange", "summary": "Scaling properties of the BUX index are similar to those observed in other\nparts of the world. The main difference is that the traditional quantities like\nvolatility, growth and autocorrelation of returns follows more closely the\nassumptions of the traditional stock market theory developed by Bachelier and\nby Black and Scholes.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9711008v1", "link": "http://arxiv.org/abs/cond-mat/9711008v1"}, {"title": "Belief Propagation Algorithm for Portfolio Optimization Problems", "summary": "The typical behavior of optimal solutions to portfolio optimization problems\nwith absolute deviation and expected shortfall models using replica analysis\nwas pioneeringly estimated by S. Ciliberti and M. M\\'ezard [Eur. Phys. B. 57,\n175 (2007)]; however, they have not yet developed an approximate derivation\nmethod for finding the optimal portfolio with respect to a given return set. In\nthis study, an approximation algorithm based on belief propagation for the\nportfolio optimization problem is presented using the Bethe free energy\nformalism, and the consistency of the numerical experimental results of the\nproposed algorithm with those of replica analysis is confirmed. Furthermore,\nthe conjecture of H. Konno and H. Yamazaki, that the optimal solutions with the\nabsolute deviation model and with the mean-variance model have the same typical\nbehavior, is verified using replica analysis and the belief propagation\nalgorithm.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1008.3746v2", "link": "http://dx.doi.org/10.1371/journal.pone.0134968"}, {"title": "Artificial Intelligence as Structural Estimation: Economic\n  Interpretations of Deep Blue, Bonanza, and AlphaGo", "summary": "Artificial intelligence (AI) has achieved superhuman performance in a growing\nnumber of tasks, but understanding and explaining AI remain challenging. This\npaper clarifies the connections between machine-learning algorithms to develop\nAIs and the econometrics of dynamic structural models through the case studies\nof three famous game AIs. Chess-playing Deep Blue is a calibrated value\nfunction, whereas shogi-playing Bonanza is an estimated value function via\nRust's (1987) nested fixed-point method. AlphaGo's \"supervised-learning policy\nnetwork\" is a deep neural network implementation of Hotz and Miller's (1993)\nconditional choice probability estimation; its \"reinforcement-learning value\nnetwork\" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional\nchoice simulation method. Relaxing these AIs' implicit econometric assumptions\nwould improve their structural interpretability.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1710.10967v3", "link": "http://arxiv.org/abs/1710.10967v3"}, {"title": "The first passage time problem for mixed-exponential jump processes with\n  applications in insurance and finance", "summary": "This paper stidies the first passage times to constant boundaries for\nmixed-exponential jump diffusion processes. Explicit solutions of the Laplace\ntransforms of the distribution of the first passage times, the joint\ndistribution of the first passage times and undershoot (overshoot) are\nobtained. As applications, we present explicit expression of the Gerber-Shiu\nfunctions for surplus processes with two-sided jumps, present the analytical\nsolutions for popular path-dependent options such as lookback and barrier\noptions in terms of Laplace transforms and give a closed-form expression on the\nprice of the zero-coupon bond under a structural credit risk model with jumps.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1302.6762v2", "link": "http://arxiv.org/abs/1302.6762v2"}, {"title": "Probability of Large Movements in Financial Markets", "summary": "Based on empirical financial time-series, we show that the \"silence-breaking\"\nprobability follows a super-universal power law: the probability of observing a\nlarge movement is inversely proportional to the length of the on-going\nlow-variability period. Such a scaling law has been previously predicted\ntheoretically [R. Kitt, J. Kalda, Physica A 353 (2005) 480], assuming that the\nlength-distribution of the low-variability periods follows a multiscaling power\nlaw.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0812.4455v2", "link": "http://dx.doi.org/10.1016/j.physa.2009.07.027"}, {"title": "Multiplicative noise, fast convolution, and pricing", "summary": "In this work we detail the application of a fast convolution algorithm\ncomputing high dimensional integrals to the context of multiplicative noise\nstochastic processes. The algorithm provides a numerical solution to the\nproblem of characterizing conditional probability density functions at\narbitrary time, and we applied it successfully to quadratic and piecewise\nlinear diffusion processes. The ability in reproducing statistical features of\nfinancial return time series, such as thickness of the tails and scaling\nproperties, makes this processes appealing for option pricing. Since exact\nanalytical results are missing, we exploit the fast convolution as a numerical\nmethod alternative to the Monte Carlo simulation both in objective and risk\nneutral settings. In numerical sections we document how fast convolution\noutperforms Monte Carlo both in velocity and efficiency terms.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1107.1451v1", "link": "http://dx.doi.org/10.1080/14697688.2012.729857"}, {"title": "Optimal Taxation with Endogenous Default under Incomplete Markets", "summary": "In a dynamic economy, we characterize the fiscal policy of the government\nwhen it levies distortionary taxes and issues defaultable bonds to finance its\nstochastic expenditure. Default may occur in equilibrium as it prevents the\ngovernment from incurring in future tax distortions that would come along with\nthe service of the debt. Households anticipate the possibility of default\ngenerating endogenous credit limits. These limits hinder the government's\nability to smooth taxes using debt, implying more volatile and less serially\ncorrelated fiscal policies, higher borrowing costs and lower levels of\nindebtedness. In order to exit temporary financial autarky following a default\nevent, the government has to repay a random fraction of the defaulted debt. We\nshow that the optimal fiscal and renegotiation policies have implications\naligned with the data.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1508.03924v2", "link": "http://arxiv.org/abs/1508.03924v2"}, {"title": "Mixed Tempered Stable distribution", "summary": "In this paper we introduce a new parametric distribution, the Mixed Tempered\nStable. It has the same structure of the Normal Variance Mean Mixtures but the\nnormality assumption leaves place to a semi-heavy tailed distribution. We show\nthat, by choosing appropriately the parameters of the distribution and under\nthe concrete specification of the mixing random variable, it is possible to\nobtain some well-known distributions as special cases.\n  We employ the Mixed Tempered Stable distribution which has many attractive\nfeatures for modeling univariate returns. Our results suggest that it is enough\nflexible to accomodate different density shapes. Furthermore, the analysis\napplied to statistical time series shows that our approach provides a better\nfit than competing distributions that are common in the practice of finance.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1405.7603v1", "link": "http://arxiv.org/abs/1405.7603v1"}, {"title": "A critique of the econometrics of happiness: Are we underestimating the\n  returns to education and income?", "summary": "A large \"happiness\", or life satisfaction, literature in economics makes use\nof Likert-like scales in assessing survey respondents' cognitive evaluations of\ntheir lives. These measures are being used to estimate economic benefits in\nevery empirical field of economics. Typically, analysis of these data have\nshown remarkably low direct returns of education for improving subjective\nwell-being. In addition, arguably, the inferred impact of material wealth and\nincome using this method is also unexpectedly low as compared with other,\nsocial factors, and as compared with economists' prior expectations which\nunderlie, in some sense, support for using GDP as a proxy for more general\nquality of life goals. Discrete response scales used ubiquitously for the\nreporting of life satisfaction pose cognitive challenges to survey respondents,\nso differing cognitive abilities result in different uses of the scale, and\nthus potential bias in statistical inference. This problem has so far gone\nunnoticed. An overlooked feature of the distribution of responses to life\nsatisfaction questions is that they exhibit certain enhancements at focal\nvalues, in particular at 0, 5, and 10 on the eleven-point scale. In this paper,\nI investigate the reasons for, and implications of, these response patterns. I\nuse a model to account for the focal-value behavior using a latent variable\napproach to capture the \"internal\" cognitive evaluation before it is translated\nto the discrete scale of a survey question. This approach, supported by other\nmore heuristic ones, finds a significant upward correction for the effects of\nboth education and income on life satisfaction.", "category": ["econ.EM", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.11835v1", "link": "http://arxiv.org/abs/1807.11835v1"}, {"title": "Transition from Pareto to Boltzmann-Gibbs behavior in a deterministic\n  economic model", "summary": "The one-dimensional deterministic economic model recently studied by\nGonzalez-Estevez et al. [Physica A 387, 4367 (2008)] is considered on a\ntwo-dimensional square lattice with periodic boundary conditions. In this\nmodel, the evolution of each agent is described by a map coupled with its\nnearest neighbors. The map has two factors: a linear term that accounts for the\nagent's own tendency to grow and an exponential term that saturates this growth\nthrough the control effect of the environment. The regions in the parameter\nspace where the system displays Pareto and Boltzmann-Gibbs statistics are\ncalculated for the cases of von Neumann and of Moore's neighborhoods. It is\nfound that, even when the parameters in the system are kept fixed, a transition\nfrom Pareto to Boltzmann-Gibbs behavior can occur when the number of neighbors\nof each agent increases.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0811.1064v1", "link": "http://dx.doi.org/10.1016/j.physa.2009.04.031"}, {"title": "Quadratic approximation of slow factor of volatility in a Multi-factor\n  Stochastic volatility Model", "summary": "In the present work, we propose a new multifactor stochastic volatility model\nin which slow factor of volatility is approximated by a parabolic arc. We\nretain ourselves to the perturbation technique to obtain approximate expression\nfor European option prices. We introduce the notion of modified Black-Scholes\nprice. We obtain a simplified expression for European option price which is\nperturbed around the modified Black-Scholes price and have also obtained the\nexpression of modified price in terms of Black-Scholes price.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1703.10825v1", "link": "http://arxiv.org/abs/1703.10825v1"}, {"title": "Local risk-minimization for Barndorff-Nielsen and Shephard models", "summary": "We obtain explicit representations of locally risk-minimizing strategies of\ncall and put options for the Barndorff-Nielsen and Shephard models, which are\nOrnstein--Uhlenbeck-type stochastic volatility models. Using Malliavin calculus\nfor Levy processes, Arai and Suzuki (2015) obtained a formula for locally\nrisk-minimizing strategies for Levy markets under many additional conditions.\nSupposing mild conditions, we make sure that the Barndorff-Nielsen and Shephard\nmodels satisfy all the conditions imposed in Arai and Suzuki (2015). Among\nothers, we investigate the Malliavin differentiability of the density of the\nminimal martingale measure. Moreover, some numerical experiments for locally\nrisk-minimizing strategies are introduced.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1503.08589v2", "link": "http://arxiv.org/abs/1503.08589v2"}, {"title": "Contest Architecture with Public Disclosures", "summary": "I study optimal disclosure policies in sequential contests. A contest\ndesigner chooses at which periods to publicly disclose the efforts of previous\ncontestants. I provide results for a wide range of possible objectives for the\ncontest designer. While different objectives involve different trade-offs, I\nshow that under many circumstances the optimal contest is one of the three\nbasic contest structures widely studied in the literature: simultaneous,\nfirst-mover, or sequential contest.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.11004v2", "link": "http://arxiv.org/abs/1905.11004v2"}, {"title": "Risk sharing for capital requirements with multidimensional security\n  markets", "summary": "We consider the risk sharing problem for capital requirements induced by\ncapital adequacy tests and security markets. The agents involved in the sharing\nprocedure may be heterogeneous in that they apply varying capital adequacy\ntests and have access to different security markets. We discuss conditions\nunder which there exists a representative agent. Thereafter, we study two\nframeworks of capital adequacy more closely, polyhedral constraints and\ndistribution based constraints. We prove existence of optimal risk allocations\nand equilibria within these frameworks and elaborate on their robustness.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1809.10015v1", "link": "http://arxiv.org/abs/1809.10015v1"}, {"title": "Equilibrium Returns with Transaction Costs", "summary": "We study how trading costs are reflected in equilibrium returns. To this end,\nwe develop a tractable continuous-time risk-sharing model, where heterogeneous\nmean-variance investors trade subject to a quadratic transaction cost. The\ncorresponding equilibrium is characterized as the unique solution of a system\nof coupled but linear forward-backward stochastic differential equations.\nExplicit solutions are obtained in a number of concrete settings. The\nsluggishness of the frictional portfolios makes the corresponding equilibrium\nreturns mean-reverting. Compared to the frictionless case, expected returns are\nhigher if the more risk-averse agents are net sellers or if the asset supply\nexpands over time.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1707.08464v4", "link": "http://arxiv.org/abs/1707.08464v4"}, {"title": "Lasso under Multi-way Clustering: Estimation and Post-selection\n  Inference", "summary": "This paper studies high-dimensional regression models with lasso when data is\nsampled under multi-way clustering. First, we establish convergence rates for\nthe lasso and post-lasso estimators. Second, we propose a novel inference\nmethod based on a post-double-selection procedure and show its asymptotic\nvalidity. Our procedure can be easily implemented with existing statistical\npackages. Simulation results demonstrate that the proposed procedure works well\nin finite sample. We illustrate the proposed method with a couple of empirical\napplications to development and growth economics.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.02107v3", "link": "http://arxiv.org/abs/1905.02107v3"}, {"title": "Application of the Kelly Criterion to Ornstein-Uhlenbeck Processes", "summary": "In this paper, we study the Kelly criterion in the continuous time framework\nbuilding on the work of E.O. Thorp and others. The existence of an optimal\nstrategy is proven in a general setting and the corresponding optimal wealth\nprocess is found. A simple formula is provided for calculating the optimal\nportfolio for a set of price processes satisfying some simple conditions.\nProperties of the optimal investment strategy for assets governed by multiple\nOrnstein-Uhlenbeck processes are studied. The paper ends with a short\ndiscussion of the implications of these ideas for financial markets.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/0903.2910v1", "link": "http://dx.doi.org/10.1007/978-3-642-02466-5_105"}, {"title": "Can an interdisciplinary field contribute to one of the parent\n  disciplines from which it emerged?", "summary": "In the light of contemporary discussions of inter and transdisciplinarity,\nthis paper approaches econophysics and sociophysics to seek a response to the\nquestion -- whether these interdisciplinary fields could contribute to physics\nand economics. Drawing upon the literature on history and philosophy of\nscience, the paper argues that the two way traffic between physics and\neconomics has a long history and this is likely to continue in the future.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1605.08354v1", "link": "http://dx.doi.org/10.1140/epjst/e2016-60115-x"}, {"title": "Diversity and its decomposition into variety, balance and disparity", "summary": "Diversity is a central concept in many fields. Despite its importance, there\nis no unified methodological framework to measure diversity and its three\ncomponents of variety, balance and disparity. Current approaches take into\naccount disparity of the types by considering their pairwise similarities.\nPairwise similarities between types do not adequately capture total disparity,\nsince they fail to take into account in which way pairs are similar. Hence,\npairwise similarities do not discriminate between similarity of types in terms\nof the same feature and similarity of types in terms of different features.\nThis paper presents an alternative approach which is based similarities of\nfeatures between types over the whole set. The proposed measure of diversity\nproperly takes into account the aspects of variety, balance and disparity, and\nwithout having to set an arbitrary weight for each aspect of diversity. Based\non this measure, the 'ABC decomposition' is introduced, which provides separate\nmeasures for the variety, balance and disparity, allowing them to enter\nanalysis separately. The method is illustrated by analyzing the industrial\ndiversity from 1850 to present while taking into account the overlap in\noccupations they employ. Finally, the framework is extended to take into\naccount disparity considering multiple features, providing a helpful tool in\nanalysis of high-dimensional data.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1902.09167v2", "link": "http://arxiv.org/abs/1902.09167v2"}, {"title": "Long Memory in Stock Trading", "summary": "Using a relationship between the moments of the probability distribution of\ntimes between the two consecutive trades (intertrade time distribution) and the\nmoments of the distribution of a daily number of trades we show, that the\nunderlying point process is essentially non-markovian. A detailed analysis of\nall trades in the EESR stock on the Moscow International Currency Exchange in\nthe period January 2003 - September 2003, including that of correlation between\nintertrade time intervals is presented. A power-law decay of the correlation\nprovides an additional evidence of the long-memory nature of the series of\ntimes of trades. A data set including all trades in Siemens, Commerzbank and\nKarstadt stocks traded on the Xetra electronic stock exchange of Deutsche\nBoerse in October 2002 is also considered.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0303222v2", "link": "http://arxiv.org/abs/cond-mat/0303222v2"}, {"title": "Portfolio Optimization under Local-Stochastic Volatility: Coefficient\n  Taylor Series Approximations & Implied Sharpe Ratio", "summary": "We study the finite horizon Merton portfolio optimization problem in a\ngeneral local-stochastic volatility setting. Using model coefficient expansion\ntechniques, we derive approximations for the both the value function and the\noptimal investment strategy. We also analyze the `implied Sharpe ratio' and\nderive a series approximation for this quantity. The zeroth-order approximation\nof the value function and optimal investment strategy correspond to those\nobtained by Merton (1969) when the risky asset follows a geometric Brownian\nmotion. The first-order correction of the value function can, for general\nutility functions, be expressed as a differential operator acting on the\nzeroth-order term. For power utility functions, higher order terms can also be\ncomputed as a differential operator acting on the zeroth-order term. We give a\nrigorous accuracy bound for the higher order approximations in this case in\npure stochastic volatility models. A number of examples are provided in order\nto demonstrate numerically the accuracy of our approximations.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1506.06180v1", "link": "http://arxiv.org/abs/1506.06180v1"}, {"title": "Corrigendum to \"Managerial Incentive Problems: A Dynamic Perspective\"", "summary": "This paper corrects some mathematical errors in Holmstr\\\"om (1999) and\nclarifies the assumptions that are sufficient for the results of Holmstr\\\"om\n(1999). The results remain qualitatively the same.", "category": [], "id": "http://arxiv.org/abs/1811.00455v2", "link": "http://arxiv.org/abs/1811.00455v2"}, {"title": "A Peek into the Unobservable: Hidden States and Bayesian Inference for\n  the Bitcoin and Ether Price Series", "summary": "Conventional financial models fail to explain the economic and monetary\nproperties of cryptocurrencies due to the latter's dual nature: their usage as\nfinancial assets on the one side and their tight connection to the underlying\nblockchain structure on the other. In an effort to examine both components via\na unified approach, we apply a recently developed Non-Homogeneous Hidden Markov\n(NHHM) model with an extended set of financial and blockchain specific\ncovariates on the Bitcoin (BTC) and Ether (ETH) price data. Based on the\nobservable series, the NHHM model offers a novel perspective on the underlying\nmicrostructure of the cryptocurrency market and provides insight on\nunobservable parameters such as the behavior of investors, traders and miners.\nThe algorithm identifies two alternating periods (hidden states) of inherently\ndifferent activity -- fundamental versus uninformed or noise traders -- in the\nBitcoin ecosystem and unveils differences in both the short/long run dynamics\nand in the financial characteristics of the two states, such as significant\nexplanatory variables, extreme events and varying series autocorrelation. In a\nsomewhat unexpected result, the Bitcoin and Ether markets are found to be\ninfluenced by markedly distinct indicators despite their perceived correlation.\nThe current approach backs earlier findings that cryptocurrencies are unlike\nany conventional financial asset and makes a first step towards understanding\ncryptocurrency markets via a more comprehensive lens.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1909.10957v1", "link": "http://arxiv.org/abs/1909.10957v1"}, {"title": "Lattice structure of the random stable set in many-to-many matching\n  market", "summary": "For a many-to-many matching market, we study the lattice structure of the set\nof random stable matchings. We define the least upper bound and the greatest\nlower bound for both sides of the matching market, and we prove that with these\nbinary operations the set of random stable matchings forms two dual lattices.", "category": [], "id": "http://arxiv.org/abs/2002.08156v4", "link": "http://arxiv.org/abs/2002.08156v4"}, {"title": "Uniform Inference for Characteristic Effects of Large Continuous-Time\n  Linear Models", "summary": "We consider continuous-time models with a large panel of moment conditions,\nwhere the structural parameter depends on a set of characteristics, whose\neffects are of interest. The leading example is the linear factor model in\nfinancial economics where factor betas depend on observed characteristics such\nas firm specific instruments and macroeconomic variables, and their effects\npick up long-run time-varying beta fluctuations. We specify the factor betas as\nthe sum of characteristic effects and an orthogonal idiosyncratic parameter\nthat captures high-frequency movements. It is often the case that researchers\ndo not know whether or not the latter exists, or its strengths, and thus the\ninference about the characteristic effects should be valid uniformly over a\nbroad class of data generating processes for idiosyncratic parameters. We\nconstruct our estimation and inference in a two-step continuous-time GMM\nframework. It is found that the limiting distribution of the estimated\ncharacteristic effects has a discontinuity when the variance of the\nidiosyncratic parameter is near the boundary (zero), which makes the usual\n\"plug-in\" method using the estimated asymptotic variance only valid pointwise\nand may produce either over- or under- coveraging probabilities. We show that\nthe uniformity can be achieved by cross-sectional bootstrap. Our procedure\nallows both known and estimated factors, and also features a bias correction\nfor the effect of estimating unknown factors.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1711.04392v2", "link": "http://arxiv.org/abs/1711.04392v2"}, {"title": "Power-Law Distributions in Circulating Money: Effect of Preferential\n  Behavior", "summary": "We introduce preferential behavior into the study on statistical mechanics of\nmoney circulation. The computer simulation results show that the preferential\nbehavior can lead to power laws on distributions over both holding time and\namount of money held by agents. However, some constraints are needed in\ngeneration mechanism to ensure the robustness of power-law distributions.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0507151v1", "link": "http://dx.doi.org/10.1142/S0217979204025981"}, {"title": "Network-based indicators of Bitcoin bubbles", "summary": "The functioning of the cryptocurrency Bitcoin relies on the open availability\nof the entire history of its transactions. This makes it a particularly\ninteresting socio-economic system to analyse from the point of view of network\nscience. Here we analyse the evolution of the network of Bitcoin transactions\nbetween users. We achieve this by using the complete transaction history from\nDecember 5th 2011 to December 23rd 2013. This period includes three bubbles\nexperienced by the Bitcoin price. In particular, we focus on the global and\nlocal structural properties of the user network and their variation in relation\nto the different period of price surge and decline. By analysing the temporal\nvariation of the heterogeneity of the connectivity patterns we gain insights on\nthe different mechanisms that take place during bubbles, and find that hubs\n(i.e., the most connected nodes) had a fundamental role in triggering the burst\nof the second bubble. Finally, we examine the local topological structures of\ninteractions between users, we discover that the relative frequency of triadic\ninteractions experiences a strong change before, during and after a bubble, and\nsuggest that the importance of the hubs grows during the bubble. These results\nprovide further evidence that the behaviour of the hubs during bubbles\nsignificantly increases the systemic risk of the Bitcoin network, and discuss\nthe implications on public policy interventions.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1805.04460v1", "link": "http://arxiv.org/abs/1805.04460v1"}, {"title": "Optimal multiple stopping with random waiting times", "summary": "In the standard models for optimal multiple stopping problems it is assumed\nthat between two exercises there is always a time period of deterministic\nlength $\\delta$, the so called refraction period. This prevents the optimal\nexercise times from bunching up together on top of the optimal stopping time\nfor the one-exercise case. In this article we generalize the standard model by\nconsidering random refraction times. We develop the theory and reduce the\nproblem to a sequence of ordinary stopping problems thus extending the results\nfor deterministic times. This requires an extension of the underlying\nfiltrations in general. Furthermore we consider the Markovian case and treat an\nexample explicitly.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1205.1966v1", "link": "http://dx.doi.org/10.1080/07474946.2013.803814"}, {"title": "A Finite Volume - Alternating Direction Implicit Approach for the\n  Calibration of Stochastic Local Volatility Models", "summary": "Calibration of stochastic local volatility (SLV) models to their underlying\nlocal volatility model is often performed by numerically solving a\ntwo-dimensional non-linear forward Kolmogorov equation. We propose a novel\nfinite volume (FV) discretization in the numerical solution of general 1D and\n2D forward Kolmogorov equations. The FV method does not require a\ntransformation of the PDE. This constitutes a main advantage in the calibration\nof SLV models as the pertinent PDE coefficients are often nonsmooth. Moreover,\nthe FV discretization has the crucial property that the total numerical mass is\nconserved. Applying the FV discretization in the calibration of SLV models\nyields a non-linear system of ODEs. Numerical time stepping is performed by the\nHundsdorfer-Verwer ADI scheme to increase the computational efficiency. The\nnon-linearity in the system of ODEs is handled by introducing an inner\niteration. Ample numerical experiments are presented that illustrate the\neffectiveness of the calibration procedure.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1611.02961v1", "link": "http://arxiv.org/abs/1611.02961v1"}, {"title": "Fractional trends in unobserved components models", "summary": "We develop a generalization of unobserved components models that allows for a\nwide range of long-run dynamics by modelling the permanent component as a\nfractionally integrated process. The model does not require stationarity and\ncan be cast in state space form. In a multivariate setup, fractional trends may\nyield a cointegrated system. We derive the Kalman filter estimator for the\ncommon fractionally integrated component and establish consistency and\nasymptotic (mixed) normality of the maximum likelihood estimator. We apply the\nmodel to extract a common long-run component of three US inflation measures,\nwhere we show that the $I(1)$ assumption is likely to be violated for the\ncommon trend.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2005.03988v1", "link": "http://arxiv.org/abs/2005.03988v1"}, {"title": "Recursive utility optimization with concave coefficients", "summary": "This paper concerns the recursive utility maximization problem. We assume\nthat the coefficients of the wealth equation and the recursive utility are\nconcave. Then some interesting and important cases with nonlinear and nonsmooth\ncoefficients satisfy our assumption. After given an equivalent backward\nformulation of our problem, we employ the Fenchel-Legendre transform and derive\nthe corresponding variational formulation. By the convex duality method, the\nprimal \"sup-inf\" problem is translated to a dual minimization problem and the\nsaddle point of our problem is derived. Finally, we obtain the optimal terminal\nwealth. To illustrate our results, three cases for investors with ambiguity\naversion are explicitly worked out under some special assumptions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1607.00721v1", "link": "http://arxiv.org/abs/1607.00721v1"}, {"title": "Critical slowing down associated with critical transition and risk of\n  collapse in cryptocurrency", "summary": "The year 2017 saw the rise and fall of the crypto-currency market, followed\nby high variability in the price of all crypto-currencies. In this work, we\nstudy the abrupt transition in crypto-currency residuals, which is associated\nwith the critical transition (the phenomenon of critical slowing down) or the\nstochastic transition phenomena. We find that, regardless of the specific\ncrypto-currency or rolling window size, the autocorrelation always fluctuates\naround a high value, while the standard deviation increases monotonically.\nTherefore, while the autocorrelation does not display signals of critical\nslowing down, the standard deviation can be used to anticipate critical or\nstochastic transitions. In particular, we have detected two sudden jumps in the\nstandard deviation, in the second quarter of 2017 and at the beginning of 2018,\nwhich could have served as early warning signals of two majors price collapses\nthat have happened in the following periods. We finally propose a mean-field\nphenomenological model for the price of crypto-currency to show how the use of\nthe standard deviation of the residuals is a better leading indicator of the\ncollapse in price than the time series' autocorrelation. Our findings represent\na first step towards a better diagnostic of the risk of critical transition in\nthe price and/or volume of crypto-currencies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1806.08386v2", "link": "http://arxiv.org/abs/1806.08386v2"}, {"title": "Structural Change Analysis of Active Cryptocurrency Market", "summary": "Structural Change Analysis of Active Cryptocurrency Market", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/1909.10679v1", "link": "http://arxiv.org/abs/1909.10679v1"}, {"title": "Model for Non-Gaussian Intraday Stock Returns", "summary": "Stock prices are known to exhibit non-Gaussian dynamics, and there is much\ninterest in understanding the origin of this behavior. Here, we present a model\nthat explains the shape and scaling of the distribution of intraday stock price\nfluctuations (called intraday returns) and verify the model using a large\ndatabase for several stocks traded on the London Stock Exchange. We provide\nevidence that the return distribution for these stocks is non-Gaussian and\nsimilar in shape, and that the distribution appears stable over intraday time\nscales. We explain these results by assuming the volatility of returns is\nconstant intraday, but varies over longer periods such that its inverse square\nfollows a gamma distribution. This produces returns that are Student\ndistributed for intraday time scales. The predicted results show excellent\nagreement with the data for all stocks in our study and over all regions of the\nreturn distribution.", "category": ["q-fin.ST", "q-fin.RM"], "id": "http://arxiv.org/abs/0906.3841v2", "link": "http://dx.doi.org/10.1103/PhysRevE.80.065102"}, {"title": "The value of power-related options under spectrally negative L\u00e9vy\n  processes", "summary": "We provide analytical tools for pricing power options with exotic features\n(capped or log payoffs, gap options ...) in the framework of exponential L\\'evy\nmodels driven by one-sided stable or tempered stable processes. Pricing\nformulas take the form of fast converging series of powers of the log-forward\nmoneyness and of the time-to-maturity; these series are obtained via a\nfactorized integral representation in the Mellin space evaluated by means of\nresidues in $\\mathbb{C}$ or $\\mathbb{C}^2$. Comparisons with numerical methods\nand efficiency tests are also discussed.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1910.07971v2", "link": "http://arxiv.org/abs/1910.07971v2"}, {"title": "The square-root impact law also holds for option markets", "summary": "Many independent studies on stocks and futures contracts have established\nthat market impact is proportional to the square-root of the executed volume.\nIs market impact quantitatively similar for option markets as well? In order to\nanswer this question, we have analyzed the impact of a large proprietary data\nset of option trades. We find that the square-root law indeed holds in that\ncase. This finding supports the argument for a universal underlying mechanism.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1602.03043v1", "link": "http://arxiv.org/abs/1602.03043v1"}, {"title": "Bid-Ask Dynamic Pricing in Financial Markets with Transaction Costs and\n  Liquidity Risk", "summary": "We introduce, in continuous time, an axiomatic approach to assign to any\nfinancial position a dynamic ask (resp. bid) price process. Taking into account\nboth transaction costs and liquidity risk this leads to the convexity (resp.\nconcavity) of the ask (resp. bid) price. Time consistency is a crucial property\nfor dynamic pricing. Generalizing the result of Jouini and Kallal, we prove\nthat the No Free Lunch condition for a time consistent dynamic pricing\nprocedure (TCPP) is equivalent to the existence of an equivalent probability\nmeasure $R$ that transforms a process between the bid process and the ask\nprocess of any financial instrument into a martingale. Furthermore we prove\nthat the ask price process associated with any financial instrument is then a\n$R$-supermartingale process which has a cadlag modification. Finally we show\nthat time consistent dynamic pricing allows both to extend the dynamics of some\nreference assets and to be consistent with any observed bid ask spreads that\none wants to take into account. It then provides new bounds reducing the bid\nask spreads for the other financial instruments.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0703074v1", "link": "http://arxiv.org/abs/math/0703074v1"}, {"title": "Exponential-growth prediction bias and compliance with safety measures\n  in the times of COVID-19", "summary": "We conduct a unique, Amazon MTurk-based global experiment to investigate the\nimportance of an exponential-growth prediction bias (EGPB) in understanding why\nthe COVID-19 outbreak has exploded. The scientific basis for our inquiry is the\nwell-established fact that disease spread, especially in the initial stages,\nfollows an exponential function meaning few positive cases can explode into a\nwidespread pandemic if the disease is sufficiently transmittable. We define\nprediction bias as the systematic error arising from faulty prediction of the\nnumber of cases x-weeks hence when presented with y-weeks of prior, actual data\non the same. Our design permits us to identify the root of this\nunder-prediction as an EGPB arising from the general tendency to underestimate\nthe speed at which exponential processes unfold. Our data reveals that the\n\"degree of convexity\" reflected in the predicted path of the disease is\nsignificantly and substantially lower than the actual path. The bias is\nsignificantly higher for respondents from countries at a later stage relative\nto those at an early stage of disease progression. We find that individuals who\nexhibit EGPB are also more likely to reveal markedly reduced compliance with\nthe WHO-recommended safety measures, find general violations of safety\nprotocols less alarming, and show greater faith in their government's actions.\nA simple behavioral nudge which shows prior data in terms of raw numbers, as\nopposed to a graph, causally reduces EGPB. Clear communication of risk via raw\nnumbers could increase accuracy of risk perception, in turn facilitating\ncompliance with suggested protective behaviors.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.01273v1", "link": "http://arxiv.org/abs/2005.01273v1"}, {"title": "A new set of cluster driven composite development indicators", "summary": "Composite development indicators used in policy making often subjectively\naggregate a restricted set of indicators. We show, using dimensionality\nreduction techniques, including Principal Component Analysis (PCA) and for the\nfirst time information filtering and hierarchical clustering, that these\ncomposite indicators miss key information on the relationship between different\nindicators. In particular, the grouping of indicators via topics is not\nreflected in the data at a global and local level. We overcome these issues by\nusing the clustering of indicators to build a new set of cluster driven\ncomposite development indicators that are objective, data driven, comparable\nbetween countries, and retain interpretabilty. We discuss their consequences on\ninforming policy makers about country development, comparing them with the top\nPageRank indicators as a benchmark. Finally, we demonstrate that our new set of\ncomposite development indicators outperforms the benchmark on a dataset\nreconstruction task.", "category": ["econ.GN", "q-fin.EC", "q-fin.ST"], "id": "http://arxiv.org/abs/1911.11226v3", "link": "http://arxiv.org/abs/1911.11226v3"}, {"title": "The Impact of Services on Economic Complexity: Service Sophistication as\n  Route for Economic Growth", "summary": "Economic complexity reflects the amount of knowledge that is embedded in the\nproductive structure of an economy. By combining tools from network science and\neconometrics, a robust and stable relationship between a country's productive\nstructure and its economic growth has been established. Here we report that not\nonly goods but also services are important for predicting the rate at which\ncountries will grow. By adopting a terminology which classifies manufactured\ngoods and delivered services as products, we investigate the influence of\nservices on the country's productive structure. In particular, we provide\nevidence that complexity indices for services are in general higher than those\nfor goods, which is reflected in a general tendency to rank countries with\ndeveloped service sector higher than countries with economy centred on\nmanufacturing of goods. By focusing on country dynamics based on experimental\ndata, we investigate the impact of services on the economic complexity of\ncountries measured in the product space (consisting of both goods and\nservices). Importantly, we show that diversification of service exports and its\nsophistication can provide an additional route for economic growth in both\ndeveloping and developed countries.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1604.06284v2", "link": "http://dx.doi.org/10.1371/journal.pone.0161633"}, {"title": "Housing Investment, Stock Market Participation and Household Portfolio\n  choice: Evidence from China's Urban Areas", "summary": "This paper employs the survey data of CHFS (2013) to investigate the impact\nof housing investment on household stock market participation and portfolio\nchoice. The results show that larger housing investment encourages the\nhousehold participation in the stock market, but reduces the proportion of\ntheir stockholding. The above conclusion remains true even when the endogeneity\nproblem is controlled with risk attitude classification, Heckman model test and\nsubsample regression. This study shows that the growth in the housing market\nwill not lead to stock market development because of lack of household\nfinancial literacy and the low expected yield on stock market.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.01641v1", "link": "http://arxiv.org/abs/2001.01641v1"}, {"title": "Tick Size Reduction and Price Clustering in a FX Order Book", "summary": "We investigate the statistical properties of the EBS order book for the\nEUR/USD and USD/JPY currency pairs and the impact of a ten-fold tick size\nreduction on its dynamics. A large fraction of limit orders are still placed\nright at or halfway between the old allowed prices. This generates price\nbarriers where the best quotes lie for much of the time, which causes the\nemergence of distinct peaks in the average shape of the book at round\ndistances. Furthermore, we argue that this clustering is mainly due to manual\ntraders who remained set to the old price resolution. Automatic traders easily\ntake price priority by submitting limit orders one tick ahead of clusters, as\nshown by the prominence of buy (sell) limit orders posted with rightmost digit\none (nine).", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1307.5440v3", "link": "http://dx.doi.org/10.1016/j.physa.2014.09.016"}, {"title": "Censored Quantile Instrumental Variable Estimation with Stata", "summary": "Many applications involve a censored dependent variable and an endogenous\nindependent variable. Chernozhukov et al. (2015) introduced a censored quantile\ninstrumental variable estimator (CQIV) for use in those applications, which has\nbeen applied by Kowalski (2016), among others. In this article, we introduce a\nStata command, cqiv, that simplifes application of the CQIV estimator in Stata.\nWe summarize the CQIV estimator and algorithm, we describe the use of the cqiv\ncommand, and we provide empirical examples.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.05305v3", "link": "http://arxiv.org/abs/1801.05305v3"}, {"title": "High-dimensional macroeconomic forecasting using message passing\n  algorithms", "summary": "This paper proposes two distinct contributions to econometric analysis of\nlarge information sets and structural instabilities. First, it treats a\nregression model with time-varying coefficients, stochastic volatility and\nexogenous predictors, as an equivalent high-dimensional static regression\nproblem with thousands of covariates. Inference in this specification proceeds\nusing Bayesian hierarchical priors that shrink the high-dimensional vector of\ncoefficients either towards zero or time-invariance. Second, it introduces the\nframeworks of factor graphs and message passing as a means of designing\nefficient Bayesian estimation algorithms. In particular, a Generalized\nApproximate Message Passing (GAMP) algorithm is derived that has low\nalgorithmic complexity and is trivially parallelizable. The result is a\ncomprehensive methodology that can be used to estimate time-varying parameter\nregressions with arbitrarily large number of exogenous predictors. In a\nforecasting exercise for U.S. price inflation this methodology is shown to work\nvery well.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/2004.11485v1", "link": "http://dx.doi.org/10.1080/07350015.2019.1677472"}, {"title": "Forecasting Financial Extremes: A Network Degree Measure of\n  Super-exponential Growth", "summary": "Investors in stock market are usually greedy during bull markets and scared\nduring bear markets. The greed or fear spreads across investors quickly. This\nis known as the herding effect, and often leads to a fast movement of stock\nprices. During such market regimes, stock prices change at a super-exponential\nrate and are normally followed by a trend reversal that corrects the previous\nover reaction. In this paper, we construct an indicator to measure the\nmagnitude of the super-exponential growth of stock prices, by measuring the\ndegree of the price network, generated from the price time series. Twelve major\ninternational stock indices have been investigated. Error diagram tests show\nthat this new indicator has strong predictive power for financial extremes,\nboth peaks and troughs. By varying the parameters used to construct the error\ndiagram, we show the predictive power is very robust. The new indicator has a\nbetter performance than the LPPL pattern recognition indicator.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1505.04060v1", "link": "http://dx.doi.org/10.1371/journal.pone.0128908"}, {"title": "Maximum Likelihood Estimation in Possibly Misspecified Dynamic Models\n  with Time-Inhomogeneous Markov Regimes", "summary": "This paper considers maximum likelihood (ML) estimation in a large class of\nmodels with hidden Markov regimes. We investigate consistency and local\nasymptotic normality of the ML estimator under general conditions which allow\nfor autoregressive dynamics in the observable process, time-inhomogeneous\nMarkov regime sequences, and possible model misspecification. A Monte Carlo\nstudy examines the finite-sample properties of the ML estimator. An empirical\napplication is also discussed.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1612.04932v2", "link": "http://arxiv.org/abs/1612.04932v2"}, {"title": "Dynamic tail inference with log-Laplace volatility", "summary": "We propose a family of models that enable predictive estimation of\ntime-varying extreme event probabilities in heavy-tailed and nonlinearly\ndependent time series. The models are a white noise process with conditionally\nlog-Laplace stochastic volatility. In contrast to other, similar stochastic\nvolatility formalisms, this process has analytic expressions for its\nconditional probabilistic structure that enable straightforward estimation of\ndynamically changing extreme event probabilities. The process and volatility\nare conditionally Pareto-tailed, with tail exponent given by the reciprocal of\nthe log-volatility's mean absolute innovation. This formalism can accommodate a\nwide variety of nonlinear dependence, as well as conditional power law-tail\nbehavior ranging from weakly non-Gaussian to Cauchy-like tails. We provide a\ncomputationally straightforward estimation procedure that uses an asymptotic\napproximation of the process' dynamic large deviation probabilities. We\ndemonstrate the estimator's utility with a simulation study. We then show the\nmethod's predictive capabilities on a simulated nonlinear time series where the\nvolatility is driven by the chaotic Lorenz system. Lastly we provide an\nempirical application, which shows that this simple modeling method can be\neffectively used for dynamic and predictive tail inference in financial time\nseries.", "category": ["econ.EM", "q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1901.02419v5", "link": "http://arxiv.org/abs/1901.02419v5"}, {"title": "Turing's Children: Representation of Sexual Minorities in STEM", "summary": "We provide the first nationally representative estimates of sexual minority\nrepresentation in STEM fields by studying 142,641 men and women in same-sex\ncouples from the 2009-2018 American Community Surveys. These data indicate that\nmen in same-sex couples are 12 percentage points less likely to have completed\na bachelor's degree in a STEM field compared to men in different-sex couples;\nthere is no gap observed for women in same-sex couples compared to women in\ndifferent-sex couples. The STEM gap between men in same-sex and different-sex\ncouples is larger than the STEM gap between white and black men but is smaller\nthan the gender STEM gap. We also document a gap in STEM occupations between\nmen in same-sex and different-sex couples, and we replicate this finding using\nindependently drawn data from the 2013-2018 National Health Interview Surveys.\nThese differences persist after controlling for demographic characteristics,\nlocation, and fertility. Our findings further the call for interventions\ndesigned at increasing representation of sexual minorities in STEM.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.06664v1", "link": "http://arxiv.org/abs/2005.06664v1"}, {"title": "Minding impacting events in a model of stochastic variance", "summary": "We introduce a generalisation of the well-known ARCH process, widely used for\ngenerating uncorrelated stochastic time series with long-term non-Gaussian\ndistributions and long-lasting correlations in the (instantaneous) standard\ndeviation exhibiting a clustering profile. Specifically, inspired by the fact\nthat in a variety of systems impacting events are hardly forgot, we split the\nprocess into two different regimes: a first one for regular periods where the\naverage volatility of the fluctuations within a certain period of time is below\na certain threshold and another one when the local standard deviation\noutnumbers it. In the former situation we use standard rules for\nheteroscedastic processes whereas in the latter case the system starts\nrecalling past values that surpassed the threshold. Our results show that for\nappropriate parameter values the model is able to provide fat tailed\nprobability density functions and strong persistence of the instantaneous\nvariance characterised by large values of the Hurst exponent is greater than\n0.8, which are ubiquitous features in complex systems.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1102.4819v2", "link": "http://dx.doi.org/10.1371/journal.pone.0018149"}, {"title": "Effects of time dependency and efficiency on information flow in\n  financial markets", "summary": "We investigated financial market data to determine which factors affect\ninformation flow between stocks. Two factors, the time dependency and the\ndegree of efficiency, were considered in the analysis of Korean, the Japanese,\nthe Taiwanese, the Canadian, and US market data. We found that the frequency of\nthe significant information decreases as the time interval increases. However,\nno significant information flow was observed in the time series from which the\ntemporal time correlation was removed. These results indicated that the\ninformation flow between stocks evidences time-dependency properties.\nFurthermore, we discovered that the difference in the degree of efficiency\nperforms a crucial function in determining the direction of the significant\ninformation flow.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0802.1500v1", "link": "http://dx.doi.org/10.1016/j.physa.2008.05.054"}, {"title": "Option Pricing and Hedging for Discrete Time Autoregressive Hidden\n  Markov Model", "summary": "In this paper we solve the discrete time mean-variance hedging problem when\nasset returns follow a multivariate autoregressive hidden Markov model. Time\ndependent volatility and serial dependence are well established properties of\nfinancial time series and our model covers both. To illustrate the relevance of\nour proposed methodology, we first compare the proposed model with the\nwell-known hidden Markov model via likelihood ratio tests and a novel\ngoodness-of-fit test on the S\\&P 500 daily returns. Secondly, we present\nout-of-sample hedging results on S\\&P 500 vanilla options as well as a trading\nstrategy based on theoretical prices, which we compare to simpler models\nincluding the classical Black-Scholes delta-hedging approach.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1707.02019v1", "link": "http://arxiv.org/abs/1707.02019v1"}, {"title": "Investigating Limit Order Book Characteristics for Short Term Price\n  Prediction: a Machine Learning Approach", "summary": "With the proliferation of algorithmic high-frequency trading in financial\nmarkets, the Limit Order Book has generated increased research interest.\nResearch is still at an early stage and there is much we do not understand\nabout the dynamics of Limit Order Books. In this paper, we employ a machine\nlearning approach to investigate Limit Order Book features and their potential\nto predict short term price movements. This is an initial broad-based\ninvestigation that results in some novel observations about LOB dynamics and\nidentifies several promising directions for further research. Furthermore, we\nobtain prediction results that are significantly superior to a baseline\npredictor.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1901.10534v1", "link": "http://arxiv.org/abs/1901.10534v1"}, {"title": "Evaluating Effects of Tuition Fees: Lasso for the Case of Germany", "summary": "We study the effect of the introduction of university tuition fees on the\nenrollment behavior of students in Germany. For this, an appropriate\nLasso-technique is crucial in order to identify the magnitude and significance\nof the effect due to potentially many relevant controlling factors and only a\nshort time frame where fees existed. We show that a post-double selection\nstrategy combined with stability selection determines a significant negative\nimpact of fees on student enrollment and identifies relevant variables. This is\nin contrast to previous empirical studies and a plain linear panel regression\nwhich cannot detect any effect of tuition fees in this case. In our study, we\nexplicitly deal with data challenges in the response variable in a transparent\nway and provide respective robust results. Moreover, we control for spatial\ncross-effects capturing the heterogeneity in the introduction scheme of fees\nacross federal states (\"Bundesl\\\"ander\"), which can set their own educational\npolicy. We also confirm the validity of our Lasso approach in a comprehensive\nsimulation study.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.08299v1", "link": "http://arxiv.org/abs/1909.08299v1"}, {"title": "At What Frequency Should the Kelly Bettor Bet?", "summary": "We study the problem of optimizing the betting frequency in a dynamic game\nsetting using Kelly's celebrated expected logarithmic growth criterion as the\nperformance metric. The game is defined by a sequence of bets with independent\nand identically distributed returns X(k). The bettor selects the fraction of\nwealth K wagered at k = 0 and waits n steps before updating the bet size.\nBetween updates, the proceeds from the previous bets remain at risk in the\nspirit of \"buy and hold.\" Within this context, the main questions we consider\nare as follows: How does the optimal performance, we call it gn*, change with\nn? Does the high-frequency case, n = 1, always lead to the best performance?\nWhat are the effects of accrued interest and transaction costs? First, we\nprovide rather complete answers to these questions for the important special\ncase when X(k) in {-1,1} is a Bernoulli random variable with probability p that\nX(k) = 1. This serves as an entry point for future research using a binomial\nlattice model for stock trading. The latter sections focus on more general\nprobability distributions for X(k) and two conjectures. The first conjecture is\nsimple to state: Absent transaction costs, gn* is non-increasing in n. The\nsecond conjecture involves the technical condition which we call the sufficient\nattractiveness inequality. We first prove that satisfaction of this inequality\nis sufficient to guarantee that the low-frequency bettor using large n can\nmatch the performance of the high-frequency bettor using n = 1. Subsequently,\nwe conjecture, and provide supporting evidence that this condition is also\nnecessary.", "category": ["q-fin.CP", "q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1801.06737v2", "link": "http://dx.doi.org/10.23919/ACC.2018.8431224"}, {"title": "Towards the Exact Simulation Using Hyperbolic Brownian Motion", "summary": "In the present paper, an expansion of the transition density of Hyperbolic\nBrownian motion with drift is given, which is potentially useful for pricing\nand hedging of options under stochastic volatility models. We work on a\ncondition on the drift which dramatically simplifies the proof.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1705.00864v1", "link": "http://arxiv.org/abs/1705.00864v1"}, {"title": "Theory of Zipf's Law and of General Power Law Distributions with\n  Gibrat's law of Proportional Growth", "summary": "We summarize a book under publication with his title written by the three\npresent authors, on the theory of Zipf's law, and more generally of power laws,\ndriven by the mechanism of proportional growth. The preprint is available upon\nrequest from the authors.\n  For clarity, consistence of language and conciseness, we discuss the origin\nand conditions of the validity of Zipf's law using the terminology of firms'\nasset values. We use firms at the entities whose size distributions are to be\nexplained. It should be noted, however, that most of the relations discussed in\nthis book, especially the intimate connection between Zipf's and Gilbrat's\nlaws, underlie Zipf's law in diverse scientific areas. The same models and\nvariations thereof can be straightforwardly applied to any of the other domains\nof application.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0808.1828v1", "link": "http://arxiv.org/abs/0808.1828v1"}, {"title": "Pivotal estimation in high-dimensional regression via linear programming", "summary": "We propose a new method of estimation in high-dimensional linear regression\nmodel. It allows for very weak distributional assumptions including\nheteroscedasticity, and does not require the knowledge of the variance of\nrandom errors. The method is based on linear programming only, so that its\nnumerical implementation is faster than for previously known techniques using\nconic programs, and it allows one to deal with higher dimensional models. We\nprovide upper bounds for estimation and prediction errors of the proposed\nestimator showing that it achieves the same rate as in the more restrictive\nsituation of fixed design and i.i.d. Gaussian errors with known variance.\nFollowing Gautier and Tsybakov (2011), we obtain the results under weaker\nsensitivity assumptions than the restricted eigenvalue or assimilated\nconditions.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1303.7092v2", "link": "http://arxiv.org/abs/1303.7092v2"}, {"title": "The Impact of Heterogeneous Trading Rules on the Limit Order Book and\n  Order Flows", "summary": "In this paper we develop a model of an order-driven market where traders set\nbids and asks and post market or limit orders according to exogenously fixed\nrules. Agents are assumed to have three components to the expectation of future\nasset returns, namely-fundamentalist, chartist and noise trader. Furthermore\nagents differ in the characteristics describing these components, such as time\nhorizon, risk aversion and the weights given to the various components. The\nmodel developed here extends a great deal of earlier literature in that the\norder submissions of agents are determined by utility maximisation, rather than\nthe mechanical unit order size that is commonly assumed. In this way the order\nflow is better related to the ongoing evolution of the market. For the given\nmarket structure we analyze the impact of the three components of the trading\nstrategies on the statistical properties of prices and order flows and observe\nthat it is the chartist strategy that is mainly responsible of the fat tails\nand clustering in the artificial price data generated by the model. The paper\nprovides further evidence that large price changes are likely to be generated\nby the presence of large gaps in the book.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0711.3581v1", "link": "http://dx.doi.org/10.1016/j.jedc.2008.08.001"}, {"title": "Asset Price Distributions and Efficient Markets", "summary": "We explore a decomposition in which returns on a large class of portfolios\nrelative to the market depend on a smooth non-negative drift and changes in the\nasset price distribution. This decomposition is obtained using general\ncontinuous semimartingale price representations, and is thus consistent with\nvirtually any asset pricing model. Fluctuations in portfolio relative returns\ndepend on stochastic time-varying dispersion in asset prices. Thus, our\nframework uncovers an asset pricing factor whose existence emerges from an\naccounting identity universal across different economic and financial\nenvironments, a fact that has deep implications for market efficiency. In\nparticular, in a closed, dividend-free market in which asset price dispersion\nis relatively constant, a large class of portfolios must necessarily outperform\nthe market portfolio over time. We show that price dispersion in commodity\nfutures markets has increased only slightly, and confirm the existence of\nsubstantial excess returns that co-vary with changes in price dispersion as\npredicted by our theory.", "category": ["q-fin.PM", "q-fin.GN"], "id": "http://arxiv.org/abs/1810.12840v1", "link": "http://arxiv.org/abs/1810.12840v1"}, {"title": "Quantum Bounds for Option Prices", "summary": "Option pricing is the most elemental challenge of mathematical finance.\nKnowledge of the prices of options at every strike is equivalent to knowing the\nentire pricing distribution for a security, as derivatives contingent on the\nsecurity can be replicated using options. The available data may be\ninsufficient to determine this distribution precisely, however, and the\nquestion arises: What are the bounds for the option price at a specified\nstrike, given the market-implied constraints?\n  Positivity of the price map imposed by the principle of no-arbitrage is here\nutilised, via the Gelfand-Naimark-Segal construction, to transform the problem\ninto the domain of operator algebras. Optimisation in this larger context is\nessentially geometric, and the outcome is simultaneously super-optimal for all\ncommutative subalgebras.\n  This generates an upper bound for the price of a basket option. With\ninnovative decomposition of the assets in the basket, the result is used to\ncreate converging families of price bounds for vanilla options, interpolate the\nvolatility smile, price options on cross FX rates, and analyse the\nrelationships between swaption and caplet prices.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1712.01385v5", "link": "http://arxiv.org/abs/1712.01385v5"}, {"title": "Mutual Information Rate-Based Networks in Financial Markets", "summary": "In the last years efforts in econophysics have been shifted to study how\nnetwork theory can facilitate understanding of complex financial markets. Main\npart of these efforts is the study of correlation-based hierarchical networks.\nThis is somewhat surprising as the underlying assumptions of research looking\nat financial markets is that they behave chaotically. In fact it's common for\neconophysicists to estimate maximal Lyapunov exponent for log returns of a\ngiven financial asset to confirm that prices behave chaotically. Chaotic\nbehaviour is only displayed by dynamical systems which are either non-linear or\ninfinite-dimensional. Therefore it seems that non-linearity is an important\npart of financial markets, which is proved by numerous studies confirming\nfinancial markets display significant non-linear behaviour, yet network theory\nis used to study them using almost exclusively correlations and partial\ncorrelations, which are inherently dealing with linear dependencies only. In\nthis paper we introduce a way to incorporate non-linear dynamics and\ndependencies into hierarchical networks to study financial markets using mutual\ninformation and its dynamical extension: the mutual information rate. We\nestimate it using multidimensional Lempel-Ziv complexity and then convert it\ninto an Euclidean metric in order to find appropriate topological structure of\nnetworks modelling financial markets. We show that this approach leads to\ndifferent results than correlation-based approach used in most studies, on the\nbasis of 15 biggest companies listed on Warsaw Stock Exchange in the period of\n2009-2012 and 91 companies listed on NYSE100 between 2003 and 2013, using\nminimal spanning trees and planar maximally filtered graphs.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1401.2548v1", "link": "http://dx.doi.org/10.1103/PhysRevE.89.052801"}, {"title": "The Poker-Litigation Game", "summary": "Is litigation a serious search for truth or simply a game of skill or luck?\nAlthough the process of litigation has been modeled as a Prisoner's Dilemma, as\na War of Attrition, as a Game of Chicken and even as a simple coin toss, no one\nhas formally modeled litigation as a game of poker. This paper is the first to\ndo so. We present a simple \"poker-litigation game\" and find the optimal\nstrategy for playing this game.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1509.01214v1", "link": "http://arxiv.org/abs/1509.01214v1"}, {"title": "The Verdoorn Law in the Portuguese Regions: A Panel Data Analysis", "summary": "This work aims to test the Verdoorn Law, with the alternative specifications\nof (1)Kaldor (1966), for five regions (NUTS II) Portuguese from 1986 to 1994\nand for the 28 NUTS III Portuguese in the period 1995 to 1999. Will, therefore,\nto analyze the existence of increasing returns to scale that characterize the\nphenomena of polarization with circular and cumulative causes and can explain\nthe processes of regional divergence. It is intended to test, even in this\nwork, the alternative interpretation of (2)Rowthorn (1975) Verdoorn's Law for\nthe same regions and periods. The results of this work will be complemented\nwith estimates of these relationships to other sectors of the economy than the\nindustry (primary and services sector), for each of the manufacturing\nindustries operating in the Portuguese regions and for the total economy of\neach region (3)(Martinho, 2011).", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1110.5544v1", "link": "http://arxiv.org/abs/1110.5544v1"}, {"title": "A Unified Framework for Efficient Estimation of General Treatment Models", "summary": "This paper presents a weighted optimization framework that unifies the\nbinary,multi-valued, continuous, as well as mixture of discrete and continuous\ntreatment, under the unconfounded treatment assignment. With a general loss\nfunction, the framework includes the average, quantile and asymmetric least\nsquares causal effect of treatment as special cases. For this general\nframework, we first derive the semiparametric efficiency bound for the causal\neffect of treatment, extending the existing bound results to a wider class of\nmodels. We then propose a generalized optimization estimation for the causal\neffect with weights estimated by solving an expanding set of equations. Under\nsome sufficient conditions, we establish consistency and asymptotic normality\nof the proposed estimator of the causal effect and show that the estimator\nattains our semiparametric efficiency bound, thereby extending the existing\nliterature on efficient estimation of causal effect to a wider class of\napplications. Finally, we discuss etimation of some causal effect functionals\nsuch as the treatment effect curve and the average outcome. To evaluate the\nfinite sample performance of the proposed procedure, we conduct a small scale\nsimulation study and find that the proposed estimation has practical value. To\nillustrate the applicability of the procedure, we revisit the literature on\ncampaign advertise and campaign contributions. Unlike the existing procedures\nwhich produce mixed results, we find no evidence of campaign advertise on\ncampaign contribution.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.04936v2", "link": "http://arxiv.org/abs/1808.04936v2"}, {"title": "The US 2000-2003 Market Descent: Clarifications", "summary": "In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:\nC6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and\nhas questioned several of our results published in [Sornette D and Zhou W-X\n2002 The US 2000-2002 market descent: how much longer and deeper? Quant.\nFinance 2: 468-81, cond-mat/0209065] and in our two consequent preprints\n[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the\nissues on (i) the analogy between rupture and crash, (ii) the Landau expansion,\n``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between\nbubbles and anti-bubbles and universality, (iv) the condition of criticality,\n(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,\n(vii) the fractal log-periodic power law patterns, (viii) the similarity\nbetween the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the\npresent status of our prediction.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0305004v1", "link": "http://arxiv.org/abs/cond-mat/0305004v1"}, {"title": "Seasonal Stochastic Volatility and Correlation together with the\n  Samuelson Effect in Commodity Futures Markets", "summary": "We introduce a multi-factor stochastic volatility model based on the\nCIR/Heston volatility process that incorporates seasonality and the Samuelson\neffect. First, we give conditions on the seasonal term under which the\ncorresponding volatility factor is well-defined. These conditions appear to be\nrather mild. Second, we calculate the joint characteristic function of two\nfutures prices for different maturities in the proposed model. This\ncharacteristic function is analytic. Finally, we provide numerical\nillustrations in terms of implied volatility and correlation produced by the\nproposed model with five different specifications of the seasonality pattern.\nThe model is found to be able to produce volatility smiles at the same time as\na volatility term-structure that exhibits the Samuelson effect with a seasonal\ncomponent. Correlation, instantaneous or implied from calendar spread option\nprices via a Gaussian copula, is also found to be seasonal.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1506.05911v1", "link": "http://arxiv.org/abs/1506.05911v1"}, {"title": "`To Have What They are Having': Portfolio Choice for Mimicking\n  Mean-Variance Savers", "summary": "We consider a group of mean-variance investors with mimicking desire such\nthat each investor is willing to penalize deviations of his portfolio\ncomposition from compositions of other group members. Penalizing norm\nconstraints are already applied for statistical improvement of Markowitz\nportfolio procedure in order to cope with estimation risk. We relate these\npenalties to individuals' wish of social learning and introduce a mutual fund\n(investment club) aggregating group member preferences unknown for individual\nsavers. We derive the explicit analytical solution for the fund's optimal\nportfolio weights and show advantages to invest in such a fund for individuals\nwilling to mimic.", "category": ["q-fin.PM", "q-fin.MF"], "id": "http://arxiv.org/abs/1611.01524v1", "link": "http://arxiv.org/abs/1611.01524v1"}, {"title": "Interest-Rate Modelling in Collateralized Markets: Multiple curves,\n  credit-liquidity effects, CCPs", "summary": "The market practice of extrapolating different term structures from different\ninstruments lacks a rigorous justification in terms of cash flows structure and\nmarket observables. In this paper, we integrate our previous consistent theory\nfor pricing under credit, collateral and funding risks into term structure\nmodelling, integrating the origination of different term structures with such\neffects. Under a number of assumptions on collateralization, wrong-way risk,\ngap risk, credit valuation adjustments and funding effects, including the\ntreasury operational model, and via an immersion hypothesis, we are able to\nderive a synthetic master equation for the multiple term structure dynamics\nthat integrates multiple curves with credit/funding adjustments.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1304.1397v1", "link": "http://arxiv.org/abs/1304.1397v1"}, {"title": "Navigating the Cryptocurrency Landscape: An Islamic Perspective", "summary": "Almost a decade on from the launch of Bitcoin, cryptocurrencies continue to\ngenerate headlines and intense debate. What started as an underground\nexperiment by a rag tag group of programmers armed with a Libertarian manifesto\nhas now resulted in a thriving $230 billion ecosystem, with constant on-going\ninnovation. Scholars and researchers alike are realizing that cryptocurrencies\nare far more than mere technical innovation; they represent a distinct and\nrevolutionary new economic paradigm tending towards decentralization.\nUnfortunately, this bold new universe is little explored from the perspective\nof Islamic economics and finance. Our work aims to address these deficiencies.\nOur paper makes the following distinct contributions We significantly expand\nthe discussion on whether cryptocurrencies qualify as \"money\" from an Islamic\nperspective and we argue that this debate necessitates rethinking certain\nfundamental definitions. We conclude that the cryptocurrency phenomenon, with\nits radical new capabilities, may hold considerable opportunity which merits\ndeeper investigation.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.05935v1", "link": "http://arxiv.org/abs/1811.05935v1"}, {"title": "Information processing constraints in travel behaviour modelling: A\n  generative learning approach", "summary": "Travel decisions tend to exhibit sensitivity to uncertainty and information\nprocessing constraints. These behavioural conditions can be characterized by a\ngenerative learning process. We propose a data-driven generative model version\nof rational inattention theory to emulate these behavioural representations. We\noutline the methodology of the generative model and the associated learning\nprocess as well as provide an intuitive explanation of how this process\ncaptures the value of prior information in the choice utility specification. We\ndemonstrate the effects of information heterogeneity on a travel choice,\nanalyze the econometric interpretation, and explore the properties of our\ngenerative model. Our findings indicate a strong correlation with rational\ninattention behaviour theory, which suggest that individuals may ignore certain\nexogenous variables and rely on prior information for evaluating decisions\nunder uncertainty. Finally, the principles demonstrated in this study can be\nformulated as a generalized entropy and utility based multinomial logit model.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.07036v2", "link": "http://arxiv.org/abs/1907.07036v2"}, {"title": "The pricing formula for cancellable European options", "summary": "This paper examines the value of a cancellable European option in a finite\ntime horizon setting. The specifications of this generalized European option\nallow the seller to cancel the option at any point in time for a fixed penalty\npaid directly to the holder. Here, we provide an explicit valuation formula for\nthe European game call where the early cancellation time is obtained\niteratively.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1304.5962v3", "link": "http://arxiv.org/abs/1304.5962v3"}, {"title": "Deep Learning in a Generalized HJM-type Framework Through Arbitrage-Free\n  Regularization", "summary": "We introduce a regularization approach to arbitrage-free factor-model\nselection. The considered model selection problem seeks to learn the closest\narbitrage-free HJM-type model to any prespecified factor-model. An asymptotic\nsolution to this, a priori computationally intractable, problem is represented\nas the limit of a 1-parameter family of optimizers to computationally tractable\nmodel selection tasks. Each of these simplified model-selection tasks seeks to\nlearn the most similar model, to the prescribed factor-model, subject to a\npenalty detecting when the reference measure is a local martingale-measure for\nthe entire underlying financial market. A simple expression for the penalty\nterms is obtained in the bond market withing the affine-term structure setting,\nand it is used to formulate a deep-learning approach to arbitrage-free affine\nterm-structure modelling. Numerical implementations are also performed to\nevaluate the performance in the bond market.", "category": ["q-fin.MF", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1710.05114v4", "link": "http://dx.doi.org/10.3390/risks8020040"}, {"title": "Generalizations of Szpilrajn's Theorem in economic and game theories", "summary": "Szpilrajn's Lemma entails that each partial order extends to a linear order.\nDushnik and Miller use Szpilrajn's Lemma to show that each partial order has a\nrelizer. Since then, many authors utilize Szpilrajn's Theorem and the\nWell-ordering principle to prove more general existence type theorems on\nextending binary relations. Nevertheless, we are often interested not only in\nthe existence of extensions of a binary relation $R$ satisfying certain axioms\nof orderability, but in something more: (A) The conditions of the sets of\nalternatives and the properties which $R$ satisfies to be inherited when one\npasses to any member of a subfamily of the family of extensions of $R$ and: (B)\nThe size of a family of ordering extensions of $R$, whose intersection is $R$,\nto be the smallest one. The key to addressing these kinds of problems is the\nszpilrajn inherited method. In this paper, we define the notion of\n$\\Lambda(m)$-consistency, where $m$ can reach the first infinite ordinal\n$\\omega$, and we give two general inherited type theorems on extending binary\nrelations, a Szpilrajn type and a Dushnik-Miller type theorem, which generalize\nall the well known existence and inherited type extension theorems in the\nliterature. \\keywords{Consistent binary relations, Extension theorems,\nIntersection of binary relations.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1708.04711v1", "link": "http://arxiv.org/abs/1708.04711v1"}, {"title": "Survey Bandits with Regret Guarantees", "summary": "We consider a variant of the contextual bandit problem. In standard\ncontextual bandits, when a user arrives we get the user's complete feature\nvector and then assign a treatment (arm) to that user. In a number of\napplications (like healthcare), collecting features from users can be costly.\nTo address this issue, we propose algorithms that avoid needless feature\ncollection while maintaining strong regret guarantees.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.09814v1", "link": "http://arxiv.org/abs/2002.09814v1"}, {"title": "Evolution of the distribution of wealth in an economic environment\n  driven by local Nash equilibria", "summary": "We present and analyze a model for the evolution of the wealth distribution\nwithin a heterogeneous economic environment. The model considers a system of\nrational agents interacting in a game theoretical framework, through fairly\ngeneral assumptions on the cost function. This evolution drives the dynamic of\nthe agents in both wealth and economic configuration variables. We consider a\nregime of scale separation where the large scale dynamics is given by a\nhydrodynamic closure with a Nash equilibrium serving as the local thermodynamic\nequilibrium. The result is a system of gas dynamics-type equations for the\ndensity and average wealth of the agents on large scales. We recover the\ninverse gamma distribution as an equilibrium in the particular case of\nquadratic cost functions which has been previously considered in the\nliterature.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1307.1685v1", "link": "http://dx.doi.org/10.1007/s10955-013-0888-4"}, {"title": "Introducing shrinkage in heavy-tailed state space models to predict\n  equity excess returns", "summary": "We forecast S&P 500 excess returns using a flexible Bayesian econometric\nstate space model with non-Gaussian features at several levels. More precisely,\nwe control for overparameterization via novel global-local shrinkage priors on\nthe state innovation variances as well as the time-invariant part of the state\nspace model. The shrinkage priors are complemented by heavy tailed state\ninnovations that cater for potential large breaks in the latent states.\nMoreover, we allow for leptokurtic stochastic volatility in the observation\nequation. The empirical findings indicate that several variants of the proposed\napproach outperform typical competitors frequently used in the literature, both\nin terms of point and density forecasts.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1805.12217v2", "link": "http://arxiv.org/abs/1805.12217v2"}, {"title": "On honest times in financial modeling", "summary": "This paper demonstrates the usefulness and importance of the concept of\nhonest times to financial modeling. It studies a financial market with asset\nprices that follow jump-diffusions with negative jumps. The central building\nblock of the market model is its growth optimal portfolio (GOP), which\nmaximizes the growth rate of strictly positive portfolios. Primary security\naccount prices, when expressed in units of the GOP, turn out to be nonnegative\nlocal martingales. In the proposed framework an equivalent risk neutral\nprobability measure need not exist. Derivative prices are obtained as\nconditional expectations of corresponding future payoffs, with the GOP as\nnumeraire and the real world probability as pricing measure. The time when the\nglobal maximum of a portfolio with no positive jumps, when expressed in units\nof the GOP, is reached, is shown to be a generic representation of an honest\ntime. We provide a general formula for the law of such honest times and compute\nthe conditional distributions of the global maximum of a portfolio in this\nframework. Moreover, we provide a stochastic integral representation for\nuniformly integrable martingales whose terminal values are functions of the\nglobal maximum of a portfolio. These formulae are model independent and\nuniversal. We also specialize our results to some examples where we hedge a\npayoff that arrives at an honest time.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/0808.2892v1", "link": "http://arxiv.org/abs/0808.2892v1"}, {"title": "Barrier Option Pricing", "summary": "We use Lie symmetry methods to price certain types of barrier options.\nUsually Lie symmetry methods cannot be used to solve the Black-Scholes equation\nfor options because the function defining the maturity condition for an option\nis not smooth. However, for barrier options, this restriction can be\naccommodated and a symmetry analysis utilised to find new solutions.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1312.3211v1", "link": "http://arxiv.org/abs/1312.3211v1"}, {"title": "Hedging without sweat: a genetic programming approach", "summary": "Hedging in the presence of transaction costs leads to complex optimization\nproblems. These problems typically lack closed-form solutions, and their\nimplementation relies on numerical methods that provide hedging strategies for\nspecific parameter values. In this paper we use a genetic programming algorithm\nto derive explicit formulas for near-optimal hedging strategies under nonlinear\ntransaction costs. The strategies are valid over a large range of parameter\nvalues and require no information about the structure of the optimal hedging\nstrategy.", "category": ["q-fin.RM", "q-fin.PR"], "id": "http://arxiv.org/abs/1305.6762v1", "link": "http://arxiv.org/abs/1305.6762v1"}, {"title": "Alonso and the Scaling of Urban Profiles", "summary": "The scaling of urban characteristics with total population has become an\nimportant research field since one needs to better understand the challenges of\nurban densification. Yet urban scaling research is largely disconnected from\nintra-urban structure. In contrast, the monocentric model of Alonso provides a\nresidential choice-based theory to urban density profiles. However, it is\nsilent about how these profiles scale with population, thus preventing\nempirical scaling studies to anchor in a strong micro-economic theory. This\npaper bridges this gap by introducing power laws for land, income and transport\ncost in the Alonso model. From this augmented model, we derive the conditions\nat which the equilibrium urban structure matches recent empirical findings\nabout the scaling of urban land and population density profiles in European\ncities. We find that the Alonso model is theoretically compatible with the\nobserved scaling of population density profiles and satisfactorily represents\nEuropean cities. This compatibility however challenges current empirical\nunderstanding of wage and transport cost elasticities with population, and\nrequires a scaling of the housing land profile that is different from the\nobserved. Our results call for revisiting theories about land development and\nhousing processes as well as the empirics of agglomeration benefits and\ntransport costs.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1801.07512v1", "link": "http://arxiv.org/abs/1801.07512v1"}, {"title": "Endogeneous Versus Exogeneous Shocks in Systems with Memory", "summary": "Systems with long-range persistence and memory are shown to exhibit different\nprecursory as well as recovery patterns in response to shocks of exogeneous\nversus endogeneous origins. By endogeneous, we envision either fluctuations\nresulting from an underlying chaotic dynamics or from a stochastic forcing\norigin which may be external or be an effective coarse-grained description of\nthe microscopic fluctuations. In this scenario, endogeneous shocks result from\na kind of constructive interference of accumulated fluctuations whose impacts\nsurvive longer than the large shocks themselves. As a consequence, the recovery\nafter an endogeneous shock is in general slower at early times and can be at\nlong times either slower or faster than after an exogeneous perturbation. This\noffers the tantalizing possibility of distinguishing between an endogeneous\nversus exogeneous cause of a given shock, even when there is no ``smoking\ngun.'' This could help in investigating the exogeneous versus self-organized\norigins in problems such as the causes of major biological extinctions, of\nchange of weather regimes and of the climate, in tracing the source of social\nupheaval and wars, and so on. Sornette, Malevergne and Muzy have already shown\nhow this concept can be applied concretely to differentiate the effects on\nfinancial markets of the Sept. 11, 2001 attack or of the coup against Gorbachev\non Aug., 19, 1991 (exogeneous) from financial crashes such as Oct. 1987\n(endogeneous).", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0206047v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01371-7"}, {"title": "Econophysics of Asset Price, Return and Multiple Expectations", "summary": "This paper describes asset price and return disturbances as result of\nrelations between transactions and multiple kinds of expectations. We show that\ndisturbances of expectations can cause fluctuations of trade volume, price and\nreturn. We model price disturbances for transactions made under all types of\nexpectations as weighted sum of partial price and trade volume disturbances for\ntransactions made under separate kinds of expectations. Relations on price\nallow present return as weighted sum of partial return and trade volume\n\"return\" for transactions made under separate expectations. Dependence of price\ndisturbances on trade volume disturbances as well as dependence of return on\ntrade volume \"return\" cause dependence of volatility and statistical\ndistributions of price and return on statistical properties of trade volume\ndisturbances and trade volume \"return\" respectively.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.05024v1", "link": "http://arxiv.org/abs/1901.05024v1"}, {"title": "Coexistence of several currencies in presence of increasing returns to\n  adoption", "summary": "We present a simplistic model of the competition between different\ncurrencies. Each individual is free to choose the currency that minimizes his\ntransaction costs, which arise whenever his exchanging relations have chosen a\ndifferent currency. We show that competition between currencies does not\nnecessarily converge to the emergence of a single currency. For large systems,\nwe prove that two distinct communities using different currencies in the\ninitial state will remain forever in this fractionalized state.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1801.04218v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.12.117"}, {"title": "The Intrafirm Complexity of Systemically Important Financial\n  Institutions", "summary": "In November, 2011, the Financial Stability Board, in collaboration with the\nInternational Monetary Fund, published a list of 29 \"systemically important\nfinancial institutions\" (SIFIs). This designation reflects a concern that the\nfailure of any one of them could have dramatic negative consequences for the\nglobal economy and is based on \"their size, complexity, and systemic\ninterconnectedness\". While the characteristics of \"size\" and \"systemic\ninterconnectedness\" have been the subject of a good deal of quantitative\nanalysis, less attention has been paid to measures of a firm's \"complexity.\" In\nthis paper we take on the challenges of measuring the complexity of a financial\ninstitution and to that end explore the use of the structure of an individual\nfirm's control hierarchy as a proxy for institutional complexity. The control\nhierarchy is a network representation of the institution and its subsidiaries.\nWe show that this mathematical representation (and various associated metrics)\nprovides a consistent way to compare the complexity of firms with often very\ndisparate business models and as such may provide the foundation for\ndetermining a SIFI designation. By quantifying the level of complexity of a\nfirm, our approach also may prove useful should firms need to reduce their\nlevel of complexity either in response to business or regulatory needs. Using a\ndata set containing the control hierarchies of many of the designated SIFIs, we\nfind that in the past two years, these firms have decreased their level of\ncomplexity, perhaps in response to regulatory requirements.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1505.02305v1", "link": "http://arxiv.org/abs/1505.02305v1"}, {"title": "Deep Neural Networks for Choice Analysis: Architectural Design with\n  Alternative-Specific Utility Functions", "summary": "Whereas deep neural network (DNN) is increasingly applied to choice analysis,\nit is challenging to reconcile domain-specific behavioral knowledge with\ngeneric-purpose DNN, to improve DNN's interpretability and predictive power,\nand to identify effective regularization methods for specific tasks. This study\ndesigns a particular DNN architecture with alternative-specific utility\nfunctions (ASU-DNN) by using prior behavioral knowledge. Unlike a fully\nconnected DNN (F-DNN), which computes the utility value of an alternative k by\nusing the attributes of all the alternatives, ASU-DNN computes it by using only\nk's own attributes. Theoretically, ASU-DNN can dramatically reduce the\nestimation error of F-DNN because of its lighter architecture and sparser\nconnectivity. Empirically, ASU-DNN has 2-3% higher prediction accuracy than\nF-DNN over the whole hyperparameter space in a private dataset that we\ncollected in Singapore and a public dataset in R mlogit package. The\nalternative-specific connectivity constraint, as a domain-knowledge-based\nregularization method, is more effective than the most popular generic-purpose\nexplicit and implicit regularization methods and architectural hyperparameters.\nASU-DNN is also more interpretable because it provides a more regular\nsubstitution pattern of travel mode choices than F-DNN does. The comparison\nbetween ASU-DNN and F-DNN can also aid in testing the behavioral knowledge. Our\nresults reveal that individuals are more likely to compute utility by using an\nalternative's own attributes, supporting the long-standing practice in choice\nmodeling. Overall, this study demonstrates that prior behavioral knowledge\ncould be used to guide the architecture design of DNN, to function as an\neffective domain-knowledge-based regularization method, and to improve both the\ninterpretability and predictive power of DNN in choice analysis.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.07481v1", "link": "http://arxiv.org/abs/1909.07481v1"}, {"title": "Market reaction to temporary liquidity crises and the permanent market\n  impact", "summary": "We study the relaxation dynamics of the bid-ask spread and of the midprice\nafter a sudden, large variation of the spread, corresponding to a temporary\ncrisis of liquidity in a double auction financial market. We find that the\nspread decays very slowly to its normal value as a consequence of the strategic\nlimit order placement of liquidity providers. We consider several quantities,\nsuch as order placement rates and distribution, that affect the decay of the\nspread. We measure the permanent impact both of a generic event altering the\nspread and of a single transaction and we find an approximately linear relation\nbetween immediate and permanent impact in both cases.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0608032v1", "link": "http://arxiv.org/abs/physics/0608032v1"}, {"title": "Safe Counterfactual Reinforcement Learning", "summary": "We develop a method for predicting the performance of reinforcement learning\nand bandit algorithms, given historical data that may have been generated by a\ndifferent algorithm. Our estimator has the property that its prediction\nconverges in probability to the true performance of a counterfactual algorithm\nat the fast $\\sqrt{N}$ rate, as the sample size $N$ increases. We also show a\ncorrect way to estimate the variance of our prediction, thus allowing the\nanalyst to quantify the uncertainty in the prediction. These properties hold\neven when the analyst does not know which among a large number of potentially\nimportant state variables are really important. These theoretical guarantees\nmake our estimator safe to use. We finally apply it to improve advertisement\ndesign by a major advertisement company. We find that our method produces\nsmaller mean squared errors than state-of-the-art methods.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.08536v1", "link": "http://arxiv.org/abs/2002.08536v1"}, {"title": "Default correlation, cluster dynamics and single names: The GPCL\n  dynamical loss model", "summary": "We extend the common Poisson shock framework reviewed for example in Lindskog\nand McNeil (2003) to a formulation avoiding repeated defaults, thus obtaining a\nmodel that can account consistently for single name default dynamics, cluster\ndefault dynamics and default counting process. This approach allows one to\nintroduce significant dynamics, improving on the standard \"bottom-up\"\napproaches, and to achieve true consistency with single names, improving on\nmost \"top-down\" loss models. Furthermore, the resulting GPCL model has\nimportant links with the previous GPL dynamical loss model in Brigo,\nPallavicini and Torresetti (2006a,b), which we point out. Model extensions\nallowing for more articulated spread and recovery dynamics are hinted at.\nCalibration to both DJi-TRAXX and CDX index and tranche data across attachments\nand maturities shows that the GPCL model has the same calibration power as the\nGPL model while allowing for consistency with single names", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0812.4163v1", "link": "http://arxiv.org/abs/0812.4163v1"}, {"title": "Greater search cost reduces prices", "summary": "The optimal price of each firm falls in the search cost of consumers, in the\nlimit to the monopoly price, despite the exit of lower-value consumers in\nresponse to costlier search. Exit means that fewer inframarginal consumers\nremain. The decrease in marginal buyers is smaller, because part of demand is\ncomposed of customers coming from rival firms. These buyers can be held up and\nare not marginal. Higher search cost reduces the fraction of incoming switchers\namong buyers, which decreases the hold-up motive, thus the price.", "category": [], "id": "http://arxiv.org/abs/2004.01238v1", "link": "http://arxiv.org/abs/2004.01238v1"}, {"title": "On the criticality of inferred models", "summary": "Advanced inference techniques allow one to reconstruct the pattern of\ninteraction from high dimensional data sets. We focus here on the statistical\nproperties of inferred models and argue that inference procedures are likely to\nyield models which are close to a phase transition. On one side, we show that\nthe reparameterization invariant metrics in the space of probability\ndistributions of these models (the Fisher Information) is directly related to\nthe model's susceptibility. As a result, distinguishable models tend to\naccumulate close to critical points, where the susceptibility diverges in\ninfinite systems. On the other, this region is the one where the estimate of\ninferred parameters is most stable. In order to illustrate these points, we\ndiscuss inference of interacting point processes with application to financial\ndata and show that sensible choices of observation time-scales naturally yield\nmodels which are close to criticality.", "category": ["q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/1102.1624v2", "link": "http://dx.doi.org/10.1088/1742-5468/2011/10/P10012"}, {"title": "A multilevel analysis to systemic exposure: insights from local and\n  system-wide information", "summary": "In the aftermath of the financial crisis, the growing literature on financial\nnetworks has widely documented the predictive power of topological\ncharacteristics (e.g. degree centrality measures) to explain the systemic\nimpact or systemic vulnerability of financial institutions. In this work, we\nshow that considering alternative topological measures based on local\nsub-network environment improves our ability to identify systemic institutions.\nTo provide empirical evidence, we apply a two-step procedure. First, we recover\nnetwork communities (i.e. close-peer environment) on a spillover network of\nfinancial institutions. Second, we regress alternative measures of\nvulnerability on three levels of topological measures: the global level (i.e.\nfirm topological characteristics computed over the whole system), local level\n(i.e. firm topological characteristics computed over the community) and\naggregated level by averaging individual characteristics over the community.\nThe sample includes $46$ financial institutions (banks, broker-dealers,\ninsurance and real-estate companies) listed in the Standard \\& Poor's 500\nindex. Our results confirm the informational content of topological metrics\nbased on close-peer environment. Such information is different from the one\nembeds in traditional system wide topological metrics and is proved to be\npredictor of distress for financial institutions in time of crisis.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1910.08611v1", "link": "http://arxiv.org/abs/1910.08611v1"}, {"title": "The Value of Timing Risk", "summary": "The aim of this paper is to provide a mathematical contribution on the\nsemi-static hedge of timing risk associated to positions in American-style\noptions under a multi-dimensional market model. Barrier options are considered\nin the paper and semi-static hedges are studied and discussed for a fairly\nlarge class of underlying price dynamics. Timing risk is identified with the\nuncertainty associated to the time at which the payoff payment of the barrier\noption is due. Starting from the work by Carr and Picron (1999), where the\nauthors show that the timing risk can be hedged via static positions in plain\nvanilla options, the present paper extends the static hedge formula proposed in\nCarr and Picron (1999) by giving sufficient conditions to decompose a\ngeneralized timing risk into an integral of knock-in options in a\nmulti-dimensional market model. A dedicated study of the semi-static hedge is\nthen conducted by defining the corresponding strategy based on positions in\nbarrier options. The proposed methodology allows to construct not only first\norder hedges but also higher order semi-static hedges, that can be interpreted\nas asymptotic expansions of the hedging error. The convergence of these higher\norder semi-static hedges to an exact hedge is shown. An illustration of the\nmain theoretical results is provided for i) a symmetric case, ii) a one\ndimensional case, where the first order and second order hedging errors are\nderived in analytic closed form. The materiality of the hedging benefit gain of\ngoing from order one to order two by re-iterating the timing risk hedging\nstrategy is discussed through numerical evidences by showing that order two can\nbring to more than 90% reduction of the hedging 'cost' w.r.t. order one\n(depending on the specific barrier option characteristics).", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1701.05695v1", "link": "http://arxiv.org/abs/1701.05695v1"}, {"title": "Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty", "summary": "The COVID-19 pandemic has caused a massive economic shock across the world\ndue to business interruptions and shutdowns from social-distancing measures. To\nevaluate the socio-economic impact of COVID-19 on individuals, a micro-economic\nmodel is developed to estimate the direct impact of distancing on household\nincome, savings, consumption, and poverty. The model assumes two periods: a\ncrisis period during which some individuals experience a drop in income and can\nuse their precautionary savings to maintain consumption; and a recovery period,\nwhen households save to replenish their depleted savings to pre-crisis level.\nThe San Francisco Bay Area is used as a case study, and the impacts of a\nlockdown are quantified, accounting for the effects of unemployment insurance\n(UI) and the CARES Act federal stimulus. Assuming a shelter-in-place period of\nthree months, the poverty rate would temporarily increase from 17.1% to 25.9%\nin the Bay Area in the absence of social protection, and the lowest income\nearners would suffer the most in relative terms. If fully implemented, the\ncombination of UI and CARES could keep the increase in poverty close to zero,\nand reduce the average recovery time, for individuals who suffer an income\nloss, from 11.8 to 6.7 months. However, the severity of the economic impact is\nspatially heterogeneous, and certain communities are more affected than the\naverage and could take more than a year to recover. Overall, this model is a\nfirst step in quantifying the household-level impacts of COVID-19 at a regional\nscale. This study can be extended to explore the impact of indirect\nmacroeconomic effects, the role of uncertainty in households' decision-making\nand the potential effect of simultaneous exogenous shocks (e.g., natural\ndisasters).", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.05945v1", "link": "http://arxiv.org/abs/2005.05945v1"}, {"title": "Asymptotic expansion for characteristic function in Heston stochastic\n  volatility model with fast mean-reverting correction", "summary": "In this note, we derive the characteristic function expansion for logarithm\nof the underlying asset price in corrected Heston model as proposed by Fouque\nand Lorig.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1310.3572v1", "link": "http://arxiv.org/abs/1310.3572v1"}, {"title": "A zero interest rate Black-Derman-Toy model", "summary": "We propose a modification of the classical Black-Derman-Toy (BDT) interest\nrate tree model, which includes the possibility of a jump with small\nprobability at each step to a practically zero interest rate. The corresponding\nBDT algorithms are consequently modified to calibrate the tree containing the\nzero interest rate scenarios. This modification is motivated by the recent\n2008-2009 crisis in the United States and it quantifies the risk of a future\ncrises in bond prices and derivatives. The proposed model is useful to price\nderivatives. This exercise also provides a tool to calibrate the probability of\nthis event. A comparison of option prices and implied volatilities on US\nTreasury bonds computed with both the proposed and the classical tree model is\nprovided, in six different scenarios along the different periods comprising the\nyears 2002-2017.", "category": ["econ.EM", "q-fin.CP"], "id": "http://arxiv.org/abs/1908.04401v1", "link": "http://arxiv.org/abs/1908.04401v1"}, {"title": "Utility maximization for L{\u00e9}vy switching models", "summary": "This article is devoted to the maximisation of HARA utilities of L{\\'e}vy\nswitching process on finite time interval via dual method. We give the\ndescription of all f-divergence minimal martingale measures in initially\nenlarged filtration, the expression of their Radon-Nikodym densities involving\nHellinger and Kulback-Leibler processes, the expressions of the optimal\nstrategies in progressively enlarged filtration for the maximisation of HARA\nutilities as well as the values of the corresponding maximal expected\nutilities. The example of Brownian switching model is presented to give the\nfinancial interpretation of the results.", "category": ["math.PR", "q-fin.CP", "q-fin.GN"], "id": "http://arxiv.org/abs/1807.08982v1", "link": "http://arxiv.org/abs/1807.08982v1"}, {"title": "On the core of normal form games with a continuum of players : a\n  correction", "summary": "We study the core of normal form games with a continuum of players and\nwithout side payments. We consider the weak-core concept, which is an\napproximation of the core, introduced by Weber, Shapley and Shubik. For payoffs\ndepending on the players' strategy profile, we prove that the weak-core is\nnonempty. The existence result establishes a weak-core element as a limit of\nelements in weak-cores of appropriate finite games. We establish by examples\nthat our regularity hypotheses are relevant in the continuum case and the\nweak-core can be strictly larger than the Aumann's $\\alpha$-core. For games\nwhere payoffs depend on the distribution of players' strategy profile, we prove\nthat analogous regularity conditions ensuring the existence of pure strategy\nNash equilibria are irrelevant for the non-vacuity of the weak-core.", "category": [], "id": "http://arxiv.org/abs/1903.09819v1", "link": "http://dx.doi.org/10.1016/j.mathsocsci.2017.06.001"}, {"title": "Inflation and unemployment in Switzerland: from 1970 to 2050", "summary": "An empirical model is presented linking inflation and unemployment rate to\nthe change in the level of labour force in Switzerland. The involved variables\nare found to be cointegrated and we estimate lagged linear deterministic\nrelationships using the method of cumulative curves, a simplified version of\nthe 1D Boundary Elements Method. The model yields very accurate predictions of\nthe inflation rate on a three year horizon. The results are coherent with the\nmodels estimated previously for the US, Japan, France and other developed\ncountries and provide additional validation of our quantitative framework based\nsolely on labour force. Finally, given the importance of inflation forecasts\nfor the Swiss monetary policy, we present a prediction extended into 2050 based\non official projections of the labour force level.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1102.5405v1", "link": "http://arxiv.org/abs/1102.5405v1"}, {"title": "Network-based Anomaly Detection for Insider Trading", "summary": "Insider trading is one of the numerous white collar crimes that can\ncontribute to the instability of the economy. Traditionally, the detection of\nillegal insider trades has been a human-driven process. In this paper, we\ncollect the insider tradings made available by the US Securities and Exchange\nCommissions (SEC) through the EDGAR system, with the aim of initiating an\nautomated large-scale and data-driven approach to the problem of identifying\nillegal insider tradings. The goal of the study is the identification of\ninteresting patterns, which can be indicators of potential anomalies. We use\nthe collected data to construct networks that capture the relationship between\ntrading behaviors of insiders. We explore different ways of building networks\nfrom insider trading data, and argue for a need of a structure that is capable\nof capturing higher order relationships among traders. Our results suggest the\ndiscovery of interesting patterns.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1702.05809v1", "link": "http://arxiv.org/abs/1702.05809v1"}, {"title": "Finite Time Identification in Unstable Linear Systems", "summary": "Identification of the parameters of stable linear dynamical systems is a\nwell-studied problem in the literature, both in the low and high-dimensional\nsettings. However, there are hardly any results for the unstable case,\nespecially regarding finite time bounds. For this setting, classical results on\nleast-squares estimation of the dynamics parameters are not applicable and\ntherefore new concepts and technical approaches need to be developed to address\nthe issue. Unstable linear systems arise in key real applications in control\ntheory, econometrics, and finance. This study establishes finite time bounds\nfor the identification error of the least-squares estimates for a fairly large\nclass of heavy-tailed noise distributions, and transition matrices of such\nsystems. The results relate the time length (samples) required for estimation\nto a function of the problem dimension and key characteristics of the true\nunderlying transition matrix and the noise distribution. To establish them,\nappropriate concentration inequalities for random matrices and for sequences of\nmartingale differences are leveraged.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1710.01852v2", "link": "http://arxiv.org/abs/1710.01852v2"}, {"title": "Spatial Effects and Convergence Theory in the Portuguese Situation", "summary": "This study analyses, through cross-section estimation methods, the influence\nof spatial effects and human capital in the conditional productivity\nconvergence (product per worker) in the economic sectors of NUTs III of\nmainland Portugal between 1995 and 2002. To analyse the data, Moran's I\nstatistics is considered, and it is stated that productivity is subject to\npositive spatial autocorrelation (productivity develops in a similar manner to\nproductivity in neighbouring regions), above all, in agriculture and services.\nIndustry and the total of all sectors present indications that they are subject\nto positive spatial autocorrelation in productivity. On the other hand, it is\nstated that the indications of convergence, specifically bearing in mind the\nconcept of absolute convergence, are greater in industry. Taking into account\nthe estimation results, it is stated once again that the indications of\nconvergence are greater in industry, and it can be seen that spatial spillover\neffects, spatial lag (capturing spatial autocorrelation through a spatially\nredundant dependent variable) and spatial error (capturing spatial\nautocorrelation through a spatially redundant error term), as well as human\ncapital, condition the convergence of productivity in the various economic\nsectors of Portuguese region in the period under consideration (Martinho,\n2011).", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1110.5571v1", "link": "http://arxiv.org/abs/1110.5571v1"}, {"title": "Assessing the state of e-Readiness for Small and Medium Companies in\n  Mexico: a Proposed Taxonomy and Adoption Model", "summary": "Emerging economies frequently show a large component of their Gross Domestic\nProduct to be dependant on the economic activity of small and medium\nenterprises. Nevertheless, e-business solutions are more likely designed for\nlarge companies. SMEs seem to follow a classical family-based management, used\nto traditional activities, rather than seeking new ways of adding value to\ntheir business strategy. Thus, a large portion of a nations economy may be at\ndisadvantage for competition. This paper aims at assessing the state of\ne-business readiness of Mexican SMEs based on already published e-business\nevolution models and by means of a survey research design. Data is being\ncollected in three cities with differing sizes and infrastructure conditions.\nStatistical results are expected to be presented. A second part of this\nresearch aims at applying classical adoption models to suggest potential causal\nrelationships, as well as more suitable recommendations for development.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1804.06709v1", "link": "http://arxiv.org/abs/1804.06709v1"}, {"title": "The Opinion Game: Stock price evolution from microscopic market\n  modelling", "summary": "We propose a class of Markovian agent based models for the time evolution of\na share price in an interactive market. The models rely on a microscopic\ndescription of a market of buyers and sellers who change their opinion about\nthe stock value in a stochastic way. The actual price is determined in\nrealistic way by matching (clearing) offers until no further transactions can\nbe performed. Some analytic results for a non-interacting model are presented.\nWe also propose basic interaction mechanisms and show in simulations that these\nalready reproduce certain particular features of prices in real stock markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0401422v1", "link": "http://arxiv.org/abs/cond-mat/0401422v1"}, {"title": "Optimal Execution in a Multiplayer Model of Transient Price Impact", "summary": "Trading algorithms that execute large orders are susceptible to exploitation\nby order anticipation strategies. This paper studies the influence of order\nanticipation strategies in a multi-investor model of optimal execution under\ntransient price impact. Existence and uniqueness of a Nash equilibrium is\nestablished under the assumption that trading incurs quadratic transaction\ncosts. A closed-form representation of the Nash equilibrium is derived for\nexponential decay kernels. With this representation, it is shown that while\norder anticipation strategies raise the execution costs of a large order\nsignificantly, they typically do not cause price overshooting in the sense of\nBrunnermeier and Pedersen.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1609.00599v4", "link": "http://dx.doi.org/10.1142/S2382626618500077"}, {"title": "How to Combine a Billion Alphas", "summary": "We give an explicit algorithm and source code for computing optimal weights\nfor combining a large number N of alphas. This algorithm does not cost O(N^3)\nor even O(N^2) operations but is much cheaper, in fact, the number of required\noperations scales linearly with N. We discuss how in the absence of binary or\nquasi-binary clustering of alphas, which is not observed in practice, the\noptimization problem simplifies when N is large. Our algorithm does not require\ncomputing principal components or inverting large matrices, nor does it require\niterations. The number of risk factors it employs, which typically is limited\nby the number of historical observations, can be sizably enlarged via using\nposition data for the underlying tradables.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1603.05937v2", "link": "http://arxiv.org/abs/1603.05937v2"}, {"title": "Modeling Financial Volatility in the Presence of Abrupt Changes", "summary": "The volatility of financial instruments is rarely constant, and usually\nvaries over time. This creates a phenomenon called volatility clustering, where\nlarge price movements on one day are followed by similarly large movements on\nsuccessive days, creating temporal clusters. The GARCH model, which treats\nvolatility as a drift process, is commonly used to capture this behavior.\nHowever research suggests that volatility is often better described by a\nstructural break model, where the volatility undergoes abrupt jumps in addition\nto drift. Most efforts to integrate these jumps into the GARCH methodology have\nresulted in models which are either very computationally demanding, or which\nmake problematic assumptions about the distribution of the instruments, often\nassuming that they are Gaussian. We present a new approach which uses ideas\nfrom nonparametric statistics to identify structural break points without\nmaking such distributional assumptions, and then models drift separately within\neach identified regime. Using our method, we investigate the volatility of\nseveral major stock indexes, and find that our approach can potentially give an\nimproved fit compared to more commonly used techniques.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1212.6016v1", "link": "http://dx.doi.org/10.1016/j.physa.2012.08.015"}, {"title": "Default contagion risks in Russian interbank market", "summary": "Systemic risks of default contagion in the Russian interbank market are\ninvestigated. The analysis is based on considering the bow-tie structure of the\nweighted oriented graph describing the structure of the interbank loans. A\nprobabilistic model of interbank contagion explicitly taking into account the\nempirical bow-tie structure reflecting functionality of the corresponding nodes\n(borrowers, lenders, borrowers and lenders simultaneously), degree\ndistributions and disassortativity of the interbank network under consideration\nbased on empirical data is developed. The characteristics of contagion-related\nsystemic risk calculated with this model are shown to be in agreement with\nthose of explicit stress tests.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1409.1071v3", "link": "http://arxiv.org/abs/1409.1071v3"}, {"title": "Sticky continuous processes have consistent price systems", "summary": "Under proportional transaction costs, a price process is said to have a\nconsistent price system, if there is a semimartingale with an equivalent\nmartingale measure that evolves within the bid-ask spread. We show that a\ncontinuous, multi-asset price process has a consistent price system, under\narbitrarily small proportional transaction costs, if it satisfies a natural\nmulti-dimensional generalization of the stickiness condition introduced by\nGuasoni [Math. Finance 16(3), 569-582 (2006)].", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1310.7857v3", "link": "http://dx.doi.org/10.1239/jap/1437658617"}, {"title": "Principal Regression Analysis and the index leverage effect", "summary": "We revisit the index leverage effect, that can be decomposed into a\nvolatility effect and a correlation effect. We investigate the latter using a\nmatrix regression analysis, that we call `Principal Regression Analysis' (PRA)\nand for which we provide some analytical (using Random Matrix Theory) and\nnumerical benchmarks. We find that downward index trends increase the average\ncorrelation between stocks (as measured by the most negative eigenvalue of the\nconditional correlation matrix), and makes the market mode more uniform. Upward\ntrends, on the other hand, also increase the average correlation between stocks\nbut rotates the corresponding market mode {\\it away} from uniformity. There are\ntwo time scales associated to these effects, a short one on the order of a\nmonth (20 trading days), and a longer time scale on the order of a year. We\nalso find indications of a leverage effect for sectorial correlations as well,\nwhich reveals itself in the second and third mode of the PRA.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1011.5810v2", "link": "http://dx.doi.org/10.1016/j.physa.2011.04.007"}, {"title": "Estimation of the size of informal employment based on administrative\n  records with non-ignorable selection mechanism", "summary": "In this study we used company level administrative data from the National\nLabour Inspectorate and The Polish Social Insurance Institution in order to\nestimate the prevalence of informal employment in Poland. Since the selection\nmechanism is non-ignorable we employed a generalization of Heckman's sample\nselection model assuming non-Gaussian correlation of errors and clustering by\nincorporation of random effects. We found that 5.7% (4.6%, 7.1%; 95% CI) of\nregistered enterprises in Poland, to some extent, take advantage of the\ninformal labour force. Our study exemplifies a new approach to measuring\ninformal employment, which can be implemented in other countries. It also\ncontributes to the existing literature by providing, to the best of our\nknowledge, the first estimates of informal employment at the level of companies\nbased solely on administrative data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1906.10957v1", "link": "http://arxiv.org/abs/1906.10957v1"}, {"title": "Testing the Stability of the 2000-2003 US Stock Market \"Antibubble\"", "summary": "Since August 2000, the stock market in the USA as well as most other western\nmarkets have depreciated almost in synchrony according to complex patterns of\ndrops and local rebounds. In \\cite{SZ02QF}, we have proposed to describe this\nphenomenon using the concept of a log-periodic power law (LPPL) antibubble,\ncharacterizing behavioral herding between investors leading to a competition\nbetween positive and negative feedbacks in the pricing process. A monthly\nprediction for the future evolution of the US S&P 500 index has been issued,\nmonitored and updated in \\cite{urlprediction}, which is still running. Here, we\ntest the possible existence of a regime switching in the US S&P 500 antibubble.\nFirst, we find some evidence that the antibubble has exhibited a transition in\nlog-periodicity described by a so-called second-order log-periodicity. Second,\n>...", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0310092v2", "link": "http://dx.doi.org/10.1016/j.physa.2004.09.032"}, {"title": "Protecting Pegged Currency Markets from Speculative Investors", "summary": "We consider a stochastic game between a trader and a central bank in a target\nzone market with a lower currency peg. This currency peg is maintained by the\ncentral bank through the generation of permanent price impact, thereby\naggregating an ever increasing risky position in foreign reserves. We describe\nthis situation mathematically by means of two coupled singular control\nproblems, where the common singularity arises from a local time along a random\ncurve. Our first result identifies a certain local time as that central bank\nstrategy for which this risk position is minimized. We then consider the\nworst-case situation the central bank may face by identifying that strategy of\nthe strategic investor that maximizes the expected inventory of the central\nbank under a cost criterion, thus establishing a Stackelberg equilibrium in our\nmodel.", "category": ["q-fin.MF", "math.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/1801.07784v4", "link": "http://arxiv.org/abs/1801.07784v4"}, {"title": "Convex Combinatorial Auction of Pipeline Network Capacities", "summary": "In this paper we propose a mechanism for the allocation of pipeline\ncapacities, assuming that the participants bidding for capacities do have\nsubjective evaluation of various network routes. The proposed mechanism is\nbased on the concept of bidding for route-quantity pairs. Each participant\ndefines a limited number of routes and places multiple bids, corresponding to\nvarious quantities, on each of these routes. The proposed mechanism assigns a\nconvex combination of the submitted bids to each participant, thus its called\nconvex combinatorial auction. The capacity payments in the proposed model are\ndetermined according to the Vickrey-Clarke-Groves principle. We compare the\nefficiency of the proposed algorithm with a simplified model of the method\ncurrently used for pipeline capacity allocation in the EU (simultaneous\nascending clock auction of pipeline capacities) via simulation, according to\nvarious measures, such as resulting utility of players, utilization of network\ncapacities, total income of the auctioneer and fairness.", "category": [], "id": "http://arxiv.org/abs/2002.06554v1", "link": "http://arxiv.org/abs/2002.06554v1"}, {"title": "Intraday pattern in bid-ask spreads and its power-law relaxation for\n  Chinese A-share stocks", "summary": "We use high-frequency data of 1364 Chinese A-share stocks traded on the\nShanghai Stock Exchange and Shenzhen Stock Exchange to investigate the intraday\npatterns in the bid-ask spreads. The daily periodicity in the spread time\nseries is confirmed by Lomb analysis and the intraday bid-ask spreads are found\nto exhibit $L$-shaped pattern with idiosyncratic fine structure. The intraday\nspread of individual stocks relaxes as a power law within the first hour of the\ncontinuous double auction from 9:30AM to 10:30AM with exponents\n$\\beta_{\\rm{SHSE}}=0.19\\pm0.069$ for the Shanghai market and\n$\\beta_{\\rm{SZSE}}=0.18\\pm0.067$ for the Shenzhen market. The power-law\nrelaxation exponent $\\beta$ of individual stocks is roughly normally\ndistributed. There is evidence showing that the accumulation of information\nwidening the spread is an endogenous process.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0710.2402v1", "link": "http://dx.doi.org/10.3938/jkps.54.786"}, {"title": "Anti-correlation and subsector structure in financial systems", "summary": "With the random matrix theory, we study the spatial structure of the Chinese\nstock market, American stock market and global market indices. After taking\ninto account the signs of the components in the eigenvectors of the\ncross-correlation matrix, we detect the subsector structure of the financial\nsystems. The positive and negative subsectors are anti-correlated each other in\nthe corresponding eigenmode. The subsector structure is strong in the Chinese\nstock market, while somewhat weaker in the American stock market and global\nmarket indices. Characteristics of the subsector structures in different\nmarkets are revealed.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1201.6418v1", "link": "http://dx.doi.org/10.1209/0295-5075/97/48006"}, {"title": "Conditional correlation in asset return and GARCH intensity model", "summary": "In an asset return series there is a conditional asymmetric dependence\nbetween current return and past volatility depending on the current return's\nsign. To take into account the conditional asymmetry, we introduce new models\nfor asset return dynamics in which frequencies of the up and down movements of\nasset price have conditionally independent Poisson distributions with\nstochastic intensities. The intensities are assumed to be stochastic recurrence\nequations of the GARCH type in order to capture the volatility clustering and\nthe leverage effect. We provide an important linkage between our model and\nexisting GARCH, explain how to apply maximum likelihood estimation to determine\nthe parameters in the intensity model and show empirical results with the S&P\n500 index return series.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1311.4977v1", "link": "http://dx.doi.org/10.1007/s10182-013-0219-8"}, {"title": "Continuous Record Asymptotics for Structural Change Models", "summary": "For a partial structural change in a linear regression model with a single\nbreak, we develop a continuous record asymptotic framework to build inference\nmethods for the break date. We have T observations with a sampling frequency h\nover a fixed time horizon [0, N] , and let T with h 0 while keeping the time\nspan N fixed. We impose very mild regularity conditions on an underlying\ncontinuous-time model assumed to generate the data. We consider the\nleast-squares estimate of the break date and establish consistency and\nconvergence rate. We provide a limit theory for shrinking magnitudes of shifts\nand locally increasing variances. The asymptotic distribution corresponds to\nthe location of the extremum of a function of the quadratic variation of the\nregressors and of a Gaussian centered martingale process over a certain time\ninterval. We can account for the asymmetric informational content provided by\nthe pre- and post-break regimes and show how the location of the break and\nshift magnitude are key ingredients in shaping the distribution. We consider a\nfeasible version based on plug-in estimates, which provides a very good\napproximation to the finite sample distribution. We use the concept of Highest\nDensity Region to construct confidence sets. Overall, our method is reliable\nand delivers accurate coverage probabilities and relatively short average\nlength of the confidence sets. Importantly, it does so irrespective of the size\nof the break.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1803.10881v2", "link": "http://arxiv.org/abs/1803.10881v2"}, {"title": "An Algorithmic Approach to Non-self-financing Hedging in a Discrete-Time\n  Incomplete Market", "summary": "We present an algorithm producing a dynamic non-self-financing hedging\nstrategy in an incomplete market corresponding to investor-relevant risk\ncriterion. The optimization is a two stage process that first determines\nadmissible model parameters that correspond to the market price of the option\nbeing hedged. The second stage applies various merit functions to bootstrapped\nsamples of model residuals to choose an optimal set of model parameters from\nthe admissible set. Results are presented for options traded on the New York\nStock Exchange.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/math/0606471v1", "link": "http://arxiv.org/abs/math/0606471v1"}, {"title": "Firm dynamics in a closed, conserved economy: A model of size\n  distribution of employment and related statistics", "summary": "We address the issue of the distribution of firm size. To this end we propose\na model of firms in a closed, conserved economy populated with\nzero-intelligence agents who continuously move from one firm to another. We\nthen analyze the size distribution and related statistics obtained from the\nmodel. Our ultimate goal is to reproduce the well known statistical features\nobtained from the panel study of the firms i.e., the power law in size (in\nterms of income and/or employment), the Laplace distribution in the growth\nrates and the slowly declining standard deviation of the growth rates\nconditional on the firm size. First, we show that the model generalizes the\nusual kinetic exchange models with binary interaction to interactions between\nan arbitrary number of agents. When the number of interacting agents is in the\norder of the system itself, it is possible to decouple the model. We provide\nsome exact results on the distributions. Our model easily reproduces the power\nlaw. The fluctuations in the growth rate falls with increasing size following a\npower law (with an exponent 1 whereas the data suggests that the exponent is\naround 1/6). However, the distribution of the difference of the firm-size in\nthis model has Laplace distribution whereas the real data suggests that the\ndifference of the log sizes has the same distribution.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1112.2168v1", "link": "http://arxiv.org/abs/1112.2168v1"}, {"title": "An Experimental Investigation of Preference Misrepresentation in the\n  Residency Match", "summary": "The development and deployment of matching procedures that incentivize\ntruthful preference reporting is considered one of the major successes of\nmarket design research. In this study, we test the degree to which these\nprocedures succeed in eliminating preference misrepresentation. We administered\nan online experiment to 1,714 medical students immediately after their\nparticipation in the medical residency match--a leading field application of\nstrategy-proof market design. When placed in an analogous, incentivized\nmatching task, we find that 23% of participants misrepresent their preferences.\nWe explore the factors that predict preference misrepresentation, including\ncognitive ability, strategic positioning, overconfidence, expectations, advice,\nand trust. We discuss the implications of this behavior for the design of\nallocation mechanisms and the social welfare in markets that use them.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1802.01990v2", "link": "http://arxiv.org/abs/1802.01990v2"}, {"title": "Introducing shrinkage in heavy-tailed state space models to predict\n  equity excess returns", "summary": "We forecast S&P 500 excess returns using a flexible Bayesian econometric\nstate space model with non-Gaussian features at several levels. More precisely,\nwe control for overparameterization via novel global-local shrinkage priors on\nthe state innovation variances as well as the time-invariant part of the state\nspace model. The shrinkage priors are complemented by heavy tailed state\ninnovations that cater for potential large breaks in the latent states.\nMoreover, we allow for leptokurtic stochastic volatility in the observation\nequation. The empirical findings indicate that several variants of the proposed\napproach outperform typical competitors frequently used in the literature, both\nin terms of point and density forecasts.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1805.12217v2", "link": "http://arxiv.org/abs/1805.12217v2"}, {"title": "A multivariate multifractal model for return fluctuations", "summary": "In this paper we briefly review the recently inrtroduced Multifractal Random\nWalk (MRW) that is able to reproduce most of recent empirical findings\nconcerning financial time-series : no correlation between price variations,\nlong-range volatility correlations and multifractal statistics. We then focus\non its extension to a multivariate context in order to model portfolio\nbehavior. Empirical estimations on real data suggest that this approach can be\npertinent to account for the nature of both linear and non-linear correlation\nbetween stock returns at all time scales.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0009260v1", "link": "http://arxiv.org/abs/cond-mat/0009260v1"}, {"title": "Optimal proportional reinsurance and investment for stochastic factor\n  models", "summary": "In this work we investigate the optimal proportional reinsurance-investment\nstrategy of an insurance company which wishes to maximize the expected\nexponential utility of its terminal wealth in a finite time horizon. Our goal\nis to extend the classical Cramer-Lundberg model introducing a stochastic\nfactor which affects the intensity of the claims arrival process, described by\na Cox process, as well as the insurance and reinsurance premia. Using the\nclassical stochastic control approach based on the Hamilton-Jacobi-Bellman\nequation we characterize the optimal strategy and provide a verification result\nfor the value function via classical solutions of two backward partial\ndifferential equations. Existence and uniqueness of these solutions are\ndiscussed. Results under various premium calculation principles are illustrated\nand a new premium calculation rule is proposed in order to get more realistic\nstrategies and to better fit our stochastic factor model. Finally, numerical\nsimulations are performed to obtain sensitivity analyses.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1806.01223v1", "link": "http://arxiv.org/abs/1806.01223v1"}, {"title": "Hedging with Temporary Price Impact", "summary": "We consider the problem of hedging a European contingent claim in a Bachelier\nmodel with transient price impact as proposed by Almgren and Chriss. Following\nthe approach of Rogers and Singh and Naujokat and Westray, the hedging problem\ncan be regarded as a cost optimal tracking problem of the frictionless hedging\nstrategy. We solve this problem explicitly for general predictable target\nhedging strategies. It turns out that, rather than towards the current target\nposition, the optimal policy trades towards a weighted average of expected\nfuture target positions. This generalizes an observation of Garleanu and\nPedersen from their homogenous Markovian optimal investment problem to a\ngeneral hedging problem. Our findings complement a number of previous studies\nin the literature on optimal strategies in illiquid markets where the\nfrictionless strategy is confined to diffusions. The consideration of general\npredictable reference strategies is made possible by the use of a convex\nanalysis approach instead of the more common dynamic programming methods.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1510.03223v2", "link": "http://arxiv.org/abs/1510.03223v2"}, {"title": "Heterogeneous Beliefs with Finite-Lived Agents", "summary": "This paper will examine a model with many agents, each of whom has a\ndifferent belief about the dynamics of a risky asset. The agents are Bayesian\nand so learn about the asset over time. All agents are assumed to have a finite\n(but random) lifetime. When an agent dies, he passes his wealth (but not his\nknowledge) onto his heir. As a result, the agents never become sure of the\ndynamics of the risky asset. We derive expressions for the stock price and\nriskless rate. We then use numerical examples to exhibit their behaviour.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0907.4953v1", "link": "http://arxiv.org/abs/0907.4953v1"}, {"title": "Exact retrospective Monte Carlo computation of arithmetic average Asian\n  options", "summary": "Taking advantage of the recent litterature on exact simulation algorithms\n(Beskos, Papaspiliopoulos and Roberts) and unbiased estimation of the\nexpectation of certain fonctional integrals (Wagner, Beskos et al. and\nFearnhead et al.), we apply an exact simulation based technique for pricing\ncontinuous arithmetic average Asian options in the Black and Scholes framework.\nUnlike existing Monte Carlo methods, we are no longer prone to the\ndiscretization bias resulting from the approximation of continuous time\nprocesses through discrete sampling. Numerical results of simulation studies\nare presented and variance reduction problems are considered.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/0704.1433v3", "link": "http://arxiv.org/abs/0704.1433v3"}, {"title": "Systemic Losses Due to Counter Party Risk in a Stylized Banking System", "summary": "We report a study of a stylized banking cascade model investigating systemic\nrisk caused by counter party failure using liabilities and assets to define\nbanks' balance sheet. In our stylized system, banks can be in two states:\nnormally operating or distressed and the state of a bank changes from normally\noperating to distressed whenever its liabilities are larger than the banks'\nassets. The banks are connected through an interbank lending network and,\nwhenever a bank is distressed, its creditor cannot expect the loan from the\ndistressed bank to be repaid, potentially becoming distressed themselves. We\nsolve the problem analytically for a homogeneous system and test the robustness\nand generality of the results with simulations of more complex systems. We\ninvestigate the parameter space and the corresponding distribution of operating\nbanks mapping the conditions under which the whole system is stable or\nunstable. This allows us to determine how financial stability of a banking\nsystem is influenced by regulatory decisions, such as leverage; we discuss the\neffect of central bank actions, such as quantitative easing and we determine\nthe cost of rescuing a distressed banking system using re-capitalisation.\nFinally, we estimate the stability of the UK and US banking systems in the\nyears 2007 and 2012 showing that both banking systems were more unstable in\n2007 and connectedness on the interbank market partly caused the banking\ncrisis.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1402.3688v1", "link": "http://dx.doi.org/10.1007/s10955-014-1040-9"}, {"title": "Electoral systems and international trade policy", "summary": "We develop a simple theoretic game a model to analyze the relationship\nbetween electoral sys tems and governments' choice in trade policies. We show\nthat existence of international pressure or foreign lobby changes a\ngovernment's final decision on trade policy, and trade policy in countries with\nproportional electoral system is more protectionist than in countries with\nmajoritarian electoral system. Moreover, lobbies pay more to affect the trade\npolicy outcomes in countries with proportional representation systems.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.05725v1", "link": "http://arxiv.org/abs/2003.05725v1"}, {"title": "The origin of fat tailed distributions in financial time series", "summary": "A classic problem in physics is the origin of fat tailed distributions\ngenerated by complex systems. We study the distributions of stock returns\nmeasured over different time lags $\\tau.$ We find that destroying all\ncorrelations without changing the $\\tau = 1$ d distribution, by shuffling the\norder of the daily returns, causes the fat tails almost to vanish for $\\tau>1$\nd. We argue that the fat tails are caused by known long-range volatility\ncorrelations. Indeed, destroying only sign correlations, by shuffling the order\nof only the signs (but not the absolute values) of the daily returns, allows\nthe fat tails to persist for $\\tau >1$ d.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0112484v4", "link": "http://arxiv.org/abs/cond-mat/0112484v4"}, {"title": "Super-exponential bubbles in lab experiments: evidence for anchoring\n  over-optimistic expectations on price", "summary": "We analyze a controlled price formation experiment in the laboratory that\nshows evidence for bubbles. We calibrate two models that demonstrate with high\nstatistical significance that these laboratory bubbles have a tendency to grow\nfaster than exponential due to positive feedback. We show that the positive\nfeedback operates by traders continuously upgrading their over-optimistic\nexpectations of future returns based on past prices rather than on realized\nreturns.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1205.0635v1", "link": "http://arxiv.org/abs/1205.0635v1"}, {"title": "A Moral Framework for Understanding of Fair ML through Economic Models\n  of Equality of Opportunity", "summary": "We map the recently proposed notions of algorithmic fairness to economic\nmodels of Equality of opportunity (EOP)---an extensively studied ideal of\nfairness in political philosophy. We formally show that through our conceptual\nmapping, many existing definition of algorithmic fairness, such as predictive\nvalue parity and equality of odds, can be interpreted as special cases of EOP.\nIn this respect, our work serves as a unifying moral framework for\nunderstanding existing notions of algorithmic fairness. Most importantly, this\nframework allows us to explicitly spell out the moral assumptions underlying\neach notion of fairness, and interpret recent fairness impossibility results in\na new light. Last but not least and inspired by luck egalitarian models of EOP,\nwe propose a new family of measures for algorithmic fairness. We illustrate our\nproposal empirically and show that employing a measure of algorithmic\n(un)fairness when its underlying moral assumptions are not satisfied, can have\ndevastating consequences for the disadvantaged group's welfare.", "category": [], "id": "http://arxiv.org/abs/1809.03400v2", "link": "http://arxiv.org/abs/1809.03400v2"}, {"title": "Resource and Competence (Internal) View vs. Environment and Market\n  (External) View when defining a Business", "summary": "Startups is a popular phenomenon that has a significant impact on global\neconomy growth, innovation and society development. However, there is still\ninsufficient understanding about startups, particularly, how to start a new\nbusiness in the relation to consequent performance. Toward this knowledge, we\nhave performed an empirical study regarding the differences between a Resource\nand Competence View (Internal) vs Environment and Market View (External) when\ndefining a Business. 701 entrepreneurs have reflected on their startups on nine\nclasses of Resources (values, vision, personal objectives, employees and\npartners, buildings and rental contracts, cash and credit, patents, IPR's and\nbrands, products and services and finally revenues and grants) and three\nelements of the Business Mission (\"KeyContribution\", \"KeyMarket\" and\n\"Distinction\"). It seems to be a tendency to favour the Internal View over the\nExternal View. This tendency is clearer in Stable Economies (Europe) than in\nEmerging Economies (South Africa). There seems to be a co-variation between the\ntendency to favour the Internal View and the tendency to focus on adding\nResources. Finally, we found that an order-based analysis seems to explain the\ndifferences between the two views better than a number-based method.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1809.01487v1", "link": "http://arxiv.org/abs/1809.01487v1"}, {"title": "An Equilibrium Model with Computationally Constrained Agents", "summary": "We study a large economy in which firms cannot compute exact solutions to the\nnon-linear equations that characterize the equilibrium price at which they can\nsell future output. Instead, firms use polynomial expansions to approximate\nprices. The precision with which they can compute prices is endogenous and\ndepends on the overall level of supply. At the same time, firms' individual\nsupplies, and thus aggregate supply, depend on the precision with which they\napproximate prices. This interrelation between supply and price forecast\ninduces multiple equilibria, with inefficiently low output, in economies that\notherwise have a unique, efficient equilibrium. Moreover, exogenous parameter\nchanges, which would increase output were there no computational frictions, can\ndiminish agents' ability to approximate future prices, and reduce output. Our\nmodel therefore accommodates the intuition that interventions, such as\nunprecedented quantitative easing, can put agents into \"uncharted territory\".", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1611.01771v1", "link": "http://arxiv.org/abs/1611.01771v1"}, {"title": "Mathematical models describing the effects of different tax evasion\n  behaviors", "summary": "Microscopic models describing a whole of economic interactions in a closed\nsociety are considered. The presence of a tax system combined with a\nredistribution process is taken into account, as well as the occurrence of tax\nevasion. In particular, the existence is postulated, in relation to the level\nof evasion, of different individual taxpayer behaviors. The effects of the\nmentioned different behaviors on shape and features of the emerging income\ndistribution profile are investigated qualitatively and quantitatively.\nNumerical solutions show that the Gini inequality index of the total population\nincreases when the evasion level is higher, but does not depend significantly\non the evasion spread. For fixed spread, the relative difference between the\naverage incomes of the worst evaders and honest taxpayers increases\napproximately as a quadratic function of the evasion level.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1701.02662v1", "link": "http://dx.doi.org/10.1007/s11403-016-0185-9"}, {"title": "Quantization-based Bermudan option pricing in the $FX$ world", "summary": "This paper proposes two numerical solution based on Product Optimal\nQuantization for the pricing of Foreign Echange (FX) linked long term Bermudan\noptions e.g. Bermudan Power Reverse Dual Currency options, where we take into\naccount stochastic domestic and foreign interest rates on top of stochastic FX\nrate, hence we consider a 3-factor model. For these two numerical methods, we\ngive an estimation of the $L^2$-error induced by such approximations and we\nillustrate them with market-based examples that highlight the speed of such\nmethods.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1911.05462v2", "link": "http://arxiv.org/abs/1911.05462v2"}, {"title": "Lie Symmetry Analysis of the Black-Scholes-Merton Model for European\n  Options with Stochastic Volatility", "summary": "We perform a classification of the Lie point symmetries for the\nBlack--Scholes--Merton Model for European options with stochastic volatility,\n$\\sigma$, in which the last is defined by a stochastic differential equation\nwith an Orstein--Uhlenbeck term. In this model, the value of the option is\ngiven by a linear (1 + 2) evolution partial differential equation in which the\nprice of the option depends upon two independent variables, the value of the\nunderlying asset, $S$, and a new variable, $y$. We find that for arbitrary\nfunctional form of the volatility, $\\sigma(y)$, the (1 + 2) evolution equation\nalways admits two Lie point symmetries in addition to the automatic linear\nsymmetry and the infinite number of solution symmetries. However, when\n$\\sigma(y)=\\sigma_{0}$ and as the price of the option depends upon the second\nBrownian motion in which the volatility is defined, the (1 + 2) evolution is\nnot reduced to the Black--Scholes--Merton Equation, the model admits five Lie\npoint symmetries in addition to the linear symmetry and the infinite number of\nsolution symmetries. We apply the zeroth-order invariants of the Lie symmetries\nand we reduce the (1 + 2) evolution equation to a linear second-order ordinary\ndifferential equation. Finally, we study two models of special interest, the\nHeston model and the Stein--Stein model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1508.06797v2", "link": "http://dx.doi.org/10.3390/math4020028"}, {"title": "Do Pareto-Zipf and Gibrat laws hold true? An analysis with European\n  Firms", "summary": "By employing exhaustive lists of large firms in European countries, we show\nthat the upper-tail of the distribution of firm size can be fitted with a\npower-law (Pareto-Zipf law), and that in this region the growth rate of each\nfirm is independent of the firm's size (Gibrat's law of proportionate effect).\nWe also find that detailed balance holds in the large-size region for periods\nwe investigated; the empirical probability for a firm to change its size from a\nvalue to another is statistically the same as that for its reverse process. We\nprove several relationships among Pareto-Zipf's law, Gibrat's law and the\ncondition of detailed balance. As a consequence, we show that the distribution\nof growth rate possesses a non-trivial relation between the positive side of\nthe distribution and the negative side, through the value of Pareto index, as\nis confirmed empirically.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0310061v2", "link": "http://dx.doi.org/10.1016/j.physa.2003.12.015"}, {"title": "The converse envelope theorem", "summary": "I prove an envelope theorem with a converse: the envelope formula is\nequivalent to a first-order condition. Like Milgrom and Segal's (2002) envelope\ntheorem, my result requires no structure on the choice set. I use the converse\nenvelope theorem to extend to abstract outcomes the canonical result in\nmechanism design that any increasing allocation is implementable, and apply\nthis to selling information.", "category": [], "id": "http://arxiv.org/abs/1909.11219v2", "link": "http://arxiv.org/abs/1909.11219v2"}, {"title": "Short-time expansions for close-to-the-money options under a L\u00e9vy jump\n  model with stochastic volatility", "summary": "In Figueroa-L\\'opez et al. (2013), a second order approximation for\nat-the-money (ATM) option prices is derived for a large class of exponential\nL\\'evy models, with or without a Brownian component. The purpose of this\narticle is twofold. First, we relax the regularity conditions imposed in\nFigueroa-L\\'opez et al. (2013) on the L\\'evy density to the weakest possible\nconditions for such an expansion to be well defined. Second, we show that the\nformulas extend both to the case of \"close-to-the-money\" strikes and to the\ncase where the continuous Brownian component is replaced by an independent\nstochastic volatility process with leverage.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1404.0601v3", "link": "http://arxiv.org/abs/1404.0601v3"}, {"title": "Activity Dependent Branching Ratios in Stocks, Solar X-ray Flux, and the\n  Bak-Tang-Wiesenfeld Sandpile Model", "summary": "We define an activity dependent branching ratio that allows comparison of\ndifferent time series $X_{t}$. The branching ratio $b_x$ is defined as $b_x=\nE[\\xi_x/x]$. The random variable $\\xi_x$ is the value of the next signal given\nthat the previous one is equal to $x$, so $\\xi_x=\\{X_{t+1}|X_t=x\\}$. If\n$b_x>1$, the process is on average supercritical when the signal is equal to\n$x$, while if $b_x<1$, it is subcritical. For stock prices we find $b_x=1$\nwithin statistical uncertainty, for all $x$, consistent with an ``efficient\nmarket hypothesis''. For stock volumes, solar X-ray flux intensities, and the\nBak-Tang-Wiesenfeld (BTW) sandpile model, $b_x$ is supercritical for small\nvalues of activity and subcritical for the largest ones, indicating a tendency\nto return to a typical value. For stock volumes this tendency has an\napproximate power law behavior. For solar X-ray flux and the BTW model, there\nis a broad regime of activity where $b_x \\simeq 1$, which we interpret as an\nindicator of critical behavior. This is true despite different underlying\nprobability distributions for $X_t$, and for $\\xi_x$. For the BTW model the\ndistribution of $\\xi_x$ is Gaussian, for $x$ sufficiently larger than one, and\nits variance grows linearly with $x$. Hence, the activity in the BTW model\nobeys a central limit theorem when sampling over past histories. The broad\nregion of activity where $b_x$ is close to one disappears once bulk dissipation\nis introduced in the BTW model -- supporting our hypothesis that it is an\nindicator of criticality.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0910.2447v1", "link": "http://dx.doi.org/10.1103/PhysRevE.81.016109"}, {"title": "Efficient computation of mean reverting portfolios using cyclical\n  coordinate descent", "summary": "The econometric challenge of finding sparse mean reverting portfolios based\non a subset of a large number of assets is well known. Many current\nstate-of-the-art approaches fall into the field of co-integration theory, where\nthe problem is phrased in terms of an eigenvector problem with sparsity\nconstraint. Although a number of approximate solutions have been proposed to\nsolve this NP-hard problem, all are based on relatively simple models and are\nlimited in their scalability. In this paper we leverage information obtained\nfrom a heterogeneous simultaneous graphical dynamic linear model (H-SGDLM) and\npropose a novel formulation of the mean reversion problem, which is phrased in\nterms of a quasi-convex minimisation with a normalisation constraint. This new\nformulation allows us to employ a cyclical coordinate descent algorithm for\nefficiently computing an exact sparse solution, even in a large universe of\nassets, while the use of H-SGDLM data allows us to easily control the required\nlevel of sparsity. We demonstrate the flexibility, speed and scalability of the\nproposed approach on S\\&P$500$, FX and ETF futures data.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1905.05841v1", "link": "http://arxiv.org/abs/1905.05841v1"}, {"title": "Insurance, Reinsurance and Dividend Payment", "summary": "The aim of this paper is to introduce an insurance model allowing reinsurance\nand dividend payment. Our model deals with several homogeneous contracts and\ntakes into account the legislation regarding the provisions to be justified by\nthe insurance companies. This translates into some restriction on the (maximal)\nnumber of contracts the company is allowed to cover. We deal with a controlled\njump process in which one has free choice of retention level and dividend\namount. The value function is given as the maximized expected discounted\ndividends. We prove that this value function is a viscosity solution of some\nfirst-order Hamilton-Jacobi-Bellman variational inequality. Moreover, a\nuniqueness result is provided.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0804.3900v1", "link": "http://arxiv.org/abs/0804.3900v1"}, {"title": "How Traders enter the Market through the Book", "summary": "Simulation of the trading activity based on the implementation of the book.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0103106v1", "link": "http://arxiv.org/abs/cond-mat/0103106v1"}, {"title": "VWAP execution and guaranteed VWAP", "summary": "Optimal liquidation using VWAP strategies has been considered in the\nliterature, though never in the presence of permanent market impact and only\nrarely with execution costs. Moreover, only VWAP strategies have been studied\nand the pricing of guaranteed VWAP contracts has never been addressed. In this\narticle, we develop a model to price guaranteed VWAP contracts in a general\nframework for market impact and we highlight the differences between an agency\nVWAP and a guaranteed VWAP contract. Numerical methods and applications are\nalso provided.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1306.2832v4", "link": "http://arxiv.org/abs/1306.2832v4"}, {"title": "Accounting for risk of non linear portfolios: a novel Fourier approach", "summary": "The presence of non linear instruments is responsible for the emergence of\nnon Gaussian features in the price changes distribution of realistic\nportfolios, even for Normally distributed risk factors. This is especially true\nfor the benchmark Delta Gamma Normal model, which in general exhibits\nexponentially damped power law tails. We show how the knowledge of the model\ncharacteristic function leads to Fourier representations for two standard risk\nmeasures, the Value at Risk and the Expected Shortfall, and for their\nsensitivities with respect to the model parameters. We detail the numerical\nimplementation of our formulae and we emphasizes the reliability and efficiency\nof our results in comparison with Monte Carlo simulation.", "category": ["q-fin.RM", "q-fin.PM"], "id": "http://arxiv.org/abs/1002.4817v3", "link": "http://dx.doi.org/10.1140/epjb/e2010-00199-9"}, {"title": "A Note on Delta Hedging in Markets with Jumps", "summary": "Modelling stock prices via jump processes is common in financial markets. In\npractice, to hedge a contingent claim one typically uses the so-called\ndelta-hedging strategy. This strategy stems from the Black--Merton--Scholes\nmodel where it perfectly replicates contingent claims. From the theoretical\nviewpoint, there is no reason for this to hold in models with jumps. However in\npractice the delta-hedging strategy is widely used and its potential\nshortcoming in models with jumps is disregarded since such models are typically\nincomplete and hence most contingent claims are non-attainable. In this note we\ninvestigate a complete model with jumps where the delta-hedging strategy is\nwell-defined for regular payoff functions and is uniquely determined via the\nrisk-neutral measure. In this setting we give examples of (admissible)\ndelta-hedging strategies with bounded discounted value processes, which\nnevertheless fail to replicate the respective bounded contingent claims. This\ndemonstrates that the deficiency of the delta-hedging strategy in the presence\nof jumps is not due to the incompleteness of the model but is inherent in the\ndiscontinuity of the trajectories.", "category": ["q-fin.PR", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1103.4965v1", "link": "http://arxiv.org/abs/1103.4965v1"}, {"title": "Pricing Asian options for NIG and VG Levy markets", "summary": "In this work, we study the value of an Asian option in the case of\nexponential Levy markets. More specifically, we are interested in the NIG\n(normal inverse Gaussian) the VG (variance gamma) models. The exponential Levy\nmodels produce incomplete markets. There are therefore an infinite number of\nequivalent martingale measures. We are interested in two methods of\nconstructing of the risk-neutral measures. The first is based on the Esscher\ntransform, and the other consists of bringing a risk-neutral correction on the\ndynamics of the trajectories. It turns out, according to the numerical results\nobtained, that the two methods generally produce the same prices.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1706.01562v1", "link": "http://arxiv.org/abs/1706.01562v1"}, {"title": "Heterogeneous volatility cascade in financial markets", "summary": "Using high frequency data, we have studied empirically the change of\nvolatility, also called volatility derivative, for various time horizons. In\nparticular, the correlation between the volatility derivative and the\nvolatility realized in the next time period is a measure of the response\nfunction of the market participants. This correlation shows explicitly the\nheterogeneous structure of the market according to the characteristic time\nhorizons of the differents agents. It reveals a volatility cascade from long to\nshort time horizons, with a structure different from the one observed in\nturbulence. Moreover, we have developed a new ARCH-type model which\nincorporates the different groups of agents, with their characteristic memory.\nThis model reproduces well the empirical response function, and allows us to\nquantify the importance of each group.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0105162v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00249-7"}, {"title": "Statistical Properties and Pre-hit Dynamics of Price Limit Hits in the\n  Chinese Stock Markets", "summary": "Price limit trading rules are adopted in some stock markets (especially\nemerging markets) trying to cool off traders' short-term trading mania on\nindividual stocks and increase market efficiency. Under such a microstructure,\nstocks may hit their up-limits and down-limits from time to time. However, the\nbehaviors of price limit hits are not well studied partially due to the fact\nthat main stock markets such as the US markets and most European markets do not\nset price limits. Here, we perform detailed analyses of the high-frequency data\nof all A-share common stocks traded on the Shanghai Stock Exchange and the\nShenzhen Stock Exchange from 2000 to 2011 to investigate the statistical\nproperties of price limit hits and the dynamical evolution of several important\nfinancial variables before stock price hits its limits. We compare the\nproperties of up-limit hits and down-limit hits. We also divide the whole\nperiod into three bullish periods and three bearish periods to unveil possible\ndifferences during bullish and bearish market states. To uncover the impacts of\nstock capitalization on price limit hits, we partition all stocks into six\nportfolios according to their capitalizations on different trading days. We\nfind that the price limit trading rule has a cooling-off effect (object to the\nmagnet effect), indicating that the rule takes effect in the Chinese stock\nmarkets. We find that price continuation is much more likely to occur than\nprice reversal on the next trading day after a limit-hitting day, especially\nfor down-limit hits, which has potential practical values for market\npractitioners.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1503.03548v1", "link": "http://dx.doi.org/10.1371/journal.pone.0120312"}, {"title": "Universal Fluctuations of AEX index", "summary": "We compute the analytic expression of the probability distributions F{AEX,+}\nand F{AEX,-} of the normalized positive and negative AEX (Netherlands) index\ndaily returns r(t). Furthermore, we define the \\alpha re-scaled AEX daily index\npositive returns r(t)^\\alpha and negative returns (-r(t))^\\alpha that we call,\nafter normalization, the \\alpha positive fluctuations and \\alpha negative\nfluctuations. We use the Kolmogorov-Smirnov statistical test, as a method, to\nfind the values of \\alpha that optimize the data collapse of the histogram of\nthe \\alpha fluctuations with the Bramwell-Holdsworth-Pinton (BHP) probability\ndensity function. The optimal parameters that we found are \\alpha+=0.46 and\n\\alpha-=0.43. Since the BHP probability density function appears in several\nother dissimilar phenomena, our results reveal universality in the stock\nexchange markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1004.1210v2", "link": "http://dx.doi.org/10.1016/j.physa.2010.06.012"}, {"title": "A Peer-based Model of Fat-tailed Outcomes", "summary": "It is well known that the distribution of returns from various financial\ninstruments are leptokurtic, meaning that the distributions have \"fatter tails\"\nthan a Normal distribution, and have skew toward zero. This paper presents a\ngraceful micro-level explanation for such fat-tailed outcomes, using agents\nwhose private valuations have Normally-distributed errors, but whose utility\nfunction includes a term for the percentage of others who also buy.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1304.0718v1", "link": "http://arxiv.org/abs/1304.0718v1"}, {"title": "Change of drift in one-dimensional diffusions", "summary": "It is generally understood that a given one-dimensional diffusion may be\ntransformed by Cameron-Martin-Girsanov measure change into another\none-dimensional diffusion with the same volatility but a different drift. But\nto achieve this we have to know that the change-of-measure local martingale\nthat we write down is a true martingale; we provide a complete characterization\nof when this happens. This is then used to discuss absence of arbitrage in a\ngeneralized Heston model including the case where the Feller condition for the\nvolatility process is violated.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1910.11904v2", "link": "http://arxiv.org/abs/1910.11904v2"}, {"title": "On Existence of Equilibrium Under Social Coalition Structures", "summary": "In a strategic form game a strategy profile is an equilibrium if no viable\ncoalition of agents (or players) benefits (in the Pareto sense) from jointly\nchanging their strategies. Weaker or stronger equilibrium notions can be\ndefined by considering various restrictions on coalition formation. In a Nash\nequilibrium, for instance, the assumption is that viable coalitions are\nsingletons, and in a super strong equilibrium, every coalition is viable.\nRestrictions on coalition formation can be justified by communication\nlimitations, coordination problems or institutional constraints. In this paper,\ninspired by social structures in various real-life scenarios, we introduce\ncertain restrictions on coalition formation, and on their basis we introduce a\nnumber of equilibrium notions. As an application we study our equilibrium\nnotions in resource selection games (RSGs), and we present a complete set of\nexistence and non-existence results for general RSGs and their important\nspecial cases.", "category": [], "id": "http://arxiv.org/abs/1910.04648v1", "link": "http://arxiv.org/abs/1910.04648v1"}, {"title": "The loss of interest for the euro in Romania", "summary": "We generalize a money demand micro-founded model to explain Romanians' recent\nloss of interest for the euro. We show that the reason behind this loss of\ninterest is a severe decline in the relative degree of the euro liquidity\nagainst that of the Romanian leu.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1609.01900v1", "link": "http://arxiv.org/abs/1609.01900v1"}, {"title": "Optimal portfolio for a robust financial system", "summary": "This study presents an ANWSER model (asset network systemic risk model) to\nquantify the risk of financial contagion which manifests itself in a financial\ncrisis. The transmission of financial distress is governed by a heterogeneous\nbank credit network and an investment portfolio of banks. Bankruptcy\nreproductive ratio of a financial system is computed as a function of the\ndiversity and risk exposure of an investment portfolio of banks, and the\ndenseness and concentration of a heterogeneous bank credit network. An analytic\nsolution of the bankruptcy reproductive ratio for a small financial system is\nderived and a numerical solution for a large financial system is obtained. For\na large financial system, Large diversity among banks in the investment\nportfolio makes financial contagion more damaging on the average. But large\ndiversity is essentially effective in eliminating the risk of financial\ncontagion in the worst case of financial crisis scenarios. A bank-unique\nspecialization portfolio is more suitable than a uniform diversification\nportfolio and a system-wide specialization portfolio in strengthening the\nrobustness of a financial system.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1211.5235v2", "link": "http://dx.doi.org/10.1109/CIFEr.2013.6611695"}, {"title": "Some Applications of Lie Groups in Theory of Technical Progress", "summary": "In recent decades, we have known some interesting applications of Lie theory\nin the theory of technological progress. Firstly, we will discuss some results\nof R. Saito in \\cite{rS1980} and \\cite{rS1981} about the application modeling\nof Lie groups in the theory of technical progress. Next, we will describe the\nresult on Romanian economy of G. Zaman and Z. Goschin in \\cite{ZG2010}.\nFinally, by using Sato's results and applying the method of G. Zaman and Z.\nGoschin, we give an estimation of the GDP function of Viet Nam for the\n1995-2018 period and give several important observations about the impact of\ntechnical progress on economic growth of Viet Nam.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2004.11118v1", "link": "http://arxiv.org/abs/2004.11118v1"}, {"title": "Fixed-Effect Regressions on Network Data", "summary": "This paper considers inference on fixed effects in a linear regression model\nestimated from network data. An important special case of our setup is the\ntwo-way regression model. This is a workhorse technique in the analysis of\nmatched data sets, such as employer-employee or student-teacher panel data. We\nformalize how the structure of the network affects the accuracy with which the\nfixed effects can be estimated. This allows us to derive sufficient conditions\non the network for consistent estimation and asymptotically-valid inference to\nbe possible. Estimation of moments is also considered. We allow for general\nnetworks and our setup covers both the dense and sparse case. We provide\nnumerical results for the estimation of teacher value-added models and\nregressions with occupational dummies.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1608.01532v4", "link": "http://arxiv.org/abs/1608.01532v4"}, {"title": "Implied volatility smile dynamics in the presence of jumps", "summary": "The main purpose of this work is to examine the behavior of the implied\nvolatility smiles around jumps, contributing to the literature with a\nhigh-frequency analysis of the smile dynamics based on intra-day option data.\nFrom our high-frequency SPX S\\&P500 index option dataset, we utilize the first\nthree principal components to characterize the implied volatility smile and\nanalyze its dynamics by the distribution of the scores' means and variances and\nother statistics for the first hour of the day, in scenarios where jumps are\ndetected and not. Our analyses clearly suggest that changes in the volatility\nsmiles have abnormal properties around jumps compared with the absence of\njumps, regardless of maturity and type of the option.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1711.02925v2", "link": "http://arxiv.org/abs/1711.02925v2"}, {"title": "Equilibrium Pricing in an Order Book Environment: Case Study for a Spin\n  Model", "summary": "When modelling stock market dynamics, the price formation is often based on\nan equilbrium mechanism. In real stock exchanges, however, the price formation\nis goverend by the order book. It is thus interesting to check if the resulting\nstylized facts of a model with equilibrium pricing change, remain the same or,\nmore generally, are compatible with the order book environment. We tackle this\nissue in the framework of a case study by embedding the\nBornholdt-Kaizoji-Fujiwara spin model into the order book dynamics. To this\nend, we use a recently developed agent based model that realistically\nincorporates the order book. We find realistic stylized facts. We conclude for\nthe studied case that equilibrium pricing is not needed and that the\ncorresponding assumption of a \"fundamental\" price may be abandoned.", "category": ["q-fin.CP", "q-fin.TR"], "id": "http://arxiv.org/abs/1502.01125v1", "link": "http://dx.doi.org/10.1016/j.physa.2016.01.073"}, {"title": "Long memory stochastic volatility in option pricing", "summary": "The aim of this paper is to present a simple stochastic model that accounts\nfor the effects of a long-memory in volatility on option pricing. The starting\npoint is the stochastic Black-Scholes equation involving volatility with\nlong-range dependence. We consider the option price as a sum of classical\nBlack-Scholes price and random deviation describing the risk from the random\nvolatility. By using the fact the option price and random volatility change on\ndifferent time scales, we find the asymptotic equation for the derivation\ninvolving fractional Brownian motion. The solution to this equation allows us\nto find the pricing bands for options.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/0403761v3", "link": "http://arxiv.org/abs/cond-mat/0403761v3"}, {"title": "Global labor flow network reveals the hierarchical organization and\n  dynamics of geo-industrial clusters in the world economy", "summary": "Groups of firms often achieve a competitive advantage through the formation\nof geo-industrial clusters. Although many exemplary clusters, such as Hollywood\nor Silicon Valley, have been frequently studied, systematic approaches to\nidentify and analyze the hierarchical structure of the geo-industrial clusters\nat the global scale are rare. In this work, we use LinkedIn's employment\nhistories of more than 500 million users over 25 years to construct a labor\nflow network of over 4 million firms across the world and apply a recursive\nnetwork community detection algorithm to reveal the hierarchical structure of\ngeo-industrial clusters. We show that the resulting geo-industrial clusters\nexhibit a stronger association between the influx of educated-workers and\nfinancial performance, compared to existing aggregation units. Furthermore, our\nadditional analysis of the skill sets of educated-workers supplements the\nrelationship between the labor flow of educated-workers and productivity\ngrowth. We argue that geo-industrial clusters defined by labor flow provide\nbetter insights into the growth and the decline of the economy than other\ncommon economic units.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1902.04613v4", "link": "http://dx.doi.org/10.1038/s41467-019-11380-w"}, {"title": "Cluster analysis for portfolio optimization", "summary": "We consider the problem of the statistical uncertainty of the correlation\nmatrix in the optimization of a financial portfolio. We show that the use of\nclustering algorithms can improve the reliability of the portfolio in terms of\nthe ratio between predicted and realized risk. Bootstrap analysis indicates\nthat this improvement is obtained in a wide range of the parameters N (number\nof assets) and T (investment horizon). The predicted and realized risk level\nand the relative portfolio composition of the selected portfolio for a given\nvalue of the portfolio return are also investigated for each considered\nfiltering method.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0507006v1", "link": "http://arxiv.org/abs/physics/0507006v1"}, {"title": "Super-Replication of the Best Pairs Trade in Hindsight", "summary": "This paper derives a robust on-line equity trading algorithm that achieves\nthe greatest possible percentage of the final wealth of the best pairs\nrebalancing rule in hindsight. A pairs rebalancing rule chooses some pair of\nstocks in the market and then perpetually executes rebalancing trades so as to\nmaintain a target fraction of wealth in each of the two. After each discrete\nmarket fluctuation, a pairs rebalancing rule will sell a precise amount of the\noutperforming stock and put the proceeds into the underperforming stock. Under\ntypical conditions, in hindsight one can find pairs rebalancing rules that\nwould have spectacularly beaten the market. Our trading strategy, which extends\nOrdentlich and Cover's (1998) \"max-min universal portfolio,\" guarantees to\nachieve an acceptable percentage of the hindsight-optimized wealth, a\npercentage which tends to zero at a slow (polynomial) rate. This means that on\na long enough investment horizon, the trader can enforce a compound-annual\ngrowth rate that is arbitrarily close to that of the best pairs rebalancing\nrule in hindsight. The strategy will \"beat the market asymptotically\" if there\nturns out to exist a pairs rebalancing rule that grows capital at a higher\nasymptotic rate than the market index. The advantages of our algorithm over the\nOrdentlich and Cover (1998) strategy are twofold. First, their strategy is\nimpossible to compute in practice. Second, in considering the more modest\nbenchmark (instead of the best all-stock rebalancing rule in hindsight), we\nreduce the \"cost of universality\" and achieve a higher learning rate.", "category": ["q-fin.PM", "econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1810.02444v3", "link": "http://arxiv.org/abs/1810.02444v3"}, {"title": "Counterfactual Inference in Duration Models with Random Censoring", "summary": "We propose a counterfactual Kaplan-Meier estimator that incorporates\nexogenous covariates and unobserved heterogeneity of unrestricted\ndimensionality in duration models with random censoring. Under some regularity\nconditions, we establish the joint weak convergence of the proposed\ncounterfactual estimator and the unconditional Kaplan-Meier (1958) estimator.\nApplying the functional delta method, we make inference on the cumulative\nhazard policy effect, that is, the change of duration dependence in response to\na counterfactual policy. We also evaluate the finite sample performance of the\nproposed counterfactual estimation method in a Monte Carlo study.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1902.08502v1", "link": "http://arxiv.org/abs/1902.08502v1"}, {"title": "Perturbation Expansion for Option Pricing with Stochastic Volatility", "summary": "We fit the volatility fluctuations of the S&P 500 index well by a Chi\ndistribution, and the distribution of log-returns by a corresponding\nsuperposition of Gaussian distributions. The Fourier transform of this is,\nremarkably, of the Tsallis type. An option pricing formula is derived from the\nsame superposition of Black-Scholes expressions. An explicit analytic formula\nis deduced from a perturbation expansion around a Black-Scholes formula with\nthe mean volatility. The expansion has two parts. The first takes into account\nthe non-Gaussian character of the stock-fluctuations and is organized by powers\nof the excess kurtosis, the second is contract based, and is organized by the\nmoments of moneyness of the option. With this expansion we show that for the\nDow Jones Euro Stoxx 50 option data, a Delta-hedging strategy is close to being\noptimal.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0708.3012v1", "link": "http://dx.doi.org/10.1016/j.physa.2009.04.027"}, {"title": "When do improved covariance matrix estimators enhance portfolio\n  optimization? An empirical comparative study of nine estimators", "summary": "The use of improved covariance matrix estimators as an alternative to the\nsample estimator is considered an important approach for enhancing portfolio\noptimization. Here we empirically compare the performance of 9 improved\ncovariance estimation procedures by using daily returns of 90 highly\ncapitalized US stocks for the period 1997-2007. We find that the usefulness of\ncovariance matrix estimators strongly depends on the ratio between estimation\nperiod T and number of stocks N, on the presence or absence of short selling,\nand on the performance metric considered. When short selling is allowed,\nseveral estimation methods achieve a realized risk that is significantly\nsmaller than the one obtained with the sample covariance method. This is\nparticularly true when T/N is close to one. Moreover many estimators reduce the\nfraction of negative portfolio weights, while little improvement is achieved in\nthe degree of diversification. On the contrary when short selling is not\nallowed and T>N, the considered methods are unable to outperform the sample\ncovariance in terms of realized risk but can give much more diversified\nportfolios than the one obtained with the sample covariance. When T<N the use\nof the sample covariance matrix and of the pseudoinverse gives portfolios with\nvery poor performance.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1004.4272v1", "link": "http://arxiv.org/abs/1004.4272v1"}, {"title": "Maximum Entropy Framework for a Universal Rank Order distribution with\n  Socio-economic Applications", "summary": "In this paper we derive the maximum entropy characteristics of a particular\nrank order distribution, namely the discrete generalized beta distribution,\nwhich has recently been observed to be extremely useful in modelling many\nseveral rank-size distributions from different context in Arts and Sciences, as\na two-parameter generalization of Zipf's law. Although it has been seen to\nprovide excellent fits for several real world empirical datasets, the\nunderlying theory responsible for the success of this particular rank order\ndistribution is not explored properly. Here we, for the first time, provide its\ngenerating process which describes it as a natural maximum entropy distribution\nunder an appropriate bivariate utility constraint. Further, considering the\nsimilarity of the proposed utility function with the usual logarithmic utility\nfunction from economic literature, we have also explored its acceptability in\nuniversal modeling of different types of socio-economic factors within a\ncountry as well as across the countries. The values of distributional\nparameters estimated through a rigorous statistical estimation method, along\nwith the $entropy$ values, are used to characterize the distributions of all\nthese socio-economic factors over the years.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1909.12542v1", "link": "http://arxiv.org/abs/1909.12542v1"}, {"title": "Information theoretic approach for accounting classification", "summary": "In this paper we consider an information theoretic approach for the\naccounting classification process. We propose a matrix formalism and an\nalgorithm for calculations of information theoretic measures associated to\naccounting classification. The formalism may be useful for further\ngeneralizations and computer-based implementation. Information theoretic\nmeasures, mutual information and symmetric uncertainty, were evaluated for\ndaily transactions recorded in the chart of accounts of a small company during\ntwo years. Variation in the information measures due the aggregation of data in\nthe process of accounting classification is observed. In particular, the\nsymmetric uncertainty seems to be a useful parameter for comparing companies\nover time or in different sectors or different accounting choices and\nstandards.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1401.2954v4", "link": "http://dx.doi.org/10.1016/j.physa.2014.09.014"}, {"title": "Look-Ahead Benchmark Bias in Portfolio Performance Evaluation", "summary": "Performance of investment managers are evaluated in comparison with\nbenchmarks, such as financial indices. Due to the operational constraint that\nmost professional databases do not track the change of constitution of\nbenchmark portfolios, standard tests of performance suffer from the \"look-ahead\nbenchmark bias,\" when they use the assets constituting the benchmarks of\nreference at the end of the testing period, rather than at the beginning of the\nperiod. Here, we report that the \"look-ahead benchmark bias\" can exhibit a\nsurprisingly large amplitude for portfolios of common stocks (up to 8% annum\nfor the S&P500 taken as the benchmark) -- while most studies have emphasized\nrelated survival biases in performance of mutual and hedge funds for which the\nbiases can be expected to be even larger. We use the CRSP database from 1926 to\n2006 and analyze the running top 500 US capitalizations to demonstrate that\nthis bias can account for a gross overestimation of performance metrics such as\nthe Sharpe ratio as well as an underestimation of risk, as measured for\ninstance by peak-to-valley drawdowns. We demonstrate the presence of a\nsignificant bias in the estimation of the survival and look-ahead biases\nstudied in the literature. A general methodology to test the properties of\ninvestment strategies is advanced in terms of random strategies with similar\ninvestment constraints.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/0810.1922v1", "link": "http://arxiv.org/abs/0810.1922v1"}, {"title": "A backward Monte Carlo approach to exotic option pricing", "summary": "We propose a novel algorithm which allows to sample paths from an underlying\nprice process in a local volatility model and to achieve a substantial variance\nreduction when pricing exotic options. The new algorithm relies on the\nconstruction of a discrete multinomial tree. The crucial feature of our\napproach is that -- in a similar spirit to the Brownian Bridge -- each random\npath runs backward from a terminal fixed point to the initial spot price. We\ncharacterize the tree in two alternative ways: in terms of the optimal grids\noriginating from the Recursive Marginal Quantization algorithm and following an\napproach inspired by the finite difference approximation of the diffusion's\ninfinitesimal generator. We assess the reliability of the new methodology\ncomparing the performance of both approaches and benchmarking them with\ncompetitor Monte Carlo methods.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1511.00848v1", "link": "http://arxiv.org/abs/1511.00848v1"}, {"title": "Awareness of crash risk improves Kelly strategies in simulated financial\n  time series", "summary": "We simulate a simplified version of the price process including bubbles and\ncrashes proposed in Kreuser and Sornette (2018). The price process is defined\nas a geometric random walk combined with jumps modelled by separate, discrete\ndistributions associated with positive (and negative) bubbles. The key\ningredient of the model is to assume that the sizes of the jumps are\nproportional to the bubble size. Thus, the jumps tend to efficiently bring back\nexcess bubble prices close to a normal or fundamental value (efficient\ncrashes). This is different from existing processes studied that assume jumps\nthat are independent of the mispricing. The present model is simplified\ncompared to Kreuser and Sornette (2018) in that we ignore the possibility of a\nchange of the probability of a crash as the price accelerates above the normal\nprice. We study the behaviour of investment strategies that maximize the\nexpected log of wealth (Kelly criterion) for the risky asset and a risk-free\nasset. We show that the method behaves similarly to Kelly on Geometric Brownian\nMotion in that it outperforms other methods in the long-term and it beats\nclassical Kelly. As a primary source of outperformance, we determine knowledge\nabout the presence of crashes, but interestingly find that knowledge of only\nthe size, and not the time of occurrence, already provides a significant and\nrobust edge. We then perform an error analysis to show that the method is\nrobust with respect to variations in the parameters. The method is most\nsensitive to errors in the expected return.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.09368v1", "link": "http://arxiv.org/abs/2004.09368v1"}, {"title": "Generational political dynamics of retirement pensions systems: An agent\n  based model", "summary": "The increasing difficulties in financing the welfare state and in particular\npublic retirement pensions have been one of the outcomes both of the decrease\nof fertility and birth rates combined with the increase of life expectancy. The\ndynamics of retirement pensions are usually studied in Economics using\noverlapping generation models. These models are based on simplifying\nassumptions like the use of a representative agent to ease the problem of\ntractability. Alternatively, we propose to use agent-based modelling (ABM),\nrelaxing the need for those assumptions and enabling the use of interacting and\nheterogeneous agents assigning special importance to the study of\ninter-generational relations. We treat pension dynamics both in economics and\npolitical perspectives. The model we build, following the ODD protocol, will\ntry to understand the dynamics of choice of public versus private retirement\npensions resulting from the conflicting preferences of different agents but\nalso from the cooperation between them. The aggregation of these individual\npreferences is done by voting. We combine a microsimulation approach following\nthe evolution of synthetic populations along time, with the ABM approach\nstudying the interactions between the different agent types. Our objective is\nto depict the conditions for the survival of the public pensions system\nemerging from the relation between egoistic and altruistic individual and\ncollective behaviours.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.08706v1", "link": "http://arxiv.org/abs/1909.08706v1"}, {"title": "Fluctuation analysis of electric power loads in Europe: Correlation\n  multifractality vs. Distribution function multifractality", "summary": "We analyze the time series of the power loads of the 35 separated countries\npublicly sharing hourly data through ENTSO-E platform for more than 5 years. We\napply the Multifractal Detrended Fluctuation Analysis for the demonstration of\nthe multifractal nature, autocorrelation and the distribution function\nfundamentals. Additionally, we improved the basic method described by\nKanterhardt, et al using uniform shuffling and surrogate the datasets to prove\nthe robustness of the results with respect to the non-linear effects of the\nprocesses. All the datasets exhibit multifractality in the distribution\nfunction as well as in the autocorrelation function. The basic differences\nbetween individual states are manifested in the width of the multifractal\nspectra and in the location of the maximum. We present the hypothesis about the\nproduction portfolio and the export/import dependences.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1706.00467v1", "link": "http://arxiv.org/abs/1706.00467v1"}, {"title": "Scaling and memory of intraday volatility return intervals in stock\n  market", "summary": "We study the return interval $\\tau$ between price volatilities that are above\na certain threshold $q$ for 31 intraday datasets, including the Standard &\nPoor's 500 index and the 30 stocks that form the Dow Jones Industrial index.\nFor different threshold $q$, the probability density function $P_q(\\tau)$\nscales with the mean interval $\\bar{\\tau}$ as\n$P_q(\\tau)={\\bar{\\tau}}^{-1}f(\\tau/\\bar{\\tau})$, similar to that found in daily\nvolatilities. Since the intraday records have significantly more data points\ncompared to the daily records, we could probe for much higher thresholds $q$\nand still obtain good statistics. We find that the scaling function $f(x)$ is\nconsistent for all 31 intraday datasets in various time resolutions, and the\nfunction is well approximated by the stretched exponential, $f(x)\\sim e^{-a\nx^\\gamma}$, with $\\gamma=0.38\\pm 0.05$ and $a=3.9\\pm 0.5$, which indicates the\nexistence of correlations. We analyze the conditional probability distribution\n$P_q(\\tau|\\tau_0)$ for $\\tau$ following a certain interval $\\tau_0$, and find\n$P_q(\\tau|\\tau_0)$ depends on $\\tau_0$, which demonstrates memory in intraday\nreturn intervals. Also, we find that the mean conditional interval\n$<\\tau|\\tau_0>$ increases with $\\tau_0$, consistent with the memory found for\n$P_q(\\tau|\\tau_0)$. Moreover, we find that return interval records have long\nterm correlations with correlation exponents similar to that of volatility\nrecords.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0511101v1", "link": "http://dx.doi.org/10.1103/PhysRevE.73.026117"}, {"title": "Short-term Market Reaction after Trading Halts in Chinese Stock Market", "summary": "In this paper, we study the dynamics of absolute return, trading volume and\nbid-ask spread after the trading halts using high-frequency data from the\nShanghai Stock Exchange. We deal with all three types of trading halts, namely\nintraday halts, one-day halts and inter-day halts, of 203 stocks in Shanghai\nStock Exchange from August 2009 to August 2011. We find that absolute return,\ntrading volume, and in case of bid-ask spread around intraday halts share the\nsame pattern with a sharp peak and a power law relaxation after that. While for\ndifferent types of trading halts, the peaks' height and the relaxation\nexponents are different. From the perspective of halt reasons or halt duration,\nthe relaxation exponents of absolute return after inter-day halts are larger\nthan that after intraday halts and one-day halts, which implies that inter-day\nhalts are most effective. From the perspective of price trends, the relaxation\nexponents of excess absolute return and excess volume for positive events are\nlarger than that for negative events in case of intraday halts and one-day\nhalts, implying that positive events are more effective than negative events\nfor intraday halts and one-day halts. In contrast, negative events are more\neffective than positive events for inter-day halts.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1309.1138v2", "link": "http://dx.doi.org/10.1016/j.physa.2014.01.044"}, {"title": "The International-Trade Network: Gravity Equations and Topological\n  Properties", "summary": "This paper begins to explore the determinants of the topological properties\nof the international - trade network (ITN). We fit bilateral-trade flows using\na standard gravity equation to build a \"residual\" ITN where trade-link weights\nare depurated from geographical distance, size, border effects, trade\nagreements, and so on. We then compare the topological properties of the\noriginal and residual ITNs. We find that the residual ITN displays, unlike the\noriginal one, marked signatures of a complex system, and is characterized by a\nvery different topological architecture. Whereas the original ITN is\ngeographically clustered and organized around a few large-sized hubs, the\nresidual ITN displays many small-sized but trade-oriented countries that,\nindependently of their geographical position, either play the role of local\nhubs or attract large and rich countries in relatively complex\ntrade-interaction patterns.", "category": ["q-fin.GN", "q-fin.TR"], "id": "http://arxiv.org/abs/0908.2086v1", "link": "http://arxiv.org/abs/0908.2086v1"}, {"title": "Arbitrage strategy", "summary": "An arbitrage strategy allows a financial agent to make certain profit out of\nnothing, i.e., out of zero initial investment. This has to be disallowed on\neconomic basis if the market is in equilibrium state, as opportunities for\nriskless profit would result in an instantaneous movement of prices of certain\nfinancial instruments. The principle of not allowing for arbitrage\nopportunities in financial markets has far-reaching consequences, most notably\nthe option-pricing and hedging formulas in complete markets.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1002.2740v1", "link": "http://arxiv.org/abs/1002.2740v1"}, {"title": "A quantitative model of trading and price formation in financial markets", "summary": "We use standard physics techniques to model trading and price formation in a\nmarket under the assumption that order arrival and cancellations are Poisson\nrandom processes. This model makes testable predictions for the most basic\nproperties of a market, such as the diffusion rate of prices, which is the\nstandard measure of financial risk, and the spread and price impact functions,\nwhich are the main determinants of transaction cost. Guided by dimensional\nanalysis, simulation, and mean field theory, we find scaling relations in terms\nof order flow rates. We show that even under completely random order flow the\nneed to store supply and demand to facilitate trading induces anomalous\ndiffusion and temporal structure in prices.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0112422v6", "link": "http://dx.doi.org/10.1103/PhysRevLett.90.108102"}, {"title": "Synthetic Control Inference for Staggered Adoption: Estimating the\n  Dynamic Effects of Board Gender Diversity Policies", "summary": "We introduce a synthetic control methodology to study policies with staggered\nadoption. Many policies, such as the board gender quota, are replicated by\nother policy setters at different time frames. Our method estimates the dynamic\naverage treatment effects on the treated using variation introduced by the\nstaggered adoption of policies. Our method gives asymptotically unbiased\nestimators of many interesting quantities and delivers asymptotically valid\ninference. By using the proposed method and national labor data in Europe, we\nfind evidence that quota regulation on board diversity leads to a decrease in\npart-time employment, and an increase in full-time employment for female\nprofessionals.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.06320v1", "link": "http://arxiv.org/abs/1912.06320v1"}, {"title": "What can we see from Investment Simulation based on Generalized\n  (m,2)-Zipf law?", "summary": "The paper revisits the investment simulation based on strategies exhibited by\nGeneralized (m,2)-Zipf law to present an interesting characterization of the\nwildness in financial time series. The investigations of dominant strategies on\neach specific time series shows that longer words dominant in larger time scale\nexhibit shorter dominant ones in smaller time scale and vice versa. Moreover,\ndenoting the term wildness based on persistence over short term trend and\nmemory represented by particular length of words, we can see how wild\nhistorical fluctuations over time series data coped with the Zipf strategies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0504210v2", "link": "http://arxiv.org/abs/physics/0504210v2"}, {"title": "Broken ergodicity and memory in the minority game", "summary": "We study the dynamics of the `batch' minority game with market-impact\ncorrection using generating functional techniques to carry out the quenched\ndisorder average. We find that the assumption of weak long-term memory, which\none usually makes in order to calculate ergodic stationary states, breaks down\nwhen the persistent autocorrelation becomes larger than c_c=0.772... We show\nthat this condition, remarkably, coincides with the AT-line found in an earlier\nstatic calculation. This result suggests a new scenario for ergodicity breaking\nin disordered systems.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0108066v2", "link": "http://dx.doi.org/10.1088/0305-4470/34/40/103"}, {"title": "Real Output Costs of Financial Crises: A Loss Distribution Approach", "summary": "We study cross-country GDP losses due to financial crises in terms of\nfrequency (number of loss events per period) and severity (loss per\noccurrence). We perform the Loss Distribution Approach (LDA) to estimate a\nmulti-country aggregate GDP loss probability density function and the\npercentiles associated to extreme events due to financial crises.\n  We find that output losses arising from financial crises are strongly\nheterogeneous and that currency crises lead to smaller output losses than debt\nand banking crises.\n  Extreme global financial crises episodes, occurring with a one percent\nprobability every five years, lead to losses between 2.95% and 4.54% of world\nGDP.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1201.0967v2", "link": "http://arxiv.org/abs/1201.0967v2"}, {"title": "Enhanced capital-asset pricing model for the reconstruction of bipartite\n  financial networks", "summary": "Reconstructing patterns of interconnections from partial information is one\nof the most important issues in the statistical physics of complex networks. A\nparamount example is provided by financial networks. In fact, the spreading and\namplification of financial distress in capital markets is strongly affected by\nthe interconnections among financial institutions. Yet, while the aggregate\nbalance sheets of institutions are publicly disclosed, information on single\npositions is mostly confidential and, as such, unavailable. Standard approaches\nto reconstruct the network of financial interconnection produce unrealistically\ndense topologies, leading to a biased estimation of systemic risk. Moreover,\nreconstruction techniques are generally designed for monopartite networks of\nbilateral exposures between financial institutions, thus failing in reproducing\nbipartite networks of security holdings (\\eg, investment portfolios). Here we\npropose a reconstruction method based on constrained entropy maximization,\ntailored for bipartite financial networks. Such a procedure enhances the\ntraditional {\\em capital-asset pricing model} (CAPM) and allows to reproduce\nthe correct topology of the network. We test this ECAPM method on a dataset,\ncollected by the European Central Bank, of detailed security holdings of\nEuropean institutional sectors over a period of six years (2009-2015). Our\napproach outperforms the traditional CAPM and the recently proposed MECAPM both\nin reproducing the network topology and in estimating systemic risk due to\nfire-sales spillovers. In general, ECAPM can be applied to the whole class of\nweighted bipartite networks described by the fitness model.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1606.07684v3", "link": "http://dx.doi.org/10.1103/PhysRevE.96.032315"}, {"title": "Risk Neutral Option Pricing With Neither Dynamic Hedging nor Complete\n  Markets", "summary": "Proof that under simple assumptions, such as constraints of Put-Call Parity,\nthe probability measure for the valuation of a European option has the mean\nderived from the forward price which can, but does not have to be the\nrisk-neutral one, under any general probability distribution, bypassing the\nBlack-Scholes-Merton dynamic hedging argument, and without the requirement of\ncomplete markets and other strong assumptions. We confirm that the heuristics\nused by traders for centuries are both more robust, more consistent, and more\nrigorous than held in the economics literature. We also show that options can\nbe priced using infinite variance (finite mean) distributions.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1405.2609v2", "link": "http://arxiv.org/abs/1405.2609v2"}, {"title": "Dynamics of state-wise prospective reserves in the presence of\n  non-monotone information", "summary": "In the presence of monotone information, stochastic Thiele equations\ndescribing the dynamics of state-wise prospective reserves are closely related\nto the classic martingale representation theorem. When the information utilized\nby the insurer is non-monotone, classic martingale theory does not apply. By\ntaking an infinitesimal approach, we derive generalized stochastic Thiele\nequations that allow for information discarding. The results and their\nimplication in practice are illustrated via examples where information is\ndiscarded upon and after stochastic retirement.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/2003.02173v1", "link": "http://arxiv.org/abs/2003.02173v1"}, {"title": "The Perfect Marriage and Much More: Combining Dimension Reduction,\n  Distance Measures and Covariance", "summary": "We develop a novel methodology based on the marriage between the\nBhattacharyya distance, a measure of similarity across distributions of random\nvariables, and the Johnson-Lindenstrauss Lemma, a technique for dimension\nreduction. The resulting technique is a simple yet powerful tool that allows\ncomparisons between data-sets representing any two distributions. The degree to\nwhich different entities, (markets, universities, hospitals, cities, groups of\nsecurities, etc.), have different distance measures of their corresponding\ndistributions tells us the extent to which they are different, aiding\nparticipants looking for diversification or looking for more of the same thing.\nWe demonstrate a relationship between covariance and distance measures based on\na generic extension of Stein's Lemma. We consider an asset pricing application\nand then briefly discuss how this methodology lends itself to numerous\nmarket-structure studies and even applications outside the realm of finance /\nsocial sciences by illustrating a biological application. We provide numerical\nillustrations using security prices, volumes and volatilities of both these\nvariables from six different countries.", "category": ["q-fin.TR", "math.PR", "q-fin.EC"], "id": "http://arxiv.org/abs/1603.09060v6", "link": "http://dx.doi.org/10.1016/j.physa.2019.04.174"}, {"title": "Computational modeling of collective human behavior: Example of\n  financial markets", "summary": "We discuss how minimal financial market models can be constructed by bridging\nthe gap between two existing, but incomplete, market models: a model in which a\npopulation of virtual traders make decisions based on common global information\nbut lack local information from their social network, and a model in which the\ntraders form a dynamically evolving social network but lack any decision-making\nbased on global information. We show that a suitable combination of these two\nmodels -- in particular, a population of virtual traders with access to both\nglobal and local information -- produces results for the price return\ndistribution which are closer to the reported stylized facts. We believe that\nthis type of model can be applied across a wide range of systems in which\ncollective human activity is observed.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0812.2603v1", "link": "http://arxiv.org/abs/0812.2603v1"}, {"title": "No-arbitrage with multiple-priors in discrete time", "summary": "In a discrete time and multiple-priors setting, we propose a new\ncharacterisation of the condition of quasi-sure no-arbitrage which has become a\nstandard assumption. This characterisation shows that it is indeed a\nwell-chosen condition being equivalent to several previously used alternative\nnotions of no-arbitrage and allowing the proof of important results in\nmathematical finance. We also revisit the so-called geometric and quantitative\nno-arbitrage conditions and explicit two important examples where all these\nconcepts are illustrated.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1904.08780v2", "link": "http://arxiv.org/abs/1904.08780v2"}, {"title": "New copulas based on general partitions-of-unity and their applications\n  to risk management", "summary": "We construct new multivariate copulas on the basis of a generalized infinite\npartition-of-unity approach. This approach allows - in contrast to finite\npartition-of-unity copulas - for tail-dependence as well as for asymmetry. A\npossibility of fitting such copulas to real data from quantitative risk\nmanagement is also pointed out.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1505.00288v3", "link": "http://dx.doi.org/10.1515/demo-2016-0006"}, {"title": "100+ Metrics for Software Startups - A Multi-Vocal Literature Review", "summary": "Metrics can be used by businesses to make more objective decisions based on\ndata. Software startups in particular are characterized by the uncertain or\neven chaotic nature of the contexts in which they operate. Using data in the\nform of metrics can help software startups to make the right decisions amidst\nuncertainty and limited resources. However, whereas conventional business\nmetrics and software metrics have been studied in the past, metrics in the\nspe-cific context of software startup are not widely covered within academic\nliterature. To promote research in this area and to create a starting point for\nit, we have conducted a multi-vocal literature review focusing on practitioner\nliterature in order to compile a list of metrics used by software startups.\nSaid list is intended to serve as a basis for further research in the area, as\nthe metrics in it are based on suggestions made by practitioners and not\nempirically verified.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.04819v1", "link": "http://arxiv.org/abs/1901.04819v1"}, {"title": "Forecasting of Jump Arrivals in Stock Prices: New Attention-based\n  Network Architecture using Limit Order Book Data", "summary": "The existing literature provides evidence that limit order book data can be\nused to predict short-term price movements in stock markets. This paper\nproposes a new neural network architecture for predicting return jump arrivals\nin equity markets with high-frequency limit order book data. This new\narchitecture, based on Convolutional Long Short-Term Memory with Attention, is\nintroduced to apply time series representation learning with memory and to\nfocus the prediction attention on the most important features to improve\nperformance. The data set consists of order book data on five liquid U.S.\nstocks. The use of the attention mechanism makes it possible to analyze the\nimportance of the inclusion limit order book data and other input variables. By\nusing this mechanism, we provide evidence that the use of limit order book data\nwas found to improve the performance of the proposed model in jump prediction,\neither clearly or marginally, depending on the underlying stock. This suggests\nthat path-dependence in limit order book markets is a stock specific feature.\nMoreover, we find that the proposed approach with an attention mechanism\noutperforms the multi-layer perceptron network as well as the convolutional\nneural network and Long Short-Term memory model.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1810.10845v1", "link": "http://arxiv.org/abs/1810.10845v1"}, {"title": "Modeling Market Mechanism with Minority Game", "summary": "Using the Minority Game model we study a broad spectrum of problems of market\nmechanism. We study the role of different types of agents: producers,\nspeculators as well as noise traders. The central issue here is the information\nflow : producers feed in the information whereas speculators make it away. How\nwell each agent fares in the common game depends on the market conditions, as\nwell as their sophistication. Sometimes there is much to gain with little\neffort, sometimes great effort virtually brings no more incremental gain.\nMarket impact is shown to play also an important role, a strategy should be\njudged when it is actually used in play for its quality. Though the Minority\nGame is an extremely simplified market model, it allows to ask, analyze and\nanswer many questions which arise in real markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/9909265v1", "link": "http://dx.doi.org/10.1016/S0378-4371(99)00446-X"}, {"title": "Endogenous Formation of Limit Order Books: Dynamics Between Trades", "summary": "In this work, we present a continuous-time large-population game for modeling\nmarket microstructure betweentwo consecutive trades. The proposed modeling\nframework is inspired by our previous work [23]. In this framework, the Limit\nOrder Book (LOB) arises as an outcome of an equilibrium between multiple agents\nwho have different beliefs about the future demand for the asset. The agents'\nbeliefs may change according to the information they observe, triggering\nchanges in their behavior. We present an example illustrating how the proposed\nmodels can be used to quantify the consequences of changes in relevant\ninformation signals. If these signals, themselves, depend on the LOB, then, our\napproach allows one to model the \"indirect\" market impact (as opposed to the\n\"direct\" impact that a market order makes on the LOB, by eliminating certain\nlimit orders). On the mathematical side, we formulate the proposed modeling\nframework as a continuum-player control-stopping game. We manage to split the\nequilibrium problem into two parts. The first one is described by a\ntwo-dimensional system of Reflected Backward Stochastic Differential Equations\n(RBSDEs), whose solution components reflect against each other. The second one\nleads to an infinite-dimensional fixed-point problem for a discontinuous\nmapping. Both problems are non-standard, and we prove the existence of their\nsolutions in the paper.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1605.09720v2", "link": "http://arxiv.org/abs/1605.09720v2"}, {"title": "Pricing of Basket Options Using Polynomial Approximations", "summary": "In this paper we use Bernstein and Chebyshev polynomials to approximate the\nprice of some basket options under a bivariate Black-Scholes model. The method\nconsists in expanding the price of a univariate related contract after\nconditioning on the remaining underlying assets and calculating the mixed\nexponential-power moments of a Gaussian distribution that arise as a\nconsequence of such approximation. Our numerical implementation on spread\ncontracts shows the method is as accurate as a standard Monte Carlo approach at\nconsiderable lesser computational effort.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1404.3160v1", "link": "http://arxiv.org/abs/1404.3160v1"}, {"title": "Analysis of multilevel Monte Carlo path simulation using the Milstein\n  discretisation", "summary": "The multilevel Monte Carlo path simulation method introduced by Giles ({\\it\nOperations Research}, 56(3):607-617, 2008) exploits strong convergence\nproperties to improve the computational complexity by combining simulations\nwith different levels of resolution. In this paper we analyse its efficiency\nwhen using the Milstein discretisation; this has an improved order of strong\nconvergence compared to the standard Euler-Maruyama method, and it is proved\nthat this leads to an improved order of convergence of the variance of the\nmultilevel estimator. Numerical results are also given for basket options to\nillustrate the relevance of the analysis.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1302.4676v3", "link": "http://dx.doi.org/10.3934/dcdsb.2018335"}, {"title": "Information and Trading Targets in a Dynamic Market Equilibrium", "summary": "This paper investigates the equilibrium interactions between trading targets\nand private information in a multi-period Kyle (1985) market. There are two\ninvestors who each follow dynamic trading strategies: A strategic portfolio\nrebalancer who engages in order splitting to reach a cumulative trading target\nand an unconstrained strategic insider who trades on long-lived information. We\nconsider cases in which the constrained rebalancer is partially informed as\nwell as the special case in which the rebalancer is ex ante uninformed. We\nderive a linear Bayesian Nash equilibrium, describe an algorithm for computing\nsuch equilibria, and present numerical results on properties of these\nequilibria.", "category": ["q-fin.TR", "q-fin.GN"], "id": "http://arxiv.org/abs/1502.02083v3", "link": "http://arxiv.org/abs/1502.02083v3"}, {"title": "A dynamic optimal execution strategy under stochastic price recovery", "summary": "In the present paper, we study the optimal execution problem under stochastic\nprice recovery based on limit order book dynamics. We model price recovery\nafter execution of a large order by accelerating the arrival of the refilling\norder, which is defined as a Cox process whose intensity increases by the\ndegree of the market impact. We include not only the market order but also the\nlimit order in our strategy in a restricted fashion. We formulate the problem\nas a combined stochastic control problem over a finite time horizon. The\ncorresponding Hamilton-Jacobi-Bellman quasi-variational inequality is solved\nnumerically. The optimal strategy obtained consists of three components: (i)\nthe initial large trade; (ii) the unscheduled small trades during the period;\n(iii) the terminal large trade. The size and timing of the trade is governed by\nthe tolerance for market impact depending on the state at each time step, and\nhence the strategy behaves dynamically. We also provide competitive results due\nto inclusion of the limit order, even though a limit order is allowed under\nconservative evaluation of the execution price.", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1502.04521v1", "link": "http://arxiv.org/abs/1502.04521v1"}, {"title": "Take a Look Around: Using Street View and Satellite Images to Estimate\n  House Prices", "summary": "When an individual purchases a home, they simultaneously purchase its\nstructural features, its accessibility to work, and the neighborhood amenities.\nSome amenities, such as air quality, are measurable while others, such as the\nprestige or the visual impression of a neighborhood, are difficult to quantify.\nDespite the well-known impacts intangible housing features have on house\nprices, limited attention has been given to systematically quantifying these\ndifficult to measure amenities. Two issues have led to this neglect. Not only\ndo few quantitative methods exist that can measure the urban environment, but\nthat the collection of such data is both costly and subjective.\n  We show that street image and satellite image data can capture these urban\nqualities and improve the estimation of house prices. We propose a pipeline\nthat uses a deep neural network model to automatically extract visual features\nfrom images to estimate house prices in London, UK. We make use of traditional\nhousing features such as age, size, and accessibility as well as visual\nfeatures from Google Street View images and Bing aerial images in estimating\nthe house price model. We find encouraging results where learning to\ncharacterize the urban quality of a neighborhood improves house price\nprediction, even when generalizing to previously unseen London boroughs.\n  We explore the use of non-linear vs. linear methods to fuse these cues with\nconventional models of house pricing, and show how the interpretability of\nlinear models allows us to directly extract proxy variables for visual\ndesirability of neighborhoods that are both of interest in their own right, and\ncould be used as inputs to other econometric methods. This is particularly\nvaluable as once the network has been trained with the training data, it can be\napplied elsewhere, allowing us to generate vivid dense maps of the visual\nappeal of London streets.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1807.07155v2", "link": "http://dx.doi.org/10.1145/3342240"}, {"title": "Stock Market Insider Trading in Continuous Time with Imperfect Dynamic\n  Information", "summary": "This paper studies the equilibrium pricing of asset shares in the presence of\ndynamic private information. The market consists of a risk-neutral informed\nagent who observes the firm value, noise traders, and competitive market makers\nwho set share prices using the total order flow as a noisy signal of the\ninsider's information. I provide a characterization of all optimal strategies,\nand prove existence of both Markovian and non Markovian equilibria by deriving\nclosed form solutions for the optimal order process of the informed trader and\nthe optimal pricing rule of the market maker. The consideration of non\nMarkovian equilibrium is relevant since the market maker might decide to\nre-weight past information after receiving a new signal. Also, I show that a)\nthere is a unique Markovian equilibrium price process which allows the insider\nto trade undetected, and that b) the presence of an insider increases the\nmarket informational efficiency, in particular for times close to dividend\npayment.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1607.00035v1", "link": "http://arxiv.org/abs/1607.00035v1"}, {"title": "Modelling Bonds & Credit Default Swaps using a Structural Model with\n  Contagion", "summary": "This paper develops a two-dimensional structural framework for valuing credit\ndefault swaps and corporate bonds in the presence of default contagion.\nModelling the values of related firms as correlated geometric Brownian motions\nwith exponential default barriers, analytical formulae are obtained for both\ncredit default swap spreads and corporate bond yields. The credit dependence\nstructure is influenced by both a longer-term correlation structure as well as\nby the possibility of default contagion. In this way, the model is able to\ngenerate a diverse range of shapes for the term structure of credit spreads\nusing realistic values for input parameters.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0710.0753v1", "link": "http://arxiv.org/abs/0710.0753v1"}, {"title": "A bank salvage model by impulse stochastic controls", "summary": "The present paper is devoted to the study of a bank salvage model with finite\ntime horizon and subjected to stochastic impulse controls. In our model, the\nbank's default time is a completely inaccessible random quantity generating its\nown filtration, then reflecting the unpredictability of the event itself. In\nthis framework the main goal is to minimize the total cost of the central\ncontroller who can inject capital to save the bank from default. We address the\nlatter task showing that the corresponding quasi-variational inequality (QVI)\nadmits a unique viscosity solution, Lipschitz continuous in space and Holder\ncontinuous in time. Furthermore, under mild assumptions on the dynamics the\nsmooth-fit $W^{(1,2),p}_{loc}$ property is achieved for any $1<p<+\\infty$.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1910.03056v1", "link": "http://arxiv.org/abs/1910.03056v1"}, {"title": "Matching with Generalized Lexicographic Choice Rules", "summary": "Motivated by the need for real-world matching problems, this paper formulates\na large class of practical choice rules, Generalized Lexicographic Choice Rules\n(GLCR), for institutions that consist of multiple divisions. Institutions fill\ntheir divisions sequentially, and each division is endowed with a sub-choice\nrule that satisfies classical substitutability and size monotonicity in\nconjunction with a new property that we introduce, quota monotonicity. We allow\nrich interactions between divisions in the form of capacity transfers. The\noverall choice rule of an institution is defined as the union of the\nsub-choices of its divisions. The cumulative offer mechanism (COM) with respect\nto GLCR is the unique stable and strategy-proof mechanism. We define a\nchoice-based improvement notion and show that the COM respects improvements. We\nemploy the theory developed in this paper in our companion paper, Ayg\\\"un and\nTurhan (2020), to design satisfactory matching mechanisms for India with\ncomprehensive affirmative action constraints.", "category": [], "id": "http://arxiv.org/abs/2004.13261v1", "link": "http://arxiv.org/abs/2004.13261v1"}, {"title": "Representation and approximation of ambit fields in Hilbert space", "summary": "We lift ambit fields as introduced by Barndorff-Nielsen and Schmiegel to a\nclass of Hilbert space-valued volatility modulated Volterra processes. We name\nthis class Hambit fields, and show that they can be expressed as a countable\nsum of weighted real-valued volatility modulated Volterra processes. Moreover,\nHambit fields can be interpreted as the boundary of the mild solution of a\ncertain first order stochastic partial differential equation. This stochastic\npartial differential equation is formulated on a suitable Hilbert space of\nfunctions on the positive real line with values in the state space of the\nHambit field. We provide an explicit construction of such a space. Finally, we\napply this interpretation of Hambit fields to develop a finite difference\nscheme, for which we prove convergence under some Lipschitz conditions.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1509.08272v1", "link": "http://arxiv.org/abs/1509.08272v1"}, {"title": "An Impossibility Theorem for Wealth in Heterogeneous-agent Models with\n  Limited Heterogeneity", "summary": "It has been conjectured that canonical Bewley--Huggett--Aiyagari\nheterogeneous-agent models cannot explain the joint distribution of income and\nwealth. The results stated below verify this conjecture and clarify its\nimplications under very general conditions. We show in particular that if (i)\nagents are infinitely-lived, (ii) saving is risk-free, and (iii) agents have\nconstant discount factors, then the wealth distribution inherits the tail\nbehavior of income shocks (e.g., light-tailedness or the Pareto exponent). Our\nrestrictions on utility require only that relative risk aversion is bounded,\nand a large variety of income processes are admitted. Our results show\nconclusively that it is necessary to go beyond standard models to explain the\nempirical fact that wealth is heavier-tailed than income. We demonstrate\nthrough examples that relaxing any of the above three conditions can generate\nPareto tails.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.08404v3", "link": "http://arxiv.org/abs/1807.08404v3"}, {"title": "An equilibrium model for spot and forward prices of commodities", "summary": "We consider a market model that consists of financial investors and producers\nof a commodity. Producers optionally store some production for future sale and\ngo short on forward contracts to hedge the uncertainty of the future commodity\nprice. Financial investors take positions in these contracts in order to\ndiversify their portfolios. The spot and forward equilibrium commodity prices\nare endogenously derived as the outcome of the interaction between producers\nand investors. Assuming that both are utility maximizers, we first prove the\nexistence of an equilibrium in an abstract setting. Then, in a framework where\nthe consumers' demand and the exogenously priced financial market are\ncorrelated, we provide semi-explicit expressions for the equilibrium prices and\nanalyze their dependence on the model parameters. The model can explain why\nincreased investors' participation in forward commodity markets and higher\ncorrelation between the commodity and the stock market could result in higher\nspot prices and lower forward premia.", "category": ["q-fin.EC", "q-fin.PR"], "id": "http://arxiv.org/abs/1502.00674v3", "link": "http://arxiv.org/abs/1502.00674v3"}, {"title": "Bayesian Model Choice of Grouped t-copula", "summary": "One of the most popular copulas for modeling dependence structures is\nt-copula. Recently the grouped t-copula was generalized to allow each group to\nhave one member only, so that a priori grouping is not required and the\ndependence modeling is more flexible. This paper describes a Markov chain Monte\nCarlo (MCMC) method under the Bayesian inference framework for estimating and\nchoosing t-copula models. Using historical data of foreign exchange (FX) rates\nas a case study, we found that Bayesian model choice criteria overwhelmingly\nfavor the generalized t-copula. In addition, all the criteria also agree on the\nsecond most likely model and these inferences are all consistent with classical\nlikelihood ratio tests. Finally, we demonstrate the impact of model choice on\nthe conditional Value-at-Risk for portfolios of six major FX rates.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1103.0606v1", "link": "http://dx.doi.org/10.1007/s11009-011-9220-4"}, {"title": "Extreme value statistics and recurrence intervals of NYMEX energy\n  futures volatility", "summary": "Energy markets and the associated energy futures markets play a crucial role\nin global economies. We investigate the statistical properties of the\nrecurrence intervals of daily volatility time series of four NYMEX energy\nfutures, which are defined as the waiting times $\\tau$ between consecutive\nvolatilities exceeding a given threshold $q$. We find that the recurrence\nintervals are distributed as a stretched exponential $P_q(\\tau)\\sim\ne^{(a\\tau)^{-\\gamma}}$, where the exponent $\\gamma$ decreases with increasing\n$q$, and there is no scaling behavior in the distributions for different\nthresholds $q$ after the recurrence intervals are scaled with the mean\nrecurrence interval $\\bar\\tau$. These findings are significant under the\nKolmogorov-Smirnov test and the Cram{\\'e}r-von Mises test. We show that\nempirical estimations are in nice agreement with the numerical integration\nresults for the occurrence probability $W_q(\\Delta{t}|t)$ of a next event above\nthe threshold $q$ within a (short) time interval after an elapsed time $t$ from\nthe last event above $q$. We also investigate the memory effects of the\nrecurrence intervals. It is found that the conditional distributions of large\nand small recurrence intervals differ from each other and the conditional mean\nof the recurrence intervals scales as a power law of the preceding interval\n$\\bar\\tau(\\tau_0)/\\bar\\tau \\sim (\\tau_0/\\bar\\tau)^\\beta$, indicating that the\nrecurrence intervals have short-term correlations. Detrended fluctuation\nanalysis and detrending moving average analysis further uncover that the\nrecurrence intervals possess long-term correlations. We confirm that the\n\"clustering\" of the volatility recurrence intervals is caused by the long-term\ncorrelations well known to be present in the volatility.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1211.5502v1", "link": "http://dx.doi.org/10.1016/j.econmod.2013.09.011"}, {"title": "Coarse-graining and Self-similarity of Price Fluctuations", "summary": "We propose a new approach for analyzing price fluctuations in their strongly\ncorrelated regime ranging from minutes to months. This is done by employing a\nself-similarity assumption for the magnitude of coarse-grained price\nfluctuation or volatility. The existence of a Cramer function, the\ncharacteristic function for self-similarity, is confirmed by analyzing real\nprice data from a stock market. We also discuss the close interrelation among\nour approach, the scaling-of-moments method and the multifractal approach for\nprice fluctuations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0101175v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00135-2"}, {"title": "Perceived Advantage in Perspective Application of Integrated Choice and\n  Latent Variable Model to Capture Electric Vehicles Perceived Advantage from\n  Consumers Perspective", "summary": "Relative advantage, or the degree to which a new technology is perceived to\nbe better over the existing technology it supersedes, has a significant impact\non individuals decision of adopting to the new technology. This paper\ninvestigates the impact of electric vehicles perceived advantage over the\nconventional internal combustion engine vehicles, from consumers perspective,\non their decision to select electric vehicles. Data is obtained from a stated\npreference survey from 1176 residents in New South Wales, Australia. The\ncollected data is used to estimate an integrated choice and latent variable\nmodel of electric vehicle choice, which incorporates the perceived advantage of\nelectric vehicles in the form of latent variables in the utility function. The\ndesign of the electric vehicle, impact on the environment, and safety are three\nidentified advantages from consumers point of view. The model is used to\nsimulate the effectiveness of various policies to promote electric vehicles on\ndifferent cohorts. Rebate on the purchase price is found to be the most\neffective strategy to promote electric vehicles adoption.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.11606v2", "link": "http://arxiv.org/abs/1905.11606v2"}, {"title": "Omega risk model with tax", "summary": "In this paper we study the Omega risk model with surplus-dependent tax\npayments in a time-homogeneous diffusion setting. The new model incorporates\npractical features from both the Omega risk model(Albrecher and Gerber and Shiu\n(2011)) and the risk model with tax(Albrecher and Hipp (2007)). We explicitly\ncharacterize the Laplace transform of the occupation time of an Azema-Yor\nprocess(e.g. a process refracted by functionals of its running maximum) below a\nconstant level until the first hitting time of another Azema-Yor process or\nuntil an independent exponential time. This result unifies and extends recent\nliterature(Li and Zhou (2013) and Zhang (2014)) incorporating some of their\nresults as special cases. We explicitly characterize the Laplace transform of\nthe time of bankruptcy in the Omega risk model with tax and discuss an\nextension to integral functionals. Finally we present examples using a Brownian\nmotion with drift.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1403.7680v1", "link": "http://arxiv.org/abs/1403.7680v1"}, {"title": "Measuring risk with multiple eligible assets", "summary": "The risk of financial positions is measured by the minimum amount of capital\nto raise and invest in eligible portfolios of traded assets in order to meet a\nprescribed acceptability constraint. We investigate nondegeneracy, finiteness\nand continuity properties of these risk measures with respect to multiple\neligible assets. Our finiteness and continuity results highlight the interplay\nbetween the acceptance set and the class of eligible portfolios. We present a\nsimple, alternative approach to the dual representation of convex risk measures\nby directly applying to the acceptance set the external characterization of\nclosed, convex sets. We prove that risk measures are nondegenerate if and only\nif the pricing functional admits a positive extension which is a supporting\nfunctional for the underlying acceptance set, and provide a characterization of\nwhen such extensions exist. Finally, we discuss applications to set-valued risk\nmeasures, superhedging with shortfall risk, and optimal risk sharing.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1308.3331v2", "link": "http://arxiv.org/abs/1308.3331v2"}, {"title": "Market Dynamics and Indirect Network Effects in Electric Vehicle\n  Diffusion", "summary": "The diffusion of electric vehicles (EVs) is studied in a two-sided market\nframework consisting of EVs on the one side and EV charging stations (EVCSs) on\nthe other. A sequential game is introduced as a model for the interactions\nbetween an EVCS investor and EV consumers. A consumer chooses to purchase an EV\nor a conventional gasoline alternative based on the upfront costs of purchase,\nthe future operating costs and the availability of charging stations. The\ninvestor, on the other hand, maximizes his profit by deciding whether to build\ncharging facilities at a set of potential EVCS sites or to defer his\ninvestments. The solution of the sequential game characterizes the EV-EVCS\nmarket equilibrium. The market solution is compared with that of a social\nplanner who invests in EVCSs with the goal of maximizing the social welfare. It\nis shown that the market solution underinvests EVCSs, leading to slower EV\ndiffusion. The effects of subsidies for EV purchase and EVCSs are also\nconsidered.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1502.03840v2", "link": "http://dx.doi.org/10.1016/j.trd.2016.06.010"}, {"title": "Entangled Economy: an ecosystems approach to modeling systemic level\n  dynamics", "summary": "We present a model of an economy inspired by individual based model\napproaches in evolutionary ecology. We demonstrate that evolutionary dynamics\nin a space of companies interconnected through a correlated interaction matrix\nproduces time dependencies of the total size of the economy total number of\ncompanies, companies age and capital distribution that compares well with\nstatistics for USA. We discuss the relevance of our modeling framework to\npolicy making.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1207.6091v1", "link": "http://dx.doi.org/10.1016/j.physa.2012.10.037"}, {"title": "Classical Option Pricing and Some Steps Further", "summary": "This paper modifies single assumption in the base of classical option pricing\nmodel and derives further extensions for the Black-Scholes-Merton equation. We\nregard the price as the ratio of the cost and the volume of market transaction\nand apply classical assumptions on stochastic Brownian motion not to the price\nbut to the cost and the volume. This simple replacement leads to 2-dimensional\nBSM-like equation with two constant volatilities. We argue that decisions on\nthe cost and the volume of market transactions are made under agents\nexpectations. Random perturbations of expectations impact the market\ntransactions and through them induce stochastic behavior of the underlying\nprice. We derive BSM-like equation driven by Brownian motion of agents\nexpectations. Agents expectations can be based on option trading data. We show\nhow such expectations can lead to nonlinear BSM-like equations. Further we show\nthat the Heston stochastic volatility option pricing model can be applied to\nour approximations and as example derive 3-dimensional BSM-like equation that\ndescribes option pricing with stochastic cost volatility and constant volume\nvolatility. Diversity of BSM-like equations with 2-5 or more dimensions\nemphasizes complexity of option pricing problem. Such variety states the\nproblem of reasonable balance between the accuracy of asset and option price\ndescription and the complexity of the equations under consideration. We hope\nthat some of BSM-like equations derived in this paper may be useful for further\ndevelopment of assets and option market modeling.", "category": ["q-fin.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/2004.13708v1", "link": "http://arxiv.org/abs/2004.13708v1"}, {"title": "Diagnostics of Rational Expectation Financial Bubbles with Stochastic\n  Mean-Reverting Termination Times", "summary": "We propose two rational expectation models of transient financial bubbles\nwith heterogeneous arbitrageurs and positive feedbacks leading to\nself-reinforcing transient stochastic faster-than-exponential price dynamics.\nAs a result of the nonlinear feedbacks, the termination of a bubble is found to\nbe characterized by a finite-time singularity in the bubble price formation\nprocess ending at some potential critical time $\\tilde{t}_c$, which follows a\nmean-reversing stationary dynamics. Because of the heterogeneity of the\nrational agents' expectations, there is a synchronization problem for the\noptimal exit times determined by these arbitrageurs, which leads to the\nsurvival of the bubble almost all the way to its theoretical end time. The\nexplicit exact analytical solutions of the two models provide nonlinear\ntransformations which allow us to develop novel tests for the presence of\nbubbles in financial time series. Avoiding the difficult problem of parameter\nestimation of the stochastic differential equation describing the price\ndynamics, the derived operational procedures allow us to diagnose bubbles that\nare in the making and to forecast their termination time. The tests performed\non three financial markets, the US S&P500 index from 1 February 1980 to 31\nOctober 2008, the US NASDAQ composite index from 1 January 1980 to 31 July 2008\nand the Hong Kong Hang Seng index from 1 December 1986 to 30 November 2008,\nsuggest the feasibility of advance bubble warning.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0911.1921v1", "link": "http://arxiv.org/abs/0911.1921v1"}, {"title": "A fractional-order difference Cournot duopoly game with long memory", "summary": "We reconsider the Cournot duopoly problem in light of the theory for long\nmemory. We introduce the Caputo fractional-order difference calculus to\nclassical duopoly theory to propose a fractional-order discrete Cournot duopoly\ngame model, which allows participants to make decisions while making full use\nof their historical information. Then we discuss Nash equilibria and local\nstability by using linear approximation. Finally, we detect the chaos of the\nmodel by employing a 0-1 test algorithm.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.04305v1", "link": "http://arxiv.org/abs/1903.04305v1"}, {"title": "Sparse Kalman Filtering Approaches to Covariance Estimation from High\n  Frequency Data in the Presence of Jumps", "summary": "Estimation of the covariance matrix of asset returns from high frequency data\nis complicated by asynchronous returns, market mi- crostructure noise and\njumps. One technique for addressing both asynchronous returns and market\nmicrostructure is the Kalman-EM (KEM) algorithm. However the KEM approach\nassumes log-normal prices and does not address jumps in the return process\nwhich can corrupt estimation of the covariance matrix.\n  In this paper we extend the KEM algorithm to price models that include jumps.\nWe propose two sparse Kalman filtering approaches to this problem. In the first\napproach we develop a Kalman Expectation Conditional Maximization (KECM)\nalgorithm to determine the un- known covariance as well as detecting the jumps.\nFor this algorithm we consider Laplace and the spike and slab jump models, both\nof which promote sparse estimates of the jumps. In the second method we take a\nBayesian approach and use Gibbs sampling to sample from the posterior\ndistribution of the covariance matrix under the spike and slab jump model.\nNumerical results using simulated data show that each of these approaches\nprovide for improved covariance estima- tion relative to the KEM method in a\nvariety of settings where jumps occur.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1602.02185v2", "link": "http://dx.doi.org/10.1007/s10107-019-01371-6"}, {"title": "Experimenting in Equilibrium", "summary": "Classical approaches to experimental design assume that intervening on one\nunit does not affect other units. There are many important settings, however,\nwhere this non-interference assumption does not hold, e.g., when running\nexperiments on supply-side incentives on a ride-sharing platform or subsidies\nin an energy marketplace. In this paper, we introduce a new approach to\nexperimental design in large-scale stochastic systems with considerable\ncross-unit interference, under an assumption that the interference is\nstructured enough that it can be captured using mean-field asymptotics. Our\napproach enables us to accurately estimate the effect of small changes to\nsystem parameters by combining unobstrusive randomization with light-weight\nmodeling, all while remaining in equilibrium. We can then use these estimates\nto optimize the system by gradient descent. Concretely, we focus on the problem\nof a platform that seeks to optimize supply-side payments p in a centralized\nmarketplace where different suppliers interact via their effects on the overall\nsupply-demand equilibrium, and show that our approach enables the platform to\noptimize p based on perturbations whose magnitude can get vanishingly small in\nlarge systems.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.02124v4", "link": "http://arxiv.org/abs/1903.02124v4"}, {"title": "Quantum attacks on Bitcoin, and how to protect against them", "summary": "The key cryptographic protocols used to secure the internet and financial\ntransactions of today are all susceptible to attack by the development of a\nsufficiently large quantum computer. One particular area at risk are\ncryptocurrencies, a market currently worth over 150 billion USD. We investigate\nthe risk of Bitcoin, and other cryptocurrencies, to attacks by quantum\ncomputers. We find that the proof-of-work used by Bitcoin is relatively\nresistant to substantial speedup by quantum computers in the next 10 years,\nmainly because specialized ASIC miners are extremely fast compared to the\nestimated clock speed of near-term quantum computers. On the other hand, the\nelliptic curve signature scheme used by Bitcoin is much more at risk, and could\nbe completely broken by a quantum computer as early as 2027, by the most\noptimistic estimates. We analyze an alternative proof-of-work called Momentum,\nbased on finding collisions in a hash function, that is even more resistant to\nspeedup by a quantum computer. We also review the available post-quantum\nsignature schemes to see which one would best meet the security and efficiency\nrequirements of blockchain applications.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1710.10377v1", "link": "http://dx.doi.org/10.5195/ledger.2018.127"}, {"title": "Experimental Design under Network Interference", "summary": "This paper discusses the problem of the design of experiments under network\ninterference. We allow for a possibly fully connected network and a general\nclass of estimands, which encompasses average treatment and average spillover\neffects, as well as estimands obtained from interactions of the two. We discuss\na near-optimal design mechanism, where the experimenter optimizes over\nparticipants and treatment assignments to minimize the variance of the\nestimators of interest, using a first-wave experiment for estimation of the\nvariance. We guarantee valid asymptotic inference on causal effects using\neither parametric or non-parametric estimators under the proposed experimental\ndesign, allowing for local dependence of potential outcomes, arbitrary\ndependence of the treatment assignment indicators, and spillovers across units.\nWe showcase asymptotic optimality and finite-sample upper bounds on the regret\nof the proposed design mechanism. Simulations illustrate the advantage of the\nmethod over state-of-art methodologies.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.08421v2", "link": "http://arxiv.org/abs/2003.08421v2"}, {"title": "The market efficiency in the stock markets", "summary": "We study the temporal evolution of the market efficiency in the stock markets\nusing the complexity, entropy density, standard deviation, autocorrelation\nfunction, and probability distribution of the log return for Standard and\nPoor's 500 (S&P 500), Nikkei stock average index, and Korean composition stock\nprice index (KOSPI). Based on the microscopic spin model, we also find that\nthese statistical quantities in stock markets depend on the market efficiency.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0701179v2", "link": "http://dx.doi.org/10.1140/epjb/e2008-00050-0"}, {"title": "Optimal dividends problem with a terminal value for spectrally positive\n  Levy processes", "summary": "In this paper we consider a modified version of the classical optimal\ndividends problem of de Finetti in which the dividend payments subject to a\npenalty at ruin. We assume that the risk process is modeled by a general\nspectrally positive Levy process before dividends are deducted. Using the\nfluctuation theory of spectrally positive Levy processes we give an explicit\nexpression of the value function of a barrier strategy. Subsequently we show\nthat a barrier strategy is the optimal strategy among all admissible ones. Our\nwork is motivated by the recent work of Bayraktar, Kyprianou and Yamazaki\n(2013).", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1302.6011v1", "link": "http://arxiv.org/abs/1302.6011v1"}, {"title": "Universal Structure of the Personal Income Distribution", "summary": "We investigate the Japanese personal income distribution in the high income\nrange over the 112 years 1887-1998, and that in the middle income range over\nthe 44 years 1955-98. It is observed that the distribution pattern of the\nlognormal with power law tail is the universal structure. However the indexes\nspecifying the distribution differ from year to year. One of the index\ncharacterizing the distribution is the mean value of the lognormal\ndistribution; the mean income in the middle income range. It is found that this\nvalue correlates linearly with the Gross Domestic Product (GDP). To clarify the\ntemporal change of the equality or inequality of the distribution, we analyze\nPareto and Gibrat indexes, which characterize the distribution in the high\nincome range and that in the middle income range respectively. It is found for\nsome years that there is no correlation between the high income and the middle\nincome. It is also shown that the mean value of Pareto index equals to 2, and\nthe change of this index is effected by the change of the asset price. From\nthese analysis we derive four constraints that must be satisfied by\nmathematical models.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0011373v1", "link": "http://arxiv.org/abs/cond-mat/0011373v1"}, {"title": "An analytic recursive method for optimal multiple stopping: Canadization\n  and phase-type fitting", "summary": "We study an optimal multiple stopping problem for call-type payoff driven by\na spectrally negative Levy process. The stopping times are separated by\nconstant refraction times, and the discount rate can be positive or negative.\nThe computation involves a distribution of the Levy process at a constant\nhorizon and hence the solutions in general cannot be attained analytically.\nMotivated by the maturity randomization (Canadization) technique by Carr\n(1998), we approximate the refraction times by independent, identically\ndistributed Erlang random variables. In addition, fitting random jumps to\nphase-type distributions, our method involves repeated integrations with\nrespect to the resolvent measure written in terms of the scale function of the\nunderlying Levy process. We derive a recursive algorithm to compute the value\nfunction in closed form, and sequentially determine the optimal exercise\nthresholds. A series of numerical examples are provided to compare our analytic\nformula to results from Monte Carlo simulation.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1505.07705v1", "link": "http://dx.doi.org/10.1142/S0219024915500326"}, {"title": "Leveraged {ETF} implied volatilities from {ETF} dynamics", "summary": "The growth of the exhange-traded fund (ETF) industry has given rise to the\ntrading of options written on ETFs and their leveraged counterparts {(LETFs)}.\nWe study the relationship between the ETF and LETF implied volatility surfaces\nwhen the underlying ETF is modeled by a general class of local-stochastic\nvolatility models. A closed-form approximation for prices is derived for\nEuropean-style options whose payoff depends on the terminal value of the ETF\nand/or LETF. Rigorous error bounds for this pricing approximation are\nestablished. A closed-form approximation for implied volatilities is also\nderived. We also discuss a scaling procedure for comparing implied volatilities\nacross leverage ratios. The implied volatility expansions and scalings are\ntested in three well-known settings: CEV, Heston and SABR.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1404.6792v4", "link": "http://arxiv.org/abs/1404.6792v4"}, {"title": "Application of multi-agent games to the prediction of financial\n  time-series", "summary": "We report on a technique based on multi-agent games which has potential use\nin the prediction of future movements of financial time-series. A third-party\ngame is trained on a black-box time-series, and is then run into the future to\nextract next-step and multi-step predictions. In addition to the possibility of\nidentifying profit opportunities, the technique may prove useful in the\ndevelopment of improved risk management strategies.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0105303v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00299-0"}, {"title": "A time before which insiders would not undertake risk", "summary": "A continuous-path semimartingale market model with wealth processes\ndiscounted by a riskless asset is considered. The numeraire portfolio is the\nunique strictly positive wealth process that, when used as a benchmark to\ndenominate all other wealth, makes all wealth processes local martingales. It\nis assumed that the numeraire portfolio exists and that its wealth increases to\ninfinity as time goes to infinity. Under this setting, an initial enlargement\nof the filtration is performed, by including the overall minimum of the\nnumeraire portfolio. It is established that all nonnegative wealth processes,\nwhen stopped at the time of the overall minimum of the numeraire portfolio,\nbecome local martingales in the enlarged filtration. This implies that\nrisk-averse insider traders would refrain from investing in the risky assets\nbefore that time. A partial converse to the previous result is also established\nin the case of complete markets, showing that the time of the overall minimum\nof the numeraire portfolio is in a certain sense unique in rendering\nundesirable the act of undertaking risky positions before it. The\naforementioned results shed light to the importance of the numeraire portfolio\nas an indicator of overall market performance.", "category": ["q-fin.PM", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1010.1961v2", "link": "http://arxiv.org/abs/1010.1961v2"}, {"title": "A General Framework for the Benchmark pricing in a Fully Collateralized\n  Market", "summary": "Collateralization with daily margining has become a new standard in the\npost-crisis market. Although there appeared vast literature on a so-called\nmulti-curve framework, a complete picture of a multi-currency setup with\ncross-currency basis can be rarely found since our initial attempts. This work\ngives its extension regarding a general framework of interest rates in a fully\ncollateralized market. It gives a new formulation of the currency funding\nspread which is better suited for the general dependence. In the last half, it\ndevelops a discretization of the HJM framework with a fixed tenor structure,\nwhich makes it implementable as a traditional Market Model.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1508.06339v2", "link": "http://arxiv.org/abs/1508.06339v2"}, {"title": "Assessing market uncertainty by means of a time-varying intermittency\n  parameter for asset price fluctuations", "summary": "Maximum likelihood estimation applied to high-frequency data allows us to\nquantify intermittency in the fluctu- ations of asset prices. From time records\nas short as one month these methods permit extraction of a meaningful\nintermittency parameter {\\lambda} characterising the degree of volatility\nclustering of asset prices. We can therefore study the time evolution of\nvolatility clustering and test the statistical significance of this\nvariability. By analysing data from the Oslo Stock Exchange, and comparing the\nresults with the investment grade spread, we find that the estimates of\n{\\lambda} are lower at times of high market uncertainty.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1202.4877v1", "link": "http://dx.doi.org/10.1016/j.physa.2013.02.010"}, {"title": "Loss-Based Risk Measures", "summary": "Starting from the requirement that risk measures of financial portfolios\nshould be based on their losses, not their gains, we define the notion of\nloss-based risk measure and study the properties of this class of risk\nmeasures. We characterize loss-based risk measures by a representation theorem\nand give examples of such risk measures. We then discuss the statistical\nrobustness of estimators of loss-based risk measures: we provide a general\ncriterion for qualitative robustness of risk estimators and compare this\ncriterion with sensitivity analysis of estimators based on influence functions.\nFinally, we provide examples of statistically robust estimators for loss-based\nrisk measures.", "category": ["q-fin.RM", "q-fin.PM"], "id": "http://arxiv.org/abs/1110.1436v3", "link": "http://dx.doi.org/10.1524/strm.2013.1132"}, {"title": "Reconstructing an economic space from a market metric", "summary": "Using a metric related to the returns correlation, a method is proposed to\nreconstruct an economic space from the market data. A reduced subspace,\nassociated to the systematic structure of the market, is identified and its\ndimension related to the number of terms in factor models. Example were worked\nout involving sets of companies from the DJIA and S&P500 indexes. Having a\nmetric defined in the space of companies, network topology coefficients may be\nused to extract further information from the data. A notion of \"continuous\nclustering\" is defined and empirically related to the occurrence of market\nshocks.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0211108v1", "link": "http://dx.doi.org/10.1016/S0378-4371(03)00014-1"}, {"title": "The Income Fluctuation Problem and the Evolution of Wealth", "summary": "We analyze the household savings problem in a general setting where returns\non assets, non-financial income and impatience are all state dependent and\nfluctuate over time. All three processes can be serially correlated and\nmutually dependent. Rewards can be bounded or unbounded and wealth can be\narbitrarily large. Extending classic results from an earlier literature, we\ndetermine conditions under which (a) solutions exist, are unique and are\nglobally computable, (b) the resulting wealth dynamics are stationary, ergodic\nand geometrically mixing, and (c) the wealth distribution has a Pareto tail. We\nshow how these results can be used to extend recent studies of the wealth\ndistribution. Our conditions have natural economic interpretations in terms of\nasymptotic growth rates for discounting and return on savings.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.13045v3", "link": "http://arxiv.org/abs/1905.13045v3"}, {"title": "A conjecture on the distribution of firm profit", "summary": "A common assumption of political economy is that profit rates across firms or\nsectors tend to uniformity, and often models are formulated in which this\ntendency is assumed to have been realised. But in reality this tendency is\nnever realised and the distribution of firm profits is not degenerate but\nskewed to the right. The mode is less than the mean and super-profits are\npresent. To understand the distribution of firm profits a general probabilistic\nargument is sketched that yields a candidate functional form. The overall\nproperties of the derived distribution are qualitatively consistent with\nempirical measures, although there is more work to be done.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0407687v2", "link": "http://arxiv.org/abs/cond-mat/0407687v2"}, {"title": "Considering pricing and uncertainty in designing a reverse logistics\n  network", "summary": "Companies try to maximize their profits by recovering returned products of\nhighly uncertain quality and quantity. In this paper, a reverse logistics\nnetwork for an Original Equipment Manufacturer (OEM) is presented. Returned\nproducts are selected for remanufacturing or scrapping, based on their quality\nand proportional prices are offered to customers. A Mixed Integer Non-linear\nProgramming (MINLP) model is proposed to determine the location of collection\ncenters, the optimum price of returned products and the sorting policy. The\nrisk in the objective function is measured using the Conditional Value at Risk\n(CVaR) metric. CVaR measures the risk of an investment in a conservative way by\nconsidering the maximum lost. The results are analyzed for various values of\nthe risk parameters ({\\alpha}, and {\\lambda}). These parameters indicate that\nconsidering risk affects prices, the classification of returned products, the\nlocation of collection centers and, consequently, the objective function. The\nmodel performs more conservatively when the weight of the CVaR part ({\\lambda})\nand the value of the confidence level {\\alpha} are increased. The results show\nthat better profits are obtained when we take CVaR into account.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1909.11633v1", "link": "http://arxiv.org/abs/1909.11633v1"}, {"title": "Empirical confirmation of creative destruction from world trade data", "summary": "We show that world trade network datasets contain empirical evidence that the\ndynamics of innovation in the world economy follows indeed the concept of\ncreative destruction, as proposed by J.A. Schumpeter more than half a century\nago. National economies can be viewed as complex, evolving systems, driven by a\nstream of appearance and disappearance of goods and services. Products appear\nin bursts of creative cascades. We find that products systematically tend to\nco-appear, and that product appearances lead to massive disappearance events of\nexisting products in the following years. The opposite - disappearances\nfollowed by periods of appearances - is not observed. This is an empirical\nvalidation of the dominance of cascading competitive replacement events on the\nscale of national economies, i.e. creative destruction. We find a tendency that\nmore complex products drive out less complex ones, i.e. progress has a\ndirection. Finally we show that the growth trajectory of a country's product\noutput diversity can be understood by a recently proposed evolutionary model of\nSchumpeterian economic dynamics.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1112.2984v1", "link": "http://dx.doi.org/10.1371/journal.pone.0038924"}, {"title": "American options with gradual exercise under proportional transaction\n  costs", "summary": "American options in a multi-asset market model with proportional transaction\ncosts are studied in the case when the holder of an option is able to exercise\nit gradually at a so-called mixed (randomised) stopping time. The introduction\nof gradual exercise leads to tighter bounds on the option price when compared\nto the case studied in the existing literature, where the standard assumption\nis that the option can only be exercised instantly at an ordinary stopping\ntime. Algorithmic constructions for the bid and ask prices and the associated\nsuperhedging strategies and optimal mixed stoping times for an American option\nwith gradual exercise are developed and implemented, and dual representations\nare established.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1308.2688v1", "link": "http://arxiv.org/abs/1308.2688v1"}, {"title": "Multifractal fluctuations in finance", "summary": "We consider the structure functions S^(q)(T), i.e. the moments of order q of\nthe increments X(t+T)-X(t) of the Foreign Exchange rate X(t) which give clear\nevidence of scaling (S^(q)(T)~T^z(q)). We demonstrate that the nonlinearity of\nthe observed scaling exponent z(q) is incompatible with monofractal additive\nstochastic models usually introduced in finance: Brownian motion, Levy\nprocesses and their truncated versions. This nonlinearity corresponds to\nmultifractal intermittency yielded by multiplicative processes. The\nnon-analycity of z(q) corresponds to universal multifractals, which are\nfurthermore able to produce ``hyperbolic'' pdf tails with an exponent q_D >2.\nWe argue that it is necessary to introduce stochastic evolution equations which\nare compatible with this multifractal behaviour.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0102369v1", "link": "http://arxiv.org/abs/cond-mat/0102369v1"}, {"title": "Ramsey Rule with Progressive Utility in Long Term Yield Curves Modeling", "summary": "The purpose of this paper relies on the study of long term yield curves\nmodeling. Inspired by the economic litterature, it provides a financial\ninterpretation of the Ramsey rule that links discount rate and marginal utility\nof aggregate optimal consumption. For such a long maturity modelization, the\npossibility of adjusting preferences to new economic information is crucial.\nThus, after recalling some important properties on progressive utility, this\npaper first provides an extension of the notion of a consistent progressive\nutility to a consistent pair of progressive utilities of investment and\nconsumption. An optimality condition is that the utility from the wealth\nsatisfies a second order SPDE of HJB type involving the Fenchel-Legendre\ntransform of the utility from consumption. This SPDE is solved in order to give\na full characterization of this class of consistent progressive pair of\nutilities. An application of this results is to revisit the classical backward\noptimization problem in the light of progressive utility theory, emphasizing\nintertemporal-consistency issue. Then we study the dynamics of the marginal\nutility yield curve, and give example with backward and progressive power\nutilities.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1404.1895v1", "link": "http://arxiv.org/abs/1404.1895v1"}, {"title": "Convergence in Multiscale Financial Models with Non-Gaussian Stochastic\n  Volatility", "summary": "We consider stochastic control systems affected by a fast mean reverting\nvolatility $Y(t)$ driven by a pure jump L\\'evy process. Motivated by a large\nliterature on financial models, we assume that $Y(t)$ evolves at a faster time\nscale $\\frac{t}{\\varepsilon}$ than the assets, and we study the asymptotics as\n$\\varepsilon\\to 0$. This is a singular perturbation problem that we study\nmostly by PDE methods within the theory of viscosity solutions.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1405.6514v1", "link": "http://arxiv.org/abs/1405.6514v1"}, {"title": "Fear and its implications for stock markets", "summary": "The value of stocks, indices and other assets, are examples of stochastic\nprocesses with unpredictable dynamics. In this paper, we discuss asymmetries in\nshort term price movements that can not be associated with a long term positive\ntrend. These empirical asymmetries predict that stock index drops are more\ncommon on a relatively short time scale than the corresponding raises. We\npresent several empirical examples of such asymmetries. Furthermore, a simple\nmodel featuring occasional short periods of synchronized dropping prices for\nall stocks constituting the index is introduced with the aim of explaining\nthese facts. The collective negative price movements are imagined triggered by\nexternal factors in our society, as well as internal to the economy, that\ncreate fear of the future among investors. This is parameterized by a ``fear\nfactor'' defining the frequency of synchronized events. It is demonstrated that\nsuch a simple fear factor model can reproduce several empirical facts\nconcerning index asymmetries. It is also pointed out that in its simplest form,\nthe model has certain shortcomings.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0609046v2", "link": "http://dx.doi.org/10.1140/epjb/e2007-00125-4"}, {"title": "Cash Sub-additive Risk Measures and Interest Rate Ambiguity", "summary": "A new class of risk measures called cash sub-additive risk measures is\nintroduced to assess the risk of future financial, nonfinancial and insurance\npositions. The debated cash additive axiom is relaxed into the cash sub\nadditive axiom to preserve the original difference between the numeraire of the\ncurrent reserve amounts and future positions. Consequently, cash sub-additive\nrisk measures can model stochastic and/or ambiguous interest rates or\ndefaultable contingent claims. Practical examples are presented and in such\ncontexts cash additive risk measures cannot be used. Several representations of\nthe cash sub-additive risk measures are provided. The new risk measures are\ncharacterized by penalty functions defined on a set of sub-linear probability\nmeasures and can be represented using penalty functions associated with cash\nadditive risk measures defined on some extended spaces. The issue of the\noptimal risk transfer is studied in the new framework using inf-convolution\ntechniques. Examples of dynamic cash sub-additive risk measures are provided\nvia BSDEs where the generator can locally depend on the level of the cash\nsub-additive risk measure.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/0710.4106v1", "link": "http://arxiv.org/abs/0710.4106v1"}, {"title": "Smoothing quantile regressions", "summary": "We propose to smooth the entire objective function, rather than only the\ncheck function, in a linear quantile regression context. Not only does the\nresulting smoothed quantile regression estimator yield a lower mean squared\nerror and a more accurate Bahadur-Kiefer representation than the standard\nestimator, but it is also asymptotically differentiable. We exploit the latter\nto propose a quantile density estimator that does not suffer from the curse of\ndimensionality. This means estimating the conditional density function without\nworrying about the dimension of the covariate vector. It also allows for\ntwo-stage efficient quantile regression estimation. Our asymptotic theory holds\nuniformly with respect to the bandwidth and quantile level. Finally, we propose\na rule of thumb for choosing the smoothing bandwidth that should approximate\nwell the optimal bandwidth. Simulations confirm that our smoothed quantile\nregression estimator indeed performs very well in finite samples.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.08535v3", "link": "http://arxiv.org/abs/1905.08535v3"}, {"title": "Data Analytics in Operations Management: A Review", "summary": "Research in operations management has traditionally focused on models for\nunderstanding, mostly at a strategic level, how firms should operate. Spurred\nby the growing availability of data and recent advances in machine learning and\noptimization methodologies, there has been an increasing application of data\nanalytics to problems in operations management. In this paper, we review recent\napplications of data analytics to operations management, in three major areas\n-- supply chain management, revenue management and healthcare operations -- and\nhighlight some exciting directions for the future.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.00556v1", "link": "http://arxiv.org/abs/1905.00556v1"}, {"title": "The Relationship Between Stock Market Parameters and Interbank Lending\n  Market: an Empirical Evidence", "summary": "The article presents calculations that prove practical importance of the\nearlier derived theoretical relationship between the interest rate on the\ninterbank credit market, volume of investment and the quantity of securities\ntradable on the stock exchange.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1309.5703v1", "link": "http://arxiv.org/abs/1309.5703v1"}, {"title": "On the Preference Relations with Negatively Transitive Asymmetric Part.\n  I", "summary": "Given a linearly ordered set I, every surjective map p: A --> I endows the\nset A with a structure of set of preferences by \"replacing\" the elements of I\nwith their inverse images via p considered as \"balloons\" (sets endowed with an\nequivalence relation), lifting the linear order on A, and \"agglutinating\" this\nstructure with the balloons. Every ballooning A of a structure of linearly\nordered set I is a set of preferences whose preference relation (not\nnecessarily complete) is negatively transitive and every such structure on a\ngiven set A can be obtained by ballooning of certain structure of a linearly\nordered set I, intrinsically encoded in A. In other words, the difference\nbetween linearity and negative transitivity is constituted of balloons. As a\nconsequence of this characterization, under certain natural topological\nconditions on the set of preferences A furnished with its interval topology,\nthe existence of a continuous generalized utility function on A is proved.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1302.7238v2", "link": "http://arxiv.org/abs/1302.7238v2"}, {"title": "Counterparty Credit Limits: An Effective Tool for Mitigating\n  Counterparty Risk?", "summary": "A counterparty credit limit (CCL) is a limit imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nAlthough CCLs are designed to help institutions mitigate counterparty risk by\nselective diversification of their exposures, their implementation restricts\nthe liquidity that institutions can access in an otherwise centralized pool. We\naddress the question of how this mechanism impacts trade prices and volatility,\nboth empirically and via a new model of trading with CCLs. We find empirically\nthat CCLs cause little impact on trade. However, our model highlights that in\nextreme situations, CCLs could serve to destabilize prices and thereby\ninfluence systemic risk.", "category": ["q-fin.TR", "econ.EM", "math.PR"], "id": "http://arxiv.org/abs/1709.08238v2", "link": "http://arxiv.org/abs/1709.08238v2"}, {"title": "Portfolio Risk Assessment using Copula Models", "summary": "In the paper, we use and investigate copulas models to represent multivariate\ndependence in financial time series. We propose the algorithm of risk measure\ncomputation using copula models. Using the optimal mean-$CVaR$ portfolio we\ncompute portfolio's Profit and Loss series and corresponded risk measures\ncurves. Value-at-risk and Conditional-Value-at-risk curves were simulated by\nthree copula models: full Gaussian, Student's $t$ and regular vine copula.\nThese risk curves are lower than historical values of the risk measures curve.\nAll three models have superior prediction ability than a usual empirical\nmethod. Further directions of research are described.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1707.03516v1", "link": "http://arxiv.org/abs/1707.03516v1"}, {"title": "Spatial and temporal structures of four financial markets in Greater\n  China", "summary": "We investigate the spatial and temporal structures of four financial markets\nin Greater China. In particular, we uncover different characteristics of the\nfour markets by analyzing the sector and subsector structures which are\ndetected through the random matrix theory. Meanwhile, we observe that the\nTaiwan and Hongkong stock markets show a negative return-volatility\ncorrelation, i.e., the so-called leverage effect. The Shanghai and Shenzhen\nstock markets are more complicated. Before the year 2000, the two markets\nexhibit a strong positive return-volatility correlation, which is called the\nanti-leverage effect. After 2000, however, it gradually changes to the leverage\neffect. We also find that the recurrence interval distributions of both the\ntrading volume volatilities and price volatilities follow a power law behavior,\nwhile the exponents vary among different markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1402.1046v1", "link": "http://dx.doi.org/10.1016/j.physa.2014.02.006"}, {"title": "Dual Representation of Quasiconvex Conditional Maps", "summary": "We provide a dual representation of quasiconvex maps between two lattices of\nrandom variables in terms of conditional expectations. This generalizes the\ndual representation of quasiconvex real valued functions and the dual\nrepresentation of conditional convex maps.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1001.3644v2", "link": "http://arxiv.org/abs/1001.3644v2"}, {"title": "Monotonicity-Constrained Nonparametric Estimation and Inference for\n  First-Price Auctions", "summary": "We propose a new nonparametric estimator for first-price auctions with\nindependent private values that imposes the monotonicity constraint on the\nestimated inverse bidding strategy. We show that our estimator has a smaller\nasymptotic variance than that of Guerre, Perrigne and Vuong's (2000) estimator.\nIn addition to establishing pointwise asymptotic normality of our estimator, we\nprovide a bootstrap-based approach to constructing uniform confidence bands for\nthe density function of latent valuations.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.12974v1", "link": "http://arxiv.org/abs/1909.12974v1"}, {"title": "L\u00e9vy-Ito Models in Finance", "summary": "We propose a class of financial models in which the prices of assets are\nL\\'evy-Ito processes driven by Brownian motion and a dynamic Poisson random\nmeasure. Each such model consists of a pricing kernel, a money market account,\nand one or more risky assets. The Poisson random measure is associated with an\n$n$-dimensional L\\'evy process. We show that the excess rate of return of a\nrisky asset in a pure-jump model is given by an integral of the product of a\nterm representing the riskiness of the asset and a term representing the level\nof market risk aversion. The integral is over the state space of the Poisson\nrandom measure and is taken with respect to the L\\'evy measure associated with\nthe $n$-dimensional L\\'evy process. The resulting framework is applied to a\nvariety of different asset classes, allowing one to construct new models as\nwell as non-trivial generalizations of familiar models.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1907.08499v3", "link": "http://arxiv.org/abs/1907.08499v3"}, {"title": "Augmented Factor Models with Applications to Validating Market Risk\n  Factors and Forecasting Bond Risk Premia", "summary": "We study factor models augmented by observed covariates that have explanatory\npowers on the unknown factors. In financial factor models, the unknown factors\ncan be reasonably well explained by a few observable proxies, such as the\nFama-French factors. In diffusion index forecasts, identified factors are\nstrongly related to several directly measurable economic variables such as\nconsumption-wealth variable, financial ratios, and term spread. With those\ncovariates, both the factors and loadings are identifiable up to a rotation\nmatrix even only with a finite dimension. To incorporate the explanatory power\nof these covariates, we propose a smoothed principal component analysis (PCA):\n(i) regress the data onto the observed covariates, and (ii) take the principal\ncomponents of the fitted data to estimate the loadings and factors. This allows\nus to accurately estimate the percentage of both explained and unexplained\ncomponents in factors and thus to assess the explanatory power of covariates.\nWe show that both the estimated factors and loadings can be estimated with\nimproved rates of convergence compared to the benchmark method. The degree of\nimprovement depends on the strength of the signals, representing the\nexplanatory power of the covariates on the factors. The proposed estimator is\nrobust to possibly heavy-tailed distributions. We apply the model to forecast\nUS bond risk premia, and find that the observed macroeconomic characteristics\ncontain strong explanatory powers of the factors. The gain of forecast is more\nsubstantial when the characteristics are incorporated to estimate the common\nfactors than directly used for forecasts.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1603.07041v2", "link": "http://arxiv.org/abs/1603.07041v2"}, {"title": "Mixed Models as an Alternative to Farima", "summary": "We construct a new process using a fractional Brownian motion and a\nfractional Ornstein-Uhlenbeck process of the Second Kind as building blocks. We\nconsider the increments of the new process in discrete time and, as a result,\nwe obtain a more parsimonious process with similar autocovariance structure to\nthat of a FARIMA. In practice, variance of the new increment process is a\nclosed-form expression easier to compute than that of FARIMA.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1712.03044v1", "link": "http://arxiv.org/abs/1712.03044v1"}, {"title": "Asset Allocation under the Basel Accord Risk Measures", "summary": "Financial institutions are currently required to meet more stringent capital\nrequirements than they were before the recent financial crisis; in particular,\nthe capital requirement for a large bank's trading book under the Basel 2.5\nAccord more than doubles that under the Basel II Accord. The significant\nincrease in capital requirements renders it necessary for banks to take into\naccount the constraint of capital requirement when they make asset allocation\ndecisions. In this paper, we propose a new asset allocation model that\nincorporates the regulatory capital requirements under both the Basel 2.5\nAccord, which is currently in effect, and the Basel III Accord, which was\nrecently proposed and is currently under discussion. We propose an unified\nalgorithm based on the alternating direction augmented Lagrangian method to\nsolve the model; we also establish the first-order optimality of the limit\npoints of the sequence generated by the algorithm under some mild conditions.\nThe algorithm is simple and easy to implement; each step of the algorithm\nconsists of solving convex quadratic programming or one-dimensional\nsubproblems. Numerical experiments on simulated and real market data show that\nthe algorithm compares favorably with other existing methods, especially in\ncases in which the model is non-convex.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1308.1321v1", "link": "http://arxiv.org/abs/1308.1321v1"}, {"title": "Efficiency in Truthful Auctions via a Social Network", "summary": "In this paper, we study efficiency in truthful auctions via a social network,\nwhere a seller can only spread the information of an auction to the buyers\nthrough the buyers' network. In single-item auctions, we show that no mechanism\nis strategy-proof, individually rational, efficient, and weakly budget\nbalanced. In addition, we propose $\\alpha$-APG mechanisms, a class of\nmechanisms which operate a trade-off between efficiency and weakly budget\nbalancedness. In multi-item auctions, there already exists a strategy-proof\nmechanism when all buyers need only one item. However, we indicate a\ncounter-example to strategy-proofness in this mechanism, and to the best of our\nknowledge, the question of finding a strategy-proof mechanism remains open. We\nassume that all buyers have decreasing marginal utility and propose a\ngeneralized APG mechanism that is strategy-proof and individually rational but\nnot efficient. Importantly, we show that this mechanism achieves the largest\nefficiency measure among all strategy-proof mechanisms.", "category": [], "id": "http://arxiv.org/abs/1904.12422v1", "link": "http://arxiv.org/abs/1904.12422v1"}, {"title": "A new approach to unbiased estimation for SDE's", "summary": "In this paper, we introduce a new approach to constructing unbiased\nestimators when computing expectations of path functionals associated with\nstochastic differential equations (SDEs). Our randomization idea is closely\nrelated to multi-level Monte Carlo and provides a simple mechanism for\nconstructing a finite variance unbiased estimator with \"square root convergence\nrate\" whenever one has available a scheme that produces strong error of order\ngreater than 1/2 for the path functional under consideration.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1207.2452v1", "link": "http://arxiv.org/abs/1207.2452v1"}, {"title": "Robust Inference Using Inverse Probability Weighting", "summary": "Inverse Probability Weighting (IPW) is widely used in empirical work in\neconomics and other disciplines. As Gaussian approximations perform poorly in\nthe presence of \"small denominators,\" trimming is routinely employed as a\nregularization strategy. However, ad hoc trimming of the observations renders\nusual inference procedures invalid for the target estimand, even in large\nsamples. In this paper, we first show that the IPW estimator can have different\n(Gaussian or non-Gaussian) asymptotic distributions, depending on how \"close to\nzero\" the probability weights are and on how large the trimming threshold is.\nAs a remedy, we propose an inference procedure that is robust not only to small\nprobability weights entering the IPW estimator but also to a wide range of\ntrimming threshold choices, by adapting to these different asymptotic\ndistributions. This robustness is achieved by employing resampling techniques\nand by correcting a non-negligible trimming bias. We also propose an\neasy-to-implement method for choosing the trimming threshold by minimizing an\nempirical analogue of the asymptotic mean squared error. In addition, we show\nthat our inference procedure remains valid with the use of a data-driven\ntrimming threshold. We illustrate our method by revisiting a dataset from the\nNational Supported Work program.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.11397v2", "link": "http://arxiv.org/abs/1810.11397v2"}, {"title": "Toward A Normative Theory of Normative Marketing Theory", "summary": "We show how different approaches to developing marketing strategies depending\non the type of environment a firm faces, where environments are distinguished\nin terms of their systems properties not their context. Particular emphasis is\ngiven to turbulent environments in which outcomes are not a priori predictable\nand are not traceable to individual firm actions and we show that, in these\nconditions, the relevant unit of competitive response and understanding is no\nlonger the individual firm but the network of relations comprising\ninterdependent, interacting firms. Networks of relations are complex adaptive\nsystems that are more 'intelligent' than the individual firms that comprise\nthem and are capable of comprehending and responding to more complex and\nturbulent environments. Yet they are co-produced by the patterns of actions and\ninteractions of the firms involved. The creation and accessing of such\ndistributed intelligence cannot be centrally directed, as this necessarily\nlimits it. Instead managers and firms are involved in a kind of participatory\nplanning and adaptation process through which the network self-organises and\nadapts. Drawing on research in systems theory, complexity, biology and\ncognitive science, extensions to the resource-based theory of the firm are\nproposed that include how resources are linked across relations and network in\na dynamic and evolutionary way. The concept of an extended firm and soft\nassembled strategies are introduced to describe the nature of the strategy\ndevelopment process. This results in a more theoretically grounded basis for\nunderstanding the nature and role of relationship and network strategies in\nmarketing and management. We finish by considering the research implications of\nour analysis and the role of agent based models as a means of sensitising and\ninforming management action.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1205.5821v1", "link": "http://arxiv.org/abs/1205.5821v1"}, {"title": "Asymptotics for Greeks under the constant elasticity of variance model", "summary": "This paper is concerned with the asymptotics for Greeks of European-style\noptions and the risk-neutral density function calculated under the constant\nelasticity of variance model. Formulae obtained help financial engineers to\nconstruct a perfect hedge with known behaviour and to price any options on\nfinancial assets.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1707.04149v2", "link": "http://arxiv.org/abs/1707.04149v2"}, {"title": "Macroeconomic Dynamics of Assets, Leverage and Trust", "summary": "A macroeconomic model based on the economic variables (i) assets, (ii)\nleverage (defined as debt over asset) and (iii) trust (defined as the maximum\nsustainable leverage) is proposed to investigate the role of credit in the\ndynamics of economic growth, and how credit may be associated with both\neconomic performance and confidence. Our first notable finding is the mechanism\nof reward/penalty associated with patience, as quantified by the return on\nassets. In regular economies where the EBITA/Assets ratio is larger than the\ncost of debt, starting with a trust higher than leverage results in the highest\nlong-term return on assets (which can be seen as a proxy for economic growth).\nOur second main finding concerns a recommendation for the reaction of a central\nbank to an external shock that affects negatively the economic growth. We find\nthat late policy intervention in the model economy results in the highest\nlong-term return on assets and largest asset value. But this comes at the cost\nof suffering longer from the crisis until the intervention occurs. The\nphenomenon can be ascribed to the fact that postponing intervention allows\ntrust to increase first, and it is most effective to intervene when trust is\nhigh. These results derive from two fundamental assumptions underlying our\nmodel: (a) trust tends to increase when it is above leverage; (b) economic\nagents learn optimally to adjust debt for a given level of trust and amount of\nassets. Using a Markov Switching Model for the EBITA/Assets ratio, we have\nsuccessfully calibrated our model to the empirical data of the return on equity\nof the EURO STOXX 50 for the time period 2000-2013. We find that dynamics of\nleverage and trust can be highly non-monotonous with curved trajectories, as a\nresult of the nonlinear coupling between the variables.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1512.03618v1", "link": "http://dx.doi.org/10.1142/S0218127416501339"}, {"title": "On lower and upper bounds for Asian-type options: a unified approach", "summary": "In the context of dealing with financial risk management problems it is\ndesirable to have accurate bounds for option prices in situations when pricing\nformulae do not exist in the closed form. A unified approach for obtaining\nupper and lower bounds for Asian-type options, including options on VWAP, is\nproposed in this paper. The bounds obtained are applicable to the continuous\nand discrete-time frameworks for the case of time-dependent interest rates.\nNumerical examples are provided to illustrate the accuracy of the bounds.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1309.2383v1", "link": "http://arxiv.org/abs/1309.2383v1"}, {"title": "Robust Optimization of Credit Portfolios", "summary": "We introduce a dynamic credit portfolio framework where optimal investment\nstrategies are robust against misspecifications of the reference credit model.\nThe risk-averse investor models his fear of credit risk misspecification by\nconsidering a set of plausible alternatives whose expected log likelihood\nratios are penalized. We provide an explicit characterization of the optimal\nrobust bond investment strategy, in terms of default state dependent value\nfunctions associated with the max-min robust optimization criterion. The value\nfunctions can be obtained as the solutions of a recursive system of HJB\nequations. We show that each HJB equation is equivalent to a suitably truncated\nequation admitting a unique bounded regular solution. The truncation technique\nrelies on estimates for the solution of the master HJB equation that we\nestablish.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1603.08169v1", "link": "http://arxiv.org/abs/1603.08169v1"}, {"title": "On Shortfall Risk Minimization for Game Options", "summary": "In this paper we study the existence of an optimal hedging strategy for the\nshortfall risk measure in the game options setup. We consider the continuous\ntime Black--Scholes (BS) model. Our first result says that in the case where\nthe game contingent claim (GCC) can be exercised only on a finite set of times,\nthere exists an optimal strategy. Our second and main result is an example\nwhich demonstrates that for the case where the GCC can be stopped on the all\ntime interval, optimal portfolio strategies need not always exist.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2002.01528v1", "link": "http://arxiv.org/abs/2002.01528v1"}, {"title": "Financial series prediction using Attention LSTM", "summary": "Financial time series prediction, especially with machine learning\ntechniques, is an extensive field of study. In recent times, deep learning\nmethods (especially time series analysis) have performed outstandingly for\nvarious industrial problems, with better prediction than machine learning\nmethods. Moreover, many researchers have used deep learning methods to predict\nfinancial time series with various models in recent years. In this paper, we\nwill compare various deep learning models, such as multilayer perceptron (MLP),\none-dimensional convolutional neural networks (1D CNN), stacked long short-term\nmemory (stacked LSTM), attention networks, and weighted attention networks for\nfinancial time series prediction. In particular, attention LSTM is not only\nused for prediction, but also for visualizing intermediate outputs to analyze\nthe reason of prediction; therefore, we will show an example for understanding\nthe model prediction intuitively with attention vectors. In addition, we focus\non time and factors, which lead to an easy understanding of why certain trends\nare predicted when accessing a given time series table. We also modify the loss\nfunctions of the attention models with weighted categorical cross entropy; our\nproposed model produces a 0.76 hit ratio, which is superior to those of other\nmethods for predicting the trends of the KOSPI 200.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1902.10877v1", "link": "http://arxiv.org/abs/1902.10877v1"}, {"title": "Coverage Error Optimal Confidence Intervals for Local Polynomial\n  Regression", "summary": "In this paper we develop new confidence intervals for local polynomial\nregression, which minimize their worse-case coverage error and length in large\nsamples. Our results rely on novel, valid Edgeworth expansions for\n$t$-statistics based on local polynomial methods, which are established\nuniformly over relevant classes of data generating processes and interval\nestimators. These higher-order expansions also allow for the uniform kernel and\nany derivative order, significantly improving on previous technical results\navailable in the literature. In addition, we discuss principled,\ninference-optimal tuning parameter (bandwidth) selection and kernel functions.\nThe main methodological results obtained in this paper are implemented in\ncompanion {\\sf R} and \\texttt{Stata} software packages.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.01398v2", "link": "http://arxiv.org/abs/1808.01398v2"}, {"title": "On Capital Dependent Dynamics of Knowledge", "summary": "We investigate the dynamics of growth models in terms of dynamical system\ntheory. We analyse some forms of knowledge and its influence on economic\ngrowth. We assume that the rate of change of knowledge depends on both the rate\nof change of physical and human capital. First, we study model with constant\nsavings. The model with optimised behaviour of households is also considered.\nWe show that the model where the rate of change of knowledge depends only on\nthe rate of change of physical capital can be reduced to the form of the\ntwo-dimensional autonomous dynamical system. All possible evolutional paths and\nthe stability of solutions in the phase space are discussed in details. We\nobtain that the rate of growth of capital, consumption and output are greater\nin the case of capital dependent rate of change of knowledge.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0608197v1", "link": "http://arxiv.org/abs/physics/0608197v1"}, {"title": "Local Linear Forests", "summary": "Random forests are a powerful method for non-parametric regression, but are\nlimited in their ability to fit smooth signals, and can show poor predictive\nperformance in the presence of strong, smooth effects. Taking the perspective\nof random forests as an adaptive kernel method, we pair the forest kernel with\na local linear regression adjustment to better capture smoothness. The\nresulting procedure, local linear forests, enables us to improve on asymptotic\nrates of convergence for random forests with smooth signals, and provides\nsubstantial gains in accuracy on both real and simulated data. We prove a\ncentral limit theorem valid under regularity conditions on the forest and\nsmoothness constraints, and propose a computationally efficient construction\nfor confidence intervals. Moving to a causal inference application, we discuss\nthe merits of local regression adjustments for heterogeneous treatment effect\nestimation, and give an example on a dataset exploring the effect word choice\nhas on attitudes to the social safety net. Last, we include simulation results\non real and generated data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1807.11408v3", "link": "http://arxiv.org/abs/1807.11408v3"}, {"title": "Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time\n  Series Analysis", "summary": "In this work, we develop a novel framework to measure the similarity between\ndynamic financial networks, i.e., time-varying financial networks.\nParticularly, we explore whether the proposed similarity measure can be\nemployed to understand the structural evolution of the financial networks with\ntime. For a set of time-varying financial networks with each vertex\nrepresenting the individual time series of a different stock and each edge\nbetween a pair of time series representing the absolute value of their Pearson\ncorrelation, our start point is to compute the commute time matrix associated\nwith the weighted adjacency matrix of the network structures, where each\nelement of the matrix can be seen as the enhanced correlation value between\npairwise stocks. For each network, we show how the commute time matrix allows\nus to identify a reliable set of dominant correlated time series as well as an\nassociated dominant probability distribution of the stock belonging to this\nset. Furthermore, we represent each original network as a discrete dominant\nShannon entropy time series computed from the dominant probability\ndistribution. With the dominant entropy time series for each pair of financial\nnetworks to hand, we develop a similarity measure based on the classical\ndynamic time warping framework, for analyzing the financial time-varying\nnetworks. We show that the proposed similarity measure is positive definite and\nthus corresponds to a kernel measure on graphs. The proposed kernel bridges the\ngap between graph kernels and the classical dynamic time warping framework for\nmultiple financial time series analysis. Experiments on time-varying networks\nextracted through New York Stock Exchange (NYSE) database demonstrate the\neffectiveness of the proposed approach.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1910.09153v1", "link": "http://arxiv.org/abs/1910.09153v1"}, {"title": "Renyi's information transfer between financial time series", "summary": "In this paper, we quantify the statistical coherence between financial time\nseries by means of the Renyi entropy. With the help of Campbell's coding\ntheorem we show that the Renyi entropy selectively emphasizes only certain\nsectors of the underlying empirical distribution while strongly suppressing\nothers. This accentuation is controlled with Renyi's parameter q. To tackle the\nissue of the information flow between time series we formulate the concept of\nRenyi's transfer entropy as a measure of information that is transferred only\nbetween certain parts of underlying distributions. This is particularly\npertinent in financial time series where the knowledge of marginal events such\nas spikes or sudden jumps is of a crucial importance. We apply the Renyian\ninformation flow to stock market time series from 11 world stock indices as\nsampled at a daily rate in the time period 02.01.1990 - 31.12.2009.\nCorresponding heat maps and net information flows are represented graphically.\nA detailed discussion of the transfer entropy between the DAX and S&P500\nindices based on minute tick data gathered in the period from 02.04.2008 to\n11.09.2009 is also provided. Our analysis shows that the bivariate information\nflow between world markets is strongly asymmetric with a distinct information\nsurplus flowing from the Asia-Pacific region to both European and US markets.\nAn important yet less dramatic excess of information also flows from Europe to\nthe US. This is particularly clearly seen from a careful analysis of Renyi\ninformation flow between the DAX and S&P500 indices.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1106.5913v3", "link": "http://dx.doi.org/10.1016/j.physa.2011.12.064"}, {"title": "A Closed-Form Approximation of Likelihood Functions for Discretely\n  Sampled Diffusions: the Exponent Expansion", "summary": "In this paper we discuss a closed-form approximation of the likelihood\nfunctions of an arbitrary diffusion process. The approximation is based on an\nexponential ansatz of the transition probability for a finite time step $\\Delta\nt$, and a series expansion of the deviation of its logarithm from that of a\nGaussian distribution. Through this procedure, dubbed {\\em exponent expansion},\nthe transition probability is obtained as a power series in $\\Delta t$. This\nbecomes asymptotically exact if an increasing number of terms is included, and\nprovides remarkably accurate results even when truncated to the first few (say\n3) terms. The coefficients of such expansion can be determined\nstraightforwardly through a recursion, and involve simple one-dimensional\nintegrals.\n  We present several examples of financial interest, and we compare our results\nwith the state-of-the-art approximation of discretely sampled diffusions\n[A\\\"it-Sahalia, {\\it Journal of Finance} {\\bf 54}, 1361 (1999)]. We find that\nthe exponent expansion provides a similar accuracy in most of the cases, but a\nbetter behavior in the low-volatility regime. Furthermore the implementation of\nthe present approach turns out to be simpler.\n  Within the functional integration framework the exponent expansion allows one\nto obtain remarkably good approximations of the pricing kernels of financial\nderivatives. This is illustrated with the application to simple path-dependent\ninterest rate derivatives. Finally we discuss how these results can also be\nused to increase the efficiency of numerical (both deterministic and\nstochastic) approaches to derivative pricing.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0703180v1", "link": "http://arxiv.org/abs/physics/0703180v1"}, {"title": "Machine Learning Algorithms for Financial Asset Price Forecasting", "summary": "This research paper explores the performance of Machine Learning (ML)\nalgorithms and techniques that can be used for financial asset price\nforecasting. The prediction and forecasting of asset prices and returns remains\none of the most challenging and exciting problems for quantitative finance and\npractitioners alike. The massive increase in data generated and captured in\nrecent years presents an opportunity to leverage Machine Learning algorithms.\nThis study directly compares and contrasts state-of-the-art implementations of\nmodern Machine Learning algorithms on high performance computing (HPC)\ninfrastructures versus the traditional and highly popular Capital Asset Pricing\nModel (CAPM) on U.S equities data. The implemented Machine Learning models -\ntrained on time series data for an entire stock universe (in addition to\nexogenous macroeconomic variables) significantly outperform the CAPM on\nout-of-sample (OOS) test data.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/2004.01504v1", "link": "http://arxiv.org/abs/2004.01504v1"}, {"title": "Concave Distortion Semigroups", "summary": "The problem behind this paper is the proper measurement of the degree of\nquality/acceptability/distance to arbitrage of trades. We are narrowing the\nclass of coherent acceptability indices introduced by Cherny and Madan (2007)\nby imposing an additional mathematical property. For this, we introduce the\nnotion of a concave distortion semigroup as a family $(\\Psi_t)_{t\\ge0}$ of\nconcave increasing functions $[0,1]\\to[0,1]$ satisfying the semigroup property\n$$ \\Psi_s\\circ\\Psi_t=\\Psi_{s+t},\\quad s,t\\ge0. $$ The goal of the paper is the\ninvestigation of these semigroups with regard to the following aspects:\nrepresentation of distortion semigroups; properties of distortion semigroups\ndesirable from the economical or mathematical perspective; determining which\nconcave distortions belong to some distortion semigroup.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1104.0508v1", "link": "http://arxiv.org/abs/1104.0508v1"}, {"title": "Adapting the CVA model to Leland's framework", "summary": "We consider the framework proposed by Burgard and Kjaer (2011) that derives\nthe PDE which governs the price of an option including bilateral counterparty\nrisk and funding. We extend this work by relaxing the assumption of absence of\ntransaction costs in the hedging portfolio by proposing a cost proportional to\nthe amount of assets traded and the traded price. After deriving the nonlinear\nPDE, we prove the existence of a solution for the corresponding\ninitial-boundary value problem. Moreover, we develop a numerical scheme that\nallows to find the solution of the PDE by setting different values for each\nparameter of the model. To understand the impact of each variable within the\nmodel, we analyze the Greeks of the option and the sensitivity of the price to\nchanges in all the risk factors.", "category": ["q-fin.MF", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1802.04837v1", "link": "http://arxiv.org/abs/1802.04837v1"}, {"title": "Tails of weakly dependent random vectors", "summary": "We introduce a new functional measure of tail dependence for weakly dependent\n(asymptotically independent) random vectors, termed weak tail dependence\nfunction. The new measure is defined at the level of copulas and we compute it\nfor several copula families such as the Gaussian copula, copulas of a class of\nGaussian mixture models, certain Archimedean copulas and extreme value copulas.\nThe new measure allows to quantify the tail behavior of certain functionals of\nweakly dependent random vectors at the log scale.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1402.4683v3", "link": "http://dx.doi.org/10.1016/j.jmva.2015.12.008"}, {"title": "L\u00e9vy Processes For Finance: An Introduction In R", "summary": "This brief manuscript provides an introduction to L\\'evy processes and their\napplications in finance as the random process that drives asset models.\nCharacteristic functions and random variable generators of popular L\\'evy\nprocesses are presented in R.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1503.03902v1", "link": "http://arxiv.org/abs/1503.03902v1"}, {"title": "Machine Learning for Better Models for Predicting Bond Prices", "summary": "Bond prices are a reflection of extremely complex market interactions and\npolicies, making prediction of future prices difficult. This task becomes even\nmore challenging due to the dearth of relevant information, and accuracy is not\nthe only consideration--in trading situations, time is of the essence. Thus,\nmachine learning in the context of bond price predictions should be both fast\nand accurate. In this course project, we use a dataset describing the previous\n10 trades of a large number of bonds among other relevant descriptive metrics\nto predict future bond prices. Each of 762,678 bonds in the dataset is\ndescribed by a total of 61 attributes, including a ground truth trade price. We\nevaluate the performance of various supervised learning algorithms for\nregression followed by ensemble methods, with feature and model selection\nconsiderations being treated in detail. We further evaluate all methods on both\naccuracy and speed. Finally, we propose a novel hybrid time-series aided\nmachine learning method that could be applied to such datasets in future work.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1705.01142v1", "link": "http://arxiv.org/abs/1705.01142v1"}, {"title": "Non-concave optimal investment and no-arbitrage: a measure theoretical\n  approach", "summary": "We consider non-concave and non-smooth random utility functions with do- main\nof definition equal to the non-negative half-line. We use a dynamic pro-\ngramming framework together with measurable selection arguments to establish\nboth the no-arbitrage condition characterization and the existence of an\noptimal portfolio in a (generically incomplete) discrete-time financial market\nmodel with finite time horizon. In contrast to the existing literature, we\npropose to consider a probability space which is not necessarily complete.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1602.06685v3", "link": "http://arxiv.org/abs/1602.06685v3"}, {"title": "Volatility swaps valuation under stochastic volatility with jumps and\n  stochastic intensity", "summary": "In this paper, a pricing formula for volatility swaps is delivered when the\nunderlying asset follows the stochastic volatility model with jumps and\nstochastic intensity. By using Feynman-Kac theorem, a partial integral\ndifferential equation is obtained to derive the joint moment generating\nfunction of the previous model.\n  Moreover, discrete and continuous sampled volatility swap pricing formulas\nare given by employing transform techniques and the relationship between two\npricing formulas is discussed. Finally, some numerical simulations are reported\nto support the results presented in this paper.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1805.06226v2", "link": "http://arxiv.org/abs/1805.06226v2"}, {"title": "Fast Training Algorithms for Deep Convolutional Fuzzy Systems with\n  Application to Stock Index Prediction", "summary": "A deep convolutional fuzzy system (DCFS) on a high-dimensional input space is\na multi-layer connection of many low-dimensional fuzzy systems, where the input\nvariables to the low-dimensional fuzzy systems are selected through a moving\nwindow across the input spaces of the layers. To design the DCFS based on\ninput-output data pairs, we propose a bottom-up layer-by-layer scheme.\nSpecifically, by viewing each of the first-layer fuzzy systems as a weak\nestimator of the output based only on a very small portion of the input\nvariables, we design these fuzzy systems using the WM Method. After the\nfirst-layer fuzzy systems are designed, we pass the data through the first\nlayer to form a new data set and design the second-layer fuzzy systems based on\nthis new data set in the same way as designing the first-layer fuzzy systems.\nRepeating this process layer-by-layer we design the whole DCFS. We also propose\na DCFS with parameter sharing to save memory and computation. We apply the DCFS\nmodels to predict a synthetic chaotic plus random time-series and the real Hang\nSeng Index of the Hong Kong stock market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1812.11226v2", "link": "http://arxiv.org/abs/1812.11226v2"}, {"title": "Time-dependent lead-lag relationship between the onshore and offshore\n  Renminbi exchange rates", "summary": "We employ the thermal optimal path method to explore both the long-term and\nshort-term interaction patterns between the onshore CNY and offshore CNH\nexchange rates (2012-2015). For the daily data, the CNY and CNH exchange rates\nshow a weak alternate lead-lag structure in most of the time periods. When CNY\nand CNH display a large disparity, the lead-lag relationship is uncertain and\ndepends on the prevailing market factors. The minute-scale interaction pattern\nbetween the CNY and CNH exchange rates change over time according to different\nmarket situations. We find that US dollar appreciation is associated with a\nlead-lag relationship running from offshore to onshore, while a (contrarian)\nRenminbi appreciation is associated with a lead-lag relationship running from\nonshore to offshore. These results are robust with respect to different\nsub-sample analyses and variations of the key smoothing parameter of the TOP\nmethod.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1803.09432v1", "link": "http://dx.doi.org/10.1016/j.intfin.2017.05.001"}, {"title": "Model reduction for calibration of American options", "summary": "American put options are among the most frequently traded single stock\noptions, and their calibration is computationally challenging since no\nclosed-form expression is available. Due to the higher flexibility in\ncomparison to European options, the mathematical model involves additional\nconstraints, and a variational inequality is obtained. We use the Heston\nstochastic volatility model to describe the price of a single stock option. In\norder to speed up the calibration process, we apply two model reduction\nstrategies. Firstly, a reduced basis method (RBM) is used to define a suitable\nlow-dimensional basis for the numerical approximation of the\nparameter-dependent partial differential equation ($\\mu$PDE) model. By doing so\nthe computational complexity for solving the $\\mu$PDE is drastically reduced,\nand applications of standard minimization algorithms for the calibration are\nsignificantly faster than working with a high-dimensional finite element basis.\nSecondly, so-called de-Americanization strategies are applied. Here, the main\nidea is to reformulate the calibration problem for American options as a\nproblem for European options and to exploit closed-form solutions. Both\nreduction techniques are systematically compared and tested for both synthetic\nand market data sets.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1611.06452v1", "link": "http://arxiv.org/abs/1611.06452v1"}, {"title": "A No-Arbitrage Model of Liquidity in Financial Markets involving\n  Brownian Sheets", "summary": "We consider a dynamic market model where buyers and sellers submit limit\norders. If at a given moment in time, the buyer is unable to complete his\nentire order due to the shortage of sell orders at the required limit price,\nthe unmatched part of the order is recorded in the order book. Subsequently\nthese buy unmatched orders may be matched with new incoming sell orders. The\nresulting demand curve constitutes the sole input to our model. The clearing\nprice is then mechanically calculated using the market clearing condition. We\nuse a Brownian sheet to model the demand curve, and provide some theoretical\nassumptions under which such a model is justified.\n  Our main result is the proof that if there exists a unique equivalent\nmartingale measure for the clearing price, then under some mild assumptions\nthere is no arbitrage. We use the Ito- Wentzell formula to obtain that result,\nand also to characterize the dynamics of the demand curve and of the clearing\nprice in the equivalent measure. We find that the volatility of the clearing\nprice is (up to a stochastic factor) inversely proportional to the sum of buy\nand sell order flow density (evaluated at the clearing price), which confirms\nthe intuition that volatility is inversely proportional to volume. We also\ndemonstrate that our approach is implementable. We use real order book data and\nsimulate option prices under a particularly simple parameterization of our\nmodel.\n  The no-arbitrage conditions we obtain are applicable to a wide class of\nmodels, in the same way that the Heath-Jarrow-Morton conditions apply to a wide\nclass of interest rate models.", "category": ["q-fin.CP", "q-fin.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/1206.4804v1", "link": "http://arxiv.org/abs/1206.4804v1"}, {"title": "Optimal Hedging for Fund & Insurance Managers with Partially Observable\n  Investment Flows", "summary": "All the financial practitioners are working in incomplete markets full of\nunhedgeable risk-factors. Making the situation worse, they are only equipped\nwith the imperfect information on the relevant processes. In addition to the\nmarket risk, fund and insurance managers have to be prepared for sudden and\npossibly contagious changes in the investment flows from their clients so that\nthey can avoid the over- as well as under-hedging. In this work, the prices of\nsecurities, the occurrences of insured events and (possibly a network of) the\ninvestment flows are used to infer their drifts and intensities by a stochastic\nfiltering technique. We utilize the inferred information to provide the optimal\nhedging strategy based on the mean-variance (or quadratic) risk criterion. A\nBSDE approach allows a systematic derivation of the optimal strategy, which is\nshown to be implementable by a set of simple ODEs and the standard Monte Carlo\nsimulation. The presented framework may also be useful for manufactures and\nenergy firms to install an efficient overlay of dynamic hedging by financial\nderivatives to minimize the costs.", "category": ["q-fin.CP", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1401.2314v2", "link": "http://arxiv.org/abs/1401.2314v2"}, {"title": "Advertising effects in Sznajd marketing model", "summary": "The traditional Sznajd model, as well as its Ochrombel simplification for\nopinion spreading, are applied to marketing with the help of advertising. The\nlarger the lattice is the smaller is the amount of advertising needed to\nconvince the whole market", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0207181v1", "link": "http://dx.doi.org/10.1142/S0129183103004255"}, {"title": "Can one hear the size of a target zone?", "summary": "We develop a target zone model with realistic features such as finite exit\ntime, non-stationary dynamics and heavy tails. Our rigorous characterization of\nrisk corresponds to the dynamic counterpart of a mean-preserving spread. We\nexplicitly solve for both stationary and transient exchange rate paths, and\nshow how they are influenced by the distance to both the time horizon and the\ntarget zone bands. This enables us to show how central bank intervention is\nendogenous to both the distance of the fundamental to the band and the\nunderlying risk. We discuss how the credibility of the target zone is shaped by\nthe set horizon and the degree of underlying risk, and we determine a minimum\ntime at which the required parity can be reached. We prove that the interplay\nof the diffusive component and the destabilizing risk component can yield an\nendogenous regime shift characterized by a threshold level of risk above which\nthe target zone ceases to exist. All the previous results cannot obtain by\nmeans of the standard Gaussian and affine models. We recover by numerical\nsimulations the different exchange rate densities established by the target\nzone literature.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.00948v1", "link": "http://arxiv.org/abs/2002.00948v1"}, {"title": "Geobiodynamics and Roegenian Economic Systems", "summary": "This mathematical essay brings together ideas from Economics, Geobiodynamics\nand Thermodynamics. Its purpose is to obtain real models of complex\nevolutionary systems. More specifically, the essay defines Roegenian Economy\nand links Geobiodynamics and Roegenian Economy. In this context, we discuss the\nisomorphism between the concepts and techniques of Thermodynamics and\nEconomics. Then we describe a Roegenian economic system like a Carnot group.\nAfter we analyse the phase equilibrium for two heterogeneous economic systems.\nThe European Union Economics appears like Cartesian product of Roegenian\neconomic systems and its Balance is analysed in details. A Section at the end\ndescribes the \"economic black holes\" as small parts of a a global economic\nsystem in which national income is so great that it causes others poor\nenrichment. These ideas can be used to improve our knowledge and understanding\nof the nature of development and evolution of thermodynamic-economic systems.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1812.07961v1", "link": "http://arxiv.org/abs/1812.07961v1"}, {"title": "Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and\n  Monetary Interactions", "summary": "We consider a frictionless constant endowment economy based on Leeper (1991).\nIn this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc\nfiscal rule, there are two equilibria. One has active monetary policy and\npassive fiscal policy, while the other has passive monetary policy and active\nfiscal policy. We consider an extended setup in which the policy maker\nminimizes a loss function under quasi-commitment, as in Schaumburg and\nTambalotti (2007). Under this formulation there exists a unique Ramsey\nequilibrium, with an interest rate peg and a passive fiscal policy. We thank\nJohn P. Conley, Luis de Araujo and one referree for their very helpful\ncomments.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.04508v1", "link": "http://arxiv.org/abs/2002.04508v1"}, {"title": "Parallel Experimentation in a Competitive Advertising Marketplace", "summary": "When multiple firms are simultaneously running experiments on a platform, the\ntreatment effects for one firm may depend on the experimentation policies of\nothers. This paper presents a set of causal estimands that are relevant to such\nan environment. We also present an experimental design that is suitable for\nfacilitating experimentation across multiple competitors in such an\nenvironment. Together, these can be used by a platform to run experiments \"as a\nservice,\" on behalf of its participating firms. We show that the causal\nestimands we develop are identified nonparametrically by the variation induced\nby the design, and present two scalable estimators that help measure them in\ntypical high-dimensional situations. We implement the design on the advertising\nplatform of JD.com, an eCommerce company, which is also a publisher of digital\nads in China. We discuss how the design is engineered within the platform's\nauction-driven ad-allocation system, which is typical of modern, digital\nadvertising marketplaces. Finally, we present results from a parallel\nexperiment involving 16 advertisers and millions of JD.com users. These results\nshowcase the importance of accommodating a role for interactions across\nexperimenters and demonstrates the viability of the framework.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.11198v2", "link": "http://arxiv.org/abs/1903.11198v2"}, {"title": "Determinants of Social-economic Mobility in the Northern Region of\n  Malaysia", "summary": "Colleting the data through a survey in the Northern region of Malaysia;\nKedah, Perlis, Penang and Perak, this study investigates intergenerational\nsocial mobility in Malaysia. We measure and analyzed the factors that influence\nsocial-economic mobility by using binary choice model (logit model). Social\nmobility can be measured in several ways, by income, education, occupation or\nsocial class. More often, economic research has focused on some measure of\nincome. Social mobility variable is measured using the difference between\neducational achievement between a father and son. If there is a change of at\nleast of two educational levels between a father and son, then this study will\nassign the value one which means that social mobility has occurred.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.03043v1", "link": "http://arxiv.org/abs/2001.03043v1"}, {"title": "Electoral Crime Under Democracy: Information Effects from Judicial\n  Decisions in Brazil", "summary": "This paper examines voters' responses to the disclosure of electoral crime\ninformation in large democracies. I focus on Brazil, where the electoral court\nmakes candidates' criminal records public before every election. Using a sample\nof local candidates running for office between 2004 and 2016, I find that a\nconviction for an electoral crime reduces candidates' probability of election\nand vote share by 10.3 and 12.9 percentage points (p.p.), respectively. These\nresults are not explained by (potential) changes in judge, voter, or candidate\nbehavior over the electoral process. I additionally perform machine\nclassification of court documents to estimate heterogeneous punishment for\nsevere and trivial crimes. I document a larger electoral penalty (6.5 p.p.) if\ncandidates are convicted for severe crimes. These results supplement the\ninformation shortcut literature by examining how judicial information\ninfluences voters' decisions and showing that voters react more strongly to\nmore credible sources of information.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.10958v1", "link": "http://arxiv.org/abs/1912.10958v1"}, {"title": "International trade of fruits between Portugal and the world", "summary": "For Portugal there are few or none works about the international trade of\nfruits between Portugal and the other countries. In this work it aims to\nanalyze the more recent data for the Portuguese international trade of fruits.\nThey were used data for the years from 2006 to 2010, available by the INE\n(Statistics Portugal), gently given by the AICEP (Trade & Investment Agency).\nTo complement this data analysis they were made some estimations with several\neconometrics method and based in the neoclassical theory, with the absolute\nconvergence model. It was concluded that the biggest relationship, in the\ninternational trade of fruits, is with the European countries and there are not\nstatistical regularity in the estimations and the data are not stationary.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1206.3385v1", "link": "http://arxiv.org/abs/1206.3385v1"}, {"title": "Risk and Utility in Portfolio Optimization", "summary": "Modern portfolio theory(MPT) addresses the problem of determining the optimum\nallocation of investment resources among a set of candidate assets. In the\noriginal mean-variance approach of Markowitz, volatility is taken as a proxy\nfor risk, conflating uncertainty with risk. There have been many subsequent\nattempts to alleviate that weakness which, typically, combine utility and risk.\nWe present here a modification of MPT based on the inclusion of separate risk\nand utility criteria. We define risk as the probability of failure to meet a\npre-established investment goal. We define utility as the expectation of a\nutility function with positive and decreasing marginal value as a function of\nyield. The emphasis throughout is on long investment horizons for which\nrisk-free assets do not exist. Analytic results are presented for a Gaussian\nprobability distribution. Risk-utility relations are explored via empirical\nstock-price data, and an illustrative portfolio is optimized using the\nempirical data.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/cond-mat/0212187v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01957-X"}, {"title": "Topologically Mapping the Macroeconomy", "summary": "An understanding of the economic landscape in a world of ever increasing data\nnecessitates representations of data that can inform policy, deepen\nunderstanding and guide future research. Topological Data Analysis offers a set\nof tools which deliver on all three calls. Abstract two-dimensional snapshots\nof multi-dimensional space readily capture non-monotonic relationships, inform\nof similarity between points of interest in parameter space, mapping such to\noutcomes. Specific examples show how some, but not all, countries have returned\nto Great Depression levels, and reappraise the links between real private\ncapital growth and the performance of the economy. Theoretical and empirical\nexpositions alike remind on the dangers of assuming monotonic relationships and\ndiscounting combinations of factors as determinants of outcomes; both dangers\nTopological Data Analysis addresses. Policy-makers can look at outcomes and\ntarget areas of the input space where such are not satisfactory, academics may\nadditionally find evidence to motivate theoretical development, and\npractitioners can gain a rapid and robust base for decision making.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1911.10476v1", "link": "http://arxiv.org/abs/1911.10476v1"}, {"title": "Agent-based mapping of credit risk for sustainable microfinance", "summary": "Inspired by recent ideas on how the analysis of complex financial risks can\nbenefit from analogies with independent research areas, we propose an\nunorthodox framework for mapping microfinance credit risk---a major obstacle to\nthe sustainability of lenders outreaching to the poor. Specifically, using the\nelements of network theory, we constructed an agent-based model that obeys the\nstylised rules of microfinance industry. We found that in a deteriorating\neconomic environment confounded with adverse selection, a form of latent moral\nhazard may cause a regime shift from a high to a low loan repayment\nprobability. An after-the-fact recovery, when possible, required the economic\nenvironment to improve beyond that which led to the shift in the first place.\nThese findings suggest a small set of measurable quantities for mapping\nmicrofinance credit risk and, consequently, for balancing the requirements to\nreasonably price loans and to operate on a fully self-financed basis. We\nillustrate how the proposed mapping works using a 10-year monthly data set from\none of the best-known microfinance representatives, Grameen Bank in Bangladesh.\nFinally, we discuss an entirely new perspective for managing microfinance\ncredit risk based on enticing spontaneous cooperation by building social\ncapital.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1504.05737v1", "link": "http://dx.doi.org/10.1371/journal.pone.0126447"}, {"title": "Degree stability of a minimum spanning tree of price return and\n  volatility", "summary": "We investigate the time series of the degree of minimum spanning trees\nobtained by using a correlation based clustering procedure which is starting\nfrom (i) asset return and (ii) volatility time series. The minimum spanning\ntree is obtained at different times by computing correlation among time series\nover a time window of fixed length $T$. We find that the minimum spanning tree\nof asset return is characterized by stock degree values, which are more stable\nin time than the ones obtained by analyzing a minimum spanning tree computed\nstarting from volatility time series. Our analysis also shows that the degree\nof stocks has a very slow dynamics with a time-scale of several years in both\ncases.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0212338v1", "link": "http://dx.doi.org/10.1016/S0378-4371(03)00002-5"}, {"title": "A Classifiers Voting Model for Exit Prediction of Privately Held\n  Companies", "summary": "Predicting the exit (e.g. bankrupt, acquisition, etc.) of privately held\ncompanies is a current and relevant problem for investment firms. The\ndifficulty of the problem stems from the lack of reliable, quantitative and\npublicly available data. In this paper, we contribute to this endeavour by\nconstructing an exit predictor model based on qualitative data, which blends\nthe outcomes of three classifiers, namely, a Logistic Regression model, a\nRandom Forest model, and a Support Vector Machine model. The output of the\ncombined model is selected on the basis of the majority of the output classes\nof the component models. The models are trained using data extracted from the\nThomson Reuters Eikon repository of 54697 US and European companies over the\n1996-2011 time span. Experiments have been conducted for predicting whether the\ncompany eventually either gets acquired or goes public (IPO), against the\ncomplementary event that it remains private or goes bankrupt, in the considered\ntime window. Our model achieves a 63\\% predictive accuracy, which is quite a\nvaluable figure for Private Equity investors, who typically expect very high\nreturns from successful investments.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1910.13969v1", "link": "http://arxiv.org/abs/1910.13969v1"}, {"title": "Analysis of high-resolution foreign exchange data of USD-JPY for 13\n  years", "summary": "We analyze high-resolution foreign exchange data consisting of 20 million\ndata points of USD-JPY for 13 years to report firm statistical laws in\ndistributions and correlations of exchange rate fluctuations. A conditional\nprobability density analysis clearly shows the existence of trend-following\nmovements at time scale of 8-ticks, about 1 minute.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0211162v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01881-2"}, {"title": "Fairness State with Plastic Preferences", "summary": "The definition of preferences assigned to individuals is a concept that\nconcerns many disciplines, from economics, with the search of an acceptable\noutcome for an ensemble of individuals, to decision making an analysis of vote\nsystems. We are concerned in the phenomena of good selection and economic\nfairness. In Arrow's theorem this situation is expressed as the impossibility\nof aggregate preferences among individuals falling down into some unfairness\nstate. This situation was also analyzed in a previous model in a network of\nindividuals with a random allocation. Both analysis are based on static\npreferences.\n  In a real society the individuals are confronted to information exchange\nwhich can modify the way that they think. Also, the preference formation of\neach individual is influenced by this exchange. This consideration reveals why\nthe actual theory is not able to make an accurate analysis of the influence of\nthe individual, or cluster of individuals, in the fairness state. The aim of\nthis investigation is to consider the coupling of two systems, one for the\nformation of preferences and a second where an allocation of goods is done in\nan evolutionary environment.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0609100v1", "link": "http://arxiv.org/abs/physics/0609100v1"}, {"title": "A Bayesian GED-Gamma stochastic volatility model for return data: a\n  marginal likelihood approach", "summary": "Several studies explore inferences based on stochastic volatility (SV)\nmodels, taking into account the stylized facts of return data. The common\nproblem is that the latent parameters of many volatility models are\nhigh-dimensional and analytically intractable, which means inferences require\napproximations using, for example, the Markov Chain Monte Carlo or Laplace\nmethods. Some SV models are expressed as a linear Gaussian state-space model\nthat leads to a marginal likelihood, reducing the dimensionality of the\nproblem. Others are not linearized, and the latent parameters are integrated\nout. However, these present a quite restrictive evolution equation. Thus, we\npropose a Bayesian GED-Gamma SV model with a direct marginal likelihood that is\na product of the generalized Student's t-distributions in which the latent\nstates are related across time through a stationary Gaussian evolution\nequation. Then, an approximation is made for the prior distribution of\nlog-precision/volatility, without the need for model linearization. This also\nallows for the computation of the marginal likelihood function, where the\nhigh-dimensional latent states are integrated out and easily sampled in blocks\nusing a smoothing procedure. In addition, extensions of our GED-Gamma model are\neasily made to incorporate skew heavy-tailed distributions. We use the Bayesian\nestimator for the inference of static parameters, and perform a simulation\nstudy on several properties of the estimator. Our results show that the\nproposed model can be reasonably estimated. Furthermore, we provide case\nstudies of a Brazilian asset and the pound/dollar exchange rate to show the\nperformance of our approach in terms of fit and prediction.\n  Keywords: SV model, New sequential and smoothing procedures, Generalized\nStudent's t-distribution, Non-Gaussian errors, Heavy tails, Skewness", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1809.01489v1", "link": "http://arxiv.org/abs/1809.01489v1"}, {"title": "The Cross-section of Expected Returns on Penny Stocks: Are Low-hanging\n  Fruits Not-so Sweet?", "summary": "In this paper, we study the determinants of expected returns on the listed\npenny stocks from two perspectives. Traditionally financial economics\nliterature has been devoted to study the macro and micro determinants of\nexpected returns on stocks (Subrahmanyam, 2010). Very few research has been\ncarried out on penny stocks (Liu, Rhee, & Zhang, 2011; Nofsinger & Verma,\n2014). Our study is an effort to contribute more empirical evidence on penny\nstocks in the emerging market context. We see the return dynamics of penny\nstocks from corporate governance perspective. Issues such as shareholding\npatters are considered to be of much significance when it comes to understand\nthe price movements. Using cross-sectional data on 167 penny stocks listed in\nthe National Stock Exchange of India, we show that (i) Returns of portfolio of\nlower market-cap penny stocks are significantly different(higher) than that of\nhigher market-cap penny stocks. (ii)Returns of portfolio lower P/E stocks are\nsignificantly different (higher) than that of higher P/E stocks. Similarly,\nreturns of portfolio of lower P/B stocks are significantly different (higher)\nthan that of higher P/B stocks, and returns of portfolio of lower priced penny\nstocks are significantly different(higher) than that of higher priced penny\nstocks. (iii) Trading volume differences due to alphabetism are insignificant.\n(iv)Differences in returns of portfolios based on beta and shareholding\npatterns are insignificant.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1610.01338v1", "link": "http://arxiv.org/abs/1610.01338v1"}, {"title": "From Risk Measures to Research Measures", "summary": "In order to evaluate the quality of the scientific research, we introduce a\nnew family of scientific performance measures, called Scientific Research\nMeasures (SRM). Our proposal originates from the more recent developments in\nthe theory of risk measures and is an attempt to resolve the many problems of\nthe existing bibliometric indices. The SRM that we introduce are based on the\nwhole scientist's citation record and are: coherent, as they share the same\nstructural properties; flexible to fit peculiarities of different areas and\nseniorities; granular, as they allow a more precise comparison between\nscientists, and inclusive, as they comprehend several popular indices. Another\nkey feature of our SRM is that they are planned to be calibrated to the\nparticular scientific community. We also propose a dual formulation of this\nproblem and explain its relevance in this context.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1205.1012v1", "link": "http://arxiv.org/abs/1205.1012v1"}, {"title": "With or without replacement? Sampling uncertainty in Shepp's urn scheme", "summary": "We introduce a variant of Shepp's classical urn problem in which the optimal\nstopper does not know whether sampling from the urn is done with or without\nreplacement. By considering the problem's continuous-time analog, we provide\nbounds on the value function and in the case of a balanced urn (with an equal\nnumber of each ball type) an explicit solution is found. Surprisingly, the\noptimal strategy for the balanced urn is the same as in the classical urn\nproblem.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1911.11971v1", "link": "http://arxiv.org/abs/1911.11971v1"}, {"title": "Listening to Chaotic Whispers: A Deep Learning Framework for\n  News-oriented Stock Trend Prediction", "summary": "Stock trend prediction plays a critical role in seeking maximized profit from\nstock investment. However, precise trend prediction is very difficult since the\nhighly volatile and non-stationary nature of stock market. Exploding\ninformation on Internet together with advancing development of natural language\nprocessing and text mining techniques have enable investors to unveil market\ntrends and volatility from online content. Unfortunately, the quality,\ntrustworthiness and comprehensiveness of online content related to stock market\nvaries drastically, and a large portion consists of the low-quality news,\ncomments, or even rumors. To address this challenge, we imitate the learning\nprocess of human beings facing such chaotic online news, driven by three\nprinciples: sequential content dependency, diverse influence, and effective and\nefficient learning. In this paper, to capture the first two principles, we\ndesigned a Hybrid Attention Networks to predict the stock trend based on the\nsequence of recent related news. Moreover, we apply the self-paced learning\nmechanism to imitate the third principle. Extensive experiments on real-world\nstock market data demonstrate the effectiveness of our approach.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1712.02136v3", "link": "http://arxiv.org/abs/1712.02136v3"}, {"title": "Heterogeneous Impact of the Minimum Wage: Implications for Changes in\n  Between- and Within-group Inequality", "summary": "Workers who earn at or below the minimum wage in the United States are mostly\neither less educated, young, or female. Little is known, however, concerning\nthe extent to which the minimum wage influences wage differentials among\nworkers with different observed characteristics and among workers with the same\nobserved characteristics. This paper shows that changes in the real value of\nthe minimum wage over recent decades have affected the relationship of hourly\nwages with education, experience, and gender. The results suggest that changes\nin the real value of the minimum wage account in part for the patterns of\nchanges in education, experience, and gender wage differentials and mostly for\nthe patterns of changes in within-group wage differentials among female workers\nwith lower levels of experience.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.03925v2", "link": "http://arxiv.org/abs/1903.03925v2"}, {"title": "Influence network in Chinese stock market", "summary": "In a stock market, the price fluctuations are interactive, that is, one\nlisted company can influence others. In this paper, we seek to study the\ninfluence relationships among listed companies by constructing a directed\nnetwork on the basis of Chinese stock market. This influence network shows\ndistinct topological properties, particularly, a few large companies that can\nlead the tendency of stock market are recognized. Furthermore, by analyzing the\nsubnetworks of listed companies distributed in several significant economic\nsectors, it is found that the influence relationships are totally different\nfrom one economic sector to another, of which three types of connectivity as\nwell as hub-like listed companies are identified. In addition, the rankings of\nlisted companies obtained from the centrality metrics of influence network are\ncompared with that according to the assets, which gives inspiration to uncover\nand understand the importance of listed companies in the stock market. These\nempirical results are meaningful in providing these topological properties of\nChinese stock market and economic sectors as well as revealing the\ninteractively influence relationships among listed companies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1503.00823v1", "link": "http://dx.doi.org/10.1088/1742-5468/2015/03/P03017"}, {"title": "The macroeconomic effect of the information and communication technology\n  in Hungary", "summary": "It was not until the beginning of the 1990s that the effects of information\nand communication technology on economic growth as well as on the profitability\nof enterprises raised the interest of researchers. After giving a general\ndescription on the relationship between a more intense use of ICT devices and\ndynamic economic growth, the author identified and explained those four\nchannels that had a robust influence on economic growth and productivity. When\ncomparing the use of information technonology devices in developed as well as\nin developing countries, the author highlighted the importance of the available\nadditional human capital and the elimination of organizational inflexibilities\nin the attempt of narrowing the productivity gap between the developed and\ndeveloping nations. By processing a large quantitiy of information gained from\nHungarian enterprises operating in several economic sectors, the author made an\nattempt to find a strong correlation between the development level of using ICT\ndevices and profitability together with total factor productivity. Although the\nimpact of using ICT devices cannot be measured unequivocally at the\nmicroeconomic level because of certain statistical and methodological\nimperfections, by applying such analytical methods as cluster analysis and\ncorrelation and regression calculation, the author managed to prove that both\nthe correlation coefficient and the gradient of the regression trend line\nshowed a positive relationship between the extensive use of information and\ncommunication technology and the profitability of enterprises.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1204.1561v1", "link": "http://arxiv.org/abs/1204.1561v1"}, {"title": "Uncovering the Dynamics of Correlation Structures Relative to the\n  Collective Market Motion", "summary": "The measured correlations of financial time series in subsequent epochs\nchange considerably as a function of time. When studying the whole correlation\nmatrices, quasi-stationary patterns, referred to as market states, are seen by\napplying clustering methods. They emerge, disappear or reemerge, but they are\ndominated by the collective motion of all stocks. In the jargon, one speaks of\nthe market motion, it is always associated with the largest eigenvalue of the\ncorrelation matrices. Thus the question arises, if one can extract more refined\ninformation on the system by subtracting the dominating market motion in a\nproper way. To this end we introduce a new approach by clustering reduced-rank\ncorrelation matrices which are obtained by subtracting the dyadic matrix\nbelonging to the largest eigenvalue from the standard correlation matrices. We\nanalyze daily data of 262 companies of the S&P 500 index over a period of\nalmost 15 years from 2002 to 2016. The resulting dynamics is remarkably\ndifferent, and the corresponding market states are quasi-stationary over a long\nperiod of time. Our approach adds to the attempts to separate endogenous from\nexogenous effects.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2004.12336v1", "link": "http://arxiv.org/abs/2004.12336v1"}, {"title": "Stochastic mortality models: An infinite dimensional approach", "summary": "Demographic projections of future mortality rates involve a high level of\nuncertainty and require stochastic mortality models. The current paper\ninvestigates forward mortality models driven by a (possibly infinite\ndimensional) Wiener process and a compensated Poisson random measure. A major\ninnovation of the paper is the introduction of a family of processes called\nforward mortality improvements which provide a flexible tool for a simple\nconstruction of stochastic forward mortality models. In practice, the notion of\nmortality improvements are a convenient device for the quantification of\nchanges in mortality rates over time that enables, for example, the detection\nof cohort effects.\n  We show that the forward mortality rates satisfy Heath-Jarrow-Morton-type\nconsistency conditions which translate to the forward mortality improvements.\nWhile the consistency conditions of the forward mortality rates are analogous\nto the classical conditions in the context of bond markets, the conditions of\nthe forward mortality improvements possess a different structure: forward\nmortality models include a cohort parameter besides the time horizon; these two\ndimensions are coupled in the dynamics of consistent models of forwards\nmortality improvements. In order to obtain a unified framework, we transform\nthe systems of It\\^o-processes which describe the forward mortality rates and\nimprovements: in contrast to term-structure models, the corresponding\nstochastic partial differential equations (SPDEs) describe the random dynamics\nof two-dimensional surfaces rather than curves.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1907.05157v1", "link": "http://arxiv.org/abs/1907.05157v1"}, {"title": "A growth adjusted price-earnings ratio", "summary": "The purpose of this paper is to introduce a new growth adjusted\nprice-earnings measure (GA-P/E) and assess its efficacy as measure of value and\npredictor of future stock returns. Taking inspiration from the interpretation\nof the traditional price-earnings ratio as a period of time, the new measure\ncomputes the requisite payback period whilst accounting for earnings growth.\nHaving derived the measure, we outline a number of its properties before\nconducting an extensive empirical study utilising a sorted portfolio\nmethodology. We find that the returns of the low GA-P/E stocks exceed those of\nthe high GA-P/E stocks, both in an absolute sense and also on a risk-adjusted\nbasis. Furthermore, the returns from the low GA-P/E porfolio was found to\nexceed those of the value portfolio arising from a P/E sort on the same pool of\nstocks. Finally, the returns of our GA-P/E sorted porfolios were subjected to\nanalysis by conducting regressions against the standard Fama and French risk\nfactors.", "category": ["q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/2001.08240v1", "link": "http://arxiv.org/abs/2001.08240v1"}, {"title": "Reconstruction of density functions by sk-splines", "summary": "Reconstruction of density functions and their characteristic functions by\nradial basis functions with scattered data points is a popular topic in the\ntheory of pricing of basket options. Such functions are usually entire or admit\nan analytic extension into an appropriate tube and \"bell-shaped\" with rapidly\ndecaying tails. Unfortunately, the domain of such functions is not compact\nwhich creates various technical difficulties. We solve interpolation problem on\nan infinite rectangular grid for a wide range of kernel functions and calculate\nexplicitly their Fourier transform to obtain representations for the respective\ndensity functions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1404.5271v1", "link": "http://arxiv.org/abs/1404.5271v1"}, {"title": "Market-wide price co-movement around crashes in the Tokyo Stock Exchange", "summary": "As described in this paper, we study market-wide price co-movements around\ncrashes by analyzing a dataset of high-frequency stock returns of the\nconstituent issues of Nikkei 225 Index listed on the Tokyo Stock Exchange for\nthe three years during 2007--2009. Results of day-to-day principal component\nanalysis of the time series sampled at the 1 min time interval during the\ncontinuous auction of the daytime reveal the long range up to a couple of\nmonths significant auto-correlation of the maximum eigenvalue of the\ncorrelation matrix, which express the intensity of market-wide co-movement of\nstock prices. It also strongly correlates with the open-to-close intraday\nreturn and daily return of Nikkei 225 Index. We also study the market mode,\nwhich is the first principal component corresponding to the maximum eigenvalue,\nin the framework of Multi-fractal random walk model. The parameter of the model\nestimated in a sliding time window, which describes the covariance of the\nlogarithm of the stochastic volatility, grows before almost all large intraday\nprice declines of less than -5%. This phenomenon signifies the upwelling of the\nmarket-wide collective behavior before the crash, which might reflect a herding\nof market participants.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1306.2188v1", "link": "http://arxiv.org/abs/1306.2188v1"}, {"title": "Variety and Volatility in Financial Markets", "summary": "We study the price dynamics of stocks traded in a financial market by\nconsidering the statistical properties both of a single time series and of an\nensemble of stocks traded simultaneously. We use the $n$ stocks traded in the\nNew York Stock Exchange to form a statistical ensemble of daily stock returns.\nFor each trading day of our database, we study the ensemble return\ndistribution. We find that a typical ensemble return distribution exists in\nmost of the trading days with the exception of crash and rally days and of the\ndays subsequent to these extreme events. We analyze each ensemble return\ndistribution by extracting its first two central moments. We observe that these\nmoments are fluctuating in time and are stochastic processes themselves. We\ncharacterize the statistical properties of ensemble return distribution central\nmoments by investigating their probability density functions and temporal\ncorrelation properties. In general, time-averaged and portfolio-averaged price\nreturns have different statistical properties. We infer from these differences\ninformation about the relative strength of correlation between stocks and\nbetween different trading days. Lastly, we compare our empirical results with\nthose predicted by the single-index model and we conclude that this simple\nmodel is unable to explain the statistical properties of the second moment of\nthe ensemble return distribution.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0006065v1", "link": "http://dx.doi.org/10.1103/PhysRevE.62.6126"}, {"title": "Adversarial Generalized Method of Moments", "summary": "We provide an approach for learning deep neural net representations of models\ndescribed via conditional moment restrictions. Conditional moment restrictions\nare widely used, as they are the language by which social scientists describe\nthe assumptions they make to enable causal inference. We formulate the problem\nof estimating the underling model as a zero-sum game between a modeler and an\nadversary and apply adversarial training. Our approach is similar in nature to\nGenerative Adversarial Networks (GAN), though here the modeler is learning a\nrepresentation of a function that satisfies a continuum of moment conditions\nand the adversary is identifying violating moments. We outline ways of\nconstructing effective adversaries in practice, including kernels centered by\nk-means clustering, and random forests. We examine the practical performance of\nour approach in the setting of non-parametric instrumental variable regression.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1803.07164v2", "link": "http://arxiv.org/abs/1803.07164v2"}, {"title": "Economic Inequality: Is it Natural?", "summary": "Mounting evidences are being gathered suggesting that income and wealth\ndistribution in various countries or societies follow a robust pattern, close\nto the Gibbs distribution of energy in an ideal gas in equilibrium, but also\ndeviating significantly for high income groups. Application of physics models\nseem to provide illuminating ideas and understanding, complimenting the\nobservations.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0703201v2", "link": "http://arxiv.org/abs/physics/0703201v2"}, {"title": "Capital Regulation under Price Impacts and Dynamic Financial Contagion", "summary": "We construct a continuous time model for price-mediated contagion\nprecipitated by a common exogenous stress to the banking book of all firms in\nthe financial system. In this setting, firms are constrained so as to satisfy a\nrisk-weight based capital ratio requirement. We use this model to find\nanalytical bounds on the risk-weights for assets as a function of the market\nliquidity. Under these appropriate risk-weights, we find existence and\nuniqueness for the joint system of firm behavior and the asset prices. We\nfurther consider an analytical bound on the firm liquidations, which allows us\nto construct exact formulas for stress testing the financial system with\ndeterministic or random stresses. Numerical case studies are provided to\ndemonstrate various implications of this model and analytical bounds.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1807.02711v3", "link": "http://arxiv.org/abs/1807.02711v3"}, {"title": "Relatively growth optimal investment strategies in a market model with\n  competition", "summary": "We consider a game-theoretic model of a market where investors compete for\npayoffs yielded by several assets. The main result consists in a proof of\nexistence and uniqueness of a strategy, called relatively growth optimal, such\nthat the logarithm of the share of its wealth in the total wealth of the market\nis a submartingale for any strategies of the other investors. It is also shown\nthat this strategy is asymptotically optimal in the sense that it achieves the\nmaximal capital growth rate when compared to competing strategies. Based on the\nobtained results, we study the asymptotic structure of the market when all the\ninvestors use the relatively growth optimal strategy.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1908.01171v1", "link": "http://arxiv.org/abs/1908.01171v1"}, {"title": "Financial Applications of Gaussian Processes and Bayesian Optimization", "summary": "In the last five years, the financial industry has been impacted by the\nemergence of digitalization and machine learning. In this article, we explore\ntwo methods that have undergone rapid development in recent years: Gaussian\nprocesses and Bayesian optimization. Gaussian processes can be seen as a\ngeneralization of Gaussian random vectors and are associated with the\ndevelopment of kernel methods. Bayesian optimization is an approach for\nperforming derivative-free global optimization in a small dimension, and uses\nGaussian processes to locate the global maximum of a black-box function. The\nfirst part of the article reviews these two tools and shows how they are\nconnected. In particular, we focus on the Gaussian process regression, which is\nthe core of Bayesian machine learning, and the issue of hyperparameter\nselection. The second part is dedicated to two financial applications. We first\nconsider the modeling of the term structure of interest rates. More precisely,\nwe test the fitting method and compare the GP prediction and the random walk\nmodel. The second application is the construction of trend-following\nstrategies, in particular the online estimation of trend and covariance\nwindows.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1903.04841v1", "link": "http://arxiv.org/abs/1903.04841v1"}, {"title": "Density of Skew Brownian motion and its functionals with application in\n  finance", "summary": "We derive the joint density of a Skew Brownian motion, its last visit to the\norigin, local and occupation times. The result is applied to option pricing in\na two valued local volatility model and in a displaced diffusion model with\nconstrained volatility.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1407.1715v3", "link": "http://arxiv.org/abs/1407.1715v3"}, {"title": "Representation of the penalty term of dynamic concave utilities", "summary": "In this paper we will provide a representation of the penalty term of general\ndynamic concave utilities (hence of dynamic convex risk measures) by applying\nthe theory of g-expectations.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/0802.1121v3", "link": "http://dx.doi.org/10.1007/s00780-009-0119-7"}, {"title": "Optimal Dynamic Strategies on Gaussian Returns", "summary": "Dynamic trading strategies, in the spirit of trend-following or\nmean-reversion, represent an only partly understood but lucrative and pervasive\narea of modern finance. Assuming Gaussian returns and Gaussian dynamic weights\nor signals, (e.g., linear filters of past returns, such as simple moving\naverages, exponential weighted moving averages, forecasts from ARIMA models),\nwe are able to derive closed-form expressions for the first four moments of the\nstrategy's returns, in terms of correlations between the random signals and\nunknown future returns. By allowing for randomness in the asset-allocation and\nmodelling the interaction of strategy weights with returns, we demonstrate that\npositive skewness and excess kurtosis are essential components of all positive\nSharpe dynamic strategies, which is generally observed empirically; demonstrate\nthat total least squares (TLS) or orthogonal least squares is more appropriate\nthan OLS for maximizing the Sharpe ratio, while canonical correlation analysis\n(CCA) is similarly appropriate for the multi-asset case; derive standard errors\non Sharpe ratios which are tighter than the commonly used standard errors from\nLo; and derive standard errors on the skewness and kurtosis of strategies,\napparently new results. We demonstrate these results are applicable\nasymptotically for a wide range of stationary time-series.", "category": ["q-fin.PM", "q-fin.RM", "q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1906.01427v1", "link": "http://arxiv.org/abs/1906.01427v1"}, {"title": "Distress propagation on production networks: Coarse-graining and\n  modularity of linkages", "summary": "Distress propagation occurs in connected networks, its rate and extent being\ndependent on network topology. To study this, we choose economic production\nnetworks as a paradigm. An economic network can be examined at many levels:\nlinkages among individual agents (microscopic), among firms/sectors\n(mesoscopic) or among countries (macroscopic). New emergent dynamical\nproperties appear at every level, so the granularity matters. For viral\nepidemics, even an individual node may act as an epicenter of distress and\npotentially affect the entire network. Economic networks, however, are known to\nbe immune at the micro-levels and more prone to failure in the\nmeso/macro-levels. We propose a dynamical interaction model to characterize the\nmechanism of distress propagation, across different modules of a network,\ninitiated at different epicenters. Vulnerable modules often lead to large\ndegrees of destabilization. We demonstrate our methodology using a unique\nempirical data-set of input-output linkages across 0.14 million firms in one\nadministrative state of India, a developing economy. The network has multiple\nhub-and-spoke structures that exhibits moderate disassortativity, which varies\nwith the level of coarse-graining. The novelty lies in characterizing the\nproduction network at different levels of granularity or modularity, and\nfinding `too-big-to-fail' modules supersede `too-central-to-fail' modules in\ndistress propagation.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.14485v1", "link": "http://arxiv.org/abs/2004.14485v1"}, {"title": "Prospective strict no-arbitrage and the fundamental theorem of asset\n  pricing under transaction costs", "summary": "In discrete time markets with proportional transaction costs, Schachermayer\n(2004) shows that robust no-arbitrage is equivalent to the existence of a\nstrictly consistent price system.\n  In this paper, we introduce the concept of prospective strict no-arbitrage\nthat is a variant of the strict no-arbitrage property from Kabanov, R\\'asonyi,\nand Stricker (2002). The prospective strict no-arbitrage condition is slightly\nweaker than robust no-arbitrage, and it implies that the set of portfolios\nattainable from zero initial endowment is closed in probability. A weak version\nof prospective strict no-arbitrage turns out to be equivalent to the existence\nof a consistent price system. In contrast to the fundamental theorem of asset\npricing of Schachermayer (2004), the consistent frictionless prices may lie on\nthe boundary of the bid-ask spread.\n  On the technical level, a crucial difference to Schachermayer (2004) and\nKabanov-R\\'asonyi-Stricker (2003) is that we prove closedness without having at\nhand that the null-strategies form a linear space.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1811.11621v2", "link": "http://dx.doi.org/10.1007/s00780-019-00403-5"}, {"title": "Argentum: a collaborative saving and investment platform for unstable\n  countries", "summary": "A crypto coin designed to provide a stabilization instrument backed up by\nminded like financial investments instruments to maintain the purchase value of\nsavings across time, in order to construct new tools for unstable economies.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1912.09569v1", "link": "http://arxiv.org/abs/1912.09569v1"}, {"title": "Simulations of financial markets in a Potts-like model", "summary": "A three-state model based on the Potts model is proposed to simulate\nfinancial markets. The three states are assigned to \"buy\", \"sell\" and\n\"inactive\" states. The model shows the main stylized facts observed in the\nfinancial market: fat-tailed distributions of returns and long time\ncorrelations in the absolute returns. At low inactivity rate, the model\neffectively reduces to the two-state model of Bornholdt and shows similar\nresults to the Bornholdt model. As the inactivity increases, we observe the\nexponential distributions of returns.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0503156v1", "link": "http://dx.doi.org/10.1142/S0129183105007923"}, {"title": "Portfolio optimisation beyond semimartingales: shadow prices and\n  fractional Brownian motion", "summary": "While absence of arbitrage in frictionless financial markets requires price\nprocesses to be semimartingales, non-semimartingales can be used to model\nprices in an arbitrage-free way, if proportional transaction costs are taken\ninto account. In this paper, we show, for a class of price processes which are\nnot necessarily semimartingales, the existence of an optimal trading strategy\nfor utility maximisation under transaction costs by establishing the existence\nof a so-called shadow price. This is a semimartingale price process, taking\nvalues in the bid ask spread, such that frictionless trading for that price\nprocess leads to the same optimal strategy and utility as the original problem\nunder transaction costs. Our results combine arguments from convex duality with\nthe stickiness condition introduced by P. Guasoni. They apply in particular to\nexponential utility and geometric fractional Brownian motion. In this case, the\nshadow price is an Ito process. As a consequence we obtain a rather surprising\nresult on the pathwise behaviour of fractional Brownian motion: the\ntrajectories may touch an Ito process in a one-sided manner without reflection.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1505.02416v2", "link": "http://arxiv.org/abs/1505.02416v2"}, {"title": "Confronting the Kaya Identity with Investment and Capital Stocks", "summary": "Scaling relations, such as the IPAT equation and the Kaya identity, are\nuseful for quickly gauging the scale of economic, technological, and\ndemographic changes required to reduce environmental impacts and pressures; in\nthe case of the Kaya identity, the environmental pressure is greenhouse gas\nemissions. However, when considering large-scale economic transformation, as\nwith a shift to a low-carbon economy, the IPAT and Kaya identities and their\ncousins fail to capture the legacy of existing capital, on the one hand, and\nthe need for new investment, on the other. While detailed models can capture\nthese factors, they do not allow for rapid exploration of widely different\nalternatives, which is the appeal of the IPAT and Kaya identities. In this\npaper we present an extended Kaya identity that includes investment and capital\nstocks. The identity we propose is a sum of terms, rather than a simple scaling\nrelation. Nevertheless, it allows for quick analysis and rapid exploration of a\nvariety of different possible paths toward a low-carbon economy.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1112.0758v2", "link": "http://arxiv.org/abs/1112.0758v2"}, {"title": "Pricing sovereign contingent convertible debt", "summary": "We develop a pricing model for Sovereign Contingent Convertible bonds\n(S-CoCo) with payment standstills triggered by a sovereign's Credit Default\nSwap (CDS) spread. We model CDS spread regime switching, which is prevalent\nduring crises, as a hidden Markov process, coupled with a mean-reverting\nstochastic process of spread levels under fixed regimes, in order to obtain\nS-CoCo prices through simulation. The paper uses the pricing model in a\nLongstaff-Schwartz American option pricing framework to compute future state\ncontingent S-CoCo prices for risk management. Dual trigger pricing is also\ndiscussed using the idiosyncratic CDS spread for the sovereign debt together\nwith a broad market index. Numerical results are reported using S-CoCo designs\nfor Greece, Italy and Germany with both the pricing and contingent pricing\nmodels.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1804.01475v1", "link": "http://arxiv.org/abs/1804.01475v1"}, {"title": "On the Optimal Dividend Problem in the Dual Model with Surplus-Dependent\n  Premiums", "summary": "This paper concerns the dual risk model, dual to the risk model for insurance\napplications, where premiums are surplus-dependent. In such a model premiums\nare regarded as costs, while claims refer to profits. We calculate the mean of\nthe cumulative discounted dividends paid until ruin, if the barrier strategy is\napplied. We formulate associated Hamilton-Jacobi-Bellman equation and identify\nsufficient conditions for a barrier strategy to be optimal. Some numerical\nexamples are provided when profits have exponential law.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1605.04584v1", "link": "http://arxiv.org/abs/1605.04584v1"}, {"title": "Entropy and Uncertainty Analysis in Financial Markets", "summary": "The investor is interested in the expected return and he is also concerned\nabout the risk and the uncertainty assumed by the investment. One of the most\npopular concepts used to measure the risk and the uncertainty is the variance\nand/or the standard-deviation. In this paper we explore the following issues:\nIs the standard-deviation a good measure of risk and uncertainty? What are the\npotentialities of the entropy in this context? Can entropy present some\nadvantages as a measure of uncertainty and simultaneously verify some basic\nassumptions of the portfolio management theory, namely the effect of\ndiversification?", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0709.0668v1", "link": "http://arxiv.org/abs/0709.0668v1"}, {"title": "About the Justification of Experience Rating: Bonus Malus System and a\n  new Poisson Mixture Model", "summary": "The claim experience of the past is a very important information to calculate\nthe fair price of an insurance contract. In a lot of European countries for\ninstance the prices for motor car insurance depend on the number of claims the\ndriver has reported to the insurance company during the last years. Classically\nthese prices are calculated on the basis of a mixed Poisson model with a gamma\nmixing distribution. The mixing distribution models the car drivers' qualities\nacross the insured portfolio. This is just one example for experience rating.\nIn the classical context the price is equal to the expectation of the Bayesian\nposterior distribution.\n  In some lines of business (especially third party liability and lines with\nexposure to extreme weather events) we that the real world data cannot be\ndescribed well enough by the classical Poisson - gamma model. Therefore we\ninvestigate the influence of the mixing distribution on the posterior\ndistribution conditional on the experienced number of claims. This enables the\napplication of other - more risk adequate premium principles than the\nexpectation principle. We introduce the inverse - gamma distribution as a new\nmixing distribution to model claim numbers and compare it to the classical\ngamma distribution. In both cases a closed analytic representation of the mixed\ndistribution can be found: In the classic case the well known negative binomial\ndistribution, in our new one a representation using the Bessel functions.\nAdditionally we present numerical results about the tail behaviour of the mixed\nPoisson - inverse - gamma distribution. Finally we introduce the concept of\nresolution. It enables us to decide if the classification of risk groups via\nthe number of experienced claims is a risk adequate procedure.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1009.4142v1", "link": "http://arxiv.org/abs/1009.4142v1"}, {"title": "Fundamental Measurements in Economics and in the Theory of Consciousness", "summary": "A new constructivist approach to modeling in economics and theory of\nconsciousness is proposed. The state of elementary object is defined as a set\nof its measurable consumer properties. A proprietor's refusal or consent for\nthe offered transaction is considered as a result of elementary economic\nmeasurement. Elementary (indivisible) technology, in which the object's\nconsumer values are variable, in this case can be formalized as a generalized\neconomic measurement. The algebra of such measurements has been constructed. It\nhas been shown that in the general case the quantum-mechanical formalism of the\ntheory of selective measurements is required for description of such\nconditions. The economic analogs of the elementary slit experiments in physics\nhave been created. The proposed approach can be also used for consciousness\nmodeling.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1110.5283v1", "link": "http://arxiv.org/abs/1110.5283v1"}, {"title": "Transition probability of Brownian motion in the octant and its\n  application to default modeling", "summary": "We derive a semi-analytic formula for the transition probability of\nthree-dimensional Brownian motion in the positive octant with absorption at the\nboundaries. Separation of variables in spherical coordinates leads to an\neigenvalue problem for the resulting boundary value problem in the two angular\ncomponents. The main theoretical result is a solution to the original problem\nexpressed as an expansion into special functions and an eigenvalue which has to\nbe chosen to allow a matching of the boundary condition. We discuss and test\nseveral computational methods to solve a finite-dimensional approximation to\nthis nonlinear eigenvalue problem. Finally, we apply our results to the\ncomputation of default probabilities and credit valuation adjustments in a\nstructural credit model with mutual liabilities.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1801.00362v2", "link": "http://arxiv.org/abs/1801.00362v2"}, {"title": "Suitability of Capital Allocations for Performance Measurement", "summary": "Capital allocation principles are used in various contexts in which a risk\ncapital or a cost of an aggregate position has to be allocated among its\nconstituent parts. We study capital allocation principles in a performance\nmeasurement framework. We introduce the notation of suitability of allocations\nfor performance measurement and show under different assumptions on the\ninvolved reward and risk measures that there exist suitable allocation methods.\nThe existence of certain suitable allocation principles generally is given\nunder rather strict assumptions on the underlying risk measure. Therefore we\nshow, with a reformulated definition of suitability and in a slightly modified\nsetting, that there is a known suitable allocation principle that does not\nrequire any properties of the underlying risk measure. Additionally we extend a\nprevious characterization result from the literature from a mean-risk to a\nreward-risk setting. Formulations of this theory are also possible in a game\ntheoretic setting.", "category": ["q-fin.RM", "q-fin.PM"], "id": "http://arxiv.org/abs/1301.5497v7", "link": "http://arxiv.org/abs/1301.5497v7"}, {"title": "Modelling the impact of financialization on agricultural commodity\n  markets", "summary": "We propose a stylized model of production and exchange in which long-term\ninvestors set their production decision over a horizon {\\tau} , the \"time to\nproduce\", and are liquidity constrained, while financial investors trade over a\nmuch shorter horizon {\\delta} (<< {\\tau} ) and are therefore more duly informed\non the exogenous shocks affecting the production output. The equilibrium\nsolution proves that: (i) long-term producers modify their production decisions\nto anticipate the impact of short-term investors allocations on prices; (ii)\nshort-term investments return a positive expected profit commensurate to the\ninformational advantage. While the presence of financial investors improves the\nefficiency of risk allocation in the short-term and reduces price volatility,\nthe model shows that the aggregate effect of commodity market financialization\nresults in rising the volatility of both farms' default risk and production\noutput.", "category": ["q-fin.MF", "q-fin.GN"], "id": "http://arxiv.org/abs/1607.07582v1", "link": "http://arxiv.org/abs/1607.07582v1"}, {"title": "Optimal control of predictive mean-field equations and applications to\n  finance", "summary": "We study a coupled system of controlled stochastic differential equations\n(SDEs) driven by a Brownian motion and a compensated Poisson random measure,\nconsisting of a forward SDE in the unknown process $X(t)$ and a\n\\emph{predictive mean-field} backward SDE (BSDE) in the unknowns $Y(t), Z(t),\nK(t,\\cdot)$. The driver of the BSDE at time $t$ may depend not just upon the\nunknown processes $Y(t), Z(t), K(t,\\cdot)$, but also on the predicted future\nvalue $Y(t+\\delta)$, defined by the conditional expectation $A(t):=\nE[Y(t+\\delta) | \\mathcal{F}_t]$. \\\\ We give a sufficient and a necessary\nmaximum principle for the optimal control of such systems, and then we apply\nthese results to the following two problems:\\\\ (i) Optimal portfolio in a\nfinancial market with an \\emph{insider influenced asset price process.} \\\\ (ii)\n  Optimal consumption rate from a cash flow modeled as a geometric It\\^ o-L\\'\nevy SDE, with respect to \\emph{predictive recursive utility}.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1505.04921v1", "link": "http://arxiv.org/abs/1505.04921v1"}, {"title": "A Theoretical Approach for Dynamic Modelling of Sustainable Development", "summary": "This article presents a theoretical model for a dynamic system based on\nsustainable development. Due to the relatively absence of theoretical studies\nand practical issues in the area of sustainable development, Romania aspires to\nthe principles of sustainable development. Based on the concept as a process in\nwhich economic, social, political and natural environment are combined in order\nto sustain planet management, our goal is to promote an economic tool for\nRomanian decision-makers in order to evaluate scenarios and planning options.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1102.5752v1", "link": "http://arxiv.org/abs/1102.5752v1"}, {"title": "Credit contagion and credit risk", "summary": "We study a simple, solvable model that allows us to investigate effects of\ncredit contagion on the default probability of individual firms, in both\nportfolios of firms and on an economy wide scale. While the effect of\ninteractions may be small in typical (most probable) scenarios they are\nmagnified, due to feedback, by situations of economic stress, which in turn\nleads to fatter tails in loss distributions of large loan portfolios.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/physics/0609164v1", "link": "http://arxiv.org/abs/physics/0609164v1"}, {"title": "Consistent Price Systems under Model Uncertainty", "summary": "We develop a version of the fundamental theorem of asset pricing for\ndiscrete-time markets with proportional transaction costs and model\nuncertainty. A robust notion of no-arbitrage of the second kind is defined and\nshown to be equivalent to the existence of a collection of strictly consistent\nprice systems.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1408.5510v1", "link": "http://arxiv.org/abs/1408.5510v1"}, {"title": "Risk Apportionment: The Dual Story", "summary": "By specifying model free preferences towards simple nested classes of lottery\npairs, we develop the dual story to stand on equal footing with that of\n(primal) risk apportionment. The dual story provides an intuitive\ninterpretation, and full characterization, of dual counterparts of such\nconcepts as prudence and temperance. The direction of preference between these\nnested classes of lottery pairs is equivalent to signing the successive\nderivatives of the probability weighting function within Yaari's (1987) dual\ntheory. We explore implications of our results for optimal portfolio choice and\nshow that the sign of the third derivative of the probability weighting\nfunction may be naturally linked to a self-protection problem.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1712.02182v1", "link": "http://arxiv.org/abs/1712.02182v1"}, {"title": "Structure and evolution of the foreign exchange networks", "summary": "We investigate topology and temporal evolution of the foreign currency\nexchange market viewed from a weighted network perspective. Based on exchange\nrates for a set of 46 currencies (including precious metals), we construct\ndifferent representations of the FX network depending on a choice of the base\ncurrency. Our results show that the network structure is not stable in time,\nbut there are main clusters of currencies, which persist for a long period of\ntime despite the fact that their size and content are variable. We find a\nlong-term trend in the network's evolution which affects the USD and EUR nodes.\nIn all the network representations, the USD node gradually loses its\ncentrality, while, on contrary, the EUR node has become slightly more central\nthan it used to be in its early years. Despite this directional trend, the\noverall evolution of the network is noisy.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0901.4793v1", "link": "http://arxiv.org/abs/0901.4793v1"}, {"title": "Random matrix approach for primal-dual portfolio optimization problems", "summary": "In this paper, we revisit the portfolio optimization problems of the\nminimization/maximization of investment risk under constraints of budget and\ninvestment concentration (primal problem) and the maximization/minimization of\ninvestment concentration under constraints of budget and investment risk (dual\nproblem) for the case that the variances of the return rates of the assets are\nidentical. We analyze both optimization problems by using the Lagrange\nmultiplier method and the random matrix approach. Thereafter, we compare the\nresults obtained from our proposed approach with the results obtained in\nprevious work. Moreover, we use numerical experiments to validate the results\nobtained from the replica approach and the random matrix approach as methods\nfor analyzing both the primal and dual portfolio optimization problems.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1709.04620v2", "link": "http://dx.doi.org/10.7566/JPSJ.86.124804"}, {"title": "Polytopes associated with lattices of subsets and maximising expectation\n  of random variables", "summary": "The present paper originated from a problem in Financial Mathematics\nconcerned with calculating the value of a European call option based on\nmultiple assets each following the binomial model. The model led to an\ninteresting family of polytopes $P(b)$ associated with the power-set\n$\\mathcal{L} = \\wp\\{1,\\dots,m\\}$ and parameterized by $b \\in \\mathbb{R}^m$,\neach of which is a collection of probability density function on $\\mathcal{L}$.\nFor each non-empty $P(b)$ there results a family of probability measures on\n$\\mathcal{L}^n$ and, given a function $F \\colon \\mathcal{L}^n \\to \\mathbb{R}$,\nour goal is to find among these probability measures one which maximises (resp.\nminimises) the expectation of $F$. In this paper we identify a family of such\nfunctions $F$, all of whose expectations are maximised (resp. minimised under\nsome conditions) by the same {\\em product} probability measure defined by a\ndistinguished vertex of $P(b)$ called the supervertex (resp. the subvertex).\nThe pay-offs of European call options belong to this family of functions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2002.06253v1", "link": "http://arxiv.org/abs/2002.06253v1"}, {"title": "On Mean-Variance Analysis", "summary": "This paper considers the mean variance portfolio management problem. We\nexamine portfolios which contain both primary and derivative securities. The\nchallenge in this context is due to portfolio's nonlinearities. The delta-gamma\napproximation is employed to overcome it. Thus, the optimization problem is\nreduced to a well posed quadratic program. The methodology developed in this\npaper can be also applied to pricing and hedging in incomplete markets.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1102.5078v3", "link": "http://arxiv.org/abs/1102.5078v3"}, {"title": "Failure of Equilibrium Selection Methods for Multiple-Principal,\n  Multiple-Agent Problems with Non-Rivalrous Goods: An Analysis of Data Markets", "summary": "The advent of machine learning tools has led to the rise of data markets.\nThese data markets are characterized by multiple data purchasers interacting\nwith a set of data sources. Data sources have more information about the\nquality of data than the data purchasers; additionally, data itself is a\nnon-rivalrous good that can be shared with multiple parties at negligible\nmarginal cost. In this paper, we study the multiple-principal, multiple-agent\nproblem with non-rivalrous goods. Under the assumption that the principal's\npayoff is quasilinear in the payments given to agents, we show that there is a\nfundamental degeneracy in the market of non-rivalrous goods. Specifically, for\na general class of payment contracts, there will be an infinite set of\ngeneralized Nash equilibria. This multiplicity of equilibria also affects\ncommon refinements of equilibrium definitions intended to uniquely select an\nequilibrium: both variational equilibria and normalized equilibria will be\nnon-unique in general. This implies that most existing equilibrium concepts\ncannot provide predictions on the outcomes of data markets emerging today. The\nresults support the idea that modifications to payment contracts themselves are\nunlikely to yield a unique equilibrium, and either changes to the models of\nstudy or new equilibrium concepts will be required to determine unique\nequilibria in settings with multiple principals and a non-rivalrous good.", "category": [], "id": "http://arxiv.org/abs/2004.00196v1", "link": "http://arxiv.org/abs/2004.00196v1"}, {"title": "Coherent CVA and FVA with Liability Side Pricing of Derivatives", "summary": "This article presents FVA and CVA of a bilateral derivative in a coherent\nmanner, based on recent developments in fair value accounting and ISDA\nstandards. We argue that a derivative liability, after primary risk factors\nbeing hedged, resembles in economics an issued variable funding note, and\nshould be priced at the market rate of the issuer's debt. For the purpose of\ndetermining the fair value, the party on the liability side is economically\nneutral to make a deposit to the other party, which earns his current debt rate\nand effectively provides funding and hedging for the party holding the\nderivative asset. The newly derived partial differential equation for an option\ndiscounts the derivative's receivable part with counterparty's curve and\npayable part with own financing curve. The price difference from the\ncounterparty risk free price, or total counterparty risk adjustment, is\nprecisely defined by discounting the product of the risk free price and the\ncredit spread at the local liability curve. Subsequently the adjustment can be\nbroken into a default risk component -- CVA and a funding component -- FVA,\nconsistent with a simple note's fair value treatment and in accordance with the\nusual understanding of a bond's credit spread consisting of a CDS spread and a\nbasis. As for FVA, we define a cost -- credit funding adjustment (CFA) and a\nbenefit -- debit funding adjustment (DFA), in parallel to CVA and DVA and\nattributed to counterparty's and own funding basis. This resolves a number of\noutstanding FVA debate issues, such as double counting, violation of the law of\none price, misuse of cash flow discounting, and controversial hedging of own\ndefault risk. It also allows an integrated implementation strategy and reuse of\nexisting CVA infrastructure.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1510.07199v1", "link": "http://arxiv.org/abs/1510.07199v1"}, {"title": "Feasibility of Portfolio Optimization under Coherent Risk Measures", "summary": "It is shown that the axioms for coherent risk measures imply that whenever\nthere is an asset in a portfolio that dominates the others in a given sample\n(which happens with finite probability even for large samples), then this\nportfolio cannot be optimized under any coherent measure on that sample, and\nthe risk measure diverges to minus infinity. This instability was first\ndiscovered on the special example of Expected Shortfall which is used here both\nas an illustration and as a prompt for generalization.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/0803.2283v3", "link": "http://arxiv.org/abs/0803.2283v3"}, {"title": "A Precursor of Market Crashes", "summary": "In this paper, we quantitatively investigate the properties of a statistical\nensemble of stock prices. We focus attention on the relative price defined as $\nX(t) = S(t)/S(0) $, where $ S(0) $ is the initial price. We selected\napproximately 3200 stocks traded on the Japanese Stock Exchange and formed a\nstatistical ensemble of daily relative prices for each trading day in the\n3-year period from January 4, 1999 to December 28, 2001, corresponding to the\nperiod in which the {\\it internet Bubble} formed and {\\it crashes} in the\nJapanese stock market. We found that the upper tail of the complementary\ncumulative distribution function of the ensemble of the relative prices in the\nhigh value of the price is well described by a power-law distribution, $ P(S>x)\n\\sim x^{-\\alpha} $, with an exponent that moves over time. Furthermore, we\nfound that as the power-law exponents $ \\alpha $ approached {\\it two}, the\nbubble burst. It is reasonable to assume that when the power-law exponents\napproached {\\it two}, it indicates the bubble is about to burst.\n  PACS: 89.65.Gh; Keywords: Market crashes, Power law, Precursor", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0510055v4", "link": "http://dx.doi.org/10.1140/epjb/e2006-00142-9"}, {"title": "Counterfactual Sensitivity and Robustness", "summary": "Researchers frequently make parametric assumptions about the distribution of\nunobservables when formulating structural models. Such assumptions are\ntypically motived by computational convenience rather than economic theory and\nare often untestable. Counterfactuals can be particularly sensitive to such\nassumptions, threatening the credibility of structural modeling exercises. To\naddress this issue, we leverage insights from the literature on ambiguity and\nmodel uncertainty to propose a tractable econometric framework for\ncharacterizing the sensitivity of counterfactuals with respect to a\nresearcher's assumptions about the distribution of unobservables in a class of\nstructural models. In particular, we show how to construct the smallest and\nlargest values of the counterfactual as the distribution of unobservables spans\nnonparametric neighborhoods of the researcher's assumed specification while\nother `structural' features of the model, e.g. equilibrium conditions, are\nmaintained. Our methods are computationally simple to implement, with the\nnuisance distribution effectively profiled out via a low-dimensional convex\nprogram. Our procedure delivers sharp bounds for the identified set of\ncounterfactuals (i.e. without parametric assumptions about the distribution of\nunobservables) as the neighborhoods become large. Over small neighborhoods, we\nrelate our procedure to a measure of local sensitivity which is further\ncharacterized using an influence function representation. We provide a suitable\nsampling theory for plug-in estimators and apply our procedure to models of\nstrategic interaction and dynamic discrete choice.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1904.00989v2", "link": "http://arxiv.org/abs/1904.00989v2"}, {"title": "ElecSim: Monte-Carlo Open-Source Agent-Based Model to Inform Policy for\n  Long-Term Electricity Planning", "summary": "Due to the threat of climate change, a transition from a fossil-fuel based\nsystem to one based on zero-carbon is required. However, this is not as simple\nas instantaneously closing down all fossil fuel energy generation and replacing\nthem with renewable sources -- careful decisions need to be taken to ensure\nrapid but stable progress. To aid decision makers, we present a new tool,\nElecSim, which is an open-sourced agent-based modelling framework used to\nexamine the effect of policy on long-term investment decisions in electricity\ngeneration. ElecSim allows non-experts to rapidly prototype new ideas.\n  Different techniques to model long-term electricity decisions are reviewed\nand used to motivate why agent-based models will become an important strategic\ntool for policy. We motivate why an open-source toolkit is required for\nlong-term electricity planning.\n  Actual electricity prices are compared with our model and we demonstrate that\nthe use of a Monte-Carlo simulation in the system improves performance by\n$52.5\\%$. Further, using ElecSim we demonstrate the effect of a carbon tax to\nencourage a low-carbon electricity supply. We show how a {\\pounds}40 ($\\$50$)\nper tonne of CO2 emitted would lead to 70% renewable electricity by 2050.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1911.01203v1", "link": "http://dx.doi.org/10.1145/3307772.3335321"}, {"title": "China's First Workforce Skill Taxonomy", "summary": "China is the world's second largest economy. After four decades of economic\nmiracles, China's economy is transitioning into an advanced, knowledge-based\neconomy. Yet, we still lack a detailed understanding of the skills that underly\nthe Chinese labor force, and the development and spatial distribution of these\nskills. For example, the US standardized skill taxonomy O*NET played an\nimportant role in understanding the dynamics of manufacturing and\nknowledge-based work, as well as potential risks from automation and\noutsourcing. Here, we use Machine Learning techniques to bridge this gap,\ncreating China's first workforce skill taxonomy, and map it to O*NET. This\nenables us to reveal workforce skill polarization into social-cognitive skills\nand sensory-physical skills, and to explore the China's regional inequality in\nlight of workforce skills, and compare it to traditional metrics such as\neducation. We build an online tool for the public and policy makers to explore\nthe skill taxonomy: skills.sysu.edu.cn. We will also make the taxonomy dataset\npublicly available for other researchers upon publication.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.02863v1", "link": "http://arxiv.org/abs/2001.02863v1"}, {"title": "Non-parametric and semi-parametric asset pricing", "summary": "We find that the CAPM fails to explain the small firm effect even if its\nnon-parametric form is used which allows time-varying risk and non-linearity in\nthe pricing function. Furthermore, the linearity of the CAPM can be rejected,\nthus the widely used risk and performance measures, the beta and the alpha, are\nbiased and inconsistent. We deduce semi-parametric measures which are\nnon-constant under extreme market conditions in a single factor setting; on the\nother hand, they are not significantly different from the linear estimates of\nthe Fama-French three-factor model. If we extend the single factor model with\nthe Fama-French factors, the simple linear model is able to explain the US\nstock returns correctly.", "category": ["q-fin.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1703.09500v1", "link": "http://dx.doi.org/10.1016/j.econmod.2010.12.008"}, {"title": "Classical and quantum randomness and the financial market", "summary": "We analyze complexity of financial (and general economic) processes by\ncomparing classical and quantum-like models for randomness. Our analysis\nimplies that it might be that a quantum-like probabilistic description is more\nnatural for financial market than the classical one. A part of our analysis is\ndevoted to study the possibility of application of the quantum probabilistic\nmodel to agents of financial market. We show that, although the direct quantum\n(physical) reduction (based on using the scales of quantum mechanics) is\nmeaningless, one may apply so called quantum-like models. In our approach\nquantum-like probabilistic behaviour is a consequence of contextualy of\nstatistical data in finances (and economics in general). However, our\nhypothesis on \"quantumness\" of financial data should be tested experimentally\n(as opposed to the conventional description based on the noncontextual\nclassical probabilistic approach). We present a new statistical test based on a\ngeneralization of the well known in quantum physics Bell's inequality.", "category": ["q-fin.ST", "math.PR"], "id": "http://arxiv.org/abs/0704.2865v1", "link": "http://arxiv.org/abs/0704.2865v1"}, {"title": "Evaluating the role of risk networks on risk identification,\n  classification and emergence", "summary": "Modern society heavily relies on strongly connected, socio-technical systems.\nAs a result, distinct risks threatening the operation of individual systems can\nno longer be treated in isolation. Consequently, risk experts are actively\nseeking for ways to relax the risk independence assumption that undermines\ntypical risk management models. Prominent work has advocated the use of risk\nnetworks as a way forward. Yet, the inevitable biases introduced during the\ngeneration of these survey-based risk networks limit our ability to examine\ntheir topology, and in turn challenge the utility of the very notion of a risk\nnetwork. To alleviate these concerns, we proposed an alternative methodology\nfor generating weighted risk networks. We subsequently applied this methodology\nto an empirical dataset of financial data. This paper reports our findings on\nthe study of the topology of the resulting risk network. We observed a modular\ntopology, and reasoned on its use as a robust risk classification framework.\nUsing these modules, we highlight a tendency of specialization during the risk\nidentification process, with some firms being solely focused on a subset of the\navailable risk classes. Finally, we considered the independent and systemic\nimpact of some risks and attributed possible mismatches to their emerging\nnature.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1801.05759v1", "link": "http://dx.doi.org/10.21314/JNTF.2017.032"}, {"title": "Can One Make Any Crash Prediction in Finance Using the Local Hurst\n  Exponent Idea?", "summary": "We apply the Hurst exponent idea for investigation of DJIA index time-series\ndata. The behavior of the local Hurst exponent prior to drastic changes in\nfinancial series signal is analyzed. The optimal length of the time-window over\nwhich this exponent can be calculated in order to make some meaningful\npredictions is discussed. Our prediction hypothesis is verified with examples\nof '29 and '87 crashes, as well as with more recent phenomena in stock market\nfrom the period 1995-2003.Some interesting agreements are found.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0311627v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.01.018"}, {"title": "Pandemic, Shutdown and Consumer Spending: Lessons from Scandinavian\n  Policy Responses to COVID-19", "summary": "This paper uses transaction data from a large bank in Scandinavia to estimate\nthe effect of social distancing laws on consumer spending in the COVID-19\npandemic. The analysis exploits a natural experiment to disentangle the effects\nof the virus and the laws aiming to contain it: Denmark and Sweden were\nsimilarly exposed to the pandemic but only Denmark imposed significant\nrestrictions on social and economic activities. We estimate that aggregate\nspending dropped by around 25 percent in Sweden and, as a result of the\nshutdown, by 4 additional percentage points in Denmark. This implies that most\nof the economic contraction is caused by the virus itself and occurs regardless\nof social distancing laws. The age gradient in the estimates suggest that\nsocial distancing reinforces the virus-induced drop in spending for low\nhealth-risk individuals but attenuates it for high-risk individuals by lowering\nthe overall prevalence of the virus in the society.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.04630v1", "link": "http://arxiv.org/abs/2005.04630v1"}, {"title": "A Multiple Network Approach to Corporate Governance", "summary": "In this work, we consider Corporate Governance (CG) ties among companies from\na multiple network perspective. Such a structure naturally arises from the\nclose interrelation between the Shareholding Network (SH) and the Board of\nDirectors network (BD). In order to capture the simultaneous effects of both\nnetworks on CG, we propose to model the CG multiple network structure via\ntensor analysis. In particular, we consider the TOPHITS model, based on the\nPARAFAC tensor decomposition, to show that tensor techniques can be\nsuccessfully applied in this context. By providing some empirical results from\nthe Italian financial market in the univariate case, we then show that a\ntensor--based multiple network approach can reveal important information.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1401.4387v2", "link": "http://arxiv.org/abs/1401.4387v2"}, {"title": "Detect & Describe: Deep learning of bank stress in the news", "summary": "News is a pertinent source of information on financial risks and stress\nfactors, which nevertheless is challenging to harness due to the sparse and\nunstructured nature of natural text. We propose an approach based on\ndistributional semantics and deep learning with neural networks to model and\nlink text to a scarce set of bank distress events. Through unsupervised\ntraining, we learn semantic vector representations of news articles as\npredictors of distress events. The predictive model that we learn can signal\ncoinciding stress with an aggregated index at bank or European level, while\ncrucially allowing for automatic extraction of text descriptions of the events,\nbased on passages with high stress levels. The method offers insight that\nmodels based on other types of data cannot provide, while offering a general\nmeans for interpreting this type of semantic-predictive model. We model bank\ndistress with data on 243 events and 6.6M news articles for 101 large European\nbanks.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1507.07870v1", "link": "http://arxiv.org/abs/1507.07870v1"}, {"title": "Stochastic Switching Games", "summary": "We study nonzero-sum stochastic switching games. Two players compete for\nmarket dominance through controlling (via timing options) the discrete-state\nmarket regime $M$. Switching decisions are driven by a continuous stochastic\nfactor $X$ that modulates instantaneous revenue rates and switching costs. This\ngenerates a competitive feedback between the short-term fluctuations due to $X$\nand the medium-term advantages based on $M$. We construct threshold-type\nFeedback Nash Equilibria which characterize stationary strategies describing\nlong-run dynamic equilibrium market organization. Two sequential approximation\nschemes link the switching equilibrium to (i) constrained optimal switching,\n(ii) multi-stage timing games. We provide illustrations using an\nOrnstein-Uhlenbeck $X$ that leads to a recurrent equilibrium $M^\\ast$ and a\nGeometric Brownian Motion $X$ that makes $M^\\ast$ eventually \"absorbed\" as one\nplayer eventually gains permanent advantage. Explicit computations and\ncomparative statics regarding the emergent macroscopic market equilibrium are\nalso provided.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.03893v1", "link": "http://arxiv.org/abs/1807.03893v1"}, {"title": "Inventory Management for High-Frequency Trading with Imperfect\n  Competition", "summary": "We study Nash equilibria for inventory-averse high-frequency traders (HFTs),\nwho trade to exploit information about future price changes. For discrete\ntrading rounds, the HFTs' optimal trading strategies and their equilibrium\nprice impact are described by a system of nonlinear equations; explicit\nsolutions obtain around the continuous-time limit. Unlike in the risk-neutral\ncase, the optimal inventories become mean-reverting and vanish as the number of\ntrading rounds becomes large. In contrast, the HFTs' risk-adjusted profits and\nthe equilibrium price impact converge to their risk-neutral counterparts.\nCompared to a social-planner solution for cooperative HFTs, Nash competition\nleads to excess trading, so that marginal transaction taxes in fact decrease\nmarket liquidity.", "category": ["q-fin.TR", "q-fin.PM"], "id": "http://arxiv.org/abs/1808.05169v3", "link": "http://arxiv.org/abs/1808.05169v3"}, {"title": "Relatedness, Knowledge Diffusion, and the Evolution of Bilateral Trade", "summary": "During the last decades two important contributions have reshaped our\nunderstanding of international trade. First, countries trade more with those\nwith whom they share history, language, and culture, suggesting that trade is\nlimited by information frictions. Second, countries are more likely to start\nexporting products that are similar to their current exports, suggesting that\nknowledge diffusion among related industries is a key constrain shaping the\ndiversification of exports. But does knowledge about how to export to a\ndestination also diffuses among related products and geographic neighbors? Do\ncountries need to learn how to trade each product to each destination? Here, we\nuse bilateral trade data from 2000 to 2015 to show that countries are more\nlikely to increase their exports of a product to a destination when: (i) they\nexport related products to it, (ii) they export the same product to the\nneighbor of a destination, (iii) they have neighbors who export the same\nproduct to that destination. Then, we explore the magnitude of these effects\nfor new, nascent, and experienced exporters, (exporters with and without\ncomparative advantage in a product) and also for groups of products with\ndifferent level of technological sophistication. We find that the effects of\nproduct and geographic relatedness are stronger for new exporters, and also,\nthat the effect of product relatedness is stronger for more technologically\nsophisticated products. These findings support the idea that international\ntrade is shaped by information frictions that are reduced in the presence of\nrelated products and experienced geographic neighbors.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1709.05392v1", "link": "http://arxiv.org/abs/1709.05392v1"}, {"title": "Portfolio Benchmarking under Drawdown Constraint and Stochastic Sharpe\n  Ratio", "summary": "We consider an investor who seeks to maximize her expected utility derived\nfrom her terminal wealth relative to the maximum performance achieved over a\nfixed time horizon, and under a portfolio drawdown constraint, in a market with\nlocal stochastic volatility (LSV). In the absence of closed-form formulas for\nthe value function and optimal portfolio strategy, we obtain approximations for\nthese quantities through the use of a coefficient expansion technique and\nnonlinear transformations. We utilize regularity properties of the risk\ntolerance function to numerically compute the estimates for our approximations.\nIn order to achieve similar value functions, we illustrate that, compared to a\nconstant volatility model, the investor must deploy a quite different portfolio\nstrategy which depends on the current level of volatility in the stochastic\nvolatility model.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1610.08558v1", "link": "http://arxiv.org/abs/1610.08558v1"}, {"title": "Game theoretic derivation of discrete distributions and discrete pricing\n  formulas", "summary": "In this expository paper we illustrate the generality of game theoretic\nprobability protocols of Shafer and Vovk (2001) in finite-horizon discrete\ngames. By restricting ourselves to finite-horizon discrete games, we can\nexplicitly describe how discrete distributions with finite support and the\ndiscrete pricing formulas, such as the Cox-Ross-Rubinstein formula, are\nnaturally derived from game-theoretic probability protocols. Corresponding to\nany discrete distribution with finite support, we construct a finite-horizon\ndiscrete game, a replicating strategy of Skeptic, and a neutral forecasting\nstrategy of Forecaster, such that the discrete distribution is derived from the\ngame. Construction of a replicating strategy is the same as in the standard\narbitrage arguments of pricing European options in the binomial tree models.\nHowever the game theoretic framework is advantageous because no a priori\nprobabilistic assumption is needed.", "category": ["math.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/math/0509367v1", "link": "http://arxiv.org/abs/math/0509367v1"}, {"title": "The Opportunity Process for Optimal Consumption and Investment with\n  Power Utility", "summary": "We study the utility maximization problem for power utility random fields in\na semimartingale financial market, with and without intermediate consumption.\nThe notion of an opportunity process is introduced as a reduced form of the\nvalue process of the resulting stochastic control problem. We show how the\nopportunity process describes the key objects: optimal strategy, value\nfunction, and dual problem. The results are applied to obtain monotonicity\nproperties of the optimal consumption.", "category": ["q-fin.PM", "q-fin.CP"], "id": "http://arxiv.org/abs/0912.1879v2", "link": "http://dx.doi.org/10.1007/s11579-010-0031-0"}, {"title": "Averaging plus Learning in financial markets", "summary": "This paper develops original models to study interacting agents in financial\nmarkets. The key feature of these models is how interactions are formulated and\nanalysed. Agents learn from their observations and learning ability to\ninterpret news or private information. Central limit theorems are developed but\nthey arise rather unexpectedly. Under certain type of conditions governing the\nlearning, agents beliefs converge in distribution that can be even fractal. The\nunderlying randomness in the systems is not restricted to be of a certain\nclass. Fresh insights are gained not only from developing new non-linear social\nlearning models but also from using different techniques to study discrete time\nrandom linear dynamical systems.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1904.08131v2", "link": "http://arxiv.org/abs/1904.08131v2"}, {"title": "Non-equilibrium statistical mechanics of Minority Games", "summary": "In this paper I give a brief introduction to a family of simple but\nnon-trivial models designed to increase our understanding of collective\nprocesses in markets, the so-called Minority Games, and their non-equilibrium\nstatistical mathematical analysis. Since the most commonly studied members of\nthis family define disordered stochastic processes without detailed balance,\nthe canonical technique for finding exact solutions is found to be generating\nfunctional analysis a la De Dominicis, as originally developed in the\nspin-glass community.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0205262v1", "link": "http://arxiv.org/abs/cond-mat/0205262v1"}, {"title": "Trust! Why it Has Been Lost and How to Regain It", "summary": "This essay suggests that a proper assessment of the presently unfolding\nfinancial crisis, and its cure, requires going back at least to the late 1990s,\naccounting for the cumulative effect of the ITC, real-estate and financial\nderivative bubbles. We focus on the deep loss of trust, not only in Wall\nStreet, but more importantly in Main Street, and how to recover it on the short\nand long terms. A multi-disciplinary approach is needed to deal with the\nnonlinear complex systems of the present world, in order to develop a culture\nof fairness, and of upside opportunities associated with a risky world.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0810.4608v1", "link": "http://arxiv.org/abs/0810.4608v1"}, {"title": "Impact of Stock Market Structure on Intertrade Time and Price Dynamics", "summary": "The NYSE and NASDAQ stock markets have very different structures and there is\ncontinuing controversy over whether differences in stock price behaviour are\ndue to market structure or company characteristics. As the influence of market\nstructure on stock prices may be obscured by exogenous factors such as demand\nand supply, we hypothesize that modulation of the flow of transactions due to\nmarket operations may carry a stronger imprint of the internal market\nmechanism. We analyse times between consecutive transactions (ITT) for NYSE and\nNASDAQ stocks, and we relate the dynamical properties of the ITT with those of\nthe corresponding price fluctuations. We find a robust scale-invariant temporal\norganisation in the ITT of stocks which is independent of individual company\ncharacteristics and industry sector, but which depends on market structure. We\nfind that stocks registered on the NASDAQ exhibit stronger correlations in\ntheir transaction timing within a trading day, compared with NYSE stocks.\nFurther, we find that companies that transfer from the NASDAQ to the NYSE show\na reduction in the correlation strength of transaction timing within a trading\nday, after the move, suggesting influences of market structure. Surprisingly,\nwe also observe that stronger power-law correlations in the ITT are coupled\nwith stronger power-law correlations in absolute price returns and higher price\nvolatility, suggesting a strong link between the dynamical properties of ITT\nand the corresponding price fluctuations over a broad range of time scales.\nComparing the NYSE and NASDAQ, we demonstrate that the higher correlations we\nfind in ITT for NASDAQ stocks are matched by higher correlations in absolute\nprice returns and by higher volatility, suggesting that market structure may\naffect price behaviour through information contained in transaction timing.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0508203v1", "link": "http://arxiv.org/abs/physics/0508203v1"}, {"title": "Extracting Geography from Trade Data", "summary": "Understanding international trade is a fundamental problem in economics --\none standard approach is via what is commonly called the \"gravity equation\",\nwhich predicts the total amount of trade $F_ij$ between two countries $i$ and\n$j$ as $$ F_{ij} = G \\frac{M_i M_j}{D_{ij}},$$ where $G$ is a constant, $M_i,\nM_j$ denote the \"economic mass\" (often simply the gross domestic product) and\n$D_{ij}$ the \"distance\" between countries $i$ and $j$, where \"distance\" is a\ncomplex notion that includes geographical, historical, linguistic and\nsociological components. We take the \\textit{inverse} route and ask ourselves\nto which extent it is possible to reconstruct meaningful information about\ncountries simply from knowing the bilateral trade volumes $F_{ij}$: indeed, we\nshow that a remarkable amount of geopolitical information can be extracted. The\nmain tool is a spectral decomposition of the Graph Laplacian as a tool to\nperform nonlinear dimensionality reduction. This may have further applications\nin economic analysis and provides a data-based approach to \"trade distance\".", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1607.05235v2", "link": "http://dx.doi.org/10.1016/j.physa.2017.01.037"}, {"title": "Sectoral co-movements in the Indian stock market: A mesoscopic network\n  analysis", "summary": "In this article we review several techniques to extract information from\nstock market data. We discuss recurrence analysis of time series, decomposition\nof aggregate correlation matrices to study co-movements in financial data,\nstock level partial correlations with market indices, multidimensional scaling\nand minimum spanning tree. We apply these techniques to daily return time\nseries from the Indian stock market. The analysis allows us to construct\nnetworks based on correlation matrices of individual stocks in one hand and on\nthe other, we discuss dynamics of market indices. Thus both micro level and\nmacro level dynamics can be analyzed using such tools. We use the\nmulti-dimensional scaling methods to visualize the sectoral structure of the\nstock market, and analyze the comovements among the sectoral stocks. Finally,\nwe construct a mesoscopic network based on sectoral indices. Minimum spanning\ntree technique is seen to be extremely useful in order to separate\ntechnologically related sectors and the mapping corresponds to actual\nproduction relationship to a reasonable extent.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1607.05514v1", "link": "http://arxiv.org/abs/1607.05514v1"}, {"title": "Rational hyperbolic discounting", "summary": "How much should you receive in a week to be indifferent to \\$ 100 in six\nmonths? Note that the indifference requires a rule to ensure the similarity\nbetween early and late payments. Assuming that rational individuals have low\naccuracy, then the following rule is valid: if the amounts to be paid are much\nless than the personal wealth, then the $q$-exponential discounting guarantees\nindifference in several periods. Thus, the discounting can be interpolated\nbetween hyperbolic and exponential functions due to the low accuracy to\ndistinguish time averages when the payments have low impact on personal wealth.\nTherefore, there are physical conditions that allow the hyperbolic discounting\nregardless psycho-behavioral assumption.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.05209v2", "link": "http://arxiv.org/abs/1910.05209v2"}, {"title": "Inference on the Sharpe ratio via the upsilon distribution", "summary": "The upsilon distribution, the sum of independent chi random variates and a\nnormal, is introduced. As a special case, the upsilon distribution includes\nLecoutre's lambda-prime distribution. The upsilon distribution finds\napplication in Frequentist inference on the Sharpe ratio, including hypothesis\ntests on independent samples, confidence intervals, and prediction intervals,\nas well as their Bayesian counterparts. These tests are extended to the case of\nfactor models of returns.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1505.00829v2", "link": "http://arxiv.org/abs/1505.00829v2"}, {"title": "Entropy-Based Financial Asset Pricing", "summary": "We investigate entropy as a financial risk measure. Entropy explains the\nequity premium of securities and portfolios in a simpler way and, at the same\ntime, with higher explanatory power than the beta parameter of the capital\nasset pricing model. For asset pricing we define the continuous entropy as an\nalternative measure of risk. Our results show that entropy decreases in the\nfunction of the number of securities involved in a portfolio in a similar way\nto the standard deviation, and that efficient portfolios are situated on a\nhyperbola in the expected return - entropy system. For empirical investigation\nwe use daily returns of 150 randomly selected securities for a period of 27\nyears. Our regression results show that entropy has a higher explanatory power\nfor the expected return than the capital asset pricing model beta. Furthermore\nwe show the time varying behaviour of the beta along with entropy.", "category": ["q-fin.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1501.01155v1", "link": "http://dx.doi.org/10.1371/journal.pone.0115742"}, {"title": "Optimal redeeming strategy of stock loans under drift uncertainty", "summary": "In practice, one must recognize the inevitable incompleteness of information\nwhile making decisions. In this paper, we consider the optimal redeeming\nproblem of stock loans under a state of incomplete information presented by the\nuncertainty in the (bull or bear) trends of the underlying stock. This is\ncalled drift uncertainty. Due to the unavoidable need for the estimation of\ntrends while making decisions, the related Hamilton-Jacobi-Bellman (HJB)\nequation is of a degenerate parabolic type. Hence, it is very hard to obtain\nits regularity using the standard approach, making the problem different from\nthe existing optimal redeeming problems without drift uncertainty. We present a\nthorough and delicate probabilistic and functional analysis to obtain the\nregularity of the value function and the optimal redeeming strategies. The\noptimal redeeming strategies of stock loans appear significantly different in\nthe bull and bear trends.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1901.06680v1", "link": "http://arxiv.org/abs/1901.06680v1"}, {"title": "Mathew Effect in Artificial Stock Market", "summary": "In this article, we established a stock market model based on agents'\ninvesting mentality. The agents decide whether to purchase the shares at the\nprobability, according to their anticipation of the market's behaviors. The\nexpectation of the amount of shares they want to buy is directly proportional\nto the value of asset they hold. The agents sell their shares because of the\ngaining-profit psychology, stopping-loss psychology, or dissatisfaction with\nthe long-time congealing of the assets. We studied how the distribution of\nagent's assets varies along with systemic evolution. The experiments show us\nobvious Mathew effect on asset distribution in the artificial stock market, and\nwe have found that the Mathew effect on asset distribution was more and more\nsalient along with the increasing of system running time, stock market size and\nagents' activity extent.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0406365v1", "link": "http://arxiv.org/abs/cond-mat/0406365v1"}, {"title": "Financial Markets and Persistence", "summary": "Persistence is studied in a financial context by mapping the time evolution\nof the values of the shares quoted on the London Financial Times Stock Exchange\n100 index (FTSE 100) onto Ising spins. By following the time dependence of the\nspins, we find evidence for power law decay of the proportion of shares that\nremain either above or below their ` starting\\rq values. As a result, we\nestimate a persistence exponent for the underlying financial market to be\n$\\theta_f\\sim 0.5$.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0510028v3", "link": "http://arxiv.org/abs/physics/0510028v3"}, {"title": "Generalized Beta Prime Distribution: Stochastic Model of Economic\n  Exchange and Properties of Inequality Indices", "summary": "We argue that a stochastic model of economic exchange, whose steady-state\ndistribution is a Generalized Beta Prime (also known as GB2), and some unique\nproperties of the latter, are the reason for GB2's success in describing\nwealth/income distributions. We use housing sale prices as a proxy to\nwealth/income distribution to numerically illustrate this point. We also\nexplore parametric limits of the distribution to do so analytically. We discuss\nparametric properties of the inequality indices -- Gini, Hoover, Theil T and\nTheil L -- vis-a-vis those of GB2 and introduce a new inequality index, which\nserves a similar purpose. We argue that Hoover and Theil L are more appropriate\nmeasures for distributions with power-law dependencies, especially fat tails,\nsuch as GB2.", "category": ["econ.EM", "q-fin.MF", "q-fin.ST"], "id": "http://arxiv.org/abs/1906.04822v1", "link": "http://arxiv.org/abs/1906.04822v1"}, {"title": "Improved Algorithms for Computing Worst Value-at-Risk: Numerical\n  Challenges and the Adaptive Rearrangement Algorithm", "summary": "Numerical challenges inherent in algorithms for computing worst Value-at-Risk\nin homogeneous portfolios are identified and solutions as well as words of\nwarning concerning their implementation are provided. Furthermore, both\nconceptual and computational improvements to the Rearrangement Algorithm for\napproximating worst Value-at-Risk for portfolios with arbitrary marginal loss\ndistributions are given. In particular, a novel Adaptive Rearrangement\nAlgorithm is introduced and investigated. These algorithms are implemented\nusing the R package qrmtools.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1505.02281v2", "link": "http://arxiv.org/abs/1505.02281v2"}, {"title": "Markovian Nash equilibrium in financial markets with asymmetric\n  information and related forward-backward systems", "summary": "This paper develops a new methodology for studying continuous-time Nash\nequilibrium in a financial market with asymmetrically informed agents. This\napproach allows us to lift the restriction of risk neutrality imposed on market\nmakers by the current literature. It turns out that, when the market makers are\nrisk averse, the optimal strategies of the agents are solutions of a\nforward-backward system of partial and stochastic differential equations. In\nparticular, the price set by the market makers solves a nonstandard \"quadratic\"\nbackward stochastic differential equation. The main result of the paper is the\nexistence of a Markovian solution to this forward-backward system on an\narbitrary time interval, which is obtained via a fixed-point argument on the\nspace of absolutely continuous distribution functions. Moreover, the\nequilibrium obtained in this paper is able to explain several stylized facts\nwhich are not captured by the current asymmetric information models.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1407.2420v3", "link": "http://dx.doi.org/10.1214/15-AAP1138"}, {"title": "Internal migration and education: A cross-national comparison", "summary": "Migration the main process shaping patterns of human settlement within and\nbetween countries. It is widely acknowledged to be integral to the process of\nhuman development as it plays a significant role in enhancing educational\noutcomes. At regional and national levels, internal migration underpins the\nefficient functioning of the economy by bringing knowledge and skills to the\nlocations where they are needed. It is the multi-dimensional nature of\nmigration that underlines its significance in the process of human development.\nHuman mobility extends in the spatial domain from local travel to international\nmigration, and in the temporal dimension from short-term stays to permanent\nrelocations. Classification and measurement of such phenomena is inevitably\ncomplex, which has severely hindered progress in comparative research, with\nvery few large-scale cross-national comparisons of migration. The linkages\nbetween migration and education have been explored in a separate line of\ninquiry that has predominantly focused on country-specific analyses as to the\nways in which migration affects educational outcomes and how educational\nattainment affects migration behaviour. A recurrent theme has been the\neducational selectivity of migrants, which in turn leads to an increase of\nhuman capital in some regions, primarily cities, at the expense of others.\nQuestions have long been raised as to the links between education and migration\nin response to educational expansion, but have not yet been fully answered\nbecause of the absence, until recently, of adequate data for comparative\nanalysis of migration. In this paper, we bring these two separate strands of\nresearch together to systematically explore links between internal migration\nand education across a global sample of 57 countries at various stages of\ndevelopment, using data drawn from the IPUMS database.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1812.08913v1", "link": "http://arxiv.org/abs/1812.08913v1"}, {"title": "World currency exchange rate cross-correlations", "summary": "World currency network constitutes one of the most complex structures that is\nassociated with the contemporary civilization. On a way towards quantifying its\ncharacteristics we study the cross correlations in changes of the daily foreign\nexchange rates within the basket of 60 currencies in the period December 1998\n-- May 2005. Such a dynamics turns out to predominantly involve one outstanding\neigenvalue of the correlation matrix. The magnitude of this eigenvalue depends\nhowever crucially on which currency is used as a base currency for the\nremaining ones. Most prominent it looks from the perspective of a peripheral\ncurrency. This largest eigenvalue is seen to systematically decrease and thus\nthe structure of correlations becomes more heterogeneous, when more significant\ncurrencies are used as reference. An extreme case in this later respect is the\nUSD in the period considered. Besides providing further insight into subtle\nnature of complexity, these observations point to a formal procedure that in\ngeneral can be used for practical purposes of measuring the relative currencies\nsignificance on various time horizons.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0708.4347v1", "link": "http://dx.doi.org/10.1140/epjb/e2007-00246-8"}, {"title": "A model of coopetitive game and the Greek crisis", "summary": "In the present work we propose an original analytical model of coopetitive\ngame. We try to apply this analytical model of coopetition - based on game\ntheory and conceived at a macro level - to the Greek crisis, suggesting\nfeasible solutions in a cooperative perspective for the divergent interests\nwhich drive the economic policies in the euro area.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1106.3543v3", "link": "http://arxiv.org/abs/1106.3543v3"}, {"title": "Nonlocal Diffusions and The Quantum Black-Scholes Equation: Modelling\n  the Market Fear Factor", "summary": "In this paper, we establish a link between quantum stochastic processes, and\nnonlocal diffusions. We demonstrate how the non-commutative Black-Scholes\nequation of Accardi & Boukas (Luigi Accardi, Andreas Boukas, 'The Quantum\nBlack-Scholes Equation', Jun 2007, available at arXiv:0706.1300v1) can be\nwritten in integral form. This enables the application of the Monte-Carlo\nmethods adapted to McKean stochastic differential equations (H. P. McKean, 'A\nclass of Markov processes associated with nonlinear parabolic equations', Proc.\nNatl. Acad. Sci. U.S.A., 56(6):1907-1911, 1966) for the simulation of\nsolutions. We show how unitary transformations can be applied to classical\nBlack-Scholes systems to introduce novel quantum effects. These have a simple\neconomic interpretation as a market `fear factor', whereby recent market\nturbulence causes an increase in volatility going forward, that is not linked\nto either the local volatility function or an additional stochastic variable.\nLastly, we extend this system to 2 variables, and consider Quantum models for\nbid-offer spread dynamics.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1806.07983v2", "link": "http://arxiv.org/abs/1806.07983v2"}, {"title": "A spring-block analogy for the dynamics of stock indexes", "summary": "A spring-block chain placed on a running conveyor belt is considered for\nmodeling stylized facts observed in the dynamics of stock indexes. Individual\nstocks are modeled by the blocks, while the stock-stock correlations are\nintroduced via simple elastic forces acting in the springs. The dragging effect\nof the moving belt corresponds to the expected economic growth. The\nspring-block system produces collective behavior and avalanche like phenomena,\nsimilar to the ones observed in stock markets. An artificial index is defined\nfor the spring-block chain, and its dynamics is compared with the one measured\nfor the Dow Jones Industrial Average. For certain parameter regions the model\nreproduces qualitatively well the dynamics of the logarithmic index, the\nlogarithmic returns, the distribution of the logarithmic returns, the\navalanche-size distribution and the distribution of the investment horizons. A\nnoticeable success of the model is that it is able to account for the gain-loss\nasymmetry observed in the inverse statistics. Our approach has mainly a\npedagogical value, bridging between a complex socio-economic phenomena and a\nbasic (mechanical) model in physics.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1409.1748v1", "link": "http://dx.doi.org/10.1016/j.physa.2015.01.079"}, {"title": "An Analysis of the Japanese Credit Network", "summary": "An analysis of the Japanese credit market in 2004 between banks and quoted\nfirms is done in this paper using the tools of the networks theory. It can be\npointed out that: (i) a backbone of the credit channel emerges, where some\nlinks play a crucial role; (ii) big banks privilege long-term contracts; the\n\"minimal spanning trees\" (iii) disclose a highly hierarchical backbone, where\nthe central positions are occupied by the largest banks, and emphasize (iv) a\nstrong geographical characterization, while (v) the clusters of firms do not\nhave specific common properties. Moreover, (vi) while larger firms have\nmultiple lending in large, (vii) the demand for credit (long vs. short term\ndebt and multi-credit lines) of firms with similar sizes is very heterogeneous.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0901.2384v2", "link": "http://arxiv.org/abs/0901.2384v2"}, {"title": "The Multivariate Mixture Dynamics Model: Shifted dynamics and\n  correlation skew", "summary": "The Multi Variate Mixture Dynamics model is a tractable, dynamical,\narbitrage-free multivariate model characterized by transparency on the\ndependence structure, since closed form formulae for terminal correlations,\naverage correlations and copula function are available. It also allows for\ncomplete decorrelation between assets and instantaneous variances. Each single\nasset is modelled according to a lognormal mixture dynamics model, and this\nunivariate version is widely used in the industry due to its flexibility and\naccuracy. The same property holds for the multivariate process of all assets,\nwhose density is a mixture of multivariate basic densities. This allows for\nconsistency of single asset and index/portfolio smile. In this paper, we\ngeneralize the MVMD model by introducing shifted dynamics and we propose a\ndefinition of implied correlation under this model. We investigate whether the\nmodel is able to consistently reproduce the implied volatility of FX cross\nrates once the single components are calibrated to univariate shifted lognormal\nmixture dynamics models. We consider in particular the case of the Chinese\nrenminbi FX rate, showing that the shifted MVMD model correctly recovers the\nCNY/EUR smile given the EUR/USD smile and the USD/CNY smile, thus highlighting\nthat the model can also work as an arbitrage free volatility smile\nextrapolation tool for cross currencies that may not be liquid or fully\nobservable. We compare the performance of the shifted MVMD model in terms of\nimplied correlation with those of the shifted Simply Correlated Mixture\nDynamics model where the dynamics of the single assets are connected naively by\nintroducing correlation among their Brownian motions. Finally, we introduce a\nmodel with uncertain volatilities and correlation. The Markovian projection of\nthis model is a generalization of the shifted MVMD model.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1512.04741v3", "link": "http://arxiv.org/abs/1512.04741v3"}, {"title": "Model Risk in Credit Risk", "summary": "The issue of model risk in default modeling has been known since inception of\nthe Academic literature in the field. However, a rigorous treatment requires a\ndescription of all the possible models, and a measure of the distance between a\nsingle model and the alternatives, consistent with the applications. This is\nthe purpose of the current paper. We first analytically describe all possible\njoint models for default, in the class of finite sequences of exchangeable\nBernoulli random variables. We then measure how the model risk of choosing or\ncalibrating one of them affects the portfolio loss from default, using two\npopular and economically sensible metrics, Value-at-Risk (VaR) and Expected\nShortfall (ES).", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1906.06164v1", "link": "http://arxiv.org/abs/1906.06164v1"}, {"title": "Market Impact in a Latent Order Book", "summary": "We revisit the classical problem of market impact through the lens of a new\nagent-based model. Drawing from the mean-field approach in Statistical\nMechanics and Physics, we assume a large number of agents interacting in the\norder book. By taking the 'continuum' limit we obtain a set of nonlinear\ndifferential equations, the core of our dynamical theory of price formation.\nAnd we explicitly solve them using Fourier analysis. One could talk as well of\na \"micro-macro\" approach of equilibrium, where the market price is the\nconsequence of each (\"microscopic\") agent behaving with respect to his\npreferences and to global (\"macroscopic\") information. When a large market\norder (or metaorder) perturbs the market, our model recovers the square-root\nlaw of impact, providing new insights on the price formation process. In\naddition, we give various limiting cases, examples and possible extensions.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1802.06101v2", "link": "http://arxiv.org/abs/1802.06101v2"}, {"title": "An analytical perturbative solution to the Merton Garman model using\n  symmetries", "summary": "In this paper, we introduce an analytical perturbative solution to the Merton\nGarman model. It is obtained by doing perturbation theory around the exact\nanalytical solution of a model which possesses a two-dimensional Galilean\nsymmetry. We compare our perturbative solution of the Merton Garman model to\nMonte Carlo simulations and find that our solutions performs surprisingly well\nfor a wide range of parameters. We also show how to use symmetries to build\noption pricing models. Our results demonstrate that the concept of symmetry is\nimportant in mathematical finance.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1909.01413v1", "link": "http://arxiv.org/abs/1909.01413v1"}, {"title": "Calibration to American Options: Numerical Investigation of the\n  de-Americanization", "summary": "American options are the reference instruments for the model calibration of a\nlarge and important class of single stocks. For this task, a fast and accurate\npricing algorithm is indispensable. The literature mainly discusses pricing\nmethods for American options that are based on Monte Carlo, tree and partial\ndifferential equation methods. We present an alternative approach that has\nbecome popular under the name de-Americanization in the financial industry. The\nmethod is easy to implement and enjoys fast run-times. Since it is based on ad\nhoc simplifications, however, theoretical results guaranteeing reliability are\nnot available. To quantify the resulting methodological risk, we empirically\ntest the performance of the de-Americanization method for calibration. We\nclassify the scenarios in which de-Americanization performs very well. However,\nwe also identify the cases where de-Americanization oversimplifies and can\nresult in large errors.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1611.06181v1", "link": "http://arxiv.org/abs/1611.06181v1"}, {"title": "A procedure for loss-optimising default definitions across simulated\n  credit risk scenarios", "summary": "A new procedure is presented for the objective comparison and evaluation of\ndefault definitions. This allows the lender to find a default threshold at\nwhich the financial loss of a loan portfolio is minimised, in accordance with\nBasel II. Alternative delinquency measures, other than simply measuring\npayments in arrears, can also be evaluated using this optimisation procedure.\nFurthermore, a simulation study is performed in testing the procedure from\n`first principles' across a wide range of credit risk scenarios. Specifically,\nthree probabilistic techniques are used to generate cash flows, while the\nparameters of each are varied, as part of the simulation study. The results\nshow that loss minima can exist for a select range of credit risk profiles,\nwhich suggests that the loss optimisation of default thresholds can become a\nviable practice. The default decision is therefore framed anew as an\noptimisation problem in choosing a default threshold that is neither too early\nnor too late in loan life. These results also challenges current practices\nwherein default is pragmatically defined as `90 days past due', with little\nobjective evidence for its overall suitability or financial impact, at least\nbeyond flawed roll rate analyses or a regulator's decree.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1907.12615v1", "link": "http://arxiv.org/abs/1907.12615v1"}, {"title": "Political Openness and Armed Conflict: Evidence from Local Councils in\n  Colombia", "summary": "In this paper, I empirically investigate how the openness of political\ninstitutions to diverse representation can impact conflict-related violence. By\nexploiting plausibly exogenous variations in the number of councillors in\nColombian municipalities, I develop two sets of results. First, regression\ndiscontinuity estimates show that larger municipal councils have a considerably\ngreater number of political parties with at least one elected representative. I\ninterpret this result as evidence that larger municipal councils are more open\nto diverse political participation. The estimates also reveal that\nnon-traditional parties are the main beneficiaries of this greater political\nopenness. Second, regression discontinuity estimates show that political\nopenness substantially decreases conflict-related violence, namely the killing\nof civilian non-combatants. By exploiting plausibly exogenous variations in\nlocal election results, I show that the lower level of political violence stems\nfrom greater participation by parties with close links to armed groups. Using\ndata about the types of violence employed by these groups, and representation\nat higher levels of government, I argue that armed violence has decreased not\nbecause of power-sharing arrangements involving armed groups linked to the\nparties with more political representation, but rather because armed groups\nwith less political power and visibility are deterred from initiating certain\ntypes of violence.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.03712v1", "link": "http://arxiv.org/abs/1910.03712v1"}, {"title": "Phase-type Approximation of the Gerber-Shiu Function", "summary": "The Gerber-Shiu function provides a way of measuring the risk of an insurance\ncompany. It is given by the expected value of a function that depends on the\nruin time, the deficit at ruin, and the surplus prior to ruin. Its computation\nrequires the evaluation of the overshoot/undershoot distributions of the\nsurplus process at ruin. In this paper, we use the recent developments of the\nfluctuation theory and approximate it in a closed form by fitting the\nunderlying process by phase-type Levy processes. A sequence of numerical\nresults are given.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1701.02798v1", "link": "http://arxiv.org/abs/1701.02798v1"}, {"title": "Mean Field Limit of a Behavioral Financial Market Model", "summary": "In the past decade there has been a growing interest in agent-based\neconophysical financial market models. The goal of these models is to gain\nfurther insights into stylized facts of financial data. We derive the mean\nfield limit of the econophysical model by Cross, Grinfeld, Lamba and Seaman\n(Physica A, 354) and show that the kinetic limit is a good approximation of the\noriginal model. Our kinetic model is able to replicate some of the most\nprominent stylized facts, namely fat-tails of asset returns, uncorrelated stock\nprice returns and volatility clustering. Interestingly, psychological\nmisperceptions of investors can be accounted to be the origin of the appearance\nof stylized facts. The mesoscopic model allows us to study the model\nanalytically. We derive steady state solutions and entropy bounds of the\ndeterministic skeleton. These first analytical results already guide us to\nexplanations for the complex dynamics of the model.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1711.02573v1", "link": "http://dx.doi.org/10.1016/j.physa.2018.03.079"}, {"title": "Resilience of Volatility", "summary": "The problem of non-stationarity in financial markets is discussed and related\nto the dynamic nature of price volatility. A new measure is proposed for\nestimation of the current asset volatility. A simple and illustrative\nexplanation is suggested of the emergence of significant serial\nautocorrelations in volatility and squared returns. It is shown that when\nnon-stationarity is eliminated, the autocorrelations substantially reduce and\nbecome statistically insignificant. The causes of non-Gaussian nature of the\nprobability of returns distribution are considered. For both stock and currency\nmarkets data samples, it is shown that removing the non-stationary component\nsubstantially reduces the kurtosis of distribution, bringing it closer to the\nGaussian one. A statistical criterion is proposed for controlling the degree of\nsmoothing of the empirical values of volatility. The hypothesis of smooth,\nnon-stochastic nature of volatility is put forward, and possible causes of\nvolatility shifts are discussed.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/0911.5048v1", "link": "http://arxiv.org/abs/0911.5048v1"}, {"title": "Rough fractional diffusions as scaling limits of nearly unstable heavy\n  tailed Hawkes processes", "summary": "We investigate the asymptotic behavior as time goes to infinity of Hawkes\nprocesses whose regression kernel has $L^1$ norm close to one and power law\ntail of the form $x^{-(1+\\alpha)}$, with $\\alpha\\in(0,1)$. We in particular\nprove that when $\\alpha\\in(1/2,1)$, after suitable rescaling, their law\nconverges to that of a kind of integrated fractional Cox-Ingersoll-Ross\nprocess, with associated Hurst parameter $H=\\alpha-1/2$. This result is in\ncontrast to the case of a regression kernel with light tail, where a classical\nBrownian CIR process is obtained at the limit. Interestingly, it shows that\npersistence properties in the point process can lead to an irregular behavior\nof the limiting process. This theoretical result enables us to give an\nagent-based foundation to some recent findings about the rough nature of\nvolatility in financial markets.", "category": ["math.PR", "q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1504.03100v1", "link": "http://arxiv.org/abs/1504.03100v1"}, {"title": "Spurious trend switching phenomena in financial markets", "summary": "The observation of power laws in the time to extrema of volatility, volume\nand intertrade times, from milliseconds to years, are shown to result\nstraightforwardly from the selection of biased statistical subsets of\nrealizations in otherwise featureless processes such as random walks. The bias\nstems from the selection of price peaks that imposes a condition on the\nstatistics of price change and of trade volumes that skew their distributions.\nFor the intertrade times, the extrema and power laws results from the format of\ntransaction data.", "category": ["q-fin.ST", "q-fin.GN"], "id": "http://arxiv.org/abs/1112.3868v1", "link": "http://dx.doi.org/10.1140/epjb/e2012-21060-1"}, {"title": "Rank-optimal weighting or \"How to be best in the OECD Better Life\n  Index?\"", "summary": "We present a method of rank-optimal weighting which can be used to explore\nthe best possible position of a subject in a ranking based on a composite\nindicator by means of a mathematical optimization problem. As an example, we\nexplore the dataset of the OECD Better Life Index and compute for each country\na weight vector which brings it as far up in the ranking as possible with the\ngreatest advance of the immediate rivals. The method is able to answer the\nquestion \"What is the best possible rank a country can achieve with a given set\nof weighted indicators?\" Typically, weights in composite indicators are\njustified normatively and not empirically. Our approach helps to give bounds on\nwhat is achievable by such normative judgments from a purely output-oriented\nand strongly competitive perspective. The method can serve as a basis for exact\nbounds in sensitivity analysis focused on ranking positions.\n  In the OECD Better Life Index data we find that 19 out the 36 countries in\nthe OECD Better Life Index 2014 can be brought to the top of the ranking by\nspecific weights. We give a table of weights for each country which brings it\nto its highest possible position. Many countries achieve their best rank by\nfocusing on their strong dimensions and setting the weights of many others to\nzero. Although setting dimensions to zero is possible in the OECD's online\ntool, this contradicts the idea of better life being multidimensional in\nessence. We discuss modifications of the optimization problem which could take\nthis into account, e.g. by allowing only a minimal weight of one.\n  Methods to find rank-optimal weights can be useful for various\nmultidimensional datasets like the ones used to rank universities or employers.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1608.04556v1", "link": "http://dx.doi.org/10.1007/s11205-016-1416-0"}, {"title": "Mean-Variance and Expected Utility: The Borch Paradox", "summary": "The model of rational decision-making in most of economics and statistics is\nexpected utility theory (EU) axiomatised by von Neumann and Morgenstern, Savage\nand others. This is less the case, however, in financial economics and\nmathematical finance, where investment decisions are commonly based on the\nmethods of mean-variance (MV) introduced in the 1950s by Markowitz. Under the\nMV framework, each available investment opportunity (\"asset\") or portfolio is\nrepresented in just two dimensions by the ex ante mean and standard deviation\n$(\\mu,\\sigma)$ of the financial return anticipated from that investment.\nUtility adherents consider that in general MV methods are logically incoherent.\nMost famously, Norwegian insurance theorist Borch presented a proof suggesting\nthat two-dimensional MV indifference curves cannot represent the preferences of\na rational investor (he claimed that MV indifference curves \"do not exist\").\nThis is known as Borch's paradox and gave rise to an important but generally\nlittle-known philosophical literature relating MV to EU. We examine the main\nearly contributions to this literature, focussing on Borch's logic and the\narguments by which it has been set aside.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1306.2728v1", "link": "http://dx.doi.org/10.1214/12-STS408"}, {"title": "Against the Norm: Modeling Daily Stock Returns with the Laplace\n  Distribution", "summary": "Modeling stock returns is not a new task for mathematicians, investors, and\nportfolio managers, but it remains a difficult objective due to the ebb and\nflow of stock markets. One common solution is to approximate the distribution\nof stock returns with a normal distribution. However, normal distributions\nplace infinitesimal probabilities on extreme outliers, but these outliers are\nof particular importance in the practice of investing. In this paper, we\ninvestigate the normality of the distribution of daily returns of major stock\nmarket indices. We find that the normal distribution is not a good model for\nstock returns, even over several years' worth of data. Moreover, we propose\nusing the Laplace distribution as a model for daily stock returns.", "category": ["q-fin.MF", "q-fin.ST"], "id": "http://arxiv.org/abs/1906.10325v1", "link": "http://arxiv.org/abs/1906.10325v1"}, {"title": "From the decompositions of a stopping time to risk premium\n  decompositions", "summary": "We build a general model for pricing defaultable claims. In addition to the\nusual absence of arbitrage assumption, we assume that one defaultable asset (at\nleast) looses value when the default occurs. We prove that under this\nassumption, in some standard market filtrations, default times are totally\ninaccessible stopping times; we therefore proceed to a systematic construction\nof default times with particular emphasis on totally inaccessible stopping\ntimes. Surprisingly, this abstract mathematical construction, reveals a very\nspecific and useful way in which default models can be built, using both market\nfactors and idiosyncratic factors. We then provide all the relevant\ncharacteristics of a default time (i.e. the Az\\'ema supermartingale and its\nDoob-Meyer decomposition) given the information about these factors. We also\nprovide explicit formulas for the prices of defaultable claims and analyze the\nrisk premiums that form in the market in anticipation of losses which occur at\nthe default event. The usual reduced-form framework is extended in order to\ninclude possible economic shocks, in particular jumps of the recovery process\nat the default time. This formulas are not classic and we point out that the\nknowledge of the default compensator or the intensity process is not anymore a\nsufficient quantity for finding explicit prices, but we need indeed the Az\\'ema\nsupermartingale and its Doob-Meyer decomposition.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0912.4312v2", "link": "http://arxiv.org/abs/0912.4312v2"}, {"title": "Information Cascades and Online Rating Games", "summary": "Through mathematical analysis and simulations, online ratings and their\nimpact on businesses are characterized through two parameters: an inherent and\nobjective restaurant quality factor, and the accuracy of customers' gut feeling\nabout a business. Within this model, it is found that online ratings are seldom\naccurate mainly because of the low or high accuracy in customers' gut feelings.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1508.00893v1", "link": "http://arxiv.org/abs/1508.00893v1"}, {"title": "Optimal Real-Time Bidding Strategies", "summary": "The ad-trading desks of media-buying agencies are increasingly relying on\ncomplex algorithms for purchasing advertising inventory. In particular,\nReal-Time Bidding (RTB) algorithms respond to many auctions -- usually Vickrey\nauctions -- throughout the day for buying ad-inventory with the aim of\nmaximizing one or several key performance indicators (KPI). The optimization\nproblems faced by companies building bidding strategies are new and interesting\nfor the community of applied mathematicians. In this article, we introduce a\nstochastic optimal control model that addresses the question of the optimal\nbidding strategy in various realistic contexts: the maximization of the\ninventory bought with a given amount of cash in the framework of audience\nstrategies, the maximization of the number of conversions/acquisitions with a\ngiven amount of cash, etc. In our model, the sequence of auctions is modeled by\na Poisson process and the \\textit{price to beat} for each auction is modeled by\na random variable following almost any probability distribution. We show that\nthe optimal bids are characterized by a Hamilton-Jacobi-Bellman equation, and\nthat almost-closed form solutions can be found by using a fluid limit.\nNumerical examples are also carried out.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1511.08409v2", "link": "http://arxiv.org/abs/1511.08409v2"}, {"title": "Yet on statistical properties of traded volume: correlation and mutual\n  information at different value magnitudes", "summary": "In this article we analyse linear correlation and non-linear dependence of\ntraded volume, $v$, of the 30 constituents of Dow Jones Industrial Average at\ndifferent value scales. Specifically, we have raised $v$ to some real value\n$\\alpha $ or $\\beta $, which introduces a bias for small ($ \\alpha, \\beta <0$)\nor large ($\\alpha, \\beta >1$) values. Our results show that small values of $v$\nare regularly \\emph{anti-correlated} with values at other scales of traded\nvolume. This is consistent with the high liquidity of the 30 equities analysed\nand the asymmetric form of the multi-fractal spectrum for traded volume which\nhas supported the dynamical scenario presented by us.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0702185v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.04.082"}, {"title": "An application to credit risk of a hybrid Monte Carlo-Optimal\n  quantization method", "summary": "In this paper we use a hybrid Monte Carlo-Optimal quantization method to\napproximate the conditional survival probabilities of a firm, given a\nstructural model for its credit defaul, under partial information. We consider\nthe case when the firm's value is a non-observable stochastic process $(V_t)_{t\n\\geq 0}$ and inverstors in the market have access to a process $(S_t)_{t \\geq\n0}$, whose value at each time t is related to $(V_s, s \\leq t)$. We are\ninterested in the computation of the conditional survival probabilities of the\nfirm given the \"investor information\". As a application, we analyse the shape\nof the credit spread curve for zero coupon bonds in two examples.", "category": ["q-fin.CP", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/0907.0645v1", "link": "http://arxiv.org/abs/0907.0645v1"}, {"title": "Gini estimation under infinite variance", "summary": "We study the problems related to the estimation of the Gini index in presence\nof a fat-tailed data generating process, i.e. one in the stable distribution\nclass with finite mean but infinite variance (i.e. with tail index\n$\\alpha\\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be\nreliably estimated using conventional nonparametric methods, because of a\ndownward bias that emerges under fat tails. This has important implications for\nthe ongoing discussion about economic inequality.\n  We start by discussing how the nonparametric estimator of the Gini index\nundergoes a phase transition in the symmetry structure of its asymptotic\ndistribution, as the data distribution shifts from the domain of attraction of\na light-tailed distribution to that of a fat-tailed one, especially in the case\nof infinite variance. We also show how the nonparametric Gini bias increases\nwith lower values of $\\alpha$. We then prove that maximum likelihood estimation\noutperforms nonparametric methods, requiring a much smaller sample size to\nreach efficiency.\n  Finally, for fat-tailed data, we provide a simple correction mechanism to the\nsmall sample bias of the nonparametric estimator based on the distance between\nthe mode and the mean of its asymptotic distribution.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1707.01370v4", "link": "http://dx.doi.org/10.1016/j.physa.2018.02.102"}, {"title": "Comparing Asset Pricing Models: Distance-based Metrics and Bayesian\n  Interpretations", "summary": "In light of the power problems of statistical tests and undisciplined use of\nalpha-based statistics to compare models, this paper proposes a unified set of\ndistance-based performance metrics, derived as the square root of the sum of\nsquared alphas and squared standard errors. The Bayesian investor views model\nperformance as the shortest distance between his dogmatic belief (model-implied\ndistribution) and complete skepticism (data-based distribution) in the model,\nand favors models that produce low dispersion of alphas with high explanatory\npower. In this view, the momentum factor is a crucial addition to the\nfive-factor model of Fama and French (2015), alleviating his prior concern of\nmodel mispricing by -8% to 8% per annum. The distance metrics complement the\nfrequentist p-values with a diagnostic tool to guard against bad models.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1803.01389v1", "link": "http://arxiv.org/abs/1803.01389v1"}, {"title": "David vs Goliath (You against the Markets), A Dynamic Programming\n  Approach to Separate the Impact and Timing of Trading Costs", "summary": "A trader's conundrum is whether (and how much) to trade during a given\ninterval or wait for the next interval when the price momentum is more\nfavorable to his direction of trading. We develop a fundamentally different\nstochastic dynamic programming model of trading costs based on the Bellman\nprinciple of optimality. Built on a strong theoretical foundation, this model\ncan provide insights to market participants by splitting the overall move of\nthe security price during the duration of an order into the Market Impact\n(price move caused by their actions) and Market Timing (price move caused by\neveryone else) components. Plugging different distributions of prices and\nvolumes into this framework can help traders decide when to bear higher Market\nImpact by trading more in the hope of offsetting the cost of trading at a\nhigher price later. We derive formulations of this model under different laws\nof motion of the security prices. We start with a benchmark scenario and extend\nthis to include multiple sources of uncertainty, liquidity constraints due to\nvolume curve shifts and relate trading costs to the spread.\n  We develop a numerical framework that can be used to obtain optimal\nexecutions under any law of motion of prices and demonstrate the tremendous\npractical applicability of our theoretical methodology including the powerful\nnumerical techniques to implement them.\n  This decomposition of trading costs into Market Impact and Market Timing\nallows us to deduce the zero sum game nature of trading costs. It holds\nnumerous lessons for dealing with complex systems, especially in the social\nsciences, wherein reducing the complexity by splitting the many sources of\nuncertainty can lead to better insights in the decision process.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1603.00984v6", "link": "http://arxiv.org/abs/1603.00984v6"}, {"title": "South African Riots: Repercussion of the Global Food Crisis and US\n  Drought", "summary": "High and volatile global food prices have led to food riots and played a\ncritical role in triggering the Arab Spring revolutions in recent years. The\nsevere drought in the US in the summer of 2012 led to a new increase in food\nprices. Through the fall, they remained at a threshold above which the riots\nand revolutions had predominantly occurred. Global prices at this level create\nconditions where an exacerbating local circumstance can trigger unrest. Global\ncorn (maize) prices reached new highs, and countries that depend mostly on\nmaize are more likely to experience high local food prices and associated\npressures toward social unrest. Here we analyze the conditions in South Africa,\nwhich is a heavily maize-dependent country. Coinciding with increased consumer\nfood indices this summer, massive labor strikes in mining and agriculture have\nled to the greatest single incident of social violence since the fall of\napartheid in 1994. Worker demands for dramatic pay increases reflect that their\nwages have not kept up with drastic increases in the prices of necessities,\nespecially food. Without attention to the global food price situation, more\nincidents of food-based social instability are likely to arise. Other countries\nthat have manifested food-related protests and riots in 2012 include Haiti and\nArgentina. Moreover, these cases of unrest are just the most visible symptom of\nwidespread suffering of poor populations worldwide due to elevated food prices.\nPolicy decisions that would directly impact food prices are decreasing the\nconversion of maize to ethanol in the US, and reimposing regulations on\ncommodity futures markets to prevent excessive speculation, which we have shown\ncauses bubbles and crashes in these markets. Absent such policy actions,\ngovernments and companies should track and mitigate the impact of high and\nvolatile food prices on citizens and employees.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1307.5268v1", "link": "http://arxiv.org/abs/1307.5268v1"}, {"title": "Long-run dynamics of the U.S. patent classification system", "summary": "Almost by definition, radical innovations create a need to revise existing\nclassification systems. In this paper, we argue that classification system\nchanges and patent reclassification are common and reveal interesting\ninformation about technological evolution. To support our argument, we present\nthree sets of findings regarding classification volatility in the U.S. patent\nclassification system. First, we study the evolution of the number of distinct\nclasses. Reconstructed time series based on the current classification scheme\nare very different from historical data. This suggests that using the current\nclassification to analyze the past produces a distorted view of the evolution\nof the system. Second, we study the relative sizes of classes. The size\ndistribution is exponential so classes are of quite different sizes, but the\nlargest classes are not necessarily the oldest. To explain this pattern with a\nsimple stochastic growth model, we introduce the assumption that classes have a\nregular chance to be split. Third, we study reclassification. The share of\npatents that are in a different class now than they were at birth can be quite\nhigh. Reclassification mostly occurs across classes belonging to the same\n1-digit NBER category, but not always. We also document that reclassified\npatents tend to be more cited than non-reclassified ones, even after\ncontrolling for grant year and class of origin.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1703.02104v2", "link": "http://dx.doi.org/10.1007/s00191-018-0603-3"}, {"title": "Self-organized criticality auction model for selling products in real\n  time", "summary": "Consumer markets are quickly growing, creating the need to design new sales\nmechanisms. Here we introduce a new auction model for selling products in real\ntime and without production limitations. Interested buyers continuously offer\nbids and if the price is 'right', the bid is accepted. The model exhibits\nself-organized criticality; it presents a critical price from which a bid is\naccepted with probability one, and avalanches of sales above this value are\nobserved. We also discuss how to implement the model and consider the impact of\ninformation sharing on total income, as well as the impact of setting a base\nprice.", "category": ["q-fin.TR", "q-fin.GN"], "id": "http://arxiv.org/abs/1805.09763v2", "link": "http://arxiv.org/abs/1805.09763v2"}, {"title": "Generalized stochastic differential utility and preference for\n  information", "summary": "This paper develops, in a Brownian information setting, an approach for\nanalyzing the preference for information, a question that motivates the\nstochastic differential utility (SDU) due to Duffie and Epstein [Econometrica\n60 (1992) 353-394]. For a class of backward stochastic differential equations\n(BSDEs) including the generalized SDU [Lazrak and Quenez Math. Oper. Res. 28\n(2003) 154-180], we formulate the information neutrality property as an\ninvariance principle when the filtration is coarser (or finer) and characterize\nit. We also provide concrete examples of heterogeneity in information that\nillustrate explicitly the nonneutrality property for some GSDUs. Our results\nsuggest that, within the GSDUs class of intertemporal utilities, risk aversion\nor ambiguity aversion are inflexibly linked to the preference for information.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0503579v1", "link": "http://dx.doi.org/10.1214/105051604000000756"}, {"title": "Ordered Kripke Model, Permissibility, and Convergence of Probabilistic\n  Kripke Model", "summary": "We define a modification of the standard Kripke model, called the ordered\nKripke model, by introducing a linear order on the set of accessible states of\neach state. We first show this model can be used to describe the lexicographic\nbelief hierarchy in epistemic game theory, and perfect rationalizability can be\ncharacterized within this model. Then we show that each ordered Kripke model is\nthe limit of a sequence of standard probabilistic Kripke models with a modified\n(common) belief operator, in the senses of structure and the\n(epsilon-)permissibilities characterized within them.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.08767v1", "link": "http://arxiv.org/abs/1801.08767v1"}, {"title": "Nonparametric Regression with Selectively Missing Covariates", "summary": "We consider the problem of regressions with selectively observed covariates\nin a nonparametric framework. Our approach relies on instrumental variables\nthat explain variation in the latent covariates but have no direct effect on\nselection. The regression function of interest is shown to be a weighted\nversion of observed conditional expectation where the weighting function is a\nfraction of selection probabilities. Nonparametric identification of the\nfractional probability weight (FPW) function is achieved via a partial\ncompleteness assumption. We provide primitive functional form assumptions for\npartial completeness to hold. The identification result is constructive for the\nFPW series estimator. We derive the rate of convergence and also the pointwise\nasymptotic distribution. In both cases, the asymptotic performance of the FPW\nseries estimator does not suffer from the inverse problem which derives from\nthe nonparametric instrumental variable approach. In a Monte Carlo study, we\nanalyze the finite sample properties of our estimator and we demonstrate the\nusefulness of our method in analyses based on survey data. In the empirical\napplication, we focus on two different applications. We estimate the\nassociation between income and health using linked data from the SHARE survey\ndata and administrative pension information and use pension entitlements as an\ninstrument. In the second application we revisit the question how income\naffects the demand for housing based on data from the Socio-Economic Panel\nStudy. In this application we use regional income information on the\nresidential block level as an instrument. In both applications we show that\nincome is selectively missing and we demonstrate that standard methods that do\nnot account for the nonrandom selection process lead to significantly biased\nestimates for individuals with low income.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.00411v2", "link": "http://arxiv.org/abs/1810.00411v2"}, {"title": "Market impact as anticipation of the order flow imbalance", "summary": "In this paper, we assume that the permanent market impact of metaorders is\nlinear and that the price is a martingale. Those two hypotheses enable us to\nderive the evolution of the price from the dynamics of the flow of market\norders. For example, if the market order flow is assumed to follow a nearly\nunstable Hawkes process, we retrieve the apparent long memory of the flow\ntogether with a power law impact function which is consistent with the\ncelebrated square root law. We also link the long memory exponent of the sign\nof market orders with the impact function exponent. One of the originalities of\nour approach is that our results are derived without assuming that market\nparticipants are able to detect the beginning of metaorders.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1402.1288v1", "link": "http://arxiv.org/abs/1402.1288v1"}, {"title": "Spurious seasonality detection: a non-parametric test proposal", "summary": "This paper offers a general and comprehensive definition of the\nday-of-the-week effect. Using symbolic dynamics, we develop a unique test based\non ordinal patterns in order to detect it. This test uncovers the fact that the\nso-called \"day-of-the-week\" effect is partly an artifact of the hidden\ncorrelation structure of the data. We present simulations based on artificial\ntime series as well. Whereas time series generated with long memory are prone\nto exhibit daily seasonality, pure white noise signals exhibit no pattern\npreference. Since ours is a non parametric test, it requires no assumptions\nabout the distribution of returns so that it could be a practical alternative\nto conventional econometric tests. We made also an exhaustive application of\nthe here proposed technique to 83 stock indices around the world. Finally, the\npaper highlights the relevance of symbolic analysis in economic time series\nstudies.", "category": ["q-fin.ST", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1801.07941v1", "link": "http://dx.doi.org/10.3390/econometrics6010003"}, {"title": "Clarifications to Questions and Criticisms on the\n  Johansen-Ledoit-Sornette Bubble Model", "summary": "The Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles with\nfinite-time singular crash hazard rates has been developed to describe the\ndynamics of financial bubbles and crashes. It has been applied successfully to\na large variety of financial bubbles in many different markets. Having been\ndeveloped for more than one decade, the JLS model has been studied, analyzed,\nused and criticized by several researchers. Much of this discussion is helpful\nfor advancing the research. However, several serious misconceptions seem to be\npresent within this collective conversation both on theoretical and empirical\naspects. Several of these problems appear to stem from the fast evolution of\nthe literature on the JLS model and related works. In the hope of removing\npossible misunderstanding and of catalyzing useful future developments, we\nsummarize these common questions and criticisms concerning the JLS model and\noffer a synthesis of the existing state-of-the-art and best-practice advices.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1107.3171v3", "link": "http://dx.doi.org/10.1016/j.physa.2013.05.011"}, {"title": "Efficient hedging under ambiguity in continuous time", "summary": "It is well known that the minimal superhedging price of a contingent claim is\ntoo high for practical use. In a continuous-time model uncertainty framework,\nwe consider a relaxed hedging criterion based on acceptable shortfall risks.\nCombining existing aggregation and convex dual representation theorems, we\nderive duality results for the minimal price on the set of upper semicontinuous\ndiscounted claims.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1812.10876v2", "link": "http://arxiv.org/abs/1812.10876v2"}, {"title": "Driving Force in Investment", "summary": "We study investment strategy in different models of financial markets, where\nthe investors cannot reach a perfect knowledge about available assets. The\ninvestor spends a certain effort to get information; this allows him to better\nchoose the investment strategy, and puts a selective pressure upon assets. The\nbest strategy is then a compromise between diversification and effort to get\ninformation.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/cond-mat/9912330v1", "link": "http://arxiv.org/abs/cond-mat/9912330v1"}, {"title": "Optimal Investment with Correlated Stochastic Volatility Factors", "summary": "The problem of portfolio allocation in the context of stocks evolving in\nrandom environments, that is with volatility and returns depending on random\nfactors, has attracted a lot of attention. The problem of maximizing a power\nutility at a terminal time with only one random factor can be linearized thanks\nto a classical distortion transformation. In the present paper, we address the\nproblem with several factors using a perturbation technique around the case\nwhere these factors are perfectly correlated reducing the problem to the case\nwith a single factor. We illustrate our result with a particular model for\nwhich we have explicit formulas. A rigorous accuracy result is also derived\nusing sub- and super-solutions of the HJB equation involved. In order to keep\nthe notations as explicit as possible, we treat the case with one stock and two\nfactors and we describe an extension to the case with two stocks and two\nfactors.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1908.07626v1", "link": "http://arxiv.org/abs/1908.07626v1"}, {"title": "Feynman-Kac formula for L\u00e9vy processes with discontinuous killing rate", "summary": "The challenge to fruitfully merge state-of-the-art techniques from\nmathematical finance and numerical analysis has inspired researchers to develop\nfast deterministic option pricing methods. As a result, highly efficient\nalgorithms to compute option prices in L\\'evy models by solving partial integro\ndifferential equations have been developed. In order to provide a solid\nmathematical foundation for these methods, we derive a Feynman-Kac\nrepresentation of variational solutions to partial integro differential\nequations that characterize conditional expectations of functionals of killed\ntime-inhomogeneous L\\'evy processes. We allow for a wide range of underlying\nstochastic processes, comprising processes with Brownian part, and a broad\nclass of pure jump processes such as generalized hyperbolic, multivariate\nnormal inverse Gaussian, tempered stable, and $\\alpha$-semi stable L\\'evy\nprocesses. By virtue of our mild regularity assumptions as to the killing rate\nand the initial condition of the partial differential equation, our results\nprovide a rigorous basis for numerous applications, not only in financial\nmathematics but also in probability theory and relativistic quantum mechanics.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1502.07531v3", "link": "http://arxiv.org/abs/1502.07531v3"}, {"title": "U.S. Core Inflation: A Wavelet Analysis", "summary": "This paper proposes the use of wavelet methods to estimate U.S. core\ninflation. It explains wavelet methods and suggests they are ideally suited to\nthis task. Comparisons are made with traditional CPI-based and regression-based\nmeasures for their performance in following trend inflation and predicting\nfuture inflation. Results suggest that wavelet-based measures perform better,\nand sometimes much better, than the traditional approaches. These results\nsuggest that wavelet methods are a promising avenue for future research on core\ninflation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1103.5659v1", "link": "http://arxiv.org/abs/1103.5659v1"}, {"title": "Diversification and Endogenous Financial Networks", "summary": "We test the hypothesis that interconnections across financial institutions\ncan be explained by a diversification motive. This idea stems from the\nempirical evidence of the existence of long-term exposures that cannot be\nexplained by a liquidity motive (maturity or currency mismatch). We model\nendogenous interconnections of heterogenous financial institutions facing\nregulatory constraints using a maximization of their expected utility. Both\ntheoretical and simulation-based results are compared to a stylized genuine\nfinancial network. The diversification motive appears to plausibly explain\ninterconnections among key players. Using our model, the impact of regulation\non interconnections between banks -currently discussed at the Basel Committee\non Banking Supervision- is analyzed.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1408.4618v3", "link": "http://arxiv.org/abs/1408.4618v3"}, {"title": "The effect of stock market indexing on corporate tax avoidance", "summary": "Membership in the Russell 1000 and 2000 Indices is based on a ranking of\nmarket capitalization in May. Each index is separately value weighted such that\nfirms just inside the Russell 2000 are comparable in size to firms just outside\n(i.e. at the bottom of the Russell 1000) but have much higher index weights.\nThese features allow for the the annual reconstitution of these indices to be\nused as part of a regression discontinuity design to identify the effect of\nstock market indexing. Using this design, I investigate whether stock market\nindexing affects corporate tax avoidance. I find no evidence that firms just\ninside the Russell 2000 have significantly different effective tax rates than\nfirms just outside.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1509.00136v1", "link": "http://arxiv.org/abs/1509.00136v1"}, {"title": "Using real-time cluster configurations of streaming asynchronous\n  features as online state descriptors in financial markets", "summary": "We present a scheme for online, unsupervised state discovery and detection\nfrom streaming, multi-featured, asynchronous data in high-frequency financial\nmarkets. Online feature correlations are computed using an unbiased, lossless\nFourier estimator. A high-speed maximum likelihood clustering algorithm is then\nused to find the feature cluster configuration which best explains the\nstructure in the correlation matrix. We conjecture that this feature\nconfiguration is a candidate descriptor for the temporal state of the system.\nUsing a simple cluster configuration similarity metric, we are able to\nenumerate the state space based on prevailing feature configurations. The\nproposed state representation removes the need for human-driven data\npre-processing for state attribute specification, allowing a learning agent to\nfind structure in streaming data, discern changes in the system, enumerate its\nperceived state space and learn suitable action-selection policies.", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1603.06805v2", "link": "http://arxiv.org/abs/1603.06805v2"}, {"title": "Fundamental Limits of Testing the Independence of Irrelevant\n  Alternatives in Discrete Choice", "summary": "The Multinomial Logit (MNL) model and the axiom it satisfies, the\nIndependence of Irrelevant Alternatives (IIA), are together the most widely\nused tools of discrete choice. The MNL model serves as the workhorse model for\na variety of fields, but is also widely criticized, with a large body of\nexperimental literature claiming to document real-world settings where IIA\nfails to hold. Statistical tests of IIA as a modelling assumption have been the\nsubject of many practical tests focusing on specific deviations from IIA over\nthe past several decades, but the formal size properties of hypothesis testing\nIIA are still not well understood. In this work we replace some of the\nambiguity in this literature with rigorous pessimism, demonstrating that any\ngeneral test for IIA with low worst-case error would require a number of\nsamples exponential in the number of alternatives of the choice problem. A\nmajor benefit of our analysis over previous work is that it lies entirely in\nthe finite-sample domain, a feature crucial to understanding the behavior of\ntests in the common data-poor settings of discrete choice. Our lower bounds are\nstructure-dependent, and as a potential cause for optimism, we find that if one\nrestricts the test of IIA to violations that can occur in a specific collection\nof choice sets (e.g., pairs), one obtains structure-dependent lower bounds that\nare much less pessimistic. Our analysis of this testing problem is unorthodox\nin being highly combinatorial, counting Eulerian orientations of cycle\ndecompositions of a particular bipartite graph constructed from a data set of\nchoices. By identifying fundamental relationships between the comparison\nstructure of a given testing problem and its sample efficiency, we hope these\nrelationships will help lay the groundwork for a rigorous rethinking of the IIA\ntesting problem as well as other testing problems in discrete choice.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2001.07042v1", "link": "http://arxiv.org/abs/2001.07042v1"}, {"title": "Solving Quadratic Multi-Leader-Follower Games by Smoothing the\n  Follower's Best Response", "summary": "We derive Nash equilibria for a class of quadratic multi-leader-follower\ngames using the nonsmooth best response function. To overcome the challenge of\nnonsmoothness, we pursue a smoothing approach resulting in a reformulation as a\nsmooth Nash equilibrium problem. The existence and uniqueness of solutions are\nproven for all smoothing parameters. Accumulation points of Nash equilibria\nexist for a decreasing sequence of these smoothing parameters and we show that\nthese candidates fulfill the conditions of s-stationarity and are Nash\nequilibria to the multi-leader-follower game. Finally, we propose an update on\nthe leader variables for efficient computation and numerically compare\nnonsmooth Newton and subgradient methods.", "category": [], "id": "http://arxiv.org/abs/1808.07941v4", "link": "http://arxiv.org/abs/1808.07941v4"}, {"title": "L'effet de levier de tr\u00e9sorerie", "summary": "The effect of leverage on liquidity is a tool for analysing the level of\nliquidity for a given production process. It measures the sensitivity of the\nlevel of liquidity that results from changes in the volume of production and\nunit operating margin. A commercial activity is liquid at the moment when all\ncosts are covered by revenues. However, not all of the cash flows from\nproduction influence liquidity levels. The estimated costs do not directly\ninfluence the level of liquidity. Therefore, two indicators are to be taken\ninto consideration: the elasticity of ongoing liquidity - fixed costs include\nestimated costs, and, the elasticity of immediate liquidity - fixed costs only\ninclude costs that are payable. The coefficients of leverage of ongoing\nliquidity and of leverage of immediate liquidity in relation to the operating\nmargin have a behaviour that is identical to that calculated in relation to\nproduction. If the productive capacity remains unchanged, the regulation of the\nchange in elasticity of the costs and of its influence on the unitary operating\nmargin is the sole parameter available to the entrepreneur to maintain the\nliquidity of the company at the desired level. But, if the productive capacity\nis variable, the entrepreneur can use the volume of sales to control liquidity\nbut then the transformation of the production process must be analysed so as to\nadjust the relevant elements to retain in the operating structure the degree of\nliquidity wished for.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1004.0682v1", "link": "http://arxiv.org/abs/1004.0682v1"}, {"title": "A Thermodynamic Picture of Financial Market and Model Risk", "summary": "By treating the financial market as a thermodynamic system, we establish a\none-to-one correspondence between thermodynamic variables and economic\nquantities. Measured by the expected loss under the worst-case scenario,\nfinancial risk caused by model uncertainty is regarded as a result of the\ninteraction between financial market and external information sources. This\nforms a thermodynamic picture in which a closed system interacts with an\nexternal reservoir, reaching its equilibrium at the worst-case scenario. The\nseverity of the worst-case scenario depends on the rate of heat dissipation,\ncaused by information sources reducing the entropy of the system. This\nthermodynamic picture leads to simple and natural derivation of the\ncharacterization rules of the worst-case risk, and gives its Lagrangian and\nHamiltonian forms. With its help financial practitioners may evaluate risks\nutilizing both equilibrium and non-equilibrium thermodynamics.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1904.00151v1", "link": "http://arxiv.org/abs/1904.00151v1"}, {"title": "Analyzing money distributions in `ideal gas' models of markets", "summary": "We analyze an ideal gas like models of a trading market. We propose a new fit\nfor the money distribution in the fixed or uniform saving market. For the\nmarketwith quenched random saving factors for its agents we show that the\nsteady state income ($m$) distribution $P(m)$ in the model has a power law tail\nwith Pareto index $\\nu$ exactly equal to unity, confirming the earlier\nnumerical studies on this model. We analyze the distribution of mutual money\ndifference and also develop a master equation for the time development of\n$P(m)$. Precise solutions are then obtained in some special cases.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0505047v1", "link": "http://arxiv.org/abs/physics/0505047v1"}, {"title": "An exact and explicit formula for pricing lookback options with regime\n  switching", "summary": "This paper investigates the pricing of European-style lookback options when\nthe price dynamics of the underlying risky asset are assumed to follow a\nMarkov-modulated Geo-metric Brownian motion; that is, the appreciation rate and\nthe volatility of the underlying risky asset depend on unobservable states of\nthe economy described by a continuous-time hidden Markov chain process. We\nderive an exact, explicit and closed-form solution for European-style lookback\noptions in a two-state regime switching model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1407.4864v1", "link": "http://arxiv.org/abs/1407.4864v1"}, {"title": "An instantaneous market volatility estimation", "summary": "Working on different aspects of algorithmic trading we empirically discovered\na new market invariant. It links together the volatility of the instrument with\nits traded volume, the average spread and the volume in the order book. The\ninvariant has been tested on different markets and different asset classes. In\nall cases we did not find significant violation of the invariant. The formula\nfor the invariant was used for the volatility estimation, which we called the\ninstantaneous volatility. Quantitative comparison showed that it reproduces\nrealised volatility better than one-day-ahead GARCH(1,1) prediction. Because of\nthe short-term prediction nature, the instantaneous volatility could be used by\nalgo developers, volatility traders and other market professionals.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1908.02847v2", "link": "http://arxiv.org/abs/1908.02847v2"}, {"title": "Expressive mechanisms for equitable rent division on a budget", "summary": "We study the incentive properties of envy-free mechanisms for the allocation\nof rooms and payments of rent among financially constrained roommates. Each\nagent reports her values for rooms, her housing earmark (soft budget), and an\nindex that reflects the difficulty the agent experiences from having to pay\nover this amount. Then an envy-free allocation for these reports is\nrecommended. The complete information non-cooperative outcomes of each of these\nmechanisms are exactly the envy-free allocations with respect to true\npreferences if and only if the admissible budget violation indices have a\nbound.", "category": [], "id": "http://arxiv.org/abs/1902.02935v3", "link": "http://arxiv.org/abs/1902.02935v3"}, {"title": "A confidence-based model for asset and derivative prices in the BitCoin\n  market", "summary": "We endorse the idea, suggested in recent literature, that BitCoin prices are\ninfluenced by sentiment and confidence about the underlying technology; as a\nconsequence, an excitement about the BitCoin system may propagate to BitCoin\nprices causing a Bubble effect, the presence of which is documented in several\npapers about the cryptocurrency. In this paper we develop a bivariate model in\ncontinuous time to describe the price dynamics of one BitCoin as well as the\nbehavior of a second factor affecting the price itself, which we name\nconfidence indicator. The two dynamics are possibly correlated and we also take\ninto account a delay between the confidence indicator and its delivered effect\non the BitCoin price. Statistical properties of the suggested model are\ninvestigated and its arbitrage-free property is shown. Further, based on\nrisk-neutral evaluation, a quasi-closed formula is derived for European style\nderivatives on the BitCoin. A short numerical application is finally provided.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1702.00215v1", "link": "http://dx.doi.org/10.1007/s10203-019-00262-x"}, {"title": "Statistical Signatures in Times of Panic: Markets as a Self-Organizing\n  System", "summary": "We study properties of the cross-sectional distribution of returns. A\nsignificant anti-correlation between dispersion and cross-sectional kurtosis is\nfound such that dispersion is high but kurtosis is low in panic times, and the\nopposite in normal times. The co-movement of stock returns also increases in\npanic times. We define a simple statistic $s$, the normalized sum of signs of\nreturns on a given day, to capture the degree of correlation in the system. $s$\ncan be seen as the order parameter of the system because if $s= 0$ there is no\ncorrelation (a disordered state), whereas for $s \\ne 0$ there is correlation\namong stocks (an ordered state). We make an analogy to non-equilibrium phase\ntransitions and hypothesize that financial markets undergo self-organization\nwhen the external volatility perception rises above some critical value.\nIndeed, the distribution of $s$ is unimodal in normal times, shifting to\nbimodal in times of panic. This is consistent with a second order phase\ntransition. Simulations of a joint stochastic process for stocks use a multi\ntimescale process in the temporal direction and an equation for the order\nparameter $s$ for the dynamics of the cross-sectional correlation. Numerical\nresults show good qualitative agreement with the stylized facts of real data,\nin both normal and panic times.", "category": ["q-fin.ST", "q-fin.GN"], "id": "http://arxiv.org/abs/0908.0111v2", "link": "http://arxiv.org/abs/0908.0111v2"}, {"title": "The art of fitting financial time series with Levy stable distributions", "summary": "This paper illustrates a procedure for fitting financial data with\n$\\alpha$-stable distributions. After using all the available methods to\nevaluate the distribution parameters, one can qualitatively select the best\nestimate and run some goodness-of-fit tests on this estimate, in order to\nquantitatively assess its quality. It turns out that, for the two investigated\ndata sets (MIB30 and DJIA from 2000 to present), an $\\alpha$-stable fit of\nlog-returns is reasonably good.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0608224v1", "link": "http://arxiv.org/abs/physics/0608224v1"}, {"title": "Direct and indirect transactions and requirements", "summary": "The indirect transactions between sectors of an economic system has been a\nlong-standing open problem. There have been numerous attempts to define and\nmathematically formulate this concept in various other scientific fields in\nliterature as well. The existing indirect effects formulations, however, can\nneither determine the direct and indirect transactions separately nor quantify\nthese transactions between two individual sectors of interest in an economic\nsystem. The novel concepts of the direct, indirect and total transactions\nbetween any two sectors are introduced, and the corresponding requirements\nmatrices are systematically formulated relative to both final demands and gross\noutputs, based on the system decomposition theory. It is demonstrated\ntheoretically and through illustrative examples that the proposed requirements\nmatrices accurately define and quantify the corresponding direct, indirect, and\ntotal interactions and relationships. The proposed requirements matrices for\nthe US economy using aggregated input-output tables for multiple years are then\npresented and briefly analyzed.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.11569v4", "link": "http://dx.doi.org/10.31219/osf.io/w2a4d"}, {"title": "Strong convergence rates for Markovian representations of fractional\n  Brownian motion", "summary": "Fractional Brownian motion can be represented as an integral over a family of\nOrnstein-Uhlenbeck processes. This representation naturally lends itself to\nnumerical discretizations, which are shown in this paper to have strong\nconvergence rates of arbitrarily high polynomial order. This explains the\npotential, but also some limitations of such representations as the basis of\nMonte Carlo schemes for fractional volatility models such as the rough Bergomi\nmodel.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1902.01471v2", "link": "http://arxiv.org/abs/1902.01471v2"}, {"title": "Optimal investment with bounded VaR for power utility functions", "summary": "We consider the optimal investment problem for Black-Scholes type financial\nmarket with bounded VaR measure on the whole investment interval $[0,T]$. The\nexplicit form for the optimal strategies is found.", "category": ["q-fin.PM", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1002.3681v2", "link": "http://arxiv.org/abs/1002.3681v2"}, {"title": "A biased view of a few possible components when reflecting on the\n  present decade financial and economic crisis", "summary": "Is the present economic and financial crisis similar to some previous one? It\nwould be so nice to prove that universality laws exist for predicting such rare\nevents under a minimum set of realistic hypotheses. First, I briefly recall\nwhether patterns, like business cycles, are indeed found, and can be modeled\nwithin a statistical physics, or econophysics, framework. I point to a\nsimulation model for describing such so called business cycles, under exo- and\nendo-genous conditions I discuss self-organized and provoked crashes and their\npredictions. I emphasize the role of an of- ten forgotten ingredient: the time\ndelay in the information flow. I wonder about the information content of\nfinancial data, its mis-interpretation and market manipulation.", "category": ["q-fin.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1412.0127v1", "link": "http://arxiv.org/abs/1412.0127v1"}, {"title": "The multiplex dependency structure of financial markets", "summary": "We propose here a multiplex network approach to investigate simultaneously\ndifferent types of dependency in complex data sets. In particular, we consider\nmultiplex networks made of four layers corresponding respectively to linear,\nnon-linear, tail, and partial correlations among a set of financial time\nseries. We construct the sparse graph on each layer using a standard network\nfiltering procedure, and we then analyse the structural properties of the\nobtained multiplex networks. The study of the time evolution of the multiplex\nconstructed from financial data uncovers important changes in intrinsically\nmultiplex properties of the network, and such changes are associated with\nperiods of financial stress. We observe that some features are unique to the\nmultiplex structure and would not be visible otherwise by the separate analysis\nof the single-layer networks corresponding to each dependency measure.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1606.04872v1", "link": "http://arxiv.org/abs/1606.04872v1"}, {"title": "Using Artificial Intelligence to Recapture Norms: Did #metoo change\n  gender norms in Sweden?", "summary": "Norms are challenging to define and measure, but this paper takes advantage\nof text data and the recent development in machine learning to create an\nencompassing measure of norms. An LSTM neural network is trained to detect\ngendered language. The network functions as a tool to create a measure on how\ngender norms changes in relation to the Metoo movement on Swedish Twitter. This\npaper shows that gender norms on average are less salient half a year after the\ndate of the first appearance of the hashtag #Metoo. Previous literature\nsuggests that gender norms change over generations, but the current result\nsuggests that norms can change in the short run.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.00690v1", "link": "http://arxiv.org/abs/1903.00690v1"}, {"title": "Diffusion and Aggregation in an Agent Based Model of Stock Market\n  Fluctuations", "summary": "We describe a new model to simulate the dynamic interactions between market\nprice and the decisions of two different kind of traders. They possess spatial\nmobility allowing to group together to form coalitions. Each coalition follows\na strategy chosen from a proportional voting ``dominated'' by a leader's\ndecision. The interplay of both kind of agents gives rise to complex price\ndynamics that is consistent with the main stylized facts of financial time\nseries.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0006463v1", "link": "http://dx.doi.org/10.1142/S0129183100000754"}, {"title": "Design of High-Frequency Trading Algorithm Based on Machine Learning", "summary": "Based on iterative optimization and activation function in deep learning, we\nproposed a new analytical framework of high-frequency trading information, that\nreduced structural loss in the assembly of Volume-synchronized probability of\nInformed Trading ($VPIN$), Generalized Autoregressive Conditional\nHeteroscedasticity (GARCH) and Support Vector Machine (SVM) to make full use of\nthe order book information. Amongst the return acquisition procedure in\nmarket-making transactions, uncovering the relationship between discrete\ndimensional data from the projection of high-dimensional time-series would\nsignificantly improve the model effect. $VPIN$ would prejudge market liquidity,\nand this effectiveness backtested with CSI300 futures return.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1912.10343v1", "link": "http://arxiv.org/abs/1912.10343v1"}, {"title": "High-order ADI scheme for option pricing in stochastic volatility models", "summary": "We propose a new high-order alternating direction implicit (ADI) finite\ndifference scheme for the solution of initial-boundary value problems of\nconvection-diffusion type with mixed derivatives and non-constant coefficients,\nas they arise from stochastic volatility models in option pricing. Our approach\ncombines different high-order spatial discretisations with Hundsdorfer and\nVerwer's ADI time-stepping method, to obtain an efficient method which is\nfourth-order accurate in space and second-order accurate in time. Numerical\nexperiments for the European put option pricing problem using Heston's\nstochastic volatility model confirm the high-order convergence.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1512.02529v1", "link": "http://dx.doi.org/10.1016/j.cam.2016.09.040"}, {"title": "Unexpected Default in an Information Based Model", "summary": "This paper provides sufficient conditions for the time of bankruptcy (of a\ncompany or a state) for being a totally inaccessible stopping time and provides\nthe explicit computation of its compensator in a framework where the flow of\nmarket information on the default is modelled explicitly with a Brownian bridge\nbetween 0 and 0 on a random time interval.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1611.02952v1", "link": "http://arxiv.org/abs/1611.02952v1"}, {"title": "Unit Root Testing with Slowly Varying Trends", "summary": "A unit root test is proposed for time series with a general nonlinear\ndeterministic trend component. It is shown that asymptotically the pooled OLS\nestimator of overlapping blocks filters out any trend component that satisfies\nsome Lipschitz condition. Under both fixed-$b$ and small-$b$ block asymptotics,\nthe limiting distribution of the t-statistic for the unit root hypothesis is\nderived. Nuisance parameter corrections provide heteroskedasticity-robust\ntests, and serial correlation is accounted for by pre-whitening. A Monte Carlo\nstudy that considers slowly varying trends yields both good size and improved\npower results for the proposed tests when compared to conventional unit root\ntests.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.04066v2", "link": "http://arxiv.org/abs/2003.04066v2"}, {"title": "Non-concave utility maximisation on the positive real axis in discrete\n  time", "summary": "We treat a discrete-time asset allocation problem in an arbitrage-free,\ngenerically incomplete financial market, where the investor has a possibly\nnon-concave utility function and wealth is restricted to remain non-negative.\nUnder easily verifiable conditions, we establish the existence of optimal\nportfolios.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1501.03123v2", "link": "http://arxiv.org/abs/1501.03123v2"}, {"title": "Political elections and uncertainty -Are BRICS markets equally exposed\n  to Trump's agenda?", "summary": "There certainly is little or no doubt that politicians, sometimes consciously\nand sometimes not, exert a significant impact on stock markets. The evolving\nvolatility over the Republican Donald Trump's surprise victory in the US\npresidential election is a perfect example when politicians, through announced\npolicies, send signals to financial markets. The present paper seeks to address\nwhether BRICS (Brazil, Russia, India, China and South Africa) stock markets\nequally vulnerable to Trump's plans. For this purpose, two methods were\nadopted. The first presents an event-study methodology based on regression\nestimation of abnormal returns. The second is based on vote intentions by\nintegrating data from social media (Twitter), search queries (Google Trends)\nand public opinion polls. Our results robustly reveal that although some\nmarkets emerged losers, others took the opposite route. China took the biggest\nhit with Brazil, while the damage was much more limited for India and South\nAfrica. These adverse responses can be explained by the Trump's\nneo-mercantilist attitude revolving around tearing up trade deals, instituting\ntariffs, and labeling China a \"currency manipulator\". However, Russia looks to\nbe benefiting due to Trump's sympathetic attitude towards Vladimir Putin and\nexpectations about the scaling down of sanctions imposed on Russia over its\nrole in the conflict in Ukraine.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1701.02182v1", "link": "http://arxiv.org/abs/1701.02182v1"}, {"title": "Optimal valuation of American callable credit default swaps under\n  drawdown of L\u00e9vy insurance risk process", "summary": "This paper discusses the valuation of credit default swaps, where default is\nannounced when the reference asset price has gone below certain level from the\nlast record maximum, also known as the high-water mark or drawdown. We assume\nthat the protection buyer pays premium at fixed rate when the asset price is\nabove a pre-specified level and continuously pays whenever the price increases.\nThis payment scheme is in favour of the buyer as she only pays the premium when\nthe market is in good condition for the protection against financial downturn.\nUnder this framework, we look at an embedded option which gives the issuer an\nopportunity to call back the contract to a new one with reduced premium payment\nrate and slightly lower default coverage subject to paying a certain cost. We\nassume that the buyer is risk neutral investor trying to maximize the expected\nmonetary value of the option over a class of stopping time. We discuss optimal\nsolution to the stopping problem when the source of uncertainty of the asset\nprice is modelled by L\\'evy process with only downward jumps. Using recent\ndevelopment in excursion theory of L\\'evy process, the results are given\nexplicitly in terms of scale function of the L\\'evy process. Furthermore, the\nvalue function of the stopping problem is shown to satisfy continuous and\nsmooth pasting conditions regardless of regularity of the sample paths of the\nL\\'evy process. Optimality and uniqueness of the solution are established using\nmartingale approach for drawdown process and convexity of the scale function\nunder Esscher transform of measure. Some numerical examples are discussed to\nillustrate the main results.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1904.10063v2", "link": "http://arxiv.org/abs/1904.10063v2"}, {"title": "Hyperparameter Optimization for Forecasting Stock Returns", "summary": "In recent years, hyperparameter optimization (HPO) has become an increasingly\nimportant issue in the field of machine learning for the development of more\naccurate forecasting models. In this study, we explore the potential of HPO in\nmodeling stock returns using a deep neural network (DNN). The potential of this\napproach was evaluated using technical indicators and fundamentals examined\nbased on the effect the regularization of dropouts and batch normalization for\nall input data. We found that the model using technical indicators and dropout\nregularization significantly outperforms three other models, showing a positive\npredictability of 0.53% in-sample and 1.11% out-of-sample, thereby indicating\nthe possibility of beating the historical average. We also demonstrate the\nstability of the model in terms of the changes in its feature importance over\ntime.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/2001.10278v1", "link": "http://arxiv.org/abs/2001.10278v1"}, {"title": "Asymptotically optimal discretization of hedging strategies with jumps", "summary": "In this work, we consider the hedging error due to discrete trading in models\nwith jumps. Extending an approach developed by Fukasawa [In Stochastic Analysis\nwith Financial Applications (2011) 331-346 Birkh\\\"{a}user/Springer Basel AG]\nfor continuous processes, we propose a framework enabling us to\n(asymptotically) optimize the discretization times. More precisely, a\ndiscretization rule is said to be optimal if for a given cost function, no\nstrategy has (asymptotically, for large cost) a lower mean square\ndiscretization error for a smaller cost. We focus on discretization rules based\non hitting times and give explicit expressions for the optimal rules within\nthis class.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1108.5940v3", "link": "http://dx.doi.org/10.1214/13-AAP940"}, {"title": "Bayesian Inference on Volatility in the Presence of Infinite Jump\n  Activity and Microstructure Noise", "summary": "Volatility estimation based on high-frequency data is key to accurately\nmeasure and control the risk of financial assets. A L\\'{e}vy process with\ninfinite jump activity and microstructure noise is considered one of the\nsimplest, yet accurate enough, models for financial data at high-frequency.\nUtilizing this model, we propose a \"purposely misspecified\" posterior of the\nvolatility obtained by ignoring the jump-component of the process. The\nmisspecified posterior is further corrected by a simple estimate of the\nlocation shift and re-scaling of the log likelihood. Our main result\nestablishes a Bernstein-von Mises (BvM) theorem, which states that the proposed\nadjusted posterior is asymptotically Gaussian, centered at a consistent\nestimator, and with variance equal to the inverse of the Fisher information. In\nthe absence of microstructure noise, our approach can be extended to inferences\nof the integrated variance of a general It\\^o semimartingale. Simulations are\nprovided to demonstrate the accuracy of the resulting credible intervals, and\nthe frequentist properties of the approximate Bayesian inference based on the\nadjusted posterior.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1909.04853v1", "link": "http://arxiv.org/abs/1909.04853v1"}, {"title": "Forecasting extreme events in collective dynamics: an analytic signal\n  approach to detecting discrete scale invariance", "summary": "A challenging problem in physics concerns the possibility of forecasting rare\nbut extreme phenomena such as large earthquakes, financial market crashes, and\nmaterial rupture. A promising line of research involves the early detection of\nprecursory log-periodic oscillations to help forecast extreme events in\ncollective phenomena where discrete scale invariance plays an important role.\nHere I investigate two distinct approaches towards the general problem of how\nto detect log-periodic oscillations in arbitrary time series without prior\nknowledge of the location of the moveable singularity. I first show that the\nproblem has a definite solution in Fourier space, however the technique\ninvolved requires an unrealistically large signal to noise ratio. I then show\nthat the quadrature signal obtained via analytic continuation onto the\nimaginary axis, using the Hilbert transform, necessarily retains the\nlog-periodicities found in the original signal. This finding allows the\ndevelopment of a new method of detecting log-periodic oscillations that relies\non calculation of the instantaneous phase of the analytic signal. I illustrate\nthe method by applying it to the well documented stock market crash of 1987.\nFinally, I discuss the relevance of these findings for parametric rather than\nnonparametric estimation of critical times.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0611281v2", "link": "http://arxiv.org/abs/physics/0611281v2"}, {"title": "Identification in Nonparametric Models for Dynamic Treatment Effects", "summary": "This paper develops a nonparametric model that represents how sequences of\noutcomes and treatment choices influence one another in a dynamic manner. In\nthis setting, we are interested in identifying the average outcome for\nindividuals in each period, had a particular treatment sequence been assigned.\nThe identification of this quantity allows us to identify the average treatment\neffects (ATE's) and the ATE's on transitions, as well as the optimal treatment\nregimes, namely, the regimes that maximize the (weighted) sum of the average\npotential outcomes, possibly less the cost of the treatments. The main\ncontribution of this paper is to relax the sequential randomization assumption\nwidely used in the biostatistics literature by introducing a flexible\nchoice-theoretic framework for a sequence of endogenous treatments. We show\nthat the parameters of interest are identified under each period's two-way\nexclusion restriction, i.e., with instruments excluded from the\noutcome-determining process and other exogenous variables excluded from the\ntreatment-selection process. We also consider partial identification in the\ncase where the latter variables are not available. Lastly, we extend our\nresults to a setting where treatments do not appear in every period.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1805.09397v3", "link": "http://arxiv.org/abs/1805.09397v3"}, {"title": "Noncooperative dynamics in election interference", "summary": "Foreign power interference in domestic elections is an existential threat to\nsocieties. Manifested through myriad methods from war to words, such\ninterference is a timely example of strategic interaction between economic and\npolitical agents. We model this interaction between rational game players as a\ncontinuous-time differential game, constructing an analytical model of this\ncompetition with a variety of payoff structures. All-or-nothing attitudes by\nonly one player regarding the outcome of the game lead to an arms race in which\nboth countries spend increasing amounts on interference and\ncounter-interference operations. We then confront our model with data\npertaining to the Russian interference in the 2016 United States presidential\nelection contest. We introduce and estimate a Bayesian structural time series\nmodel of election polls and social media posts by Russian Twitter troll\naccounts. Our analytical model, while purposefully abstract and simple,\nadequately captures many temporal characteristics of the election and social\nmedia activity. We close with a discussion of our model's shortcomings and\nsuggestions for future research.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.02793v4", "link": "http://dx.doi.org/10.1103/PhysRevE.101.022307"}, {"title": "On the time consistency of collective preferences", "summary": "A dynamic model of collective consumption and saving decisions made by a\nfinite number of agents with constant but different discount rates is\ndeveloped. Collective utility is a weighted sum of individual utilities with\ntime-varying utility weights. Under standard separability assumptions, it is\nshown that collective preferences may be nonstationary but still satisfy time\nconsistency. The assumption of time-varying weights is key to balance the need\nof the group for a changing distribution of consumption among its members over\ntime with their tolerance for consumption fluctuations.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1607.02688v3", "link": "http://arxiv.org/abs/1607.02688v3"}, {"title": "No need for conspiracy: Self-organized cartel formation in a modified\n  trust game", "summary": "We investigate the dynamics of a trust game on a mixed population where\nindividuals with the role of buyers are forced to play against a predetermined\nnumber of sellers, whom they choose dynamically. Agents with the role of\nsellers are also allowed to adapt the level of value for money of their\nproducts, based on payoff. The dynamics undergoes a transition at a specific\nvalue of the strategy update rate, above which an emergent cartel organization\nis observed, where sellers have similar values of below optimal value for\nmoney. This cartel organization is not due to an explicit collusion among\nagents; instead it arises spontaneously from the maximization of the individual\npayoffs. This dynamics is marked by large fluctuations and a high degree of\nunpredictability for most of the parameter space, and serves as a plausible\nqualitative explanation for observed elevated levels and fluctuations of\ncertain commodity prices.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1201.3798v3", "link": "http://dx.doi.org/10.1103/PhysRevLett.108.218702"}, {"title": "Covariance of random stock prices in the Stochastic Dividend Discount\n  Model", "summary": "Dividend discount models have been developed in a deterministic setting. Some\nauthors (Hurley and Johnson, 1994 and 1998; Yao, 1997) have introduced\nrandomness in terms of stochastic growth rates, delivering closed-form\nexpressions for the expected value of stock prices. This paper extends such\nprevious results by determining a formula for the covariance between random\nstock prices when the dividends' rates of growth are correlated. The formula is\neventually applied to real market data.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1609.03029v2", "link": "http://arxiv.org/abs/1609.03029v2"}, {"title": "Persuading part of an audience", "summary": "I propose a cheap-talk model in which the sender can use private messages and\nonly cares about persuading a subset of her audience. For example, a candidate\nonly needs to persuade a majority of the electorate in order to win an\nelection. I find that senders can gain credibility by speaking truthfully to\nsome receivers while lying to others. In general settings, the model admits\ninformation transmission in equilibrium for some prior beliefs. The sender can\napproximate her preferred outcome when the fraction of the audience she needs\nto persuade is sufficiently small. I characterize the sender-optimal\nequilibrium and the benefit of not having to persuade your whole audience in\nseparable environments. I also analyze different applications and verify that\nthe results are robust to some perturbations of the model, including\nnon-transparent motives as in Crawford and Sobel (1982), and full commitment as\nin Kamenica and Gentzkow (2011).", "category": [], "id": "http://arxiv.org/abs/1903.00129v1", "link": "http://arxiv.org/abs/1903.00129v1"}, {"title": "Implementing result-based agri-environmental payments by means of\n  modelling", "summary": "From a theoretical point of view, result-based agri-environmental payments\nare clearly preferable to action-based payments. However, they suffer from two\nmajor practical disadvantages: costs of measuring the results and payment\nuncertainty for the participating farmers. In this paper, we propose an\nalternative design to overcome these two disadvantages by means of modelling\n(instead of measuring) the results. We describe the concept of model-informed\nresult-based agri-environmental payments (MIRBAP), including a hypothetical\nexample of payments for the protection and enhancement of soil functions. We\noffer a comprehensive discussion of the relative advantages and disadvantages\nof MIRBAP, showing that it not only unites most of the advantages of\nresult-based and action-based schemes, but also adds two new advantages: the\npotential to address trade-offs among multiple policy objectives and management\nfor long-term environmental effects. We argue that MIRBAP would be a valuable\naddition to the agri-environmental policy toolbox and a reflection of recent\nadvancements in agri-environmental modelling.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.08219v1", "link": "http://arxiv.org/abs/1908.08219v1"}, {"title": "Numerical analysis for a unified 2 factor model of structural and\n  reduced form types for corporate bonds with fixed discrete coupon", "summary": "Conditions of Stability for explicit finite difference scheme and some\nresults of numerical analysis for a unified 2 factor model of structural and\nreduced form types for corporate bonds with fixed discrete coupon are provided.\nIt seems to be difficult to get solution formula for PDE model which\ngeneralizes Agliardi's structural model [1] for discrete coupon bonds into a\nunified 2 factor model of structural and reduced form types and we study a\nnumerical analysis for it by explicit finite difference scheme. These equations\nare parabolic equations with 3 variables and they include mixed derivatives, so\nthe explicit finite difference scheme is not stable in general. We find\nconditions for the explicit finite difference scheme to be stable, in the case\nthat it is stable, numerically compute the price of the bond and analyze its\ncredit spread and duration.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1709.06517v2", "link": "http://arxiv.org/abs/1709.06517v2"}, {"title": "Cleaning large correlation matrices: tools from random matrix theory", "summary": "This review covers recent results concerning the estimation of large\ncovariance matrices using tools from Random Matrix Theory (RMT). We introduce\nseveral RMT methods and analytical techniques, such as the Replica formalism\nand Free Probability, with an emphasis on the Marchenko-Pastur equation that\nprovides information on the resolvent of multiplicatively corrupted noisy\nmatrices. Special care is devoted to the statistics of the eigenvectors of the\nempirical correlation matrix, which turn out to be crucial for many\napplications. We show in particular how these results can be used to build\nconsistent \"Rotationally Invariant\" estimators (RIE) for large correlation\nmatrices when there is no prior on the structure of the underlying process. The\nlast part of this review is dedicated to some real-world applications within\nfinancial markets as a case in point. We establish empirically the efficacy of\nthe RIE framework, which is found to be superior in this case to all previously\nproposed methods. The case of additively (rather than multiplicatively)\ncorrupted noisy matrices is also dealt with in a special Appendix. Several open\nproblems and interesting technical developments are discussed throughout the\npaper.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1610.08104v1", "link": "http://dx.doi.org/10.1016/j.physrep.2016.10.005"}, {"title": "The Chinese Equity Bubble: Ready to Burst", "summary": "Amid the current financial crisis, there has been one equity index beating\nall others: the Shanghai Composite. Our analysis of this main Chinese equity\nindex shows clear signatures of a bubble build up and we go on to predict its\nmost likely crash date: July 17-27, 2009 (20%/80% quantile confidence\ninterval).", "category": ["q-fin.ST", "q-fin.PM"], "id": "http://arxiv.org/abs/0907.1827v1", "link": "http://arxiv.org/abs/0907.1827v1"}, {"title": "k-Generalized Statistics in Personal Income Distribution", "summary": "Starting from the generalized exponential function\n$\\exp_{\\kappa}(x)=(\\sqrt{1+\\kappa^{2}x^{2}}+\\kappa x)^{1/\\kappa}$, with\n$\\exp_{0}(x)=\\exp(x)$, proposed in Ref. [G. Kaniadakis, Physica A \\textbf{296},\n405 (2001)], the survival function $P_{>}(x)=\\exp_{\\kappa}(-\\beta x^{\\alpha})$,\nwhere $x\\in\\mathbf{R}^{+}$, $\\alpha,\\beta>0$, and $\\kappa\\in[0,1)$, is\nconsidered in order to analyze the data on personal income distribution for\nGermany, Italy, and the United Kingdom. The above defined distribution is a\ncontinuous one-parameter deformation of the stretched exponential function\n$P_{>}^{0}(x)=\\exp(-\\beta x^{\\alpha})$\\textemdash to which reduces as $\\kappa$\napproaches zero\\textemdash behaving in very different way in the $x\\to0$ and\n$x\\to\\infty$ regions. Its bulk is very close to the stretched exponential one,\nwhereas its tail decays following the power-law\n$P_{>}(x)\\sim(2\\beta\\kappa)^{-1/\\kappa}x^{-\\alpha/\\kappa}$. This makes the\n$\\kappa$-generalized function particularly suitable to describe simultaneously\nthe income distribution among both the richest part and the vast majority of\nthe population, generally fitting different curves. An excellent agreement is\nfound between our theoretical model and the observational data on personal\nincome over their entire range.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0607293v2", "link": "http://dx.doi.org/10.1140/epjb/e2007-00120-9"}, {"title": "Quantum Annealing Algorithm for Expected Shortfall based Dynamic Asset\n  Allocation", "summary": "The 2008 mortgage crisis is an example of an extreme event. Extreme value\ntheory tries to estimate such tail risks. Modern finance practitioners prefer\nExpected Shortfall based risk metrics (which capture tail risk) over\ntraditional approaches like volatility or even Value-at-Risk. This paper\nprovides a quantum annealing algorithm in QUBO form for a dynamic asset\nallocation problem using expected shortfall constraint. It was motivated by the\nneed to refine the current quantum algorithms for Markowitz type problems which\nare academically interesting but not useful for practitioners. The algorithm is\ndynamic and the risk target emerges naturally from the market volatility.\nMoreover, it avoids complicated statistics like generalized pareto\ndistribution. It translates the problem into qubit form suitable for\nimplementation by a quantum annealer like D-Wave. Such QUBO algorithms are\nexpected to be solved faster using quantum annealing systems than any classical\nalgorithm using classical computer (but yet to be demonstrated at scale).", "category": ["q-fin.RM", "q-fin.PM"], "id": "http://arxiv.org/abs/1909.12904v2", "link": "http://arxiv.org/abs/1909.12904v2"}, {"title": "Using network science to quantify economic disruptions in regional\n  input-output networks", "summary": "Input Output (IO) tables provide a standardised way of looking at monetary\nflows between all industries in an economy. IO tables can be thought of as\nnetworks - with the nodes being different industries and the edges being the\nflows between them. We develop a network-based analysis to consider a\nmulti-regional IO network at regional and subregional level within a country.\nWe calculate both traditional matrix-based IO measures (e.g. 'multipliers') and\nnew network theory-based measures at this higher spatial resolution. We\ncontrast these methods with the results of a disruption model applied to the\nsame IO data in order to demonstrate that betweenness centrality gives a good\nindication of flow on economic disruption, while eigenvector-type centrality\nmeasures give results comparable to traditional IO multipliers.We also show the\neffects of treating IO networks at different levels of spatial aggregation.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.12498v1", "link": "http://arxiv.org/abs/1910.12498v1"}, {"title": "How on Earth: Flourishing in a Not-for-Profit World by 2050", "summary": "In this book, we outline a model of a non-capitalist market economy based on\nnot-for-profit forms of business. This work presents both a critique of the\ncurrent economic system and a vision of a more socially, economically, and\necologically sustainable economy. The point of departure is the purpose and\nprofit-orientation embedded in the legal forms used by businesses (e.g.,\nfor-profit or not-for-profit) and the ramifications of this for global\nsustainability challenges such as environmental pollution, resource use,\nclimate change, and economic inequality. We document the rapid rise of\nnot-for-profit forms of business in the global economy and offer a conceptual\nframework and an analytical lens through which to view these relatively new\neconomic actors and their potential for transforming the economy. The book\nexplores how a market consisting of only or mostly not-for-profit forms of\nbusiness might lead to better financial circulation, economic equality, social\nwell-being, and environmental regeneration as compared to for-profit markets.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1902.01398v1", "link": "http://arxiv.org/abs/1902.01398v1"}, {"title": "Model-free portfolio theory and its functional master formula", "summary": "We use pathwise It\\^o calculus to prove two strictly pathwise versions of the\nmaster formula in Fernholz' stochastic portfolio theory. Our first version is\nset within the framework of F\\\"ollmer's pathwise It\\^o calculus and works for\nportfolios generated from functions that may depend on the current states of\nthe market portfolio and an additional path of finite variation. The second\nversion is formulated within the functional pathwise It\\^o calculus of Dupire\n(2009) and Cont \\& Fourni\\'e (2010) and allows for portfolio-generating\nfunctionals that may depend additionally on the entire path of the market\nportfolio. Our results are illustrated by several examples and shown to work on\nempirical market data.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1606.03325v5", "link": "http://arxiv.org/abs/1606.03325v5"}, {"title": "Estimation in a Generalization of Bivariate Probit Models with Dummy\n  Endogenous Regressors", "summary": "The purpose of this paper is to provide guidelines for empirical researchers\nwho use a class of bivariate threshold crossing models with dummy endogenous\nvariables. A common practice employed by the researchers is the specification\nof the joint distribution of the unobservables as a bivariate normal\ndistribution, which results in a bivariate probit model. To address the problem\nof misspecification in this practice, we propose an easy-to-implement\nsemiparametric estimation framework with parametric copula and nonparametric\nmarginal distributions. We establish asymptotic theory, including root-n\nnormality, for the sieve maximum likelihood estimators that can be used to\nconduct inference on the individual structural parameters and the average\ntreatment effect (ATE). In order to show the practical relevance of the\nproposed framework, we conduct a sensitivity analysis via extensive Monte Carlo\nsimulation exercises. The results suggest that the estimates of the parameters,\nespecially the ATE, are sensitive to parametric specification, while\nsemiparametric estimation exhibits robustness to underlying data generating\nprocesses. We then provide an empirical illustration where we estimate the\neffect of health insurance on doctor visits. In this paper, we also show that\nthe absence of excluded instruments may result in identification failure, in\ncontrast to what some practitioners believe.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.05792v2", "link": "http://arxiv.org/abs/1808.05792v2"}, {"title": "Martingale Option Pricing", "summary": "We show that our generalization of the Black-Scholes partial differential\nequation (pde) for nontrivial diffusion coefficients is equivalent to a\nMartingale in the risk neutral discounted stock price. Previously, this was\nproven for the case of the Gaussian logarithmic returns model by Harrison and\nKreps, but we prove it for much a much larger class of returns models where the\ndiffusion coefficient depends on both returns x and time t. That option prices\nblow up if fat tails in logarithmic returns x are included in the market\ndynamics is also explained.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/physics/0606011v4", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.038"}, {"title": "Optimizing a basket against the efficient market hypothesis", "summary": "The possibility that the collective dynamics of a set of stocks could lead to\na specific basket violating the efficient market hypothesis is investigated.\nPrecisely, we show that it is systematically possible to form a basket with a\nnon-trivial autocorrelation structure when the examined time scales are at the\norder of tens of seconds. Moreover, we show that this situation is persistent\nenough to allow some kind of forecasting.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1006.5230v1", "link": "http://arxiv.org/abs/1006.5230v1"}, {"title": "On time scaling of semivariance in a jump-diffusion process", "summary": "The aim of this paper is to examine the time scaling of the semivariance when\nreturns are modeled by various types of jump-diffusion processes, including\nstochastic volatility models with jumps in returns and in volatility. In\nparticular, we derive an exact formula for the semivariance when the volatility\nis kept constant, explaining how it should be scaled when considering a lower\nfrequency. We also provide and justify the use of a generalization of the\nBall-Torous approximation of a jump-diffusion process, this new model appearing\nto deliver a more accurate estimation of the downside risk. We use Markov Chain\nMonte Carlo (MCMC) methods to fit our stochastic volatility model. For the\ntests, we apply our methodology to a highly skewed set of returns based on the\nBarclays US High Yield Index, where we compare different time scalings for the\nsemivariance. Our work shows that the square root of the time horizon seems to\nbe a poor approximation in the context of semivariance and that our methodology\nbased on jump-diffusion processes gives much better results.", "category": ["q-fin.ST", "q-fin.RM"], "id": "http://arxiv.org/abs/1311.1122v1", "link": "http://arxiv.org/abs/1311.1122v1"}, {"title": "On coherency and other properties of MAXVAR", "summary": "This paper is concerned with the MAXVAR risk measure on L^2 space. We present\nan elementary and direct proof of its coherency and averseness. Based on the\nobservation that the MAXVAR measure is a continuous convex combination of the\nCVaR measure, we provide an explicit formula for the risk envelope of MAXVAR.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1703.10981v2", "link": "http://dx.doi.org/10.1007/s10013-017-0262-y"}, {"title": "Volatility distribution in the S&P500 Stock Index", "summary": "We study the volatility of the S&P500 stock index from 1984 to 1996 and find\nthat the volatility distribution can be very well described by a log-normal\nfunction. Further, using detrended fluctuation analysis we show that the\nvolatility is power-law correlated with Hurst exponent $\\alpha\\cong0.9$.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9708143v1", "link": "http://dx.doi.org/10.1016/S0378-4371(97)00417-2"}, {"title": "A Comparison of Economic Agent-Based Model Calibration Methods", "summary": "Interest in agent-based models of financial markets and the wider economy has\nincreased consistently over the last few decades, in no small part due to their\nability to reproduce a number of empirically-observed stylised facts that are\nnot easily recovered by more traditional modelling approaches. Nevertheless,\nthe agent-based modelling paradigm faces mounting criticism, focused\nparticularly on the rigour of current validation and calibration practices,\nmost of which remain qualitative and stylised fact-driven. While the literature\non quantitative and data-driven approaches has seen significant expansion in\nrecent years, most studies have focused on the introduction of new calibration\nmethods that are neither benchmarked against existing alternatives nor\nrigorously tested in terms of the quality of the estimates they produce. We\ntherefore compare a number of prominent ABM calibration methods, both\nestablished and novel, through a series of computational experiments in an\nattempt to determine the respective strengths and weaknesses of each approach\nand the overall quality of the resultant parameter estimates. We find that\nBayesian estimation, though less popular in the literature, consistently\noutperforms frequentist, objective function-based approaches and results in\nreasonable parameter estimates in many contexts. Despite this, we also find\nthat agent-based model calibration techniques require further development in\norder to definitively calibrate large-scale models. We therefore make\nsuggestions for future research.", "category": ["q-fin.CP", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1902.05938v1", "link": "http://arxiv.org/abs/1902.05938v1"}, {"title": "Patterns in high-frequency FX data: Discovery of 12 empirical scaling\n  laws", "summary": "We have discovered 12 independent new empirical scaling laws in foreign\nexchange data-series that hold for close to three orders of magnitude and\nacross 13 currency exchange rates. Our statistical analysis crucially depends\non an event-based approach that measures the relationship between different\ntypes of events. The scaling laws give an accurate estimation of the length of\nthe price-curve coastline, which turns out to be surprisingly long. The new\nlaws substantially extend the catalogue of stylised facts and sharply constrain\nthe space of possible theoretical explanations of the market mechanisms.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0809.1040v2", "link": "http://dx.doi.org/10.1080/14697688.2010.481632"}, {"title": "Optimal inventory management and order book modeling", "summary": "We model the behavior of three agent classes acting dynamically in a limit\norder book of a financial asset. Namely, we consider market makers (MM),\nhigh-frequency trading (HFT) firms, and institutional brokers (IB). Given a\nprior dynamic of the order book, similar to the one considered in the\nQueue-Reactive models [14, 20, 21], the MM and the HFT define their trading\nstrategy by optimizing the expected utility of terminal wealth, while the IB\nhas a prescheduled task to sell or buy many shares of the considered asset. We\nderive the variational partial differential equations that characterize the\nvalue functions of the MM and HFT and explain how almost optimal control can be\ndeduced from them. We then provide a first illustration of the interactions\nthat can take place between these different market participants by simulating\nthe dynamic of an order book in which each of them plays his own (optimal)\nstrategy.", "category": ["q-fin.TR", "math.PR"], "id": "http://arxiv.org/abs/1802.08135v2", "link": "http://arxiv.org/abs/1802.08135v2"}, {"title": "Portfolio Optimization with Entropic Value-at-Risk", "summary": "The entropic value-at-risk (EVaR) is a new coherent risk measure, which is an\nupper bound for both the value-at-risk (VaR) and conditional value-at-risk\n(CVaR). As important properties, the EVaR is strongly monotone over its domain\nand strictly monotone over a broad sub-domain including all continuous\ndistributions, while well-known monotone risk measures, such as VaR and CVaR\nlack these properties. A key feature for a risk measure, besides its financial\nproperties, is its applicability in large-scale sample-based portfolio\noptimization. If the negative return of an investment portfolio is a\ndifferentiable convex function, the portfolio optimization with the EVaR\nresults in a differentiable convex program whose number of variables and\nconstraints is independent of the sample size, which is not the case for the\nVaR and CVaR. This enables us to design an efficient algorithm using\ndifferentiable convex optimization. Our extensive numerical study shows the\nhigh efficiency of the algorithm in large scales, compared to the existing\nconvex optimization software packages. The computational efficiency of the EVaR\nportfolio optimization approach is also compared with that of CVaR-based\nportfolio optimization. This comparison shows that the EVaR approach generally\nperforms similarly, and it outperforms as the sample size increases. Moreover,\nthe comparison of the portfolios obtained for a real case by the EVaR and CVaR\napproaches shows that the EVaR approach can find portfolios with better\nexpectations and VaR values at high confidence levels.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1708.05713v1", "link": "http://dx.doi.org/10.1016/j.ejor.2019.02.007"}, {"title": "Optimization of Fire Sales and Borrowing in Systemic Risk", "summary": "This paper provides a framework for modeling financial contagion in a network\nsubject to fire sales and price impacts, but allowing for firms to borrow to\ncover their shortfall as well. We consider both uncollateralized and\ncollateralized loans. The main results of this work are providing sufficient\nconditions for existence and uniqueness of the clearing solutions (i.e.,\npayments, liquidations, and borrowing); in such a setting any clearing solution\nis the Nash equilibrium of an aggregation game.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1802.04232v3", "link": "http://arxiv.org/abs/1802.04232v3"}, {"title": "Confidentiality and linked data", "summary": "Data providers such as government statistical agencies perform a balancing\nact: maximising information published to inform decision-making and research,\nwhile simultaneously protecting privacy. The emergence of identified\nadministrative datasets with the potential for sharing (and thus linking)\noffers huge potential benefits but significant additional risks. This article\nintroduces the principles and methods of linking data across different sources\nand points in time, focusing on potential areas of risk. We then consider\nconfidentiality risk, focusing in particular on the \"intruder\" problem central\nto the area, and looking at both risks from data producer outputs and from the\nrelease of micro-data for further analysis. Finally, we briefly consider\npotential solutions to micro-data release, both the statistical solutions\nconsidered in other contributed articles and non-statistical solutions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.06465v1", "link": "http://arxiv.org/abs/1907.06465v1"}, {"title": "Duality formulas for robust pricing and hedging in discrete time", "summary": "In this paper we derive robust super- and subhedging dualities for contingent\nclaims that can depend on several underlying assets. In addition to strict\nsuper- and subhedging, we also consider relaxed versions which, instead of\neliminating the shortfall risk completely, aim to reduce it to an acceptable\nlevel. This yields robust price bounds with tighter spreads. As examples we\nstudy strict super- and subhedging with general convex transaction costs and\ntrading constraints as well as risk-based hedging with respect to robust\nversions of the average value at risk and entropic risk measure. Our approach\nis based on representation results for increasing convex functionals and allows\nfor general financial market structures. As a side result it yields a robust\nversion of the fundamental theorem of asset pricing.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1602.06177v5", "link": "http://arxiv.org/abs/1602.06177v5"}, {"title": "On Rational Bubbles and Fat Tails", "summary": "This paper addresses the statistical properties of time series driven by\nrational bubbles a la Blanchard and Watson (1982), corresponding to\nmultiplicative maps, whose study has recently be revived recently in physics as\na mechanism of intermittent dynamics generating power law distributions. Using\ninsights on the behavior of multiplicative stochastic processes, we demonstrate\nthat the tails of the unconditional distribution emerging from such bubble\nprocesses follow power-laws (exhibit hyperbolic decline). More precisely, we\nfind that rational bubbles predict a 'fat' power tail for both the bubble\ncomponent and price differences with an exponent smaller than 1, implying\nabsence of convergence of the mean. The distribution of returns is dominated by\nthe same power-law over an extended range of large returns. Although power-law\ntails are a pervasive feature of empirical data, these numerical predictions\nare in disagreement with the usual empirical estimates of an exponent between 2\nand 4. It, therefore, appears that exogenous rational bubbles are hardly\nreconcilable with some of the stylized facts of financial data at a very\nelementary level.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9910141v1", "link": "http://arxiv.org/abs/cond-mat/9910141v1"}, {"title": "Game-Theoretic Optimal Portfolios in Continuous Time", "summary": "We consider a two-person trading game in continuous time whereby each player\nchooses a constant rebalancing rule $b$ that he must adhere to over $[0,t]$. If\n$V_t(b)$ denotes the final wealth of the rebalancing rule $b$, then Player 1\n(the `numerator player') picks $b$ so as to maximize\n$\\mathbb{E}[V_t(b)/V_t(c)]$, while Player 2 (the `denominator player') picks\n$c$ so as to minimize it. In the unique Nash equilibrium, both players use the\ncontinuous-time Kelly rule $b^*=c^*=\\Sigma^{-1}(\\mu-r\\textbf{1})$, where\n$\\Sigma$ is the covariance of instantaneous returns per unit time, $\\mu$ is the\ndrift vector of the stock market, and $\\textbf{1}$ is a vector of ones. Thus,\neven over very short intervals of time $[0,t]$, the desire to perform well\nrelative to other traders leads one to adopt the Kelly rule, which is\nordinarily derived by maximizing the asymptotic exponential growth rate of\nwealth. Hence, we find agreement with Bell and Cover's (1988) result in\ndiscrete time.", "category": ["q-fin.PM", "econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.MF"], "id": "http://arxiv.org/abs/1906.02216v1", "link": "http://arxiv.org/abs/1906.02216v1"}, {"title": "Vibrato and automatic differentiation for high order derivatives and\n  sensitivities of financial options", "summary": "This paper deals with the computation of second or higher order greeks of\nfinancial securities. It combines two methods, Vibrato and automatic\ndifferentiation and compares with other methods. We show that this combined\ntechnique is faster than standard finite difference, more stable than automatic\ndifferentiation of second order derivatives and more general than Malliavin\nCalculus. We present a generic framework to compute any greeks and present\nseveral applications on different types of financial contracts: European and\nAmerican options, multidimensional Basket Call and stochastic volatility models\nsuch as Heston's model. We give also an algorithm to compute derivatives for\nthe Longstaff-Schwartz Monte Carlo method for American options. We also extend\nautomatic differentiation for second order derivatives of options with\nnon-twice differentiable payoff. 1. Introduction. Due to BASEL III regulations,\nbanks are requested to evaluate the sensitivities of their portfolios every day\n(risk assessment). Some of these portfolios are huge and sensitivities are time\nconsuming to compute accurately. Faced with the problem of building a software\nfor this task and distrusting automatic differentiation for non-differentiable\nfunctions, we turned to an idea developed by Mike Giles called Vibrato. Vibrato\nat core is a differentiation of a combination of likelihood ratio method and\npathwise evaluation. In Giles [12], [13], it is shown that the computing time,\nstability and precision are enhanced compared with numerical differentiation of\nthe full Monte Carlo path. In many cases, double sensitivities, i.e. second\nderivatives with respect to parameters, are needed (e.g. gamma hedging). Finite\ndifference approximation of sensitivities is a very simple method but its\nprecision is hard to control because it relies on the appropriate choice of the\nincrement. Automatic differentiation of computer programs bypass the difficulty\nand its computing cost is similar to finite difference, if not cheaper. But in\nfinance the payoff is never twice differentiable and so generalized derivatives\nhave to be used requiring approximations of Dirac functions of which the\nprecision is also doubtful. The purpose of this paper is to investigate the\nfeasibility of Vibrato for second and higher derivatives. We will first compare\nVibrato applied twice with the analytic differentiation of Vibrato and show\nthat it is equivalent, as the second is easier we propose the best compromise\nfor second derivatives: Automatic Differentiation of Vibrato. In [8], Capriotti\nhas recently investigated the coupling of different mathematical methods --\nnamely pathwise and likelihood ratio methods -- with an Automatic differ", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1606.06143v1", "link": "http://arxiv.org/abs/1606.06143v1"}, {"title": "Informe-pais Brasil", "summary": "Structural socioeconomic analysis of Brazil. All basic information about this\nSouth American country is gathered in a comprehensive outlook that includes the\nchallenges Brazil faces, as well as their causes and posible economic\nsolutions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.08564v1", "link": "http://arxiv.org/abs/1909.08564v1"}, {"title": "Rationality parameter for exercising American put", "summary": "The main result of this paper is a probabilistic proof of the penalty method\nfor approximating the price of an American put in the Black-Scholes market. The\nmethod gives a parametrized family of partial differential equations, and by\nvarying the parameter the corresponding solutions converge to the price of an\nAmerican put. For each PDE the parameter may be interpreted as a rationality\nparameter of the holder of the option. The method may be extended to other\nvaluation situations given as an optimal stopping problem with no explicit\nsolution. The method may also be used for valuations where actors do not behave\ncompletely rationally but instead have randomness affecting their choices. The\nrationality parameter is a measure for this randomness.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1410.1287v1", "link": "http://arxiv.org/abs/1410.1287v1"}, {"title": "Marked point processes and intensity ratios for limit order book\n  modeling", "summary": "This paper extends the analysis of Muni Toke and Yoshida (2020) to the case\nof marked point processes. We consider multiple marked point processes with\nintensities defined by three multiplicative components, namely a common\nbaseline intensity, a state-dependent component specific to each process, and a\nstate-dependent component specific to each mark within each process. We show\nthat for specific mark distributions, this model is a combination of the ratio\nmodels defined in Muni Toke and Yoshida (2020). We prove convergence results\nfor the quasi-maximum and quasi-Bayesian likelihood estimators of this model\nand provide numerical illustrations of the asymptotic variances. We use these\nratio processes in order to model transactions occuring in a limit order book.\nModel flexibility allows us to investigate both state-dependency (emphasizing\nthe role of imbalance and spread as significant signals) and clustering.\nCalibration, model selection and prediction results are reported for\nhigh-frequency trading data on multiple stocks traded on Euronext Paris. We\nshow that the marked ratio model outperforms other intensity-based methods\n(such as \"pure\" Hawkes-based methods) in predicting the sign and aggressiveness\nof market orders on financial markets.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/2001.08442v1", "link": "http://arxiv.org/abs/2001.08442v1"}, {"title": "The uniqueness of the profits distribution function in the middle scale\n  region", "summary": "We report the proof that the expression of extended Gibrat's law is unique\nand the probability distribution function (pdf) is also uniquely derived from\nthe law of detailed balance and the extended Gibrat's law. In the proof, two\napproximations are employed that the pdf of growth rate is described as\ntent-shaped exponential functions and that the value of the origin of growth\nrate is constant. These approximations are confirmed in profits data of\nJapanese companies 2003 and 2004. The resultant profits pdf fits with the\nempirical data with high accuracy. This guarantees the validity of the\napproximations.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0607217v1", "link": "http://arxiv.org/abs/physics/0607217v1"}, {"title": "Portfolio Selection with Mandatory Bequest", "summary": "In this paper, optimal consumption and investment decisions are studied for\nan investor who can invest in a fixed interest rate bank account and a stock\nwhose price is a log normal diffusion. We present the method of the HJB\nequation in order to explicitly solve problems of this type with modifications\nsuch as a fixed percentage transaction cost and a mandatory bequest function.\nIt is shown that the investor treats the mandatory bequest as an expense that\nshe factors into her personal wealth when making consumption and transaction\ndecisions. Furthermore, the investor keeps her portfolio proportions inside a\nfixed boundary relating to Merton's optimal proportion and the transaction\ncosts.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1409.3969v1", "link": "http://arxiv.org/abs/1409.3969v1"}, {"title": "An Iterated Az\u00e9ma-Yor Type Embedding for Finitely Many Marginals", "summary": "We solve the $n$-marginal Skorokhod embedding problem for a continuous local\nmartingale and a sequence of probability measures $\\mu_1,...,\\mu_n$ which are\nin convex order and satisfy an additional technical assumption. Our\nconstruction is explicit and is a multiple marginal generalisation of the Azema\nand Yor (1979) solution. In particular, we recover the stopping boundaries\nobtained by Brown et al. (2001) and Madan and Yor (2002). Our technical\nassumption is necessary for the explicit embedding, as demonstrated with a\ncounterexample. We discuss extensions to the general case giving details when\n$n=3$.\n  In our analysis we compute the law of the maximum at each of the n stopping\ntimes. This is used in Henry-Labordere et al. (2013) to show that the\nconstruction maximises the distribution of the maximum among all solutions to\nthe $n$-marginal Skorokhod embedding problem. The result has direct\nimplications for robust pricing and hedging of Lookback options.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1304.0368v2", "link": "http://arxiv.org/abs/1304.0368v2"}, {"title": "On collective non-gaussian dependence patterns in high frequency\n  financial data", "summary": "The analysis of observed conditional distributions of both lagged and\nsimultaneous intraday price increments of a basket of stocks reveals phenomena\nof dependence - induced volatility smile and kurtosis reduction. A model based\non multivariate t-Student distribution shows that the observed effects are\ncaused by colelctive non-gaussian dependence properties of financial time\nseries.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0506072v3", "link": "http://arxiv.org/abs/physics/0506072v3"}, {"title": "Hospitality Students' Perceptions towards Working in Hotels: a case\n  study of the faculty of tourism and hotels in Alexandria University", "summary": "The tourism and hospitality industry worldwide has been confronted with the\nproblem of attracting and retaining quality employees. If today's students are\nto become the effective practitioners of tomorrow, it is fundamental to\nunderstand their perceptions of tourism employment. Therefore, this research\naims at investigating the perceptions of hospitality students at the Faculty of\nTourism in Alexandria University towards the industry as a career choice. A\nself-administrated questionnaire was developed to rate the importance of 20\nfactors in influencing career choice, and the extent to which hospitality as a\ncareer offers these factors. From the results, it is clear that students\ngenerally do not believe that the hospitality career will offer them the\nfactors they found important. However, most of respondents (70.6%) indicated\nthat they would work in the industry after graduation. Finally, a set of\nspecific remedial actions that hospitality stakeholders could initiate to\nimprove the perceptions of hospitality career are discussed.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.09660v1", "link": "http://arxiv.org/abs/1807.09660v1"}, {"title": "Structural distortions in the Euro interbank market: The role of 'key\n  players' during the recent market turmoil", "summary": "We study the frictions in the patterns of trades in the Euro money market. We\ncharacterize the structure of lending relations during the period of recent\nfinancial turmoil. We use network-topology method on data from overnight\ntransactions in the Electronic Market for Interbank Deposits (e-Mid) to\ninvestigate on two main issues. First, we characterize the division of roles\nbetween borrowers and lenders in long-run relations by providing evidence on\nnetwork formation at a yearly frequency. Second, we identify the 'key players'\nin the marketplace and study their behaviour. Key players are 'locally-central\nbanks' within a network that lend (or borrow) large volumes to (from) several\ncounterparties, while borrowing (or lending) small volumes from (to) a small\nnumber of institutions. Our results are twofold. We show that the aggregate\ntrading patterns in e-Mid are characterized by largely asymmetric relations.\nThis implies a clear division of roles between lenders and borrowers. Second,\nthe key players do not exploit their position of network leaders by imposing\nopportunistic pricing policies. We find that only a fraction of the networks\ncomposed by big players are characterized by interest rates that are\nstatistically different from the average market rate throughout the turmoil\nperiod.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1207.5269v1", "link": "http://arxiv.org/abs/1207.5269v1"}, {"title": "Rebalancing Frequency Considerations for Kelly-Optimal Stock Portfolios\n  in a Control-Theoretic Framework", "summary": "In this paper, motivated by the celebrated work of Kelly, we consider the\nproblem of portfolio weight selection to maximize expected logarithmic growth.\nGoing beyond existing literature, our focal point here is the rebalancing\nfrequency which we include as an additional parameter in our analysis. The\nproblem is first set in a control-theoretic framework, and then, the main\nquestion we address is as follows: In the absence of transaction costs, does\nhigh-frequency trading always lead to the best performance? Related to this is\nour prior work on betting, also in the Kelly context, which examines the impact\nof making a wager and letting it ride. Our results on betting frequency can be\ninterpreted in the context of weight selection for a two-asset portfolio\nconsisting of one risky asset and one riskless asset. With regard to the\nquestion above, our prior results indicate that it is often the case that there\nare no performance benefits associated with high-frequency trading. In the\npresent paper, we generalize the analysis to portfolios with multiple risky\nassets. We show that if there is an asset satisfying a new condition which we\ncall dominance, then an optimal portfolio consists of this asset alone; i.e.,\nthe trader has \"all eggs in one basket\" and performance becomes a constant\nfunction of rebalancing frequency. Said another way, the problem of rebalancing\nis rendered moot. The paper also includes simulations which address practical\nconsiderations associated with real stock prices and the dominant asset\ncondition.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1807.05265v2", "link": "http://dx.doi.org/10.1109/CDC.2018.8619189"}, {"title": "Pathwise superhedging on prediction sets", "summary": "In this paper we provide a pricing-hedging duality for the model-independent\nsuperhedging price with respect to a prediction set $\\Xi\\subseteq C[0,T]$,\nwhere the superhedging property needs to hold pathwise, but only for paths\nlying in $\\Xi$. For any Borel measurable claim $\\xi$ which is bounded from\nbelow, the superhedging price coincides with the supremum over all pricing\nfunctionals $\\mathbb{E}_{\\mathbb{Q}}[\\xi]$ with respect to martingale measures\n$\\mathbb{Q}$ concentrated on the prediction set $\\Xi$. This allows to include\nbeliefs in future paths of the price process expressed by the set $\\Xi$, while\neliminating all those which are seen as impossible. Moreover, we provide\nseveral examples to justify our setup.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1711.02764v3", "link": "http://arxiv.org/abs/1711.02764v3"}, {"title": "Controlling Human Utilization of Failure-Prone Systems via Taxes", "summary": "We consider a game-theoretic model where individuals compete over a shared\nfailure-prone system or resource. We investigate the effectiveness of a\ntaxation mechanism in controlling the utilization of the resource at the Nash\nequilibrium when the decision-makers have behavioral risk preferences, captured\nby prospect theory. We first observe that heterogeneous prospect-theoretic risk\npreferences can lead to counter-intuitive outcomes. In particular, for\nresources that exhibit network effects, utilization can increase under taxation\nand there may not exist a tax rate that achieves the socially optimal level of\nutilization. We identify conditions under which utilization is monotone and\ncontinuous, and then characterize the range of utilizations that can be\nachieved by a suitable choice of tax rate. We further show that resource\nutilization is higher when players are charged differentiated tax rates\ncompared to the case when all players are charged an identical tax rate, under\nsuitable assumptions.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1802.09490v2", "link": "http://arxiv.org/abs/1802.09490v2"}, {"title": "Optimization Method for Interval Portfolio Selection Based on\n  Satisfaction Index of Interval inequality Relation", "summary": "In this paper we consider an interval portfolio selection problem with\nuncertain returns and introduce an inclusive concept of satisfaction index for\ninterval inequality relation. Based on the satisfaction index, we propose an\napproach to reduce the interval programming problem with uncertain objective\nand constraints into a standard linear programming problem with two parameters.\nWe showed by simulation experiment that our method is capable of helping\ninvestors to find efficient portfolios according to their preference.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1207.1932v1", "link": "http://arxiv.org/abs/1207.1932v1"}, {"title": "A case for FDI in Multi-brand retail in India", "summary": "India is ranked as the third most attractive nation for retail investment\namong emerging markets and many MNCs have been looking for the potential\nbenefits to be taken from it. The development of organized retail has the\npotential of generating employment, improvement in technology, development of\nreal estate etc. On the other hand critics of the FDI feel that allowing FDI\nwould jeopardize the unorganized retail sector and would not only adversely\naffect the small retailers and consumers but will give rise to monopolies of\nlarge corporate houses also, which can adversely affect the pricing and\navailability of goods. A case for the prospects for the same is discussed in\nthis paper.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1210.6201v1", "link": "http://arxiv.org/abs/1210.6201v1"}, {"title": "Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law\n  and the LPPLS Model", "summary": "We develop a strong diagnostic for bubbles and crashes in bitcoin, by\nanalyzing the coincidence (and its absence) of fundamental and technical\nindicators. Using a generalized Metcalfe's law based on network properties, a\nfundamental value is quantified and shown to be heavily exceeded, on at least\nfour occasions, by bubbles that grow and burst. In these bubbles, we detect a\nuniversal super-exponential unsustainable growth. We model this universal\npattern with the Log-Periodic Power Law Singularity (LPPLS) model, which\nparsimoniously captures diverse positive feedback phenomena, such as herding\nand imitation. The LPPLS model is shown to provide an ex-ante warning of market\ninstabilities, quantifying a high crash hazard and probabilistic bracket of the\ncrash time consistent with the actual corrections; although, as always, the\nprecise time and trigger (which straw breaks the camel's back) being exogenous\nand unpredictable. Looking forward, our analysis identifies a substantial but\nnot unprecedented overvaluation in the price of bitcoin, suggesting many months\nof volatile sideways bitcoin prices ahead (from the time of writing, March\n2018).", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.05663v1", "link": "http://arxiv.org/abs/1803.05663v1"}, {"title": "Affine multiple yield curve models", "summary": "We provide a general and tractable framework under which all multiple yield\ncurve modeling approaches based on affine processes, be it short rate, Libor\nmarket, or HJM modeling, can be consolidated. We model a numeraire process and\nmultiplicative spreads between Libor rates and simply compounded OIS rates as\nfunctions of an underlying affine process. Besides allowing for ordered spreads\nand an exact fit to the initially observed term structures, this general\nframework leads to tractable valuation formulas for caplets and swaptions and\nembeds all existing multi-curve affine models. The proposed approach also gives\nrise to new developments, such as a short rate type model driven by a Wishart\nprocess, for which we derive a closed-form pricing formula for caplets. The\nempirical performance of two specifications of our framework is illustrated by\ncalibration to market data.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1603.00527v2", "link": "http://arxiv.org/abs/1603.00527v2"}, {"title": "Identifying Multidiemsnional Adverse Selection Models", "summary": "In this paper, I study the nonparametric identification of a multidimensional\nadverse selection model. In particular, I consider the screening model of\nRochet and Chone (1998), where products have multiple characteristics and\nconsumers have private information about their multidimensional taste for these\ncharacteristics, and determine the data features and additional condition(s)\nthat identify model parameters. The parameters include the nonparametric joint\ndensity of consumer taste, the cost function, and the utility function, and the\ndata includes individual-level data on choices and prices paid from one market.\nWhen the utility is nonlinear in product characteristics, however, data from\none market is not enough, but with data from at least two markets, or over two\nperiods, with different marginal prices is sufficient for identification as\nlong as these price differences are due to exogenous (and binary) changes in\ncost and not because the two markets are inherently different. I also derive\nall testable conditions for a joint distribution of observed choices and prices\nto be rationalized by a model of multidimensional adverse selection.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1411.6250v7", "link": "http://arxiv.org/abs/1411.6250v7"}, {"title": "Optimal Behaviour in Solar Renewable Energy Certificate (SREC) Markets", "summary": "SREC markets are a relatively novel market-based system to incentivize the\nproduction of energy from solar means. A regulator imposes a floor on the\namount of energy each regulated firm must generate from solar power in a given\nperiod and provides them with certificates for each generated MWh. Firms offset\nthese certificates against the floor and pay a penalty for any lacking\ncertificates. Certificates are tradable assets, allowing firms to purchase/sell\nthem freely. In this work, we formulate a stochastic control problem for\ngenerating and trading in SREC markets from a regulated firm's perspective. We\naccount for generation and trading costs, the impact both have on SREC prices,\nprovide a characterization of the optimal strategy, and develop a numerical\nalgorithm to solve this control problem. Through numerical experiments, we\nexplore how a firm who acts optimally behaves under various conditions. We find\nthat an optimal firm's generation and trading behaviour can be separated into\nvarious regimes, based on the marginal benefit of obtaining an additional SREC,\nand validate our theoretical characterization of the optimal strategy. We also\nconduct parameter sensitivity experiments and conduct comparisons of the\noptimal strategy to other candidate strategies.", "category": ["q-fin.MF", "econ.GN", "q-fin.EC", "q-fin.TR"], "id": "http://arxiv.org/abs/1904.06337v6", "link": "http://arxiv.org/abs/1904.06337v6"}, {"title": "Comparative Companies' Stock Valuation through Financial Metrics", "summary": "Out of the companies, Dolby is the company with the best overall financial\nand operation health. According to the table that accounted its financial\nstatements for the past three years, Dolby has stable profit margins that\ngenerates a revenue in the billions, the only company in ten figures.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1909.06332v2", "link": "http://arxiv.org/abs/1909.06332v2"}, {"title": "Unified Growth Theory: A puzzling collection of myths based on\n  hyperbolic illusions", "summary": "The Unified Growth Theory is a puzzling collection of myths based on\nillusions created by hyperbolic distributions. Some of these myths are\ndiscussed. The examination of data shows that the three stages of growth\n(Malthusian Regime, Post-Malthusian Regime and Modern Growth Regime) did not\nexist and that Industrial Revolution had no influence on the economic growth\nand on the growth of human population. All elaborate explanations revolving\naround phantom features created by hyperbolic illusions might be fascinating\nbut they are scientifically unacceptable and, consequently, they do not explain\nthe economic growth. The data clearly indicate that the economic growth was not\nas complicated as described by the Unified Growth Theory but elegantly simple.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1311.5511v3", "link": "http://arxiv.org/abs/1311.5511v3"}, {"title": "Credit risk: Taking fluctuating asset correlations into account", "summary": "In structural credit risk models, default events and the ensuing losses are\nboth derived from the asset values at maturity. Hence it is of utmost\nimportance to choose a distribution for these asset values which is in\naccordance with empirical data. At the same time, it is desirable to still\npreserve some analytical tractability. We achieve both goals by putting forward\nan ensemble approach for the asset correlations. Consistently with the data, we\nview them as fluctuating quantities, for which we may choose the average\ncorrelation as homogeneous. Thereby we can reduce the number of parameters to\ntwo, the average correlation between assets and the strength of the\nfluctuations around this average value. Yet, the resulting asset value\ndistribution describes the empirical data well. This allows us to derive the\ndistribution of credit portfolio losses. With Monte-Carlo simulations for the\nValue at Risk and Expected Tail Loss we validate the assumptions of our\napproach and demonstrate the necessity of taking fluctuating correlations into\naccount.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1601.03015v1", "link": "http://arxiv.org/abs/1601.03015v1"}, {"title": "Stochastic Revealed Preferences with Measurement Error", "summary": "A long-standing question about consumer behavior is whether individuals'\nobserved purchase decisions satisfy the revealed preference (RP) axioms of the\nutility maximization theory (UMT). Researchers using survey or experimental\npanel data sets on prices and consumption to answer this question face the\nwell-known problem of measurement error. We show that ignoring measurement\nerror in the RP approach may lead to overrejection of the UMT. To solve this\nproblem, this paper proposes a new statistical RP framework for consumption\npanel data sets that allows for testing the UMT in the presence of measurement\nerror. Our test is applicable to all consumer models that can be characterized\nby their first-order conditions. Our approach is nonparametric, allows for\nunrestricted heterogeneity in preferences, and requires only a centering\ncondition on measurement error. We develop two applications that provide new\nevidence about the UMT. First, we find support in a survey dataset for the\ndynamic and time-consistent UMT in single-individual households, in the\npresence of \\emph{nonclassical} measurement error in consumption. In the second\napplication, we cannot reject the static UMT in a widely used experimental\ndataset where measurement error in prices is assumed to be the result of price\nmisperception due to the experimental design. The first finding stands in\ncontrast to the conclusions drawn from the deterministic RP test of Browning\n(1989). The second finding reverses the conclusions drawn from the\ndeterministic RP test of Afriat (1967) and Varian (1982)", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.05287v1", "link": "http://arxiv.org/abs/1810.05287v1"}, {"title": "Linear and nonlinear correlations in order aggressiveness of Chinese\n  stocks", "summary": "The diagonal effect of orders is well documented in different markets, which\nstates that orders are more likely to be followed by orders of the same\naggressiveness and implies the presence of short-term correlations in order\nflows. Based on the order flow data of 43 Chinese stocks, we investigate if\nthere are long-range correlations in the time series of order aggressiveness.\nThe detrending moving average analysis shows that there are crossovers in the\nscaling behaviors of overall fluctuations and order aggressiveness exhibits\nlinear long-term correlations. We design an objective procedure to determine\nthe two Hurst indexes delimited by the crossover scale. We find no correlations\nin the short term and strong correlations in the long term for all stocks\nexcept for an outlier stock. The long-term correlation is found to depend on\nseveral firm specific characteristics. We also find that there are nonlinear\nlong-term correlations in the order aggressiveness when we perform the\nmultifractal detrending moving average analysis.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1707.05604v1", "link": "http://dx.doi.org/10.1142/S0218348X17500414"}, {"title": "Option-Based Pricing of Wrong Way Risk for CVA", "summary": "The two main issues for managing wrong way risk (WWR) for the credit\nvaluation adjustment (CVA, i.e. WW-CVA) are calibration and hedging. Hence we\nstart from a novel model-free worst-case approach based on static hedging of\ncounterparty exposure with liquid options. We say \"start from\" because we\ndemonstrate that a naive worst-case approach contains hidden unrealistic\nassumptions on the variance of the hazard rate (i.e. that it is infinite). We\ncorrect this by making it an explicit (finite) parameter and present an\nefficient method for solving the parametrized model optimizing the hedges. We\nalso prove that WW-CVA is theoretically, but not practically, unbounded. The\noption-based hedges serve to significantly reduce (typically halve) practical\nWW-CVA. Thus we propose a realistic and practical option-based worst case CVA.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1609.00819v3", "link": "http://arxiv.org/abs/1609.00819v3"}, {"title": "Mathematics of Market Microstructure under Asymmetric Information", "summary": "These are the lecture notes for the summer course given for 2018 Mathematical\nFinance Summer School at Shandong Unversity. It contains a brief introduction\nto the Kyle model and the related topics in filtering, enlargement of\nfiltrations and Markov bridges.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1809.03885v1", "link": "http://arxiv.org/abs/1809.03885v1"}, {"title": "Inventory growth cycles with debt-financed investment", "summary": "We propose a continuous-time stock-flow consistent model for inventory\ndynamics in an economy with firms, banks, and households. On the supply side,\nfirms decide on production based on adaptive expectations for sales demand and\na desired level of inventories. On the demand side, investment is determined as\na function of utilization and profitability and can be financed by debt,\nwhereas consumption is independently determined as a function of income and\nwealth. Prices adjust sluggishly to both changes in labour costs and inventory.\nDisequilibrium between expected sales and demand is absorbed by unplanned\nchanges in inventory. This results in a five-dimensional dynamical system for\nwage share, employment rate, private debt ratio, expected sales, and capacity\nutilization. We analyze two limiting cases: the long-run dynamics provides a\nversion of the Keen model with effective demand and varying inventories,\nwhereas the short-run dynamics gives rise to behaviour that we interpret as\nKitchin cycles.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1610.00955v1", "link": "http://arxiv.org/abs/1610.00955v1"}, {"title": "Price Clustering and Discreteness: Is there Chaos behind the Noise?", "summary": "We investigate the \"compass rose\" (Crack, T.F. and Ledoit, O. (1996), Journal\nof Finance, 51(2), pg. 751-762) patterns revealed in phase portraits (delay\nplots) of stock returns. The structures observed in these diagrams have been\nattributed mainly to price clustering and discreteness. Using wavelet based\ndenoising, we examine the noise-free versions of a set of FTSE100 stock returns\ntime series. We reveal evidence of non-periodic cyclical dynamics. As a second\nstage we apply Surrogate Data Analysis on the original and denoised stock\nreturns. Our results suggest that there is a strong nonlinear and possibly\ndeterministic signature in the data generating processes of the stock returns\nsequences.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0407471v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.09.006"}, {"title": "Inference for Extremal Conditional Quantile Models, with an Application\n  to Market and Birthweight Risks", "summary": "Quantile regression is an increasingly important empirical tool in economics\nand other sciences for analyzing the impact of a set of regressors on the\nconditional distribution of an outcome. Extremal quantile regression, or\nquantile regression applied to the tails, is of interest in many economic and\nfinancial applications, such as conditional value-at-risk, production\nefficiency, and adjustment bands in (S,s) models. In this paper we provide\nfeasible inference tools for extremal conditional quantile models that rely\nupon extreme value approximations to the distribution of self-normalized\nquantile regression statistics. The methods are simple to implement and can be\nof independent interest even in the non-regression case. We illustrate the\nresults with two empirical examples analyzing extreme fluctuations of a stock\nreturn and extremely low percentiles of live infants' birthweights in the range\nbetween 250 and 1500 grams.", "category": ["econ.EM", "q-fin.RM"], "id": "http://arxiv.org/abs/0912.5013v1", "link": "http://dx.doi.org/10.1093/restud/rdq020"}, {"title": "Fairness through Experimentation: Inequality in A/B testing as an\n  approach to responsible design", "summary": "As technology continues to advance, there is increasing concern about\nindividuals being left behind. Many businesses are striving to adopt\nresponsible design practices and avoid any unintended consequences of their\nproducts and services, ranging from privacy vulnerabilities to algorithmic\nbias. We propose a novel approach to fairness and inclusiveness based on\nexperimentation. We use experimentation because we want to assess not only the\nintrinsic properties of products and algorithms but also their impact on\npeople. We do this by introducing an inequality approach to A/B testing,\nleveraging the Atkinson index from the economics literature. We show how to\nperform causal inference over this inequality measure. We also introduce the\nconcept of site-wide inequality impact, which captures the inclusiveness impact\nof targeting specific subpopulations for experiments, and show how to conduct\nstatistical inference on this impact. We provide real examples from LinkedIn,\nas well as an open-source, highly scalable implementation of the computation of\nthe Atkinson index and its variance in Spark/Scala. We also provide over a\nyear's worth of learnings -- gathered by deploying our method at scale and\nanalyzing thousands of experiments -- on which areas and which kinds of product\ninnovations seem to inherently foster fairness through inclusiveness.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.05819v1", "link": "http://arxiv.org/abs/2002.05819v1"}, {"title": "Characterizing Shadow Price via Lagrangian Multiplier for Nonsmooth\n  Problem", "summary": "In this paper, a relation between shadow price and the Lagrangian multiplier\nfor nonsmooth problem is explored. It is shown that the Lagrangian Multiplier\nis the upper bound of shadow price for convex optimization and a class of\nLipschtzian optimizations. This work can be used in shadow pricing for\nnonsmooth situation. The several nonsmooth functions involved in this class of\nLipschtzian optimizations is listed. Finally, an application to electricity\npricing is discussed.", "category": [], "id": "http://arxiv.org/abs/1905.13622v1", "link": "http://arxiv.org/abs/1905.13622v1"}, {"title": "Valuation of a Bermudan DB underpin hybrid pension benefit", "summary": "In this paper we consider three types of embedded options in pension benefit\ndesign.\n  The first is the Florida second election (FSE) option, offered to public\nemployees in the state of Florida in 2002. Employees were given the option to\nconvert from a defined contribution (DC) plan to a defined benefit (DB) plan at\na time of their choosing. The cost of the switch was assessed in terms of the\nABO (Accrued Benefit Obligation), which is the expected present value of the\naccrued DB pension at the time of the switch. If the ABO was greater than the\nDC account, the employee was required to fund the difference.\n  The second is the DB Underpin option, also known as a floor offset, under\nwhich the employee participates in a DC plan, but with a guaranteed minimum\nbenefit based on a traditional DB formula.\n  The third option can be considered a variation on each of the first two. We\nremove the requirement from the FSE option for employees to fund any shortfall\nat the switching date. The resulting option is very similar to the DB underpin,\nbut with the possibility of early exercise. Since we assume that exercise is\nonly permitted at discrete, annual intervals, this option is a Bermudan\nvariation on the DB Underpin.\n  We adopt an arbitrage-free pricing methodology to value the option. We\nanalyse and value the optimal switching strategy for the employee by\nconstructing an exercise frontier, and illustrate numerically the difference\nbetween the FSE, DB Underpin and Bermudan DB Underpin options.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1708.04281v1", "link": "http://arxiv.org/abs/1708.04281v1"}, {"title": "The slippage paradox", "summary": "Buying or selling assets leads to transaction costs for the investor. On one\nhand, it is well know to all market practionaires that the transaction costs\nare positive on average and present therefore systematic loss. On the other\nhand, for every trade, there is a buy side and a sell side, the total amount of\nasset and the total amount of cash is conserved. I show, that the apparently\nparadoxical observation of systematic loss of all participants is intrinsic to\nthe trading process since it corresponds to a correlation of outstanding orders\nand price changes.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1103.2214v1", "link": "http://arxiv.org/abs/1103.2214v1"}, {"title": "Utility indifference pricing and hedging for structured contracts in\n  energy markets", "summary": "In this paper we study the pricing and hedging of structured products in\nenergy markets, such as swing and virtual gas storage, using the exponential\nutility indifference pricing approach in a general incomplete multivariate\nmarket model driven by finitely many stochastic factors. The buyer of such\ncontracts is allowed to trade in the forward market in order to hedge the risk\nof his position. We fully characterize the buyer's utility indifference price\nof a given product in terms of continuous viscosity solutions of suitable\nnonlinear PDEs. This gives a way to identify reasonable candidates for the\noptimal exercise strategy for the structured product as well as for the\ncorresponding hedging strategy. Moreover, in a model with two correlated\nassets, one traded and one nontraded, we obtain a representation of the price\nas the value function of an auxiliary simpler optimization problem under a risk\nneutral probability, that can be viewed as a perturbation of the minimal\nentropy martingale measure. Finally, numerical results are provided.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1407.7725v4", "link": "http://arxiv.org/abs/1407.7725v4"}, {"title": "Fractional G-White Noise Theory, Wavelet Decomposition for Fractional\n  G-Brownian Motion, and Bid-Ask Pricing Application to Finance Under\n  Uncertainty", "summary": "G-framework is presented by Peng [41] for measure risk under uncertainty. In\nthis paper, we define fractional G-Brownian motion (fGBm). Fractional\nG-Brownian motion is a centered G-Gaussian process with zero mean and\nstationary increments in the sense of sub-linearity with Hurst index $H\\in\n(0,1)$. This process has stationary increments, self-similarity, and long rang\ndependence properties in the sense of sub-linearity. These properties make the\nfractional G-Brownian motion a suitable driven process in mathematical finance.\nWe construct wavelet decomposition of the fGBm by wavelet with compactly\nsupport. We develop fractional G-white noise theory, define G-It\\^o-Wick\nstochastic integral, establish the fractional G-It\\^o formula and the\nfractional G-Clark-Ocone formula, and derive the G-Girsanov's Theorem. For\napplication the G-white noise theory, we consider the financial market modelled\nby G-Wick-It\\^o type of SDE driven by fGBm. The financial asset price modelled\nby fGBm has volatility uncertainty, using G-Girsanov's Theorem and\nG-Clark-Ocone Theorem, we derive that sublinear expectation of the discounted\nEuropean contingent claim is the bid-ask price of the claim.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1306.4070v1", "link": "http://arxiv.org/abs/1306.4070v1"}, {"title": "Counting Defiers", "summary": "The LATE monotonicity assumption of Imbens and Angrist (1994) precludes\n\"defiers,\" individuals whose treatment always runs counter to the instrument,\nin the terminology of Balke and Pearl (1993) and Angrist et al. (1996). I allow\nfor defiers in a model with a binary instrument and a binary treatment. The\nmodel is explicit about the randomization process that gives rise to the\ninstrument. I use the model to develop estimators of the counts of defiers,\nalways takers, compliers, and never takers. I propose separate versions of the\nestimators for contexts in which the parameter of the randomization process is\nunspecified, which I intend for use with natural experiments with virtual\nrandom assignment. I present an empirical application that revisits Angrist and\nEvans (1998), which examines the impact of virtual random assignment of the sex\nof the first two children on subsequent fertility. I find that subsequent\nfertility is much more responsive to the sex mix of the first two children when\ndefiers are allowed.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.05811v1", "link": "http://arxiv.org/abs/1908.05811v1"}, {"title": "Constrained Information Design: Toolkit", "summary": "These notes show the tools in Le Treust and Tomala(2017) extend to the case\nof multiple inequality and equality constraints. This showcases the power of\nthe results in that paper to analyze problems of information design subject to\nconstraints. In fact, we show in Doval and Skreta (2018) that they can be used\nto provide an upper bound on the number of posteriors a designer with limited\ncommitment uses in his optimal mechanism.", "category": [], "id": "http://arxiv.org/abs/1811.03588v1", "link": "http://arxiv.org/abs/1811.03588v1"}, {"title": "Market Implied Probability Distributions and Bayesian Skew Estimation", "summary": "We review and illustrate how the volatility smile translates into a\nprobability distribution, the market-implied probability distribution\nrepresenting believes priced in. The effects of changes in the smile are\nexamined. Special attention is given to the effects of slope, which might\nappear at first counter-intuitive.\n  We then show how Bayesian methods can be used to deal with sparse real market\ndata. With each skew in a parametric model we associate a probability. This is\nillustrated with an example, for which multivariate parameter distributions are\nderived. We introduce the fuzzy smile (or fuzzy skew) as a visual illustration\nof the skew distribution.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0911.0805v1", "link": "http://arxiv.org/abs/0911.0805v1"}, {"title": "A continuous time random walk model for financial distributions", "summary": "We apply the formalism of the continuous time random walk to the study of\nfinancial data. The entire distribution of prices can be obtained once two\nauxiliary densities are known. These are the probability densities for the\npausing time between successive jumps and the corresponding probability density\nfor the magnitude of a jump. We have applied the formalism to data on the US\ndollar/Deutsche Mark future exchange, finding good agreement between theory and\nthe observed data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0210513v1", "link": "http://dx.doi.org/10.1103/PhysRevE.67.021112"}, {"title": "SlideVaR: a risk measure with variable risk attitudes", "summary": "To find a trade-off between profitability and prudence, financial\npractitioners need to choose appropriate risk measures. Two key points are:\nFirstly, investors' risk attitudes under uncertainty conditions should be an\nimportant reference for risk measures. Secondly, risk attitudes are not\nabsolute. For different market performance, investors have different risk\nattitudes. We proposed a new risk measure named SlideVaR which sufficiently\nreflects the different subjective attitudes of investors and the impact of\nmarket changes on investors' attitudes. We proposed the concept of risk-tail\nregion and risk-tail sub-additivity and proved that SlideVaR satisfies several\nimportant mathematical properties. Moreover, SlideVaR has a simple and\nintuitive form of expression for practical application. Several simulate and\nempirical computations show that SlideVaR has obvious advantages in markets\nwhere the state changes frequently.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1907.11855v1", "link": "http://arxiv.org/abs/1907.11855v1"}, {"title": "Persuasion with Limited Signal Spaces", "summary": "Persuasion is an exceedingly difficult task. A leading cause of this\ndifficulty is the misalignment of preferences, which is studied extensively by\nthe literature on persuasion games. However, the difficulty of communication\nalso has a first order effect on outcomes and welfare of agents. Motivated by\nthis observation, we study a model of Bayesian persuasion in which the\ncommunication between the sender and the receiver is constrained. We limit the\ncardinality of the signal space to be less than the cardinality of the action\nspace and the state space. This limits the sender's ability of making\narbitrarily many action recommendations. We prove the existence of a solution\nto the sender's utility maximization problem and characterize its properties.\nIn solving this problem, we develop a novel approach for solving Bayesian\npersuasion problems, which can be applied to a wide range of settings. We\ncharacterize the sender's willingness to pay for an additional signal as a\nfunction of the prior belief, which we interpret as the value of precise\ncommunication. We show that increased precision might not be always welfare\nimproving by showing that the receiver might prefer coarse communication.", "category": [], "id": "http://arxiv.org/abs/1910.13547v3", "link": "http://arxiv.org/abs/1910.13547v3"}, {"title": "Semiparametric correction for endogenous truncation bias with Vox Populi\n  based participation decision", "summary": "We synthesize the knowledge present in various scientific disciplines for the\ndevelopment of semiparametric endogenous truncation-proof algorithm, correcting\nfor truncation bias due to endogenous self-selection. This synthesis enriches\nthe algorithm's accuracy, efficiency and applicability. Improving upon the\ncovariate shift assumption, data are intrinsically affected and largely\ngenerated by their own behavior (cognition). Refining the concept of Vox Populi\n(Wisdom of Crowd) allows data points to sort themselves out depending on their\nestimated latent reference group opinion space. Monte Carlo simulations, based\non 2,000,000 different distribution functions, practically generating 100\nmillion realizations, attest to a very high accuracy of our model.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1902.06286v1", "link": "http://dx.doi.org/10.1109/ACCESS.2018.2888575"}, {"title": "Scaling properties and universality of first-passage time probabilities\n  in financial markets", "summary": "Financial markets provide an ideal frame for the study of crossing or\nfirst-passage time events of non-Gaussian correlated dynamics mainly because\nlarge data sets are available. Tick-by-tick data of six futures markets are\nherein considered resulting in fat tailed first-passage time probabilities. The\nscaling of the return with the standard deviation collapses the probabilities\nof all markets examined, and also for different time horizons, into single\ncurves, suggesting that first-passage statistics is market independent (at\nleast for high-frequency data). On the other hand, a very closely related\nquantity, the survival probability, shows, away from the center and tails of\nthe distribution, a hyperbolic $t^{-1/2}$ decay typical of a Markovian dynamics\nalbeit the existence of memory in markets. Modifications of the Weibull and\nStudent distributions are good candidates for the phenomenological description\nof first-passage time properties under certain regimes. The scaling strategies\nshown may be useful for risk control and algorithmic trading.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1107.1174v2", "link": "http://dx.doi.org/10.1103/PhysRevE.84.066110"}, {"title": "Reducing Estimation Risk in Mean-Variance Portfolios with Machine\n  Learning", "summary": "In portfolio analysis, the traditional approach of replacing population\nmoments with sample counterparts may lead to suboptimal portfolio choices. I\nshow that optimal portfolio weights can be estimated using a machine learning\n(ML) framework, where the outcome to be predicted is a constant and the vector\nof explanatory variables is the asset returns. It follows that ML specifically\ntargets estimation risk when estimating portfolio weights, and that\n\"off-the-shelf\" ML algorithms can be used to estimate the optimal portfolio in\nthe presence of parameter uncertainty. The framework nests the traditional\napproach and recently proposed shrinkage approaches as special cases. By\nrelying on results from the ML literature, I derive new insights for existing\napproaches and propose new estimation methods. Based on simulation studies and\nseveral datasets, I find that ML significantly reduces estimation risk compared\nto both the traditional approach and the equal weight strategy.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1804.01764v2", "link": "http://arxiv.org/abs/1804.01764v2"}, {"title": "Block Sampling under Strong Dependence", "summary": "The paper considers the block sampling method for long-range dependent\nprocesses. Our theory generalizes earlier ones by Hall, Jing and Lahiri (1998)\non functionals of Gaussian processes and Nordman and Lahiri (2005) on linear\nprocesses. In particular, we allow nonlinear transforms of linear processes.\nUnder suitable conditions on physical dependence measures, we prove the\nvalidity of the block sampling method. The problem of estimating the\nself-similar index is also studied.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1312.5807v1", "link": "http://arxiv.org/abs/1312.5807v1"}, {"title": "Optimal multi-asset trading with linear costs: a mean-field approach", "summary": "Optimal multi-asset trading with Markovian predictors is well understood in\nthe case of quadratic transaction costs, but remains intractable when these\ncosts are $L_1$. We present a mean-field approach that reduces the multi-asset\nproblem to a single-asset problem, with an effective predictor that includes a\nrisk averse component. We obtain a simple approximate solution in the case of\nOrnstein-Uhlenbeck predictors and maximum position constraints. The optimal\nstrategy is of the \"bang-bang\" type similar to that obtained in [de Lataillade\net al., 2012]. When the risk aversion parameter is small, we find that the\ntrading threshold is an affine function of the instantaneous global position,\nwith a slope coefficient that we compute exactly. We relate the risk aversion\nparameter to the desired target risk and provide numerical simulations that\nsupport our analytical results.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1905.04821v2", "link": "http://arxiv.org/abs/1905.04821v2"}, {"title": "Forecasting Time Series with VARMA Recursions on Graphs", "summary": "Graph-based techniques emerged as a choice to deal with the dimensionality\nissues in modeling multivariate time series. However, there is yet no complete\nunderstanding of how the underlying structure could be exploited to ease this\ntask. This work provides contributions in this direction by considering the\nforecasting of a process evolving over a graph. We make use of the\n(approximate) time-vertex stationarity assumption, i.e., timevarying graph\nsignals whose first and second order statistical moments are invariant over\ntime and correlated to a known graph topology. The latter is combined with VAR\nand VARMA models to tackle the dimensionality issues present in predicting the\ntemporal evolution of multivariate time series. We find out that by projecting\nthe data to the graph spectral domain: (i) the multivariate model estimation\nreduces to that of fitting a number of uncorrelated univariate ARMA models and\n(ii) an optimal low-rank data representation can be exploited so as to further\nreduce the estimation costs. In the case that the multivariate process can be\nobserved at a subset of nodes, the proposed models extend naturally to Kalman\nfiltering on graphs allowing for optimal tracking. Numerical experiments with\nboth synthetic and real data validate the proposed approach and highlight its\nbenefits over state-of-the-art alternatives.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.08581v2", "link": "http://dx.doi.org/10.1109/TSP.2019.2929930"}, {"title": "Herd Behaviors in the Stock and Foreign Exchange Markets", "summary": "The herd behaviors of returns for the won-dollar exchange rate and the KOSPI\nare analyzed in Korean financial markets. It is shown that the probability\ndistribution $P(R)$ of price returns $R$ for three values of the herding\nparameter tends to a power-law behavior $P(R) \\simeq R^{-\\beta}$ with the\nexponents $ \\beta=2.2$(the won-dollar exchange rate) and 2.4(the KOSPI). The\nfinancial crashes are found to occur at $h >2.33$ when the relative increase in\nthe probability distribution of exteremely high price returns is observed.\nEspecially, the distribution of normalized returns shows a crossover to a\nGaussian distribution for the time step $\\Delta t=252$. Our results will be\nalso compared to the other well-known analyses.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0304451v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.05.052"}, {"title": "Do your volatility smiles take care of extreme events?", "summary": "In the Black-Scholes context we consider the probability distribution\nfunction (PDF) of financial returns implied by volatility smile and we study\nthe relation between the decay of its tails and the fitting parameters of the\nsmile. We show that, considering a scaling law derived from data, it is\npossible to get a new fitting procedure of the volatility smile that considers\nalso the exponential decay of the real PDF of returns observed in the financial\nmarkets. Our study finds application in the Risk Management activities where\nthe tails characterization of financial returns PDF has a central role for the\nrisk estimation.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1010.2184v1", "link": "http://arxiv.org/abs/1010.2184v1"}, {"title": "Variance Optimal Hedging for discrete time processes with independent\n  increments. Application to Electricity Markets", "summary": "We consider the discretized version of a (continuous-time) two-factor model\nintroduced by Benth and coauthors for the electricity markets. For this model,\nthe underlying is the exponent of a sum of independent random variables. We\nprovide and test an algorithm, which is based on the celebrated\nFoellmer-Schweizer decomposition for solving the mean-variance hedging problem.\nIn particular, we establish that decomposition explicitely, for a large class\nof vanilla contingent claims. Interest is devoted in the choice of rebalancing\ndates and its impact on the hedging error, regarding the payoff regularity and\nthe non stationarity of the log-price process.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1205.4089v1", "link": "http://arxiv.org/abs/1205.4089v1"}, {"title": "A Comparison of Various Electricity Tariff Price Forecasting Techniques\n  in Turkey and Identifying the Impact of Time Series Periods", "summary": "It is very vital for suppliers and distributors to predict the deregulated\nelectricity prices for creating their bidding strategies in the competitive\nmarket area. Pre requirement of succeeding in this field, accurate and suitable\nelectricity tariff price forecasting tools are needed. In the presence of\neffective forecasting tools, taking the decisions of production, merchandising,\nmaintenance and investment with the aim of maximizing the profits and benefits\ncan be successively and effectively done. According to the electricity demand,\nthere are four various electricity tariffs pricing in Turkey; monochromic, day,\npeak and night. The objective is find the best suitable tool for predicting the\nfour pricing periods of electricity and produce short term forecasts (one year\nahead-monthly). Our approach based on finding the best model, which ensures the\nsmallest forecasting error measurements of: MAPE, MAD and MSD. We conduct a\ncomparison of various forecasting approaches in total accounts for nine teen,\nat least all of those have different aspects of methodology. Our beginning step\nwas doing forecasts for the year 2015. We validated and analyzed the\nperformance of our best model and made comparisons to see how well the\nhistorical values of 2015 and forecasted data for that specific period matched.\nResults show that given the time-series data, the recommended models provided\ngood forecasts. Second part of practice, we also include the year 2015, and\ncompute all the models with the time series of January 2011 to December 2015.\nAgain by choosing the best appropriate forecasting model, we conducted the\nforecast process and also analyze the impact of enhancing of time series\nperiods (January 2007 to December 2015) to model that we used for forecasting\nprocess.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1610.08415v1", "link": "http://arxiv.org/abs/1610.08415v1"}, {"title": "An Econophysical dynamical approach of expenditure and income\n  distribution in the UK", "summary": "We extend the exploration regarding dynamical approach of macroeconomic\nvariables by tackling systematically expenditure using Statistical Physics\nmodels (for the first time to the best of our knowledge). Also, using\npolynomial distribution which characterizes the behavior of dynamical systems\nin certain situations, we extend also our analysis to mean income data from the\nUK that span for a time interval of 35 years. We find that most of the values\nfor coefficient of determination obtained from fitting the data from\nconsecutive years analysis to be above 80%. We used for our analysis first\ndegree polynomial, but higher degree polynomials and longer time intervals\nbetween the years considered can dramatically increase goodness of the fit. As\nthis methodology was applied successfully to income and wealth, we can conclude\nthat macroeconomic systems can be treated similarly to dynamic systems from\nPhysics. Subsequently, the analysis could be extended to other macroeconomic\nindicators.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1410.3851v2", "link": "http://arxiv.org/abs/1410.3851v2"}, {"title": "General framework for a portfolio theory with non-Gaussian risks and\n  non-linear correlations", "summary": "Using a family of modified Weibull distributions, encompassing both\nsub-exponentials and super-exponentials, to parameterize the marginal\ndistributions of asset returns and their natural multivariate generalizations,\nwe give exact formulas for the tails and for the moments and cumulants of the\ndistribution of returns of a portfolio make of arbitrary compositions of these\nassets. Using combinatorial and hypergeometric functions, we are in particular\nable to extend previous results to the case where the exponents of the Weibull\ndistributions are different from asset to asset and in the presence of\ndependence between assets. We treat in details the problem of risk minimization\nusing two different measures of risks (cumulants and value-at-risk) for a\nportfolio made of two assets and compare the theoretical predictions with\ndirect empirical data. While good agreement is found, the remaining discrepancy\nbetween theory and data stems from the deviations from the Weibull\nparameterization for small returns. Our extended formulas enable us to\ndetermine analytically the conditions under which it is possible to ``have your\ncake and eat it too'', i.e., to construct a portfolio with both larger return\nand smaller ``large risks''.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/cond-mat/0103020v1", "link": "http://arxiv.org/abs/cond-mat/0103020v1"}, {"title": "Threshold-Based Portfolio: The Role of the Threshold and Its\n  Applications", "summary": "This paper aims at developing a new method by which to build a data-driven\nportfolio featuring a target risk-return. We first present a comparative study\nof recurrent neural network models (RNNs), including a simple RNN, long\nshort-term memory (LSTM), and gated recurrent unit (GRU) for selecting the best\npredictor to use in portfolio construction. The models are applied to the\ninvestment universe consisted of ten stocks in the S&P500. The experimental\nresults shows that LSTM outperforms the others in terms of hit ratio of\none-month-ahead forecasts. We then build predictive threshold-based portfolios\n(TBPs) that are subsets of the universe satisfying given threshold criteria for\nthe predicted returns. The TBPs are rebalanced monthly to restore equal weights\nto each security within the TBPs. We find that the risk and return profile of\nthe realized TBP represents a monotonically increasing frontier on the\nrisk-return plane, where the equally weighted portfolio (EWP) of all ten stocks\nplays a role in their lower bound. This shows the availability of TBPs in\ntargeting specific risk-return levels, and an EWP based on all the assets plays\na role in the reference portfolio of TBPs. In the process, thresholds play\ndominant roles in characterizing risk, return, and the prediction accuracy of\nthe subset. The TBP is more data-driven in designing portfolio target risk and\nreturn than existing ones, in the sense that it requires no prior knowledge of\nfinance such as financial assumptions, financial mathematics, or expert\ninsights. In a practical application, we present the TBP management procedure\nfor a time horizon extending over multiple time periods; we also discuss their\napplication to mean-variance portfolios to reduce estimation risk.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1709.09822v2", "link": "http://arxiv.org/abs/1709.09822v2"}, {"title": "Quantifying immediate price impact of trades based on the $k$-shell\n  decomposition of stock trading networks", "summary": "Traders in a stock market exchange stock shares and form a stock trading\nnetwork. Trades at different positions of the stock trading network may contain\ndifferent information. We construct stock trading networks based on the limit\norder book data and classify traders into $k$ classes using the $k$-shell\ndecomposition method. We investigate the influences of trading behaviors on the\nprice impact by comparing a closed national market (A-shares) with an\ninternational market (B-shares), individuals and institutions, partially filled\nand filled trades, buyer-initiated and seller-initiated trades, and trades at\ndifferent positions of a trading network. Institutional traders professionally\nuse some trading strategies to reduce the price impact and individuals at the\nsame positions in the trading network have a higher price impact than\ninstitutions. We also find that trades in the core have higher price impacts\nthan those in the peripheral shell.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1611.06666v2", "link": "http://dx.doi.org/10.1209/0295-5075/116/28006"}, {"title": "La structure du capital et la profitabilit\u00e9: Le cas des entreprises\n  industrielles fran\u00e7aises", "summary": "The objective of this article is to analyze the impact of capital structure\non profitability. This impact can be explained by three essential theories:\nsignaling theory, tax theory and the agency costs theory. A sample of 1846\nFrench industrial firms are taken over the period 1999-2006, as a dynamic panel\nstudy by using the generalized method of moments (GMM). We show that capital\nstructure has no influence on the profitability of French firms, regardless the\nsize of the company.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1212.6795v1", "link": "http://arxiv.org/abs/1212.6795v1"}, {"title": "A Link between Sequential Semi-anonymous Nonatomic Games and their Large\n  Finite Counterparts", "summary": "We show that equilibria of a sequential semi-anonymous nonatomic game (SSNG)\ncan be adopted by players in corresponding large but finite dynamic games to\nachieve near-equilibrium payoffs. Such equilibria in the form of random\nstate-to-action rules are parsimonious in form and easy to execute, as they are\nboth oblivious of past history and blind to other players' present states. Our\ntransient results can be extended to a stationary case, where the finite\ncounterparts are special discounted stochastic games. The kind of equilibria we\nadopt for SSNG are similar to distributional equilibria that are well\nunderstood in literature, and they themselves are shown to exist.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1510.06809v2", "link": "http://dx.doi.org/10.1007/s00182-016-0539-5"}, {"title": "Non-zero-sum stopping games in discrete time", "summary": "We consider two-player non-zero-sum stopping games in discrete time. Unlike\nDynkin games, in our games the payoff of each player is revealed after both\nplayers stop. Moreover, each player can adjust her own stopping strategy\naccording to the other player's action. In the first part of the paper, we\nconsider the game where players act simultaneously at each stage. We show that\nthere exists a Nash equilibrium in mixed stopping strategies. In the second\npart, we assume that one player has to act first at each stage. In this case,\nwe show the existence of a Nash equilibrium in pure stopping strategies.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1508.06032v1", "link": "http://arxiv.org/abs/1508.06032v1"}, {"title": "Continuous-Time Portfolio Optimisation for a Behavioural Investor with\n  Bounded Utility on Gains", "summary": "This paper examines an optimal investment problem in a continuous-time\n(essentially) complete financial market with a finite horizon. We deal with an\ninvestor who behaves consistently with principles of Cumulative Prospect\nTheory, and whose utility function on gains is bounded above. The\nwell-posedness of the optimisation problem is trivial, and a necessary\ncondition for the existence of an optimal trading strategy is derived. This\ncondition requires that the investor's probability distortion function on\nlosses does not tend to 0 near 0 faster than a given rate, which is determined\nby the utility function. Under additional assumptions, we show that this\ncondition is indeed the borderline for attainability, in the sense that for\nslower convergence of the distortion function there does exist an optimal\nportfolio.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1309.0362v2", "link": "http://arxiv.org/abs/1309.0362v2"}, {"title": "Interacting Default Intensity with Hidden Markov Process", "summary": "In this paper we consider a reduced-form intensity-based credit risk model\nwith a hidden Markov state process. A filtering method is proposed for\nextracting the underlying state given the observation processes. The method may\nbe applied to a wide range of problems. Based on this model, we derive the\njoint distribution of multiple default times without imposing stringent\nassumptions on the form of default intensities. Closed-form formulas for the\ndistribution of default times are obtained which are then applied to solve a\nnumber of practical problems such as hedging and pricing credit derivatives.\nThe method and numerical algorithms presented may be applicable to various\nforms of default intensities.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1603.02902v1", "link": "http://arxiv.org/abs/1603.02902v1"}, {"title": "Solvency assessment within the ORSA framework: issues and quantitative\n  methodologies", "summary": "The implementation of the Own Risk and Solvency Assessment is a critical\nissue raised by Pillar II of Solvency II framework. In particular the Overall\nSolvency Needs calculation left the Insurance companies to define an optimal\nentity-specific solvency constraint on a multi-year time horizon. In a life\ninsurance society framework, the intuitive approaches to answer this problem\ncan sometimes lead to new implementation issues linked to the highly stochastic\nnature of the methodologies used to project a company Net Asset Value over\nseveral years. One alternative approach can be the use of polynomial proxies to\nreplicate the outcomes of this variable throughout the time horizon. Polynomial\nfunctions are already considered as efficient replication methodologies for the\nNet Asset Value over 1 year. The Curve Fitting and Least Squares Monte-Carlo\nprocedures are the best-known examples of such procedures. In this article we\nintroduce a possibility of adaptation for these methodologies to be used on a\nmulti-year time horizon, in order to assess the Overall Solvency Needs.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1210.6000v2", "link": "http://arxiv.org/abs/1210.6000v2"}, {"title": "Nonparametric estimation in a regression model with additive and\n  multiplicative noise", "summary": "In this paper, we consider an unknown functional estimation problem in a\ngeneral nonparametric regression model with the characteristic of having both\nmultiplicative and additive noise. We propose two wavelet estimators, which, to\nour knowledge, are new in this general context. We prove that they achieve fast\nconvergence rates under the mean integrated square error over Besov spaces. The\nrates obtained have the particularity of being established under weak\nconditions on the model. A numerical study in a context comparable to\nstochastic frontier estimation (with the difference that the boundary is not\nnecessarily a production function) supports the theory.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1906.07695v1", "link": "http://arxiv.org/abs/1906.07695v1"}, {"title": "Herd behavior and aggregate fluctuations in financial markets", "summary": "We present a simple model of a stock market where a random communication\nstructure between agents gives rise to a heavy tails in the distribution of\nstock price variations in the form of an exponentially truncated power-law,\nsimilar to distributions observed in recent empirical studies of high frequency\nmarket data. Our model provides a link between two well-known market phenomena:\nthe heavy tails observed in the distribution of stock market returns on one\nhand and 'herding' behavior in financial markets on the other hand. In\nparticular, our study suggests a relation between the excess kurtosis observed\nin asset returns, the market order flow and the tendency of market participants\nto imitate each other.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9712318v2", "link": "http://dx.doi.org/10.1017/s1365100500015029"}, {"title": "Reinforcement learning in market games", "summary": "Financial markets investors are involved in many games -- they must interact\nwith other agents to achieve their goals. Among them are those directly\nconnected with their activity on markets but one cannot neglect other aspects\nthat influence human decisions and their performance as investors.\nDistinguishing all subgames is usually beyond hope and resource consuming. In\nthis paper we study how investors facing many different games, gather\ninformation and form their decision despite being unaware of the complete\nstructure of the game. To this end we apply reinforcement learning methods to\nthe Information Theory Model of Markets (ITMM). Following Mengel, we can try to\ndistinguish a class $\\Gamma$ of games and possible actions (strategies)\n$a^{i}_{m_{i}}$ for $i-$th agent. Any agent divides the whole class of games\ninto analogy subclasses she/he thinks are analogous and therefore adopts the\nsame strategy for a given subclass. The criteria for partitioning are based on\nprofit and costs analysis. The analogy classes and strategies are updated at\nvarious stages through the process of learning. This line of research can be\ncontinued in various directions.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0710.0114v1", "link": "http://arxiv.org/abs/0710.0114v1"}, {"title": "Computing semiparametric bounds on the expected payments of insurance\n  instruments via column generation", "summary": "It has been recently shown that numerical semiparametric bounds on the\nexpected payoff of fi- nancial or actuarial instruments can be computed using\nsemidefinite programming. However, this approach has practical limitations.\nHere we use column generation, a classical optimization technique, to address\nthese limitations. From column generation, it follows that practical univari-\nate semiparametric bounds can be found by solving a series of linear programs.\nIn addition to moment information, the column generation approach allows the\ninclusion of extra information about the random variable; for instance,\nunimodality and continuity, as well as the construction of corresponding\nworst/best-case distributions in a simple way.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1601.02149v1", "link": "http://arxiv.org/abs/1601.02149v1"}, {"title": "Superstars in politics: the role of the media in the rise and success of\n  Junichiro Koizumi", "summary": "This paper explores the role of mass media in people perceptions of\ncharismatic leaders, focusing on the case of Junichiro Koizumi, Prime Minister\nof Japan from 2001 to 2006. Using survey data collected immediately after his\n2005 landslide electoral victory, this study empirically assesses the influence\nof television and newspapers on support for Koizumi and for the most\ndistinctive policy action he announced during his campaign, the privatization\nof the postal service.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1407.1726v1", "link": "http://arxiv.org/abs/1407.1726v1"}, {"title": "The tick-by-tick dynamical consistency of price impact in limit order\n  books", "summary": "Constant price impact functions, much used in financial literature, are shown\nto give rise to paradoxical outcomes since they do not allow for proper\npredictability removal: for instance the exploitation of a single large trade\nwhose size and time of execution are known in advance to some insider leaves\nthe arbitrage opportunity unchanged, which allows arbitrage exploitation\nmultiple times. We argue that chain arbitrage exploitation should not exist,\nwhich provides an a contrario consistency criterion. Remarkably, all the stocks\ninvestigated in Paris Stock Exchange have dynamically consistent price impact\nfunctions. Both the bid-ask spread and the feedback of sequential same-side\nmarket orders onto both sides of the order book are essential to ensure\nconsistency at the smallest time scale.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0702210v3", "link": "http://arxiv.org/abs/physics/0702210v3"}, {"title": "Wealth distribution on complex networks", "summary": "We study the wealth distribution of the Bouchaud--M\\'ezard (BM) model on\ncomplex networks. It has been known that this distribution depends on the\ntopology of network by numerical simulations, however, no one have succeeded to\nexplain it. Using \"adiabatic\" and \"independent\" assumptions along with the\ncentral-limit theorem, we derive equations that determine the probability\ndistribution function. The results are compared to those of simulations for\nvarious networks. We find good agreement between our theory and the\nsimulations, except the case of Watts--Strogatz networks with a low rewiring\nrate, due to the breakdown of independent assumption.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1209.2781v2", "link": "http://dx.doi.org/10.1103/PhysRevE.86.066115"}, {"title": "Volatility options in rough volatility models", "summary": "We discuss the pricing and hedging of volatility options in some rough\nvolatility models. First, we develop efficient Monte Carlo methods and\nasymptotic approximations for computing option prices and hedge ratios in\nmodels where log-volatility follows a Gaussian Volterra process. While\nproviding a good fit for European options, these models are unable to reproduce\nthe VIX option smile observed in the market, and are thus not suitable for VIX\nproducts. To accommodate these, we introduce the class of modulated Volterra\nprocesses, and show that they successfully capture the VIX smile.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1802.01641v2", "link": "http://arxiv.org/abs/1802.01641v2"}, {"title": "European Option Pricing with Transaction Costs and Stochastic\n  Volatility: an Asymptotic Analysis", "summary": "In this paper the valuation problem of a European call option in presence of\nboth stochastic volatility and transaction costs is considered. In the limit of\nsmall transaction costs and fast mean reversion, an asymptotic expression for\nthe option price is obtained. While the dominant term in the expansion it is\nshown to be the classical Black and Scholes solution, the correction terms\nappear at $O(\\varepsilon^{1/2})$ and $O(\\varepsilon)$. The optimal hedging\nstrategy is then explicitly obtained for the Scott's model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1211.4396v1", "link": "http://arxiv.org/abs/1211.4396v1"}, {"title": "Optimal Trading of a Basket of Futures Contracts", "summary": "We study the problem of dynamically trading multiple futures contracts with\ndifferent underlying assets. To capture the joint dynamics of stochastic bases\nfor all traded futures, we propose a new model involving a multi-dimensional\nscaled Brownian bridge that is stopped before price convergence. This leads to\nthe analysis of the corresponding Hamilton-Jacobi-Bellman (HJB) equations,\nwhose solutions are derived in semi-explicit form. The resulting optimal\ntrading strategy is a long-short policy that accounts for whether the futures\nare in contango or backwardation. Our model also allows us to quantify and\ncompare the values of trading in the futures markets when the underlying assets\nare traded or not. Numerical examples are provided to illustrate the optimal\nstrategies and the effects of model parameters.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1910.04943v1", "link": "http://arxiv.org/abs/1910.04943v1"}, {"title": "Paths and indices of maximal tail dependence", "summary": "We demonstrate both analytically and numerically that the existing methods\nfor measuring tail dependence in copulas may sometimes underestimate the extent\nof extreme co-movements of dependent risks and, therefore, may not always\ncomply with the new paradigm of prudent risk management. This phenomenon holds\nin the context of both symmetric and asymmetric copulas with and without\nsingularities. As a remedy, we introduce a notion of paths of maximal (tail)\ndependence and utilize it to propose several new indices of tail dependence.\nThe suggested new indices are conservative, conform with the basic concepts of\nmodern quantitative risk management, and are able to distinguish between\ndistinct risky positions in situations when the existing indices fail to do so.", "category": ["math.PR", "q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1405.1326v3", "link": "http://arxiv.org/abs/1405.1326v3"}, {"title": "Rebuttal of \"On Nonparametric Identification of Treatment Effects in\n  Duration Models\"", "summary": "In their IZA Discussion Paper 10247, Johansson and Lee claim that the main\nresult (Proposition 3) in Abbring and Van den Berg (2003b) does not hold. We\nshow that their claim is incorrect. At a certain point within their line of\nreasoning, they make a rather basic error while transforming one random\nvariable into another random variable, and this leads them to draw incorrect\nconclusions. As a result, their paper can be discarded.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.09886v1", "link": "http://arxiv.org/abs/1907.09886v1"}, {"title": "Transfer Potentials shape and equilibrate Monetary Systems", "summary": "We analyze a monetary system of random money transfer on the basis of double\nentry bookkeeping. Without boundary conditions, we do not reach a price\nequilibrium and violate text-book formulas of economists quantity theory\n(MV=PQ). To match the resulting quantity of money with the model assumption of\na constant price, we have to impose boundary conditions. They either restrict\nspecific transfers globally or impose transfers locally. Both connect through a\ngeneral framework of transfer potentials. We show that either restricted or\nimposed transfers can shape gaussian, tent-shape exponential,\nboltzmann-exponential, pareto or periodic equilibrium distributions. We derive\nthe master equation and find its general time dependent approximate solution.\nAn equivalent of quantity theory for random money transfer under the boundary\nconditions of transfer potentials is given.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0211029v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01746-6"}, {"title": "A Theory of Non_Gaussian Option Pricing", "summary": "Option pricing formulas are derived from a non-Gaussian model of stock\nreturns. Fluctuations are assumed to evolve according to a nonlinear\nFokker-Planck equation which maximizes the Tsallis nonextensive entropy of\nindex $q$. A generalized form of the Black-Scholes differential equation is\nfound, and we derive a martingale measure which leads to closed form solutions\nfor European call options. The standard Black-Scholes pricing equations are\nrecovered as a special case ($q = 1$). The distribution of stock returns is\nwell-modelled with $q$ circa 1.5. Using that value of $q$ in the option pricing\nmodel we reproduce the volatility smile. The partial derivatives (or Greeks) of\nthe model are also calculated. Empirical results are demonstrated for options\non Japanese Yen futures. Using just one value of $\\sigma$ across strikes we\nclosely reproduce market prices, for expiration times ranging from weeks to\nseveral months.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/0205078v3", "link": "http://arxiv.org/abs/cond-mat/0205078v3"}, {"title": "The structure of the climate debate", "summary": "First-best climate policy is a uniform carbon tax which gradually rises over\ntime. Civil servants have complicated climate policy to expand bureaucracies,\npoliticians to create rents. Environmentalists have exaggerated climate change\nto gain influence, other activists have joined the climate bandwagon. Opponents\nto climate policy have attacked the weaknesses in climate research. The climate\ndebate is convoluted and polarized as a result, and climate policy complex.\nClimate policy should become easier and more rational as the Paris Agreement\nhas shifted climate policy back towards national governments. Changing\npolitical priorities, austerity, and a maturing bureaucracy should lead to a\nmore constructive climate debate.", "category": ["q-fin.EC", "econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1608.05597v1", "link": "http://arxiv.org/abs/1608.05597v1"}, {"title": "Design and Evaluation of Product Aesthetics: A Human-Machine Hybrid\n  Approach", "summary": "Aesthetics are critically important to market acceptance in many product\ncategories. In the automotive industry in particular, an improved aesthetic\ndesign can boost sales by 30% or more. Firms invest heavily in designing and\ntesting new product aesthetics. A single automotive \"theme clinic\" costs\nbetween \\$100,000 and \\$1,000,000, and hundreds are conducted annually. We use\nmachine learning to augment human judgment when designing and testing new\nproduct aesthetics. The model combines a probabilistic variational autoencoder\n(VAE) and adversarial components from generative adversarial networks (GAN),\nalong with modeling assumptions that address managerial requirements for firm\nadoption. We train our model with data from an automotive partner-7,000 images\nevaluated by targeted consumers and 180,000 high-quality unrated images. Our\nmodel predicts well the appeal of new aesthetic designs-38% improvement\nrelative to a baseline and substantial improvement over both conventional\nmachine learning models and pretrained deep learning models. New automotive\ndesigns are generated in a controllable manner for the design team to consider,\nwhich we also empirically verify are appealing to consumers. These results,\ncombining human and machine inputs for practical managerial usage, suggest that\nmachine learning offers significant opportunity to augment aesthetic design.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.07786v1", "link": "http://arxiv.org/abs/1907.07786v1"}, {"title": "Double Majority and Generalized Brexit: Explaining Counterintuitive\n  Results", "summary": "A mathematical analysis of the distribution of voting power in the Council of\nthe European Union operating according to the Treaty of Lisbon is presented. We\nstudy the effects of Brexit on the voting power of the remaining members,\nmeasured by the Penrose--Banzhaf Index. We note that the effects in question\nare non-monotonic with respect to voting weights, and that some member states\nwill lose power after Brexit. We use the normal approximation of the\nPenrose--Banzhaf Index in double-majority games to show that such\nnon-monotonicity is in most cases inherent in the double-majority system, but\nis strongly exacerbated by the peculiarities of the EU population vector.\nFurthermore, we investigate consequences of a hypothetical \"generalized\nBrexit\", i.e., NN-exit of another member state (from a 28-member Union), noting\nthat the effects on voting power are non-monotonic in most cases, but strongly\ndepend on the size of the country leaving the Union.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1812.07048v1", "link": "http://arxiv.org/abs/1812.07048v1"}, {"title": "Social learning equilibria", "summary": "We consider a large class of social learning models in which a group of\nagents face uncertainty regarding a state of the world, share the same utility\nfunction, observe private signals, and interact in a general dynamic setting.\nWe introduce Social Learning Equilibria, a static equilibrium concept that\nabstracts away from the details of the given extensive form, but nevertheless\ncaptures the corresponding asymptotic equilibrium behavior. We establish\ngeneral conditions for agreement, herding, and information aggregation in\nequilibrium, highlighting a connection between agreement and information\naggregation.", "category": [], "id": "http://arxiv.org/abs/1207.5895v4", "link": "http://dx.doi.org/10.3982/ECTA16465"}, {"title": "Modelling Extremal Dependence for Operational Risk by a Bipartite Graph", "summary": "We introduce a statistical model for operational losses based on heavy-tailed\ndistributions and bipartite graphs, which captures the event type and business\nline structure of operational risk data. The model explicitly takes into\naccount the Pareto tails of losses and the heterogeneous dependence structures\nbetween them. We then derive estimators for individual as well as aggregated\ntail risk, measured in terms of Value-at-Risk and Conditional-Tail-Expectation\nfor very high confidence levels, and provide also an asymptotically full\ncapital allocation method. Estimation methods for such tail risk measures and\ncapital allocations are also proposed and tested on simulated data. Finally, by\nhaving access to real-world operational risk losses from the Italian banking\nsystem, we show that even with a small number of observations, the proposed\nestimation methods produce reliable estimates, and that quantifying dependence\nby means of the empirical network has a big impact on estimates at both\nindividual and aggregate level, as well as for capital allocations.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1902.03041v1", "link": "http://arxiv.org/abs/1902.03041v1"}, {"title": "On a Heath-Jarrow-Morton approach for stock options", "summary": "This paper aims at transferring the philosophy behind Heath-Jarrow-Morton to\nthe modelling of call options with all strikes and maturities. Contrary to the\napproach by Carmona and Nadtochiy (2009) and related to the recent contribution\nCarmona and Nadtochiy (2012) by the same authors, the key parametrisation of\nour approach involves time-inhomogeneous L\\'evy processes instead of local\nvolatility models. We provide necessary and sufficient conditions for absence\nof arbitrage. Moreover we discuss the construction of arbitrage-free models.\nSpecifically, we prove their existence and uniqueness given basic building\nblocks.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1305.5621v2", "link": "http://arxiv.org/abs/1305.5621v2"}, {"title": "On Value at Risk for foreign exchange rates - the copula approach", "summary": "The aim of this paper is to determine the Value at Risk (VaR) of the\nportfolio consisting of long positions in foreign currencies on an emerging\nmarket. Basing on empirical data we restrict ourselves to the case when the\ntail parts of distributions of logarithmic returns of these assets follow the\npower laws and the lower tail of associated copula C follows the power law of\ndegree 1. We will illustrate the practical usefulness of this approach by the\nanalysis of the exchange rates of EUR and CHF at the Polish forex market.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/physics/0608190v1", "link": "http://arxiv.org/abs/physics/0608190v1"}, {"title": "Tobit Bayesian Model Averaging and the Determinants of Foreign Direct\n  Investment", "summary": "We develop a fully Bayesian, computationally efficient framework for\nincorporating model uncertainty into Type II Tobit models and apply this to the\ninvestigation of the determinants of Foreign Direct Investment (FDI). While\ndirect evaluation of modelprobabilities is intractable in this setting, we show\nthat by using conditional Bayes Factors, which nest model moves inside a Gibbs\nsampler, we are able to incorporate model uncertainty in a straight-forward\nfashion. We conclude with a study of global FDI flows between 1988-2000.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1205.2501v1", "link": "http://arxiv.org/abs/1205.2501v1"}, {"title": "Forecasting the sustainable status of the labor market in agriculture", "summary": "In this article, a game-theoretic model is constructed that is related to the\nproblem of optimal assignments. Examples are considered. A compromise point is\nfound, the Nash equilibriums and the decision of the Nash arbitration scheme\nare constructed.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1805.09686v1", "link": "http://arxiv.org/abs/1805.09686v1"}, {"title": "Why Finnish polytechnics reject top applicants", "summary": "I use a panel of higher education clearinghouse data to study the centralized\nassignment of applicants to Finnish polytechnics. I show that on a yearly\nbasis, large numbers of top applicants unnecessarily remain unassigned to any\nprogram. There are programs which rejected applicants would find acceptable,\nbut the assignment mechanism both discourages applicants from applying, and\nstops programs from admitting those who do. A mechanism which would admit each\nyear's most eligible applicants has the potential to substantially reduce\nre-applications, thereby shortening the long queues into Finnish higher\neducation.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.05443v1", "link": "http://arxiv.org/abs/1908.05443v1"}, {"title": "Formalizing the Cox-Ross-Rubinstein pricing of European derivatives in\n  Isabelle/HOL", "summary": "We formalize in the proof assistant Isabelle essential basic notions and\nresults in financial mathematics. We provide generic formal definitions of\nconcepts such as markets, portfolios, derivative products, arbitrages or fair\nprices, and we show that, under the usual no-arbitrage condition, the existence\nof a replicating portfolio for a derivative implies that the latter admits a\nunique fair price. Then, we provide a formalization of the Cox-Rubinstein model\nand we show that the market is complete in this model, i.e., that every\nderivative product admits a replicating portfolio. This entails that in this\nmodel, every derivative product admits a unique fair price.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1807.09873v2", "link": "http://arxiv.org/abs/1807.09873v2"}, {"title": "Geometric Step Options with Jumps. Parity Relations, PIDEs, and\n  Semi-Analytical Pricing", "summary": "The present article studies geometric step options in exponential L\\'evy\nmarkets. Our contribution is manifold and extends several aspects of the\ngeometric step option pricing literature. First, we provide symmetry and parity\nrelations and derive various characterizations for both European-type and\nAmerican-type geometric double barrier step options. In particular, we are able\nto obtain a jump-diffusion disentanglement for the early exercise premium of\nAmerican-type geometric double barrier step contracts and its\nmaturity-randomized equivalent as well as to characterize the diffusion and\njump contributions to these early exercise premiums separately by means of\npartial integro-differential equations and ordinary integro-differential\nequations. As an application of our characterizations, we derive\nsemi-analytical pricing results for (regular) European-type and American-type\ngeometric down-and-out step call options under hyper-exponential jump-diffusion\nmodels. Lastly, we use the latter results to discuss the early exercise\nstructure of geometric step options once jumps are added and to subsequently\nprovide an analysis of the impact of jumps on the price and hedging parameters\nof (European-type and American-type) geometric step contracts.", "category": ["q-fin.MF", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/2002.09911v1", "link": "http://arxiv.org/abs/2002.09911v1"}, {"title": "Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper\n  Scoring Rules", "summary": "In recent years, probabilistic forecasting is an emerging topic, which is why\nthere is a growing need of suitable methods for the evaluation of multivariate\npredictions. We analyze the sensitivity of the most common scoring rules,\nespecially regarding quality of the forecasted dependency structures.\nAdditionally, we propose scoring rules based on the copula, which uniquely\ndescribes the dependency structure for every probability distribution with\ncontinuous marginal distributions. Efficient estimation of the considered\nscoring rules and evaluation methods such as the Diebold-Mariano test are\ndiscussed. In detailed simulation studies, we compare the performance of the\nrenowned scoring rules and the ones we propose. Besides extended synthetic\nstudies based on recently published results we also consider a real data\nexample. We find that the energy score, which is probably the most widely used\nmultivariate scoring rule, performs comparably well in detecting forecast\nerrors, also regarding dependencies. This contradicts other studies. The\nresults also show that a proposed copula score provides very strong distinction\nbetween models with correct and incorrect dependency structure. We close with a\ncomprehensive discussion on the proposed methodology.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.07325v1", "link": "http://arxiv.org/abs/1910.07325v1"}, {"title": "Cities and space: Common power laws and spatial fractal structures", "summary": "City size distributions are known to be well approximated by power laws\nacross a wide range of countries. But such distributions are also meaningful at\nother spatial scales, such as within certain regions of a country. Using data\nfrom China, France, Germany, India, Japan, and the US, we first document that\nlarge cities are significantly more spaced out than would be expected by chance\nalone. We next construct spatial hierarchies for countries by first\npartitioning geographic space using a given number of their largest cities as\ncell centers, and then continuing this partitioning procedure within each cell\nrecursively. We find that city size distributions in different parts of these\nspatial hierarchies exhibit power laws that are again far more similar than\nwould be expected by chance alone -- suggesting the existence of a spatial\nfractal structure.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.12289v1", "link": "http://arxiv.org/abs/1907.12289v1"}, {"title": "Entropy of the Nordic electricity market: anomalous scaling, spikes, and\n  mean-reversion", "summary": "The electricity market is a very peculiar market due to the large variety of\nphenomena that can affect the spot price. However, this market still shows many\ntypical features of other speculative (commodity) markets like, for instance,\ndata clustering and mean reversion. We apply the diffusion entropy analysis\n(DEA) to the Nordic spot electricity market (Nord Pool). We study the waiting\ntime statistics between consecutive spot price spikes and find it to show\nanomalous scaling characterized by a decaying power-law. The exponent observed\nin data follows a quite robust relationship with the one implied by the DEA\nanalysis. We also in terms of the DEA revisit topics like clustering,\nmean-reversion and periodicities. We finally propose a GARCH inspired model but\nfor the price itself. Models in the context of stochastic volatility processes\nappear under this scope to have a feasible description.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0609066v1", "link": "http://dx.doi.org/10.1088/1742-5468/2006/11/P11011"}, {"title": "Distributional Mellin calculus in $\\mathbb{C}^n$, with applications to\n  option pricing", "summary": "We discuss several aspects of Mellin transform, including distributional\nMellin transform and inversion of multiple Mellin-Barnes integrals in\n$\\mathbb{C}^n$ and its connection to residue expansion or evaluation of Laplace\nintegrals. These mathematical concepts are demonstrated on several\noption-pricing models. This includes European option models such as\nBlack-Scholes or fractional-diffusion models, as well as evaluation of\nquantities related to the optimal exercise price of American options.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1611.03239v3", "link": "http://arxiv.org/abs/1611.03239v3"}, {"title": "Topological identification in networks of dynamical systems", "summary": "The paper deals with the problem of reconstructing the topological structure\nof a network of dynamical systems. A distance function is defined in order to\nevaluate the \"closeness\" of two processes and a few useful mathematical\nproperties are derived. Theoretical results to guarantee the correctness of the\nidentification procedure for networked linear systems with tree topology are\nprovided as well. Finally, the application of the techniques to the analysis of\nan actual complex network, i.e. to high frequency time series of the stock\nmarket, is illustrated.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0804.2441v2", "link": "http://arxiv.org/abs/0804.2441v2"}, {"title": "Optimal mean-variance investment strategy under value-at-risk\n  constraints", "summary": "This paper is devoted to study the effects arising from imposing a\nvalue-at-risk (VaR) constraint in mean-variance portfolio selection problem for\nan investor who receives a stochastic cash flow which he/she must then invest\nin a continuous-time financial market. For simplicity, we assume that there is\nonly one investment opportunity available for the investor, a risky stock.\nUsing techniques of stochastic linear-quadratic (LQ) control, the optimal\nmean-variance investment strategy with and without VaR constraint are derived\nexplicitly in closed forms, based on solution of corresponding\nHamilton-Jacobi-Bellman (HJB) equation. Furthermore, some numerical examples\nare proposed to show how the addition of the VaR constraint affects the optimal\nstrategy.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1011.4991v1", "link": "http://arxiv.org/abs/1011.4991v1"}, {"title": "Portfolio optimization in the case of an asset with a given liquidation\n  time distribution", "summary": "Management of the portfolios containing low liquidity assets is a tedious\nproblem. The buyer proposes the price that can differ greatly from the paper\nvalue estimated by the seller, the seller, on the other hand, can not liquidate\nhis portfolio instantly and waits for a more favorable offer. To minimize\nlosses in this case we need to develop new methods. One of the steps moving the\ntheory towards practical needs is to take into account the time lag of the\nliquidation of an illiquid asset. This task became especially significant for\nthe practitioners in the time of the global financial crises. Working in the\nMerton's optimal consumption framework with continuous time we consider an\noptimization problem for a portfolio with an illiquid, a risky and a risk-free\nasset. While a standard Black-Scholes market describes the liquid part of the\ninvestment the illiquid asset is sold at a random moment with prescribed\nliquidation time distribution. In the moment of liquidation it generates\nadditional liquid wealth dependent on illiquid assets paper value. The investor\nhas the logarithmic utility function as a limit case of a HARA-type utility.\nDifferent distributions of the liquidation time of the illiquid asset are under\nconsideration - a classical exponential distribution and Weibull distribution\nthat is more practically relevant. Under certain conditions we show the\nexistence of the viscosity solution in both cases. Applying numerical methods\nwe compare classical Merton's strategies and the optimal consumption-allocation\nstrategies for portfolios with different liquidation-time distributions of an\nilliquid asset.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1407.3154v1", "link": "http://arxiv.org/abs/1407.3154v1"}, {"title": "Multidimensional Quasi-Monte Carlo Malliavin Greeks", "summary": "We investigate the use of Malliavin calculus in order to calculate the Greeks\nof multidimensional complex path-dependent options by simulation. For this\npurpose, we extend the formulas employed by Montero and Kohatsu-Higa to the\nmultidimensional case. The multidimensional setting shows the convenience of\nthe Malliavin Calculus approach over different techniques that have been\npreviously proposed. Indeed, these techniques may be computationally expensive\nand do not provide flexibility for variance reduction. In contrast, the\nMalliavin approach exhibits a higher flexibility by providing a class of\nfunctions that return the same expected value (the Greek) with different\naccuracies. This versatility for variance reduction is not possible without the\nuse of the generalized integral by part formula of Malliavin Calculus. In the\nmultidimensional context, we find convenient formulas that permit to improve\nthe localization technique, introduced in Fourni\\'e et al and reduce both the\ncomputational cost and the variance. Moreover, we show that the parameters\nemployed for variance reduction can be obtained \\textit{on the flight} in the\nsimulation. We illustrate the efficiency of the proposed procedures, coupled\nwith the enhanced version of Quasi-Monte Carlo simulations as discussed in\nSabino, for the numerical estimation of the Deltas of call, digital Asian-style\nand Exotic basket options with a fixed and a floating strike price in a\nmultidimensional Black-Scholes market.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1103.5722v1", "link": "http://dx.doi.org/10.1007/s10203-011-0125-z"}, {"title": "Central Counterparty Risk", "summary": "A clearing member of a Central Counterparty (CCP) is exposed to losses on\ntheir default fund and initial margin contributions. Such losses can be\nincurred whenever the CCP has insufficient funds to unwind the portfolio of a\ndefaulting clearing member. This does not necessarily require the default of\nthe CCP itself. In this note we aim to quantify the risk a financial\ninstitution has when facing a CCP.\n  We show that a clearing member's CCP risk is given by a sum of exposures to\neach of the other clearing members. This arises because of the implicit default\ninsurance that each member has provided in the form of mutualised, loss sharing\ncollateral. We calculate the exposures by explicitly modeling the capital\nstructure of a CCP as well as the loss distributions of the individual member\nportfolios.\n  An important consideration in designing the model is the limited transparency\nwith respect to the portfolio composition and collateral levels of individual\nclearing members. To overcome this we leverage the fact that, for a typical\nCCP, margin levels are risk-based. In particular, we parameterise the portfolio\nloss tail as a Pareto distribution and we calibrate this to the CCP defined\nprobability of losses exceeding the posted initial margin levels.\n  A key aspect of the model is that we explicitly take into account wrong-way\nrisk, i.e. the fact that member defaults are more likely to occur in stressed\nmarket conditions, as well as potential contagion between a member's default\nand the losses on his portfolio.", "category": ["q-fin.RM", "q-fin.PR"], "id": "http://arxiv.org/abs/1205.1533v1", "link": "http://arxiv.org/abs/1205.1533v1"}, {"title": "Tails of correlation mixtures of elliptical copulas", "summary": "Correlation mixtures of elliptical copulas arise when the correlation\nparameter is driven itself by a latent random process. For such copulas, both\npenultimate and asymptotic tail dependence are much larger than for ordinary\nelliptical copulas with the same unconditional correlation. Furthermore, for\nGaussian and Student t-copulas, tail dependence at sub-asymptotic levels is\ngenerally larger than in the limit, which can have serious consequences for\nestimation and evaluation of extreme risk. Finally, although correlation\nmixtures of Gaussian copulas inherit the property of asymptotic independence,\nat the same time they fall in the newly defined category of near asymptotic\ndependence. The consequences of these findings for modeling are assessed by\nmeans of a simulation study and a case study involving financial time series.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/0912.3516v1", "link": "http://arxiv.org/abs/0912.3516v1"}, {"title": "Quantifying the Coherence of Development Policy Priorities", "summary": "Over the last 30 years, the concept of policy coherence for development has\nreceived especial attention among academics, practitioners and international\norganizations. However, its quantification and measurement remain elusive. To\naddress this challenge, we develop a theoretical and empirical framework to\nmeasure the coherence of policy priorities for development. Our procedure takes\ninto account the country-specific constraints that governments face when trying\nto reach specific development goals. Hence, we put forward a new definition of\npolicy coherence where context-specific efficient resource allocations are\nemployed as the baseline to construct an index. To demonstrate the usefulness\nand validity of our index, we analyze the cases of Mexico, Korea and Estonia,\nthree developing countries that, arguably, joined the OECD with the aim of\ncoherently establishing policies that could enable a catch-up process. We find\nthat Korea shows significant signs of policy coherence, Estonia seems to be in\nthe process of achieving it, and Mexico has unequivocally failed. Furthermore,\nour results highlight the limitations of assessing coherence in terms of naive\nbenchmark comparisons using development-indicator data. Altogether, our\nframework sheds new light in a promising direction to develop bespoke analytic\ntools to meet the 2030 agenda.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1902.00430v1", "link": "http://arxiv.org/abs/1902.00430v1"}, {"title": "Program Evaluation with Right-Censored Data", "summary": "In a unified framework, we provide estimators and confidence bands for a\nvariety of treatment effects when the outcome of interest, typically a\nduration, is subjected to right censoring. Our methodology accommodates\naverage, distributional, and quantile treatment effects under different\nidentifying assumptions including unconfoundedness, local treatment effects,\nand nonlinear differences-in-differences. The proposed estimators are easy to\nimplement, have close-form representation, are fully data-driven upon\nestimation of nuisance parameters, and do not rely on parametric distributional\nassumptions, shape restrictions, or on restricting the potential treatment\neffect heterogeneity across different subpopulations. These treatment effects\nresults are obtained as a consequence of more general results on two-step\nKaplan-Meier estimators that are of independent interest: we provide conditions\nfor applying (i) uniform law of large numbers, (ii) functional central limit\ntheorems, and (iii) we prove the validity of the ordinary nonparametric\nbootstrap in a two-step estimation procedure where the outcome of interest may\nbe randomly censored.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1604.02642v1", "link": "http://arxiv.org/abs/1604.02642v1"}, {"title": "Impact of Investor's Varying Risk Aversion on the Dynamics of Asset\n  Price Fluctuations", "summary": "While the investors' responses to price changes and their price forecasts are\nwell accepted major factors contributing to large price fluctuations in\nfinancial markets, our study shows that investors' heterogeneous and dynamic\nrisk aversion (DRA) preferences may play a more critical role in the dynamics\nof asset price fluctuations. We propose and study a model of an artificial\nstock market consisting of heterogeneous agents with DRA, and we find that DRA\nis the main driving force for excess price fluctuations and the associated\nvolatility clustering. We employ a popular power utility function,\n$U(c,\\gamma)=\\frac{c^{1-\\gamma}-1}{1-\\gamma}$ with agent specific and\ntime-dependent risk aversion index, $\\gamma_i(t)$, and we derive an approximate\nformula for the demand function and aggregate price setting equation. The\ndynamics of each agent's risk aversion index, $\\gamma_i(t)$ (i=1,2,...,N), is\nmodeled by a bounded random walk with a constant variance $\\delta^2$. We show\nnumerically that our model reproduces most of the ``stylized'' facts observed\nin the real data, suggesting that dynamic risk aversion is a key mechanism for\nthe emergence of these stylized facts.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0506224v1", "link": "http://arxiv.org/abs/physics/0506224v1"}, {"title": "Shape-Enforcing Operators for Point and Interval Estimators", "summary": "A common problem in econometrics, statistics, and machine learning is to\nestimate and make inference on functions that satisfy shape restrictions. For\nexample, distribution functions are nondecreasing and range between zero and\none, height growth charts are nondecreasing in age, and production functions\nare nondecreasing and quasi-concave in input quantities. We propose a method to\nenforce these restrictions ex post on point and interval estimates of the\ntarget function by applying functional operators. If an operator satisfies\ncertain properties that we make precise, the shape-enforced point estimates are\ncloser to the target function than the original point estimates and the\nshape-enforced interval estimates have greater coverage and shorter length than\nthe original interval estimates. We show that these properties hold for six\ndifferent operators that cover commonly used shape restrictions in practice:\nrange, convexity, monotonicity, monotone convexity, quasi-convexity, and\nmonotone quasi-convexity. We illustrate the results with two empirical\napplications to the estimation of a height growth chart for infants in India\nand a production function for chemical firms in China.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1809.01038v3", "link": "http://arxiv.org/abs/1809.01038v3"}, {"title": "Theory of Information Pricing", "summary": "In financial markets valuable information is rarely circulated homogeneously,\nbecause of time required for information to spread. However, advances in\ncommunication technology means that the 'lifetime' of important information is\ntypically short. Hence, viewed as a tradable asset, information shares the\ncharacteristics of a perishable commodity: while it can be stored and\ntransmitted freely, its worth diminishes rapidly in time. In view of recent\ndevelopments where internet search engines and other information providers are\noffering information to financial institutions, the problem of pricing\ninformation is becoming increasingly important. With this in mind, a new\nformulation of utility-indifference argument is introduced and used as a basis\nfor pricing information. Specifically, we regard information as a quantity that\nconverts a prior distribution into a posterior distribution. The amount of\ninformation can then be quantified by relative entropy. The key to our utility\nindifference argument is to equate the maximised a posterior utility, after\npaying certain cost for the information, with the a posterior expectation of\nthe utility based on the a priori optimal strategy. This formulation leads to\none price for a given quantity of upside information; and another price for a\ngiven quantity of downside information. The ideas are illustrated by means of\nsimple examples.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1106.5706v2", "link": "http://arxiv.org/abs/1106.5706v2"}, {"title": "Inference on Estimators defined by Mathematical Programming", "summary": "We propose an inference procedure for estimators defined by mathematical\nprogramming problems, focusing on the important special cases of linear\nprogramming (LP) and quadratic programming (QP). In these settings, the\ncoefficients in both the objective function and the constraints of the\nmathematical programming problem may be estimated from data and hence involve\nsampling error. Our inference approach exploits the characterization of the\nsolutions to these programming problems by complementarity conditions; by doing\nso, we can transform the problem of doing inference on the solution of a\nconstrained optimization problem (a non-standard inference problem) into one\ninvolving inference based on a set of inequalities with pre-estimated\ncoefficients, which is much better understood. We evaluate the performance of\nour procedure in several Monte Carlo simulations and an empirical application\nto the classic portfolio selection problem in finance.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1709.09115v1", "link": "http://arxiv.org/abs/1709.09115v1"}, {"title": "Social Integration in Two-Sided Matching Markets", "summary": "When several two-sided matching markets merge into one, it is inevitable that\nsome agents will become worse off if the matching mechanism used is stable. I\nformalize this observation by defining the property of integration\nmonotonicity, which requires that every agent becomes better off after any\nnumber of matching markets merge. Integration monotonicity is also incompatible\nwith the weaker efficiency property of Pareto optimality.\n  Nevertheless, I obtain two possibility results. First, stable matching\nmechanisms never hurt more than one-half of the society after the integration\nof several matching markets occurs. Second, in random matching markets there\nare positive expected gains from integration for both sides of the market,\nwhich I quantify.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1705.08033v5", "link": "http://dx.doi.org/10.1016/j.jmateco.2018.08.003"}, {"title": "Digital Economy And Society. A Cross Country Comparison Of Hungary And\n  Ukraine", "summary": "We live in the Digital Age in which both economy and society have been\ntransforming significantly. The Internet and the connected digital devices are\ninseparable parts of our daily life and the engine of the economic growth. In\nthis paper, first I analyzed the status of digital economy and society in\nHungary, then compared it with Ukraine and made conclusions regarding the\nfuture development tendencies. Using secondary data provided by the European\nCommission I investigated the five components of the Digital Economy and\nSociety Index of Hungary. I performed cross country analysis to find out the\nsignificant differences between Ukraine and Hungary in terms of access to the\nInternet and device use including smartphones, computers and tablets. Based on\nmy findings, I concluded that Hungary is more developed in terms of the\nsignificant parameters of the digital economy and society than Ukraine, but\neven Hungary is an emerging digital nation. Considering the high growth rate of\nInternet, tablet and smartphone penetration in both countries, I expect faster\nprogress in the development of the digital economy and society in Hungary and\nUkraine.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.00283v1", "link": "http://arxiv.org/abs/1901.00283v1"}, {"title": "Optimal Convergence Trading", "summary": "This article examines arbitrage investment in a mispriced asset when the\nmispricing follows the Ornstein-Uhlenbeck process and a credit-constrained\ninvestor maximizes a generalization of the Kelly criterion. The optimal\ndifferentiable and threshold policies are derived. The optimal differentiable\npolicy is linear with respect to mispricing and risk-free in the long run. The\noptimal threshold policy calls for investing immediately when the mispricing is\ngreater than zero with the investment amount inversely proportional to the risk\naversion parameter. The investment is risky even in the long run. The results\nare consistent with the belief that credit-constrained arbitrageurs should be\nrisk-neutral if they are to engage in convergence trading.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/math/0302104v2", "link": "http://arxiv.org/abs/math/0302104v2"}, {"title": "Automatic Financial Trading Agent for Low-risk Portfolio Management\n  using Deep Reinforcement Learning", "summary": "The autonomous trading agent is one of the most actively studied areas of\nartificial intelligence to solve the capital market portfolio management\nproblem. The two primary goals of the portfolio management problem are\nmaximizing profit and restrainting risk. However, most approaches to this\nproblem solely take account of maximizing returns. Therefore, this paper\nproposes a deep reinforcement learning based trading agent that can manage the\nportfolio considering not only profit maximization but also risk restraint. We\nalso propose a new target policy to allow the trading agent to learn to prefer\nlow-risk actions. The new target policy can be reflected in the update by\nadjusting the greediness for the optimal action through the hyper parameter.\nThe proposed trading agent verifies the performance through the data of the\ncryptocurrency market. The Cryptocurrency market is the best test-ground for\ntesting our trading agents because of the huge amount of data accumulated every\nminute and the market volatility is extremely large. As a experimental result,\nduring the test period, our agents achieved a return of 1800% and provided the\nleast risky investment strategy among the existing methods. And, another\nexperiment shows that the agent can maintain robust generalized performance\neven if market volatility is large or training period is short.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1909.03278v1", "link": "http://arxiv.org/abs/1909.03278v1"}, {"title": "Mean--variance portfolio optimization when means and covariances are\n  unknown", "summary": "Markowitz's celebrated mean--variance portfolio optimization theory assumes\nthat the means and covariances of the underlying asset returns are known. In\npractice, they are unknown and have to be estimated from historical data.\nPlugging the estimates into the efficient frontier that assumes known\nparameters has led to portfolios that may perform poorly and have\ncounter-intuitive asset allocation weights; this has been referred to as the\n\"Markowitz optimization enigma.\" After reviewing different approaches in the\nliterature to address these difficulties, we explain the root cause of the\nenigma and propose a new approach to resolve it. Not only is the new approach\nshown to provide substantial improvements over previous methods, but it also\nallows flexible modeling to incorporate dynamic features and fundamental\nanalysis of the training sample of historical data, as illustrated in\nsimulation and empirical studies.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1108.0996v1", "link": "http://dx.doi.org/10.1214/10-AOAS422"}, {"title": "Robust Nearly-Efficient Estimation of Large Panels with Factor\n  Structures", "summary": "This paper studies estimation of linear panel regression models with\nheterogeneous coefficients, when both the regressors and the residual contain a\npossibly common, latent, factor structure. Our theory is (nearly) efficient,\nbecause based on the GLS principle, and also robust to the specification of\nsuch factor structure because it does not require any information on the number\nof factors nor estimation of the factor structure itself. We first show how the\nunfeasible GLS estimator not only affords an efficiency improvement but, more\nimportantly, provides a bias-adjusted estimator with the conventional limiting\ndistribution, for situations where the OLS is affected by a first-order bias.\nThe technical challenge resolved in the paper is to show how these properties\nare preserved for a class of feasible GLS estimators in a double-asymptotics\nsetting. Our theory is illustrated by means of Monte Carlo exercises and, then,\nwith an empirical application using individual asset returns and firms'\ncharacteristics data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1902.11181v1", "link": "http://arxiv.org/abs/1902.11181v1"}, {"title": "Multifractal cross wavelet analysis", "summary": "Complex systems are composed of mutually interacting components and the\noutput values of these components are usually long-range cross-correlated. We\npropose a method to characterize the joint multifractal nature of such\nlong-range cross correlations based on wavelet analysis, termed multifractal\ncross wavelet analysis (MFXWT). We assess the performance of the MFXWT method\nby performing extensive numerical experiments on the dual binomial measures\nwith multifractal cross correlations and the bivariate fractional Brownian\nmotions (bFBMs) with monofractal cross correlations. For binomial multifractal\nmeasures, the empirical joint multifractality of MFXWT is found to be in\napproximate agreement with the theoretical formula. For bFBMs, MFXWT may\nprovide spurious multifractality because of the wide spanning range of the\nmultifractal spectrum. We also apply the MFXWT method to stock market indexes\nand uncover intriguing joint multifractal nature in pairs of index returns and\nvolatilities.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1610.09519v2", "link": "http://dx.doi.org/10.1142/S0218348X17500542"}, {"title": "Econophysics Macroeconomic Model", "summary": "This paper presents macroeconomic model that is based on parallels between\nmacroeconomic multi-agent systems and multi-particle systems. We use risk\nratings of economic agents as their coordinates on economic space. Aggregates\nof economic or financial variables like Investment, Assets, Demand, Credits and\netc. of economic agents near point x define corresponding macroeconomic\nvariables as functions of time t and coordinates x on economic space. Parallels\nbetween multi-agent and multi-particle systems on economic space allow describe\ntransition from economic kinetic-like to economic hydrodynamic-like\napproximation and derive macroeconomic hydrodynamic-like equations on economic\nspace. Economic or financial transactions between economic agents determine\nevolution of macroeconomic variables This paper describes local macroeconomic\napproximation that takes into account transactions between economic agents with\ncoordinates near same point x on economic space only and describes interaction\nbetween macroeconomic variables by linear differential operators. For simple\nmodel of interaction between macroeconomic variables as Demand on Investment\nand Interest Rate we derive hydrodynamic-like equations in a closed form. For\nperturbations of these macroeconomic variables we derive macroeconomic wave\nequations. Macroeconomic waves on economic space can propagate with exponential\ngrowth of amplitude and cause irregular time fluctuations of macroeconomic\nvariables or induce economic crises.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1701.06625v1", "link": "http://arxiv.org/abs/1701.06625v1"}, {"title": "Equilibrium Effects of Intraday Order-Splitting Benchmarks", "summary": "This paper presents a continuous-time model of intraday trading, pricing, and\nliquidity with dynamic TWAP and VWAP benchmarks. The model is solved in\nclosed-form for the competitive equilibrium and also for non-price-taking\nequilibria. The intraday trajectories of TWAP trading targets cause predictable\nintraday patterns of price pressure, and randomness in VWAP target trajectories\ninduces additional randomness in intraday price-pressure patterns. TWAP and\nVWAP trading both reduce market liquidity and increase price volatility\nrelative to just terminal trading targets alone. The model is computationally\ntractable, which lets us provide a number of numerical illustrations.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1803.08336v6", "link": "http://arxiv.org/abs/1803.08336v6"}, {"title": "The impact of air transport availability on research collaboration: A\n  case study of four universities", "summary": "This paper analyzes the impact of air transport connectivity and\naccessibility on scientific collaboration.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.02106v1", "link": "http://arxiv.org/abs/1811.02106v1"}, {"title": "Mesoscopic Community Structure of Financial Markets Revealed by Price\n  and Sign Fluctuations", "summary": "The mesoscopic organization of complex systems, from financial markets to the\nbrain, is an intermediate between the microscopic dynamics of individual units\n(stocks or neurons, in the mentioned cases), and the macroscopic dynamics of\nthe system as a whole. The organization is determined by \"communities\" of units\nwhose dynamics, represented by time series of activity, is more strongly\ncorrelated internally than with the rest of the system. Recent studies have\nshown that the binary projections of various financial and neural time series\nexhibit nontrivial dynamical features that resemble those of the original data.\nThis implies that a significant piece of information is encoded into the binary\nprojection (i.e. the sign) of such increments. Here, we explore whether the\nbinary signatures of multiple time series can replicate the same complex\ncommunity organization of the financial market, as the original weighted time\nseries. We adopt a method that has been specifically designed to detect\ncommunities from cross-correlation matrices of time series data. Our analysis\nshows that the simpler binary representation leads to a community structure\nthat is almost identical with that obtained using the full weighted\nrepresentation. These results confirm that binary projections of financial time\nseries contain significant structural information.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1504.00590v1", "link": "http://dx.doi.org/10.1371/journal.pone.0133679"}, {"title": "Limitations of P-Values and $R^2$ for Stepwise Regression Building: A\n  Fairness Demonstration in Health Policy Risk Adjustment", "summary": "Stepwise regression building procedures are commonly used applied statistical\ntools, despite their well-known drawbacks. While many of their limitations have\nbeen widely discussed in the literature, other aspects of the use of individual\nstatistical fit measures, especially in high-dimensional stepwise regression\nsettings, have not. Giving primacy to individual fit, as is done with p-values\nand $R^2$, when group fit may be the larger concern, can lead to misguided\ndecision making. One of the most consequential uses of stepwise regression is\nin health care, where these tools allocate hundreds of billions of dollars to\nhealth plans enrolling individuals with different predicted health care costs.\nThe main goal of this \"risk adjustment\" system is to convey incentives to\nhealth plans such that they provide health care services fairly, a component of\nwhich is not to discriminate in access or care for persons or groups likely to\nbe expensive. We address some specific limitations of p-values and $R^2$ for\nhigh-dimensional stepwise regression in this policy problem through an\nillustrated example by additionally considering a group-level fairness metric.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1803.05513v2", "link": "http://dx.doi.org/10.1080/00031305.2018.1518269"}, {"title": "Modelling bid-ask spread conditional distributions using hierarchical\n  correlation reconstruction", "summary": "While we would like to predict exact values, available incomplete information\nis rarely sufficient - usually allowing only to predict conditional probability\ndistributions. This article discusses hierarchical correlation reconstruction\n(HCR) methodology for such prediction on example of usually unavailable bid-ask\nspreads, predicted from more accessible data like closing price, volume,\nhigh/low price, returns. In HCR methodology we first normalize marginal\ndistributions to nearly uniform like in copula theory. Then we model (joint)\ndensities as linear combinations of orthonormal polynomials, getting its\ndecomposition into (mixed) moments. Then here we model each moment (separately)\nof predicted variable as a linear combination of mixed moments of known\nvariables using least squares linear regression - getting accurate description\nwith interpretable coefficients describing linear relations between moments.\nCombining such predicted moments we get predicted density as a polynomial, for\nwhich we can e.g. calculate expected value, but also variance to evaluate\nuncertainty of such prediction, or we can use the entire distribution e.g. for\nmore accurate further calculations or generating random values. There were\nperformed 10-fold cross-validation log-likelihood tests for 22 DAX companies,\nleading to very accurate predictions, especially when using individual models\nfor each company as there were found large differences between their behaviors.\nAdditional advantage of the discussed methodology is being computationally\ninexpensive, finding and evaluation a model with hundreds of parameters and\nthousands of data points takes a second on a laptop.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1911.02361v1", "link": "http://arxiv.org/abs/1911.02361v1"}, {"title": "Robust framework for quantifying the value of information in pricing and\n  hedging", "summary": "We investigate asymmetry of information in the context of robust approach to\npricing and hedging of financial derivatives. We consider two agents, one who\nonly observes the stock prices and another with some additional information,\nand investigate when the pricing--hedging duality for the former extends to the\nlatter. We introduce a general framework to express the superhedging and market\nmodel prices for an informed agent. Our key insight is that an informed agent\ncan be seen as a regular agent who can restrict her attention to a certain\nsubset of possible paths. We use results of Hou & Ob\\l\\'oj on robust approach\nwith beliefs to establish the pricing--hedging duality for an informed agent.\nOur results cover number of scenarios, including information arriving before\ntrading starts, arriving after static position in European options is formed\nbut before dynamic trading starts or arriving at some point before the\nmaturity. For the latter we show that the superhedging value satisfies a\nsuitable dynamic programming principle, which is of independent interest.", "category": ["q-fin.MF", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1605.02539v2", "link": "http://arxiv.org/abs/1605.02539v2"}, {"title": "Chaos structures. Multicurrency adviser on the basis of NSW model and\n  social-financial nets", "summary": "Algorithm of multicurrency trading at the market of Forex is realized on the\nbasis of nonlinear stochastic wavelets. The distinctive feature of the\nalgorithm is the possibility of weakly- and strongly connected horizontal\nself-assemblies, as well as use of nested structures. On-line trading with\neight currency couples has shown high effectiveness and stability of the\nalgorithm. It is discussed the problem of possibility of excess profit earning\nin electronic markets via development of social-financial nets based on\nsynchronization of work of individual traders by means of proposed algorithm.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1106.4502v1", "link": "http://arxiv.org/abs/1106.4502v1"}, {"title": "A numeraire-free and original probability based framework for financial\n  markets", "summary": "In this paper, we introduce a numeraire-free and original probability based\nframework for financial markets. We reformulate or characterize fair markets,\nthe optional decomposition theorem, superhedging, attainable claims and\ncomplete markets in terms of martingale deflators, present a recent result of\nKramkov and Schachermayer (1999, 2001) on portfolio optimization and give a\nreview of utility-based approach to contingent claim pricing in incomplete\nmarkets.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/math/0305017v1", "link": "http://arxiv.org/abs/math/0305017v1"}, {"title": "Making Mean-Variance Hedging Implementable in a Partially Observable\n  Market", "summary": "The mean-variance hedging (MVH) problem is studied in a partially observable\nmarket where the drift processes can only be inferred through the observation\nof asset or index processes. Although most of the literatures treat the MVH\nproblem by the duality method, here we study a system consisting of three BSDEs\nderived by Mania and Tevzadze (2003) and Mania et.al.(2008) and try to provide\nmore explicit expressions directly implementable by practitioners. Under the\nBayesian and Kalman-Bucy frameworks, we find that a relevant BSDE yields a\nsemi-closed solution via a simple set of ODEs which allow a quick numerical\nevaluation. This renders remaining problems equivalent to solving European\ncontingent claims under a new forward measure, and it is straightforward to\nobtain a forward looking non-sequential Monte Carlo simulation scheme. We also\ngive a special example where the hedging position is available in a semi-closed\nform. For more generic setups, we provide explicit expressions of approximate\nhedging portfolio by an asymptotic expansion. These analytic expressions not\nonly allow the hedgers to update the hedging positions in real time but also\nmake a direct analysis of the terminal distribution of the hedged portfolio\nfeasible by standard Monte Carlo simulation.", "category": ["q-fin.CP", "q-fin.PM"], "id": "http://arxiv.org/abs/1306.3359v4", "link": "http://arxiv.org/abs/1306.3359v4"}, {"title": "Importance of Positive Feedbacks and Over-confidence in a\n  Self-Fulfilling Ising Model of Financial Markets", "summary": "Following a long tradition of physicists who have noticed that the Ising\nmodel provides a general background to build realistic models of social\ninteractions, we study a model of financial price dynamics resulting from the\ncollective aggregate decisions of agents. This model incorporates imitation,\nthe impact of external news and private information. It has the structure of a\ndynamical Ising model in which agents have two opinions (buy or sell) with\ncoupling coefficients which evolve in time with a memory of how past news have\nexplained realized market returns. We study two versions of the model, which\ndiffer on how the agents interpret the predictive power of news. We show that\nthe stylized facts of financial markets are reproduced only when agents are\nover-confident and mis-attribute the success of news to predict return to\nherding effects, thereby providing positive feedbacks leading to the model\nfunctioning close to the critical point. Our model exhibits a rich multifractal\nstructure characterized by a continuous spectrum of exponents of the power law\nrelaxation of endogenous bursts of volatility, in good agreement with previous\nanalytical predictions obtained with the multifractal random walk model and\nwith empirical facts.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0503607v2", "link": "http://dx.doi.org/10.1016/j.physa.2006.02.022"}, {"title": "Testing power-law cross-correlations: Rescaled covariance test", "summary": "We introduce a new test for detection of power-law cross-correlations among a\npair of time series - the rescaled covariance test. The test is based on a\npower-law divergence of the covariance of the partial sums of the long-range\ncross-correlated processes. Utilizing a heteroskedasticity and auto-correlation\nrobust estimator of the long-term covariance, we develop a test with desirable\nstatistical properties which is well able to distinguish between short- and\nlong-range cross-correlations. Such test should be used as a starting point in\nthe analysis of long-range cross-correlations prior to an estimation of\nbivariate long-term memory parameters. As an application, we show that the\nrelationship between volatility and traded volume, and volatility and returns\nin the financial markets can be labeled as the one with power-law\ncross-correlations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1307.4727v2", "link": "http://dx.doi.org/10.1140/epjb/e2013-40705-y"}, {"title": "An assessement of global energy resource economic potentials", "summary": "This paper presents an assessment of global economic energy potentials for\nall major natural energy resources. This work is based on both an extensive\nliterature review and calculations using natural resource assessment data.\nEconomic potentials are presented in the form of cost-supply curves, in terms\nof energy flows for renewable energy sources, or fixed amounts for fossil and\nnuclear resources, with strong emphasis on uncertainty, using a consistent\nmethodology that allow direct comparisons to be made. In order to interpolate\nthrough available resource assessment data and associated uncertainty, a\ntheoretical framework and a computational methodology are given based on\nstatistical properties of different types of resources, justified empirically\nby the data, and used throughout. This work aims to provide a global database\nfor natural energy resources ready to integrate into models of energy systems,\nenabling to introduce at the same time uncertainty over natural resource\nassessments. The supplementary material provides theoretical details and tables\nof data and parameters that enable this extensive database to be adapted to a\nvariety of energy systems modelling frameworks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1205.4693v2", "link": "http://dx.doi.org/10.1016/j.energy.2012.08.018"}, {"title": "The Bismut-Elworthy-Li formula for jump-diffusions and applications to\n  Monte Carlo pricing in finance", "summary": "We extend the Bismut-Elworthy-Li formula to non-degenerate jump diffusions\nand \"payoff\" functions depending on the process at multiple future times. In\nthe spirit of Fournie et al [13] and Davis and Johansson [9] this can improve\nMonte Carlo numerics for stochastic volatility models with jumps. To this end\none needs so-called Malliavin weights and we give explicit formulae valid in\npresence of jumps: (a) In a non-degenerate situation, the extended BEL formula\nrepresents possible Malliavin weights as Ito integrals with explicit\nintegrands; (b) in a hypoelliptic setting we review work of Arnaudon and\nThalmaier [1] and also find explicit weights, now involving the Malliavin\ncovariance matrix, but still straight-forward to implement. (This is in\ncontrast to recent work by Forster, Lutkebohmert and Teichmann where weights\nare constructed as anticipating Skorohod integrals.) We give some financial\nexamples covered by (b) but note that most practical cases of poor Monte Carlo\nperformance, Digital Cliquet contracts for instance, can be dealt with by the\nextended BEL formula and hence without any reliance on Malliavin calculus at\nall. We then discuss some of the approximations, often ignored in the\nliterature, needed to justify the use of the Malliavin weights in the context\nof standard jump diffusion models. Finally, as all this is meant to improve\nnumerics, we give some numerical results with focus on Cliquets under the\nHeston model with jumps.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0604311v3", "link": "http://arxiv.org/abs/math/0604311v3"}, {"title": "On the Existence of Shadow Prices", "summary": "For utility maximization problems under proportional transaction costs, it\nhas been observed that the original market with transaction costs can sometimes\nbe replaced by a frictionless \"shadow market\" that yields the same optimal\nstrategy and utility. However, the question of whether or not this indeed holds\nin generality has remained elusive so far. In this paper we present a\ncounterexample which shows that shadow prices may fail to exist. On the other\nhand, we prove that short selling constraints are a sufficient condition to\nwarrant their existence, even in very general multi-currency market models with\npossibly discontinuous bid-ask-spreads.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1111.6633v2", "link": "http://arxiv.org/abs/1111.6633v2"}, {"title": "Risk as Challenge: A Dual System Stochastic Model for Binary Choice\n  Behavior", "summary": "Challenge Theory (CT), a new approach to decision under risk departs\nsignificantly from expected utility, and is based on firmly psychological,\nrather than economic, assumptions. The paper demonstrates that a purely\ncognitive-psychological paradigm for decision under risk can yield excellent\npredictions, comparable to those attained by more complex economic or\npsychological models that remain attached to conventional economic constructs\nand assumptions. The study presents a new model for predicting the popularity\nof choices made in binary risk problems. A CT-based regression model is tested\non data gathered from 126 respondents who indicated their preferences with\nrespect to 44 choice problems. Results support CT's central hypothesis,\nstrongly associating between the Challenge Index (CI) attributable to every\nbinary risk problem, and the observed popularity of the bold prospect in that\nproblem (with r=-0.92 and r=-0.93 for gains and for losses, respectively). The\nnovelty of the CT perspective as a new paradigm is illuminated by its simple,\nsingle-index (CI) representation of psychological effects proposed by Prospect\nTheory for describing choice behavior (certainty effect, reflection effect,\noverweighting small probabilities and loss aversion).", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.04487v1", "link": "http://arxiv.org/abs/1910.04487v1"}, {"title": "A common trajectory recapitulated by urban economies", "summary": "Is there a general economic pathway recapitulated by individual cities over\nand over? Identifying such evolution structure, if any, would inform models for\nthe assessment, maintenance, and forecasting of urban sustainability and\neconomic success as a quantitative baseline. This premise seems to contradict\nthe existing body of empirical evidences for path-dependent growth shaping the\nunique history of individual cities. And yet, recent empirical evidences and\ntheoretical models have amounted to the universal patterns, mostly\nsize-dependent, thereby expressing many of urban quantities as a set of simple\nscaling laws. Here, we provide a mathematical framework to integrate repeated\ncross-sectional data, each of which freezes in time dimension, into a frame of\nreference for longitudinal evolution of individual cities in time. Using data\nof over 100 millions employment in thousand business categories between 1998\nand 2013, we decompose each city's evolution into a pre-factor and relative\nchanges to eliminate national and global effects. In this way, we show the\nlongitudinal dynamics of individual cities recapitulate the observed\ncross-sectional regularity. Larger cities are not only scaled-up versions of\ntheir smaller peers but also of their past. In addition, our model shows that\nboth specialization and diversification are attributed to the distribution of\nindustry's scaling exponents, resulting a critical population of 1.2 million at\nwhich a city makes an industrial transition into innovative economies.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1810.08330v1", "link": "http://arxiv.org/abs/1810.08330v1"}, {"title": "Higher order elicitability and Osband's principle", "summary": "A statistical functional, such as the mean or the median, is called\nelicitable if there is a scoring function or loss function such that the\ncorrect forecast of the functional is the unique minimizer of the expected\nscore. Such scoring functions are called strictly consistent for the\nfunctional. The elicitability of a functional opens the possibility to compare\ncompeting forecasts and to rank them in terms of their realized scores. In this\npaper, we explore the notion of elicitability for multi-dimensional functionals\nand give both necessary and sufficient conditions for strictly consistent\nscoring functions. We cover the case of functionals with elicitable components,\nbut we also show that one-dimensional functionals that are not elicitable can\nbe a component of a higher order elicitable functional. In the case of the\nvariance this is a known result. However, an important result of this paper is\nthat spectral risk measures with a spectral measure with finite support are\njointly elicitable if one adds the `correct' quantiles. A direct consequence of\napplied interest is that the pair (Value at Risk, Expected Shortfall) is\njointly elicitable under mild conditions that are usually fulfilled in risk\nmanagement applications.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1503.08123v3", "link": "http://dx.doi.org/10.1214/16-AOS1439"}, {"title": "Varadhan's formula, conditioned diffusions, and local volatilities", "summary": "Motivated by marginals-mimicking results for It\\^o processes via SDEs and by\ntheir applications to volatility modeling in finance, we discuss the weak\nconvergence of the law of a hypoelliptic diffusions conditioned to belong to a\ntarget affine subspace at final time, namely $\\mathcal{L}(Z_t|Y_t = y)$ if\n$X_{\\cdot}=(Y_\\cdot,Z_{\\cdot})$. To do so, we revisit Varadhan-type estimates\nin a small-noise regime (as opposed to small-time), studying the density of the\nlower-dimensional component $Y$. The application to stochastic volatility\nmodels include the small-time and, for certain models, the large-strike\nasymptotics of the Gyongy-Dupire's local volatility function. The final product\nare asymptotic formulae that can (i) motivate parameterizations of the local\nvolatility surface and (ii) be used to extrapolate local volatilities in a\ngiven model.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1311.1545v3", "link": "http://arxiv.org/abs/1311.1545v3"}, {"title": "Quantifying macroeconomic expectations in stock markets using Google\n  Trends", "summary": "Among other macroeconomic indicators, the monthly release of U.S.\nunemployment rate figures in the Employment Situation report by the U.S. Bureau\nof Labour Statistics gets a lot of media attention and strongly affects the\nstock markets. I investigate whether a profitable investment strategy can be\nconstructed by predicting the likely changes in U.S. unemployment before the\nofficial news release using Google query volumes for related search terms. I\nfind that massive new data sources of human interaction with the Internet not\nonly improves U.S. unemployment rate predictability, but can also enhance\nmarket timing of trading strategies when considered jointly with macroeconomic\ndata. My results illustrate the potential of combining extensive behavioural\ndata sets with economic data to anticipate investor expectations and stock\nmarket moves.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1805.00268v1", "link": "http://arxiv.org/abs/1805.00268v1"}, {"title": "Arbitrage Opportunities in Misspecified Stochastic volatility Models", "summary": "There is vast empirical evidence that given a set of assumptions on the\nreal-world dynamics of an asset, the European options on this asset are not\nefficiently priced in options markets, giving rise to arbitrage opportunities.\nWe study these opportunities in a generic stochastic volatility model and\nexhibit the strategies which maximize the arbitrage profit. In the case when\nthe misspecified dynamics is a classical Black-Scholes one, we give a new\ninterpretation of the classical butterfly and risk reversal contracts in terms\nof their (near) optimality for arbitrage strategies. Our results are\nillustrated by a numerical example including transaction costs.", "category": ["q-fin.PR", "q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1002.5041v3", "link": "http://arxiv.org/abs/1002.5041v3"}, {"title": "In search of a new economic model determined by logistic growth", "summary": "In this paper we extend the work by Ryuzo Sato devoted to the development of\neconomic growth models within the framework of the Lie group theory. We propose\na new growth model based on the assumption of logistic growth in factors. It is\nemployed to derive new production functions and introduce a new notion of wage\nshare. In the process it is shown that the new functions compare reasonably\nwell against relevant economic data. The corresponding problem of maximization\nof profit under conditions of perfect competition is solved with the aid of one\nof these functions. In addition, it is explained in reasonably rigorous\nmathematical terms why Bowley's law no longer holds true in post-1960 data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1711.02625v5", "link": "http://arxiv.org/abs/1711.02625v5"}, {"title": "Long-range dependence in Interest Rates and Monetary Policy", "summary": "This paper studies the dynamics of Brazilian interest rates for short-term\nmaturities. The paper employs developed techniques in the econophysics\nliterature and tests for long-range dependence in the term structure of these\ninterest rates for the last decade. Empirical results suggest that the degree\nof long-range dependence has changed over time due to changes in monetary\npolicy, specially in the short-end of the term structure of interest rates.\nTherefore, we show that it is possible to identify monetary arrangements using\nthese techniques from econophysics.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0607245v1", "link": "http://arxiv.org/abs/physics/0607245v1"}, {"title": "Dynamic optimal contract under parameter uncertainty with risk averse\n  agent and principal", "summary": "We consider a continuous time Principal-Agent model on a finite time horizon,\nwhere we look for the existence of an optimal contract both parties agreed on.\nContrary to the main stream, where the principal is modelled as risk-neutral,\nwe assume that both the principal and the agent have exponential utility, and\nare risk averse with same risk awareness level. Moreover, the agent's quality\nis unknown and modelled as a filtering term in the problem, which is revealed\nas time passes by. The principal can not observe the agent's real action, but\ncan only recommend action levels to the agent. Hence, we have a \\textit{moral\nhazard} problem. In this setting, we give an explicit solution to the optimal\ncontract problem.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1806.01495v1", "link": "http://arxiv.org/abs/1806.01495v1"}, {"title": "The distortion principle for insurance pricing: properties,\n  identification and robustness", "summary": "Distortion (Denneberg 1990) is a well known premium calculation principle for\ninsurance contracts. In this paper, we study sensitivity properties of\ndistortion functionals w.r.t. the assumptions for risk aversion as well as\nrobustness w.r.t. ambiguity of the loss distribution. Ambiguity is measured by\nthe Wasserstein distance. We study variances of distances for probability\nmodels and identify some worst case distributions. In addition to the direct\nproblem we also investigate the inverse problem, that is how to identify the\ndistortion density on the basis of observations of insurance premia.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1809.06592v1", "link": "http://arxiv.org/abs/1809.06592v1"}, {"title": "Asymptotic analysis of forward performance processes in incomplete\n  markets and their ill-posed HJB equations", "summary": "We consider the problem of optimal portfolio selection under forward\ninvestment performance criteria in an incomplete market. The dynamics of the\nprices of the traded assets depend on a pair of stochastic factors, namely, a\nslow factor (e.g. a macroeconomic indicator) and a fast factor (e.g. stochastic\nvolatility). We analyze the associated forward performance SPDE and provide\nexplicit formulae for the leading order and first order correction terms for\nthe forward investment process and the optimal feedback portfolios. They both\ndepend on the investor's initial preferences and the dynamically changing\ninvestment opportunities. The leading order terms resemble their time-monotone\ncounterparts, but with the appropriate stochastic time changes resulting from\naveraging phenomena. The first-order terms compile the reaction of the investor\nto both the changes in the market input and his recent performance. Our\nanalysis is based on an expansion of the underlying ill-posed HJB equation, and\nit is justified by means of an appropriate remainder estimate.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1504.03209v2", "link": "http://arxiv.org/abs/1504.03209v2"}, {"title": "Optimal dividend problems for a jump-diffusion model with capital\n  injections and proportional transaction costs", "summary": "In this paper, we study the optimal control problem for a company whose\nsurplus process evolves as an upward jump diffusion with random return on\ninvestment. Three types of practical optimization problems faced by a company\nthat can control its liquid reserves by paying dividends and injecting capital.\nIn the first problem, we consider the classical dividend problem without\ncapital injections. The second problem aims at maximizing the expected\ndiscounted dividend payments minus the expected discounted costs of capital\ninjections over strategies with positive surplus at all times. The third\nproblem has the same objective as the second one, but without the constraints\non capital injections. Under the assumption of proportional transaction costs,\nwe identify the value function and the optimal strategies for any distribution\nof gains.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1409.0407v1", "link": "http://arxiv.org/abs/1409.0407v1"}, {"title": "New measure of multifractality and its application in finances", "summary": "We provide an alternative method for analysis of multifractal properties of\ntime series. The new approach takes into account the behaviour of the whole\nmultifractal profile of the generalized Hurst exponent $h(q)$ for all moment\norders $q$, not limited only to the edge values of $h(q)$ describing in MFDFA\nscaling properties of smallest and largest fluctuations in signal. The meaning\nof this new measure is clarified and its properties are investigated for\nsynthetic multifractal data and real signals taken from stock market. We show\nthat the proposed new measure is free of problems one can meet in real\nnonstationary signals, while searching their multifractal signatures.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1309.5466v1", "link": "http://arxiv.org/abs/1309.5466v1"}, {"title": "A convergence result for the Emery topology and a variant of the proof\n  of the fundamental theorem of asset pricing", "summary": "We show that \\emph{No unbounded profit with bounded risk} (NUPBR) implies\n\\emph{predictable uniform tightness} (P-UT), a boundedness property in the\nEmery topology which has been introduced by C. Stricker \\cite{S:85}. Combining\nthis insight with well known results from J. M\\'emin and L. S{\\l}ominski\n\\cite{MS:91} leads to a short variant of the proof of the fundamental theorem\nof asset pricing initially proved by F. Delbaen and W. Schachermayer\n\\cite{DS:94}. The results are formulated in the general setting of admissible\nportfolio wealth processes as laid down by Y. Kabanov in \\cite{kab:97}.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1406.5414v2", "link": "http://arxiv.org/abs/1406.5414v2"}, {"title": "Modeling the stylized facts of wholesale system marginal price (SMP) and\n  the impacts of regulatory reforms on the Greek Electricity Market", "summary": "This work presents the results of an empirical research with the target of\nmodeling the stylized facts of the daily expost System Marginal Price (SMP) of\nthe Greek wholesale electricity market, using data from January 2004 to\nDecember of 2011. SMP is considered here as the footprint of an underline\nstochastic and nonlinear process that bears all the information reflecting not\nonly the effects of changes in endogenous or fundamental factors of the market\nbut also the impacts of a series of regulatory reforms that have continuously\nchanged the market's microstructure. To capture the dynamics of the conditional\nmean and volatility of SMP that generate the stylized facts(mean reversion,\nprice spikes, fat tails price distribution etc), a number of ARMAX GARCH models\nhave been estimated using as regressors an extensive set of fundamental factors\nin the Greek electricity market as well as dummy variables that mimic the\nhistory of Regulator's interventions. The findings show that changes in the\nmicrostructure of the market caused by the reforms have strongly affected the\ndynamic evolution of SMP and that the best found model captures adequately the\nstylized facts of the series that other electricity and financial markets\nshare. The dynamics of the conditional volatility generated by the model can be\nextremely useful in the efforts that are under way towards market restructuring\nso the Greek market to be more compatible with the requirements of the European\nTarget Model.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1401.5452v1", "link": "http://arxiv.org/abs/1401.5452v1"}, {"title": "Diversifying portfolios of U.S. stocks with crude oil and natural gas: A\n  regime-dependent optimization with several risk measures", "summary": "Energy markets are strategic to governments and economic development. Several\ncommodities compete as substitutable energy sources and energy diversifiers.\nSuch competition reduces the energy vulnerability of countries as well as\nportfolios' risk exposure. Vulnerability results mainly from price trends and\nfluctuations, following supply and demand shocks. Such energy price uncertainty\nattracts many market participants in the energy commodity markets. First,\nenergy producers and consumers hedge adverse price changes with energy\nderivatives. Second, financial market participants use commodities and\ncommodity derivatives to diversify their conventional portfolios. For that\nreason, we consider the joint dependence between the United States (U.S.)\nnatural gas, crude oil and stock markets. We use Gatfaoui's (2015) time varying\nmultivariate copula analysis and related variance regimes. Such approach\nhandles structural changes in asset prices. In this light, we draw implications\nfor portfolio optimization, when investors diversify their stock portfolios\nwith natural gas and crude oil assets. We minimize the portfolio's variance,\nsemi-variance and tail risk, in the presence and the absence of constraints on\nthe portfolio's expected return and/or U.S. stock investment. The return\nconstraint reduces the performance of the optimal portfolio. Moreover, the\nregime-specific portfolio optimization helps implement an enhanced active\nmanagement strategy over the whole sample period. Under a return constraint,\nthe semi-variance optimal portfolio offers the best risk-return tradeoff,\nwhereas the tail-risk optimal portfolio offers the best tradeoff in the absence\nof a return constraint.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1811.02382v1", "link": "http://arxiv.org/abs/1811.02382v1"}, {"title": "Option pricing: A yet simpler approach", "summary": "We provide a lean, non-technical exposition on the pricing of path-dependent\nand European-style derivatives in the Cox-Ross-Rubinstein (CRR) pricing model.\nThe main tool used in the paper for cleaning up the reasoning is applying\nstatic hedging arguments.\n  This can be accomplished by taking various routes through some auxiliary\nconsiderations, namely Arrow-Debreu securities, digital options or backward\nrandom processes. In the last case the CRR model is extended to an infinite\nstate space which leads to an interesting new phenomenon not present in the\nclassical CRR model.\n  At the end we discuss the paradox involving the drift parameter $\\mu$ in the\nBSM model pricing. We provide sensitivity analysis and the speed of converge\nfor the asymptotically vanishing drift.", "category": ["q-fin.MF", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1705.00212v3", "link": "http://arxiv.org/abs/1705.00212v3"}, {"title": "Is the Juice Worth the Squeeze? Machine Learning (ML) In and For\n  Agent-Based Modelling (ABM)", "summary": "In recent years, many scholars praised the seemingly endless possibilities of\nusing machine learning (ML) techniques in and for agent-based simulation models\n(ABM). To get a more comprehensive understanding of these possibilities, we\nconduct a systematic literature review (SLR) and classify the literature on the\napplication of ML in and for ABM according to a theoretically derived\nclassification scheme. We do so to investigate how exactly machine learning has\nbeen utilized in and for agent-based models so far and to critically discuss\nthe combination of these two promising methods. We find that, indeed, there is\na broad range of possible applications of ML to support and complement ABMs in\nmany different ways, already applied in many different disciplines. We see\nthat, so far, ML is mainly used in ABM for two broad cases: First, the\nmodelling of adaptive agents equipped with experience learning and, second, the\nanalysis of outcomes produced by a given ABM. While these are the most\nfrequent, there also exist a variety of many more interesting applications.\nThis being the case, researchers should dive deeper into the analysis of when\nand how which kinds of ML techniques can support ABM, e.g. by conducting a more\nin-depth analysis and comparison of different use cases. Nonetheless, as the\napplication of ML in and for ABM comes at certain costs, researchers should not\nuse ML for ABMs just for the sake of doing it.", "category": [], "id": "http://arxiv.org/abs/2003.11985v1", "link": "http://arxiv.org/abs/2003.11985v1"}, {"title": "Laser Welfare: First Steps in Econodynamic Engineering", "summary": "The paper starts with a brief review of present understanding of income\ndistributions; especially with regard to recent work in the field of\neconophysics that draws parallels between income, wealth and energy\ndistributions. Examples of alternative energy distributions found in physical\nsystems are discussed, and how they could be used to construct economic models\nthat might allow alternative overall distributions of wealth and income in\nsociety. These ideas are further expanded and a more detailed scheme for\nwelfare assistance is proposed that might be used to improve the incomes of the\npoorest in a more efficient way than traditional welfare schemes. Finally, and\nunusually for a paper on economics; an experiment is proposed that could be\nused to either support or disprove the ideas discussed in the rest of the\npaper.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0408227v1", "link": "http://arxiv.org/abs/cond-mat/0408227v1"}, {"title": "Illiquidity Effects in Optimal Consumption-Investment Problems", "summary": "We study the effect of liquidity freezes on an economic agent optimizing her\nutility of consumption in a perturbed Black-Scholes-Merton model. The single\nrisky asset follows a geometric Brownian motion but is subject to liquidity\nshocks, during which no trading is possible and stock dynamics are modified.\nThe liquidity regime is governed by a two-state Markov chain. We derive the\nasymptotic effect of such freezes on optimal consumption and investment\nschedules in the two cases of (i) small probability of liquidity shock; (ii)\nfast-scale liquidity regime switching. Explicit formulas are obtained for\nlogarithmic and hyperbolic utility maximizers on infinite horizon. We also\nderive the corresponding loss in utility and compare with a recent related\nfinite-horizon model of Diesinger, Kraft and Seifried (2009).", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1004.1489v2", "link": "http://arxiv.org/abs/1004.1489v2"}, {"title": "A remark on Gatheral's 'most-likely path approximation' of implied\n  volatility", "summary": "We give a new proof of the representation of implied volatility as a\ntime-average of weighted expectations of local or stochastic volatility. With\nthis proof we clarify the question of existence of 'forward implied variance'\nin the original derivation of Gatheral, who introduced this representation in\nhis book 'The Volatility Surface'.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0911.0562v2", "link": "http://arxiv.org/abs/0911.0562v2"}, {"title": "Regularization and analytic option pricing under $\u03b1$-stable\n  distribution of arbitrary asymmetry", "summary": "We consider a non-Gaussian option pricing model, into which the underlying\nlog-price is assumed to be driven by an $\\alpha$-stable distribution. We remove\nthe a priori divergence of the model by introducing a Mellin regularization for\nthe L\\'evy propagator. Using distributional and $\\mathbb{C}^n$ tools, we derive\nan analytic closed formula for the option price, valid for any stability\n$\\alpha\\in]1,2]$ and any asymmetry. This formula is very efficient and recovers\nprevious cases (Black-Scholes, Carr-Wu); we calibrate the formula on market\ndatas, make numerical tests, and discuss its many interesting properties.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1611.04320v2", "link": "http://arxiv.org/abs/1611.04320v2"}, {"title": "Subvector Inference in Partially Identified Models with Many Moment\n  Inequalities", "summary": "This paper considers inference for a function of a parameter vector in a\npartially identified model with many moment inequalities. This framework allows\nthe number of moment conditions to grow with the sample size, possibly at\nexponential rates. Our main motivating application is subvector inference,\ni.e., inference on a single component of the partially identified parameter\nvector associated with a treatment effect or a policy variable of interest.\n  Our inference method compares a MinMax test statistic (minimum over\nparameters satisfying $H_0$ and maximum over moment inequalities) against\ncritical values that are based on bootstrap approximations or analytical\nbounds. We show that this method controls asymptotic size uniformly over a\nlarge class of data generating processes despite the partially identified many\nmoment inequality setting. The finite sample analysis allows us to obtain\nexplicit rates of convergence on the size control. Our results are based on\ncombining non-asymptotic approximations and new high-dimensional central limit\ntheorems for the MinMax of the components of random matrices. Unlike the\nprevious literature on functional inference in partially identified models, our\nresults do not rely on weak convergence results based on Donsker's class\nassumptions and, in fact, our test statistic may not even converge in\ndistribution. Our bootstrap approximation requires the choice of a tuning\nparameter sequence that can avoid the excessive concentration of our test\nstatistic. To this end, we propose an asymptotically valid data-driven method\nto select this tuning parameter sequence. This method generalizes the selection\nof tuning parameter sequences to problems outside the Donsker's class\nassumptions and may also be of independent interest. Our procedures based on\nself-normalized moderate deviation bounds are relatively more conservative but\neasier to implement.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1806.11466v1", "link": "http://arxiv.org/abs/1806.11466v1"}, {"title": "Time-consistency of risk measures with GARCH volatilities and their\n  estimation", "summary": "In this paper we study time-consistent risk measures for returns that are\ngiven by a GARCH(1,1) model. We present a construction of risk measures based\non their static counterparts that overcomes the lack of time-consistency. We\nthen study in detail our construction for the risk measures Value-at-Risk (VaR)\nand Average Value-at-Risk (AVaR). While in the VaR case we can derive an\nanalytical formula for its time-consistent counterpart, in the AVaR case we\nderive lower and upper bounds to its time-consistent version. Furthermore, we\nincorporate techniques from Extreme Value Theory (EVT) to allow for a more\ntail-geared statistical analysis of the corresponding risk measures. We\nconclude with an application of our results to a data set of stock prices.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1504.04774v2", "link": "http://arxiv.org/abs/1504.04774v2"}, {"title": "A common origin of the power law distributions in models of market and\n  earthquake", "summary": "We show that there is a common mode of origin for the power laws observed in\ntwo different models: (i) the Pareto law for the distribution of money among\nthe agents with random saving propensities in an ideal gas-like market model\nand (ii) the Gutenberg-Richter law for the distribution of overlaps in a\nfractal-overlap model for earthquakes. We find that the power laws appear as\nthe asymptotic forms of ever-widening log-normal distributions for the agents'\nmoney and the overlap magnitude respectively. The identification of the generic\norigin of the power laws helps in better understanding and in developing\ngeneralized views of phenomena in such diverse areas as economics and\ngeophysics.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0510038v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.096"}, {"title": "Machine Learning on EPEX Order Books: Insights and Forecasts", "summary": "This paper employs machine learning algorithms to forecast German electricity\nspot market prices. The forecasts utilize in particular bid and ask order book\ndata from the spot market but also fundamental market data like renewable\ninfeed and expected demand. Appropriate feature extraction for the order book\ndata is developed. Using cross-validation to optimise hyperparameters, neural\nnetworks and random forests are proposed and compared to statistical reference\nmodels. The machine learning models outperform traditional approaches.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1906.06248v3", "link": "http://arxiv.org/abs/1906.06248v3"}, {"title": "Modelling Social Evolutionary Processes and Peer Effects in Agricultural\n  Trade Networks: the Rubber Value Chain in Indonesia", "summary": "Understanding market participants' channel choices is important to policy\nmakers because it yields information on which channels are effective in\ntransmitting information. These channel choices are the result of a recursive\nprocess of social interactions and determine the observable trading networks.\nThey are characterized by feedback mechanisms due to peer interaction and\ntherefore need to be understood as complex adaptive systems (CAS). When\nmodelling CAS, conventional approaches like regression analyses face severe\ndrawbacks since endogeneity is omnipresent. As an alternative, process-based\nanalyses allow researchers to capture these endogenous processes and multiple\nfeedback loops. This paper applies an agent-based modelling approach (ABM) to\nthe empirical example of the Indonesian rubber trade. The feedback mechanisms\nare modelled via an innovative approach of a social matrix, which allows\ndecisions made in a specific period to feed back into the decision processes in\nsubsequent periods, and allows agents to systematically assign different\nweights to the decision parameters based on their individual characteristics.\nIn the validation against the observed network, uncertainty in the found\nestimates, as well as under determination of the model, are dealt with via an\napproach of evolutionary calibration: a genetic algorithm finds the combination\nof parameters that maximizes the similarity between the simulated and the\nobserved network. Results indicate that the sellers' channel choice decisions\nare mostly driven by physical distance and debt obligations, as well as\npeer-interaction. Within the social matrix, the most influential individuals\nare sellers that live close by to other traders, are active in social groups\nand belong to the ethnic majority in their village.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.11476v1", "link": "http://arxiv.org/abs/1811.11476v1"}, {"title": "The Core of an Economy with an Endogenous Social Division of Labour", "summary": "This paper considers the core of a competitive market economy with an\nendogenous social division of labour. The theory is founded on the notion of a\n\"consumer-producer\", who consumes as well as produces commodities. First, we\nshow that the Core of such an economy with an endogenous social division of\nlabour can be founded on deviations of coalitions of arbitrary size, extending\nthe seminal insights of Vind and Schmeidler for pure exchange economies.\nFurthermore, we establish the equivalence between the Core and the set of\ncompetitive equilibria for continuum economies with an endogenous social\ndivision of labour. Our analysis also concludes that self-organisation in a\nsocial division of labour can be incorporated into the Edgeworthian barter\nprocess directly. This is formulated as a Core equivalence result stated for a\nStructured Core concept based on renegotiations among fully specialised\neconomic agents, i.e., coalitions that use only fully developed internal\ndivisions of labour. Our approach bridges the gap between standard economies\nwith social production and coalition production economies. Therefore, a more\nstraightforward and natural interpretation of coalitional improvement and the\nCore can be developed than for coalition production economies.", "category": [], "id": "http://arxiv.org/abs/1809.01470v1", "link": "http://arxiv.org/abs/1809.01470v1"}, {"title": "Consistent upper price bounds for exotic options given a finite number\n  of call prices and their convergence", "summary": "We consider the problem of finding a consistent upper price bound for exotic\noptions whose payoff depends on the stock price at two different predetermined\ntime points (e.g. Asian option), given a finite number of observed call prices\nfor these maturities. A model-free approach is used, only taking into account\nthat the (discounted) stock price process is a martingale under the\nno-arbitrage condition. In case the payoff is directionally convex we obtain\nthe worst case marginal pricing measures. The speed of convergence of the upper\nprice bound is determined when the number of observed stock prices increases.\nWe illustrate our findings with some numerical computations.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1907.09144v1", "link": "http://arxiv.org/abs/1907.09144v1"}, {"title": "Distributions of Historic Market Data -- Relaxation and Correlations", "summary": "We investigate relaxation and correlations in a class of mean-reverting\nmodels for stochastic variances. We derive closed-form expressions for the\ncorrelation functions and leverage for a general form of the stochastic term.\nWe also discuss correlation functions and leverage for three specific models --\nmultiplicative, Heston (Cox-Ingersoll-Ross) and combined multiplicative-Heston\n-- whose steady-state probability density functions are Gamma, Inverse Gamma\nand Beta Prime respectively, the latter two exhibiting \"fat\" tails. For the\nHeston model, we apply the eigenvalue analysis of the Fokker-Planck equation to\nderive the correlation function -- in agreement with the general analysis --\nand to identify a series of time scales, which are observable in relaxation of\ncumulants on approach to the steady state. We test our findings on a very large\nset of historic financial markets data.", "category": ["q-fin.ST", "q-fin.MF"], "id": "http://arxiv.org/abs/1907.05348v2", "link": "http://arxiv.org/abs/1907.05348v2"}, {"title": "A Stochastic Control Approach to Managed Futures Portfolios", "summary": "We study a stochastic control approach to managed futures portfolios.\nBuilding on the Schwartz 97 stochastic convenience yield model for commodity\nprices, we formulate a utility maximization problem for dynamically trading a\nsingle-maturity futures or multiple futures contracts over a finite horizon. By\nanalyzing the associated Hamilton-Jacobi-Bellman (HJB) equation, we solve the\ninvestor's utility maximization problem explicitly and derive the optimal\ndynamic trading strategies in closed form. We provide numerical examples and\nillustrate the optimal trading strategies using WTI crude oil futures data.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1811.01916v1", "link": "http://arxiv.org/abs/1811.01916v1"}, {"title": "Estimating and decomposing most productive scale size in parallel DEA\n  networks with shared inputs: A case of China's Five-Year Plans", "summary": "Attaining the optimal scale size of production systems is an issue frequently\nfound in the priority questions on management agendas of various types of\norganizations. Determining the most productive scale size (MPSS) allows the\ndecision makers not only to know the best scale size that their systems can\nachieve but also to tell the decision makers how to move the inefficient\nsystems onto the MPSS region. This paper investigates the MPSS concept for\nproduction systems consisting of multiple subsystems connected in parallel.\nFirst, we propose a relational model where the MPSS of the whole system and the\ninternal subsystems are measured in a single DEA implementation. Then, it is\nproved that the MPSS of the system can be decomposed as the weighted sum of the\nMPSS of the individual subsystems. The main result is that the system is\noverall MPSS if and only if it is MPSS in each subsystem. MPSS decomposition\nallows the decision makers to target the non-MPSS subsystems so that the\nnecessary improvements can be readily suggested. An application of China's\nFive-Year Plans (FYPs) with shared inputs is used to show the applicability of\nthe proposed model for estimating and decomposing MPSS in parallel network DEA.\nIndustry and Agriculture sectors are selected as two parallel subsystems in the\nFYPs. Interesting findings have been noticed. Using the same amount of\nresources, the Industry sector had a better economic scale than the Agriculture\nsector. Furthermore, the last two FYPs, 11th and 12th, were the perfect two\nFYPs among the others.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.03421v3", "link": "http://arxiv.org/abs/1910.03421v3"}, {"title": "From small markets to big markets", "summary": "We study the most famous example of a large financial market: the Arbitrage\nPricing Model, where investors can trade in a one-period setting with countably\nmany assets admitting a factor structure. We consider the problem of maximising\nexpected utility in this setting. Besides establishing the existence of\noptimizers under weaker assumptions than previous papers, we go on studying the\nrelationship between optimal investments in finite market segments and those in\nthe whole market. We show that certain natural (but nontrivial) continuity\nrules hold: maximal satisfaction, reservation prices and (convex combinations\nof) optimizers computed in small markets converge to their respective\ncounterparts in the big market.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1907.05593v1", "link": "http://arxiv.org/abs/1907.05593v1"}, {"title": "Modeling Movements in Oil, Gold, Forex and Market Indices using Search\n  Volume Index and Twitter Sentiments", "summary": "Study of the forecasting models using large scale microblog discussions and\nthe search behavior data can provide a good insight for better understanding\nthe market movements. In this work we collected a dataset of 2 million tweets\nand search volume index (SVI from Google) for a period of June 2010 to\nSeptember 2011. We perform a study over a set of comprehensive causative\nrelationships and developed a unified approach to a model for various market\nsecurities like equity (Dow Jones Industrial Average-DJIA and NASDAQ-100),\ncommodity markets (oil and gold) and Euro Forex rates. We also investigate the\nlagged and statistically causative relations of Twitter sentiments developed\nduring active trading days and market inactive days in combination with the\nsearch behavior of public before any change in the prices/ indices. Our results\nshow extent of lagged significance with high correlation value upto 0.82\nbetween search volumes and gold price in USD. We find weekly accuracy in\ndirection (up and down prediction) uptil 94.3% for DJIA and 90% for NASDAQ-100\nwith significant reduction in mean average percentage error for all the\nforecasting models.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1212.1037v1", "link": "http://arxiv.org/abs/1212.1037v1"}, {"title": "Volatility and Arbitrage", "summary": "The capitalization-weighted total relative variation $\\sum_{i=1}^d\n\\int_0^\\cdot \\mu_i (t) \\mathrm{d} \\langle \\log \\mu_i \\rangle (t)$ in an equity\nmarket consisting of a fixed number $d$ of assets with capitalization weights\n$\\mu_i (\\cdot)$ is an observable and nondecreasing function of time. If this\nobservable of the market is not just nondecreasing, but actually grows at a\nrate which is bounded away from zero, then strong arbitrage can be constructed\nrelative to the market over sufficiently long time horizons. It has been an\nopen issue for more than ten years, whether such strong outperformance of the\nmarket is possible also over arbitrary time horizons under the stated\ncondition. We show that this is not possible in general, thus settling this\nlong-open question. We also show that, under appropriate additional conditions,\noutperformance over any time horizon indeed becomes possible, and exhibit\ninvestment strategies that effect it.", "category": ["q-fin.PM", "math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1608.06121v1", "link": "http://arxiv.org/abs/1608.06121v1"}, {"title": "Predicting Distresses using Deep Learning of Text Segments in Annual\n  Reports", "summary": "Corporate distress models typically only employ the numerical financial\nvariables in the firms' annual reports. We develop a model that employs the\nunstructured textual data in the reports as well, namely the auditors' reports\nand managements' statements. Our model consists of a convolutional recurrent\nneural network which, when concatenated with the numerical financial variables,\nlearns a descriptive representation of the text that is suited for corporate\ndistress prediction. We find that the unstructured data provides a\nstatistically significant enhancement of the distress prediction performance,\nin particular for large firms where accurate predictions are of the utmost\nimportance. Furthermore, we find that auditors' reports are more informative\nthan managements' statements and that a joint model including both managements'\nstatements and auditors' reports displays no enhancement relative to a model\nincluding only auditors' reports. Our model demonstrates a direct improvement\nover existing state-of-the-art models.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1811.05270v1", "link": "http://arxiv.org/abs/1811.05270v1"}, {"title": "The hidden hyperbolic geometry of international trade: World Trade Atlas\n  1870-2013", "summary": "Here, we present the World Trade Atlas 1870-2013, a collection of annual\nworld trade maps in which distance combines economic size and the different\ndimensions that affect international trade beyond mere geography. Trade\ndistances, which are based on a gravity model predicting the existence of\nsignificant trade channels, are such that the closer countries are in trade\nspace, the greater their chance of becoming connected. The atlas provides us\nwith information regarding the long-term evolution of the international trade\nsystem and demonstrates that, in terms of trade, the world is not flat but\nhyperbolic, as a reflection of its complex architecture. The departure from\nflatness has been increasing since World War I, meaning that differences in\ntrade distances are growing and trade networks are becoming more hierarchical.\nSmaller-scale economies are moving away from other countries except for the\nlargest economies; meanwhile those large economies are increasing their chances\nof becoming connected worldwide. At the same time, Preferential Trade\nAgreements do not fit in perfectly with natural communities within the trade\nspace and have not necessarily reduced internal trade barriers. We discuss an\ninterpretation in terms of globalization, hierarchization, and localization;\nthree simultaneous forces that shape the international trade system.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1512.02233v2", "link": "http://arxiv.org/abs/1512.02233v2"}, {"title": "Cliquet option pricing with Meixner processes", "summary": "We investigate the pricing of cliquet options in a geometric Meixner model.\nThe considered option is of monthly sum cap style while the underlying stock\nprice model is driven by a pure-jump Meixner--L\\'{e}vy process yielding Meixner\ndistributed log-returns. In this setting, we infer semi-analytic expressions\nfor the cliquet option price by using the probability distribution function of\nthe driving Meixner--L\\'{e}vy process and by an application of Fourier\ntransform techniques. In an introductory section, we compile various facts on\nthe Meixner distribution and the related class of Meixner--L\\'{e}vy processes.\nWe also propose a customized measure change preserving the Meixner distribution\nof any Meixner process.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1803.09444v1", "link": "http://dx.doi.org/10.15559/18-VMSTA96"}, {"title": "The invisible hand and the rational agent are behind bubbles and crashes", "summary": "The substantial turmoil created by both 2000 dot-com crash and 2008 subprime\ncrisis has fueled the belief that the two classical paradigms of economics,\nwhich are the invisible hand and the rational agent, are not appropriate to\ndescribe market dynamics and should be abandoned at the benefit of alternative\nnew theoretical concepts. At odd with such a view, using a simple model of\nchoice dynamics from sociophysics, the invisible hand and the rational agent\nparadigms are given a new legitimacy. Indeed, it is sufficient to introduce the\nholding of a few intermediate mini market aggregations by agents sharing their\nown private information, to recenter the invisible hand and the rational agent\nat the heart of market self regulation including the making of bubbles and\ntheir subsequent crashes. In so doing, an elasticity is discovered in the\nmarket efficiency mechanism due to the existence of agents anticipation. This\nelasticity is found to create spontaneous bubbles, which are rationally\nfounded, and at the same time, it provokes crashes when the limit of elasticity\nis reached. Although the findings disclose a path to put an end to the\nbubble-crash phenomena, it is argued to be rationality not feasible.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1601.02990v1", "link": "http://dx.doi.org/10.1016/j.chaos.2016.03.011"}, {"title": "The structure of two-valued strategy-proof social choice functions with\n  indifference", "summary": "We give a structure theorem for all coalitionally strategy-proof social\nchoice functions whose range is a subset of cardinality two of a given larger\nset of alternatives.\n  We provide this in the case where the voters/agents are allowed to express\nindifference and the domain consists of profiles of preferences over a society\nof arbitrary cardinality. The theorem, that takes the form of a representation\nformula, can be used to construct all functions under consideration.", "category": [], "id": "http://arxiv.org/abs/2002.06341v1", "link": "http://arxiv.org/abs/2002.06341v1"}, {"title": "Quantifying the Computational Advantage of Forward Orthogonal Deviations", "summary": "Under suitable conditions, one-step generalized method of moments (GMM) based\non the first-difference (FD) transformation is numerically equal to one-step\nGMM based on the forward orthogonal deviations (FOD) transformation. However,\nwhen the number of time periods ($T$) is not small, the FOD transformation\nrequires less computational work. This paper shows that the computational\ncomplexity of the FD and FOD transformations increases with the number of\nindividuals ($N$) linearly, but the computational complexity of the FOD\ntransformation increases with $T$ at the rate $T^{4}$ increases, while the\ncomputational complexity of the FD transformation increases at the rate $T^{6}$\nincreases. Simulations illustrate that calculations exploiting the FOD\ntransformation are performed orders of magnitude faster than those using the FD\ntransformation. The results in the paper indicate that, when one-step GMM based\non the FD and FOD transformations are the same, Monte Carlo experiments can be\nconducted much faster if the FOD version of the estimator is used.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.05995v1", "link": "http://arxiv.org/abs/1808.05995v1"}, {"title": "A Mathematical Method for Deriving the Relative Effect of Serviceability\n  on Default Risk", "summary": "The writers propose a mathematical Method for deriving risk weights which\ndescribe how a borrower's income, relative to their debt service obligations\n(serviceability) affects the probability of default of the loan.\n  The Method considers the borrower's income not simply as a known quantity at\nthe time the loan is made, but as an uncertain quantity following a statistical\ndistribution at some later point in the life of the loan. This allows a\nprobability to be associated with an income level leading to default, so that\nthe relative risk associated with different serviceability levels can be\nquantified. In a sense, the Method can be thought of as an extension of the\nMerton Model to quantities that fail to satisfy Merton's 'critical' assumptions\nrelating to the efficient markets hypothesis.\n  A set of numerical examples of risk weights derived using the Method suggest\nthat serviceability may be under-represented as a risk factor in many mortgage\ncredit risk models.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1111.5397v1", "link": "http://arxiv.org/abs/1111.5397v1"}, {"title": "Portfolio Rebalancing under Uncertainty Using Meta-heuristic Algorithm", "summary": "In this paper, we solve portfolio rebalancing problem when security returns\nare represented by uncertain variables considering transaction costs. The\nperformance of the proposed model is studied using constant-proportion\nportfolio insurance (CPPI) as rebalancing strategy. Numerical results showed\nthat uncertain parameters and different belief degrees will produce different\nefficient frontiers, and affect the performance of the proposed model.\nMoreover, CPPI strategy performs as an insurance mechanism and limits downside\nrisk in bear markets while it allows potential benefit in bull markets.\nFinally, using a globally optimization solver and genetic algorithm (GA) for\nsolving the model, we concluded that the problem size is an important factor in\nsolving portfolio rebalancing problem with uncertain parameters and to gain\nbetter results, it is recommended to use a meta-heuristic algorithm rather than\na global solver.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1812.07635v1", "link": "http://arxiv.org/abs/1812.07635v1"}, {"title": "The Growth of Business Firms: Theoretical Framework and Empirical\n  Evidence", "summary": "We introduce a model of proportional growth to explain the distribution of\nbusiness firm growth rates. The model predicts that the distribution is\nexponential in the central part and depicts an asymptotic power-law behavior in\nthe tails with an exponent 3. Because of data limitations, previous studies in\nthis field have been focusing exclusively on the Laplace shape of the body of\nthe distribution. In this article, we test the model at different levels of\naggregation in the economy, from products to firms to countries, and we find\nthat the model's predictions agree with empirical growth distributions and\nsize-variance relationships.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0512005v1", "link": "http://dx.doi.org/10.1073/pnas.0509543102"}, {"title": "Long Memory and Volatility Clustering: is the empirical evidence\n  consistent across stock markets?", "summary": "Long memory and volatility clustering are two stylized facts frequently\nrelated to financial markets. Traditionally, these phenomena have been studied\nbased on conditionally heteroscedastic models like ARCH, GARCH, IGARCH and\nFIGARCH, inter alia. One advantage of these models is their ability to capture\nnonlinear dynamics. Another interesting manner to study the volatility\nphenomena is by using measures based on the concept of entropy. In this paper\nwe investigate the long memory and volatility clustering for the SP 500, NASDAQ\n100 and Stoxx 50 indexes in order to compare the US and European Markets.\nAdditionally, we compare the results from conditionally heteroscedastic models\nwith those from the entropy measures. In the latter, we examine Shannon\nentropy, Renyi entropy and Tsallis entropy. The results corroborate the\nprevious evidence of nonlinear dynamics in the time series considered.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0709.2178v3", "link": "http://dx.doi.org/10.1016/j.physa.2008.01.046"}, {"title": "Analysis of Financial Credit Risk Using Machine Learning", "summary": "Corporate insolvency can have a devastating effect on the economy. With an\nincreasing number of companies making expansion overseas to capitalize on\nforeign resources, a multinational corporate bankruptcy can disrupt the world's\nfinancial ecosystem. Corporations do not fail instantaneously; objective\nmeasures and rigorous analysis of qualitative (e.g. brand) and quantitative\n(e.g. econometric factors) data can help identify a company's financial risk.\nGathering and storage of data about a corporation has become less difficult\nwith recent advancements in communication and information technologies. The\nremaining challenge lies in mining relevant information about a company's\nhealth hidden under the vast amounts of data, and using it to forecast\ninsolvency so that managers and stakeholders have time to react. In recent\nyears, machine learning has become a popular field in big data analytics\nbecause of its success in learning complicated models. Methods such as support\nvector machines, adaptive boosting, artificial neural networks, and Gaussian\nprocesses can be used for recognizing patterns in the data (with a high degree\nof accuracy) that may not be apparent to human analysts. This thesis studied\ncorporate bankruptcy of manufacturing companies in Korea and Poland using\nexperts' opinions and financial measures, respectively. Using publicly\navailable datasets, several machine learning methods were applied to learn the\nrelationship between the company's current state and its fate in the near\nfuture. Results showed that predictions with accuracy greater than 95% were\nachievable using any machine learning technique when informative features like\nexperts' assessment were used. However, when using purely financial factors to\npredict whether or not a company will go bankrupt, the correlation is not as\nstrong.", "category": ["q-fin.ST", "econ.EM", "q-fin.MF"], "id": "http://arxiv.org/abs/1802.05326v1", "link": "http://dx.doi.org/10.13140/RG.2.2.30242.53449"}, {"title": "Copula-Based Univariate Time Series Structural Shift Identification Test", "summary": "An approach is proposed to determine structural shift in time-series assuming\nnon-linear dependence of lagged values of dependent variable. Copulas are used\nto model non-linear dependence of time series components.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1609.05056v1", "link": "http://arxiv.org/abs/1609.05056v1"}, {"title": "Fitting the Log Periodic Power Law to financial crashes: a critical\n  analysis", "summary": "A number of papers claim that a Log Periodic Power Law (LPPL) fitted to\nfinancial market bubbles that precede large market falls or 'crashes', contain\nparameters that are confined within certain ranges. The mechanism that has been\nclaimed as underlying the LPPL, is based on influence percolation and a\nmartingale condition. This paper examines these claims and the robustness of\nthe LPPL for capturing large falls in the Hang Seng stock market index, over a\n30-year period, including the current global downturn. We identify 11 crashes\non the Hang Seng market over the period 1970 to 2008. The fitted LPPLs have\nparameter values within the ranges specified post hoc by Johansen and Sornette\n(2001) for only seven of these crashes. Interestingly, the LPPL fit could have\npredicted the substantial fall in the Hang Seng index during the recent global\ndownturn. We also find that influence percolation combined with a martingale\ncondition holds for only half of the pre-crash bubbles previously reported.\nOverall, the mechanism posited as underlying the LPPL does not do so, and the\ndata used to support the fit of the LPPL to bubbles does so only partially.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1002.1010v2", "link": "http://arxiv.org/abs/1002.1010v2"}, {"title": "Understanding the explosive trend in EU ETS prices -- fundamentals or\n  speculation?", "summary": "In 2018, allowance prices in the EU Emission Trading Scheme (EU ETS)\nexperienced a run-up from persistently low levels in previous years. Regulators\nattribute this to a comprehensive reform in the same year, and are confident\nthe new price level reflects an anticipated tighter supply of allowances. We\nask if this is indeed the case, or if it is an overreaction of the market\ndriven by speculation. We combine several econometric methods - time-varying\ncoefficient regression, formal bubble detection as well as time stamping and\ncrash odds prediction - to juxtapose the regulators' claim versus the\nconcurrent explanation. We find evidence of a long period of explosive\nbehaviour in allowance prices, starting in March 2018 when the reform was\nadopted. Our results suggest that the reform triggered market participants into\nspeculation, and question regulators' confidence in its long-term outcome. This\nhas implications for both the further development of the EU ETS, and the long\nlasting debate about taxes versus emission trading schemes.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1906.10572v5", "link": "http://arxiv.org/abs/1906.10572v5"}, {"title": "Empirical Process Results for Exchangeable Arrays", "summary": "Exchangeable arrays are natural tools to model common forms of dependence\nbetween units of a sample. Jointly exchangeable arrays are well suited to\ndyadic data, where observed random variables are indexed by two units from the\nsame population. Examples include trade flows between countries or\nrelationships in a network. Separately exchangeable arrays are well suited to\nmultiway clustering, where units sharing the same cluster (e.g. geographical\nareas or sectors of activity when considering individual wages) may be\ndependent in an unrestricted way. We prove uniform laws of large numbers and\ncentral limit theorems for such exchangeable arrays. We obtain these results\nunder the same moment restrictions and conditions on the class of functions as\nthose typically assumed with i.i.d. data. We also show the convergence of\nbootstrap processes adapted to such arrays.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1906.11293v2", "link": "http://arxiv.org/abs/1906.11293v2"}, {"title": "A Numerical Method for Pricing Discrete Double Barrier Option by\n  Legendre Multiwavelet", "summary": "In this Article, a fast numerical numerical algorithm for pricing discrete\ndouble barrier option is presented. According to Black-Scholes model, the price\nof option in each monitoring date can be evaluated by a recursive formula upon\nthe heat equation solution. These recursive solutions are approximated by using\nLegendre multiwavelets as orthonormal basis functions and expressed in\noperational matrix form. The most important feature of this method is that its\nCPU time is nearly invariant when monitoring dates increase. Besides, the rate\nof convergence of presented algorithm was obtained. The numerical results\nverify the validity and efficiency of the numerical method.", "category": ["q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1703.09129v2", "link": "http://dx.doi.org/10.1016/j.cam.2017.07.033"}, {"title": "Fluctuation-Dissipation Theory of Input-Output Interindustrial\n  Correlations", "summary": "In this study, the fluctuation-dissipation theory is invoked to shed light on\ninput-output interindustrial relations at a macroscopic level by its\napplication to IIP (indices of industrial production) data for Japan.\nStatistical noise arising from finiteness of the time series data is carefully\nremoved by making use of the random matrix theory in an eigenvalue analysis of\nthe correlation matrix; as a result, two dominant eigenmodes are detected. Our\nprevious study successfully used these two modes to demonstrate the existence\nof intrinsic business cycles. Here a correlation matrix constructed from the\ntwo modes describes genuine interindustrial correlations in a statistically\nmeaningful way. Further it enables us to quantitatively discuss the\nrelationship between shipments of final demand goods and production of\nintermediate goods in a linear response framework. We also investigate\ndistinctive external stimuli for the Japanese economy exerted by the current\nglobal economic crisis. These stimuli are derived from residuals of moving\naverage fluctuations of the IIP remaining after subtracting the long-period\ncomponents arising from inherent business cycles. The observation reveals that\nthe fluctuation-dissipation theory is applicable to an economic system that is\nsupposed to be far from physical equilibrium.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/0912.1985v2", "link": "http://arxiv.org/abs/0912.1985v2"}, {"title": "Information inefficiency in a random linear economy model", "summary": "We study the effects of introducing information inefficiency in a model for a\nrandom linear economy with a representative consumer. This is done by\nconsidering statistical, instead of classical, economic general equilibria.\nEmploying two different approaches we show that inefficiency increases the\nconsumption set of a consumer but decreases her expected utility. In this\nscenario economic activity grows while welfare shrinks, that is the opposite of\nthe behavior obtained by considering a rational consumer.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1610.01270v2", "link": "http://arxiv.org/abs/1610.01270v2"}, {"title": "Network-Aware Strategies in Financial Systems", "summary": "We study the incentives of banks in a financial network, where the network\nconsists of debt contracts and credit default swaps (CDSs) between banks. One\nof the most important questions in such a system is the problem of deciding\nwhich of the banks are in default, and how much of their liabilities these\nbanks can pay. We study the payoff and preferences of the banks in the\ndifferent solutions to this problem. We also introduce a more refined model\nwhich allows assigning priorities to payment obligations; this provides a more\nexpressive and realistic model of real-life financial systems, while it always\nensures the existence of a solution.\n  The main focus of the paper is an analysis of the actions that a single bank\ncan execute in a financial system in order to influence the outcome to its\nadvantage. We show that removing an incoming debt, or donating funds to another\nbank can result in a single new solution that is strictly more favorable to the\nacting bank. We also show that increasing the bank's external funds or\nmodifying the priorities of outgoing payments cannot introduce a more favorable\nnew solution into the system, but may allow the bank to remove some unfavorable\nsolutions, or to increase its recovery rate. Finally, we show how the actions\nof two banks in a simple financial system can result in classical game\ntheoretic situations like the prisoner's dilemma or the dollar auction,\ndemonstrating the wide expressive capability of the financial system model.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/2002.07566v1", "link": "http://arxiv.org/abs/2002.07566v1"}, {"title": "Testing for Common Breaks in a Multiple Equations System", "summary": "The issue addressed in this paper is that of testing for common breaks across\nor within equations of a multivariate system. Our framework is very general and\nallows integrated regressors and trends as well as stationary regressors. The\nnull hypothesis is that breaks in different parameters occur at common\nlocations and are separated by some positive fraction of the sample size unless\nthey occur across different equations. Under the alternative hypothesis, the\nbreak dates across parameters are not the same and also need not be separated\nby a positive fraction of the sample size whether within or across equations.\nThe test considered is the quasi-likelihood ratio test assuming normal errors,\nthough as usual the limit distribution of the test remains valid with\nnon-normal errors. Of independent interest, we provide results about the rate\nof convergence of the estimates when searching over all possible partitions\nsubject only to the requirement that each regime contains at least as many\nobservations as some positive fraction of the sample size, allowing break dates\nnot separated by a positive fraction of the sample size across equations.\nSimulations show that the test has good finite sample properties. We also\nprovide an application to issues related to level shifts and persistence for\nvarious measures of inflation to illustrate its usefulness.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1606.00092v2", "link": "http://arxiv.org/abs/1606.00092v2"}, {"title": "Ordinal Tax To Sustain a Digital Economy", "summary": "Recently, the French Senate approved a law that imposes a 3% tax on revenue\ngenerated from digital services by companies above a certain size. While there\nis a lot of political debate about economic consequences of this action, it is\nactually interesting to reverse the question: We consider the long-term\nimplications of an economy with no such digital tax. More generally, we can\nthink of digital services as a special case of products with low or zero cost\nof transportation. With basic economic models we show that a market with no\ntransportation costs is prone to monopolization as minuscule, random\ndifferences in quality are rewarded disproportionally. We then propose a\ndistance-based tax to counter-balance the tendencies of random centralisation.\nUnlike a tax that scales with physical (cardinal) distance, a ranked (ordinal)\ndistance tax leverages the benefits of digitalization while maintaining a\nstable economy.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.03287v1", "link": "http://arxiv.org/abs/1908.03287v1"}, {"title": "Crunching Mortality and Life Insurance Portfolios with extended\n  CreditRisk+", "summary": "Using an extended version of the credit risk model CreditRisk+, we develop a\nflexible framework with numerous applications amongst which we find stochastic\nmortality modelling, forecasting of death causes as well as profit and loss\nmodelling of life insurance and annuity portfolios which can be used in\n(partial) internal models under Solvency II. Yet, there exists a fast and\nnumerically stable algorithm to derive loss distributions exactly, even for\nlarge portfolios. We provide various estimation procedures based on publicly\navailable data. Compared to the Lee-Carter model, we have a more flexible\nframework, get tighter bounds and can directly extract several sources of\nuncertainty. Straight-forward model validation techniques are available.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1601.04557v4", "link": "http://arxiv.org/abs/1601.04557v4"}, {"title": "Approximations and asymptotics of upper hedging prices in multinomial\n  models", "summary": "We give an exposition and numerical studies of upper hedging prices in\nmultinomial models from the viewpoint of linear programming and the\ngame-theoretic probability of Shafer and Vovk. We also show that, as the number\nof rounds goes to infinity, the upper hedging price of a European option\nconverges to the solution of the Black-Scholes-Barenblatt equation.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1007.4372v2", "link": "http://arxiv.org/abs/1007.4372v2"}, {"title": "Exponential distribution of financial returns at mesoscopic time lags: a\n  new stylized fact", "summary": "We study the probability distribution of stock returns at mesoscopic time\nlags (return horizons) ranging from about an hour to about a month. While at\nshorter microscopic time lags the distribution has power-law tails, for\nmesoscopic times the bulk of the distribution (more than 99% of the\nprobability) follows an exponential law. The slope of the exponential function\nis determined by the variance of returns, which increases proportionally to the\ntime lag. At longer times, the exponential law continuously evolves into\nGaussian distribution. The exponential-to-Gaussian crossover is well described\nby the analytical solution of the Heston model with stochastic volatility.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0401225v2", "link": "http://dx.doi.org/10.1016/j.physa.2004.06.122"}, {"title": "A subordinated CIR intensity model with application to Wrong-Way risk\n  CVA", "summary": "Credit Valuation Adjustment (CVA) pricing models need to be both flexible and\ntractable. The survival probability has to be known in closed form (for\ncalibration purposes), the model should be able to fit any valid Credit Default\nSwap (CDS) curve, should lead to large volatilities (in line with CDS options)\nand finally should be able to feature significant Wrong-Way Risk (WWR) impact.\nThe Cox-Ingersoll-Ross model (CIR) combined with independent positive jumps and\ndeterministic shift (JCIR++) is a very good candidate : the variance (and thus\ncovariance with exposure, i.e. WWR) can be increased with the jumps, whereas\nthe calibration constraint is achieved via the shift. In practice however,\nthere is a strong limit on the model parameters that can be chosen, and thus on\nthe resulting WWR impact. This is because only non-negative shifts are allowed\nfor consistency reasons, whereas the upwards jumps of the JCIR++ need to be\ncompensated by a downward shift. To limit this problem, we consider the\ntwo-side jump model recently introduced by Mendoza-Arriaga \\& Linetsky, built\nby time-changing CIR intensities. In a multivariate setup like CVA,\ntime-changing the intensity partly kills the potential correlation with the\nexposure process and destroys WWR impact. Moreover, it can introduce a forward\nlooking effect that can lead to arbitrage opportunities. In this paper, we use\nthe time-changed CIR process in a way that the above issues are avoided. We\nshow that the resulting process allows to introduce a large WWR effect compared\nto the JCIR++ model. The computation cost of the resulting Monte Carlo\nframework is reduced by using an adaptive control variate procedure.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1801.05673v1", "link": "http://arxiv.org/abs/1801.05673v1"}, {"title": "Machine Learning Techniques for Mortality Modeling", "summary": "Various stochastic models have been proposed to estimate mortality rates. In\nthis paper we illustrate how machine learning techniques allow us to analyze\nthe quality of such mortality models. In addition, we present how these\ntechniques can be used for differentiating the different causes of death in\nmortality modeling.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1705.03396v1", "link": "http://arxiv.org/abs/1705.03396v1"}, {"title": "Topology and Behaviour of Agents: Capital Markets", "summary": "On a capital market the social group is formed from traders. Individual\nbehaviour of agents is influenced by the need to associate with other agents\nand to obtain the approval of other agents in the group. Making decisions an\nindividual equates own needs with those of the other agents. Any two agents\nfrom the group may interact. The interaction consists of the exchange of\ninformation and it costs some money. We assume that agents give reference to\nthe origin of the information if asked by other agents. Thus the agent may\nverify obtained private information. Hudak recently used methods described by\nRivier to study social behaviour of such agents. He characterized the quantity\nwhich corresponds to verification of information. Quantity which characterizes\nverification of information contributes to an aversion of an agent with respect\nto a risk. The mix of investments of an agent in a given cell with an average\nmeasure A of risk aversion in the cell is found from minimum of the average per\ncell aim function $<FM>$. Absolute minimum corresponds to such a state in which\nthere is an optimal mix of the exchange of information for a given expectations\nabout the capital market. The crowd and personal /$\\approx <f>$/ contributions\nto the risk aversion of an agent are present in the aversion constant A. We\nhave discussed a stable and metastable states of the market for different\nvalues of E, an expected return for a given investment period, of EV, an\nexpected risk for a given investment period, and of b, a constant which\ncharacterizes contribution of the quantity $<f>$ to the risk aversion. Variance\nof n for the distribution of nonreducibile subgroups is found. Our model\ndescribes intermediary process effects.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0501002v2", "link": "http://arxiv.org/abs/cond-mat/0501002v2"}, {"title": "Data science for assessing possible tax income manipulation: The case of\n  Italy", "summary": "This paper explores a real-world fundamental theme under a data science\nperspective. It specifically discusses whether fraud or manipulation can be\nobserved in and from municipality income tax size distributions, through their\naggregation from citizen fiscal reports. The study case pertains to official\ndata obtained from the Italian Ministry of Economics and Finance over the\nperiod 2007-2011. All Italian (20) regions are considered. The considered data\nscience approach concretizes in the adoption of the Benford first digit law as\nquantitative tool. Marked disparities are found, - for several regions, leading\nto unexpected \"conclusions\". The most eye browsing regions are not the expected\nones according to classical imagination about Italy financial shadow matters.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1709.02129v1", "link": "http://dx.doi.org/10.1016/j.chaos.2017.08.012"}, {"title": "Entropy and credit risk in highly correlated markets", "summary": "We compare two models of corporate default by calculating the\nJeffreys-Kullback-Leibler divergence between their predicted default\nprobabilities when asset correlations are either high or low. Our main results\nshow that the divergence between the two models increases in highly correlated,\nvolatile, and large markets, but that it is closer to zero in small markets,\nwhen asset correlations are low and firms are highly leveraged. These findings\nsuggest that during periods of financial instability the single-and\nmulti-factor models of corporate default will generate increasingly\ninconsistent predictions.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1604.07042v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.02.083"}, {"title": "The Alpha-Heston Stochastic Volatility Model", "summary": "We introduce an affine extension of the Heston model where the instantaneous\nvariance process contains a jump part driven by $\\alpha$-stable processes with\n$\\alpha\\in(1,2]$. In this framework, we examine the implied volatility and its\nasymptotic behaviors for both asset and variance options. Furthermore, we\nexamine the jump clustering phenomenon observed on the variance market and\nprovide a jump cluster decomposition which allows to analyse the cluster\nprocesses.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1812.01914v1", "link": "http://arxiv.org/abs/1812.01914v1"}, {"title": "Statistical Properties of the Returns of Stock Prices of International\n  Markets", "summary": "We investigate statistical properties of daily international market indices\nof seven countries, and high-frequency $S&P500$ and KOSDAQ data, by using the\ndetrended fluctuation method and the surrogate test. We have found that the\nreturns of international stock market indices of seven countries follow a\nuniversal power-law distribution with an exponent of $\\zeta \\approx 3$, while\nthe Korean stock market follows an exponential distribution with an exponent of\n$\\beta \\approx 0.7$. The Hurst exponent analysis of the original return, and\nits magnitude and sign series, reveal that the long-term-memory property, which\nis absent in the returns and sign series, exists in the magnitude time series\nwith $0.7 \\leq H \\leq 0.8$. The surrogate test shows that the magnitude time\nseries reflects the non-linearity of the return series, which helps to reveal\nthat the KOSDAQ index, one of the emerging markets, shows higher volatility\nthan a mature market such as the {S&P} 500 index.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0601126v1", "link": "http://arxiv.org/abs/physics/0601126v1"}, {"title": "How Can We Induce More Women to Competitions?", "summary": "Why women avoid participating in a competition and how can we encourage them\nto participate in it? In this paper, we investigate how social image concerns\naffect women's decision to compete. We first construct a theoretical model and\nshow that participating in a competition, even under affirmative action\npolicies favoring women, is costly for women under public observability since\nit deviates from traditional female gender norms, resulting in women's low\nappearance in competitive environments. We propose and theoretically show that\nintroducing prosocial incentives in the competitive environment is effective\nand robust to public observability since (i) it induces women who are\nintrinsically motivated by prosocial incentives to the competitive environment\nand (ii) it makes participating in a competition not costly for women from\nsocial image point of view. We conduct a laboratory experiment where we\nrandomly manipulate the public observability of decisions to compete and test\nour theoretical predictions. The results of the experiment are fairly\nconsistent with our theoretical predictions. We suggest that when designing\npolicies to promote gender equality in competitive environments, using\nprosocial incentives through company philanthropy or other social\nresponsibility policies, either as substitutes or as complements to traditional\naffirmative action policies, could be promising.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.10518v1", "link": "http://arxiv.org/abs/1801.10518v1"}, {"title": "Influence of introducing high speed railways on intercity travel\n  behavior in Vietnam", "summary": "It is one of hottest topics in Vietnam whether to construct a High Speed Rail\n(HSR) system or not in near future. To analyze the impacts of introducing the\nHSR on the intercity travel behavior, this research develops an integrated\nintercity demand forecasting model to represent trip generation and frequency,\ndestination choice and travel mode choice behavior. For this purpose, a\ncomprehensive questionnaire survey with both Revealed Preference (RP)\ninformation (an inter-city trip diary) and Stated Preference (SP) information\nwas conducted in Hanoi in 2011. In the SP part, not only HSR, but also Low Cost\nCarrier is included in the choice set, together with other existing inter-city\ntravel modes. To make full use of the advantages of each type of data and to\novercome their disadvantages, RP and SP data are combined to describe the\ndestination choice and mode choice behavior, while trip generation and\nfrequency are represented by using the RP data. The model estimation results\nshow the inter-relationship between trip generation and frequency, destination\nchoice and travel mode choice, and confirm that those components should not\ndealt with separately.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.00155v1", "link": "http://arxiv.org/abs/1810.00155v1"}, {"title": "Small time central limit theorems for semimartingales with applications", "summary": "We give conditions under which the normalized marginal distribution of a\nsemimartingale converges to a Gaussian limit law as time tends to zero. In\nparticular, our result is applicable to solutions of stochastic differential\nequations with locally bounded and continuous coefficients. The limit theorems\nare subsequently extended to functional central limit theorems on the process\nlevel. We present two applications of the results in the field of mathematical\nfinance: to the pricing of at-the-money digital options with short maturities\nand short time implied volatility skews.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1208.4282v1", "link": "http://arxiv.org/abs/1208.4282v1"}, {"title": "Continuous-Time Public Good Contribution under Uncertainty: A Stochastic\n  Control Approach", "summary": "In this paper we study continuous-time stochastic control problems with both\nmonotone and classical controls motivated by the so-called public good\ncontribution problem. That is the problem of n economic agents aiming to\nmaximize their expected utility allocating initial wealth over a given time\nperiod between private consumption and irreversible contributions to increase\nthe level of some public good. We investigate the corresponding social planner\nproblem and the case of strategic interaction between the agents, i.e. the\npublic good contribution game. We show existence and uniqueness of the social\nplanner's optimal policy, we characterize it by necessary and sufficient\nstochastic Kuhn-Tucker conditions and we provide its expression in terms of the\nunique optional solution of a stochastic backward equation. Similar stochastic\nfirst order conditions prove to be very useful for studying any Nash equilibria\nof the public good contribution game. In the symmetric case they allow us to\nprove (qualitative) uniqueness of the Nash equilibrium, which we again\nconstruct as the unique optional solution of a stochastic backward equation. We\nfinally also provide a detailed analysis of the so-called free rider effect.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1307.2849v4", "link": "http://dx.doi.org/10.1007/s00245-016-9337-5"}, {"title": "Kernel Estimation for Panel Data with Heterogeneous Dynamics", "summary": "This paper proposes nonparametric kernel-smoothing estimation for panel data\nto examine the degree of heterogeneity across cross-sectional units. We first\nestimate the sample mean, autocovariances, and autocorrelations for each unit\nand then apply kernel smoothing to compute their density functions. The\ndependence of the kernel estimator on bandwidth makes asymptotic bias of very\nhigh order affect the required condition on the relative magnitudes of the\ncross-sectional sample size (N) and the time-series length (T). In particular,\nit makes the condition on N and T stronger and more complicated than those\ntypically observed in the long-panel literature without kernel smoothing. We\nalso consider a split-panel jackknife method to correct bias and construction\nof confidence intervals. An empirical application and Monte Carlo simulations\nillustrate our procedure in finite samples.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1802.08825v4", "link": "http://arxiv.org/abs/1802.08825v4"}, {"title": "Viscosity properties with singularities in a state-constrained expected\n  utility maximization problem", "summary": "We consider the value function originating from an expected utility\nmaximization problem with finite fuel constraint and show its close relation to\na nonlinear parabolic degenerated Hamilton-Jacobi-Bellman (HJB) equation with\nsingularity. On one hand, we give a so-called verification argument based on\nthe dynamic programming principle, which allows us to derive conditions under\nwhich a classical solution of the HJB equation coincides with our value\nfunction (provided that it is smooth enough). On the other hand, we establish a\ncomparison principle, which allows us to characterize our value function as the\nunique viscosity solution of the HJB equation.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1510.03584v1", "link": "http://arxiv.org/abs/1510.03584v1"}, {"title": "The asymptotic smile of a multiscaling stochastic volatility model", "summary": "We consider a stochastic volatility model which captures relevant stylized\nfacts of financial series, including the multi-scaling of moments. The\nvolatility evolves according to a generalized Ornstein-Uhlenbeck processes with\nsuper-linear mean reversion.\n  Using large deviations techniques, we determine the asymptotic shape of the\nimplied volatility surface in any regime of small maturity $t \\to 0$ or extreme\nlog-strike $|\\kappa| \\to \\infty$ (with bounded maturity). Even if the price has\ncontinuous paths, out-of-the-money implied volatility diverges for small\nmaturity, producing a very pronounced smile.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1501.03387v4", "link": "http://arxiv.org/abs/1501.03387v4"}, {"title": "From short to fat tails in financial markets: A unified description", "summary": "In complex systems such as turbulent flows and financial markets, the\ndynamics in long and short time-lags, signaled by Gaussian and fat-tailed\nstatistics, respectively, calls for a unified description. To address this\nissue we analyze a real dataset, namely, price fluctuations, in a wide range of\ntemporal scales to embrace both regimes. By means of Kramers-Moyal (KM)\ncoefficients evaluated from empirical time series, we obtain the evolution\nequation for the probability density function (PDF) of price returns. We also\npresent consistent asymptotic solutions for the timescale dependent equation\nthat emerges from the empirical analysis. From these solutions, new\nrelationships connecting PDF characteristics, such as tail exponents, to\nparameters of KM coefficients arise. The results reveal a dynamical path that\nleads from Gaussian to fat-tailed statistics, furnishing insights on other\ncomplex systems where akin crossover is observed.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0801.3263v1", "link": "http://arxiv.org/abs/0801.3263v1"}, {"title": "Michael Milken: The Junk Dealer", "summary": "We take a closer look at the life and legacy of Micheal Milken. We discuss\nwhy Michael Milken, also know as the Junk Bond King, was not just any other\nKing or run-of-the-mill Junk Dealer, but \"The Junk Dealer\". We find parallels\nbetween the three parts to any magic act and what Micheal Milken did, showing\nthat his accomplishments were nothing short of a miracle. His compensation at\nthat time captures to a certain extent the magnitude of the changes he brought\nabout, the eco-system he created for businesses to flourish, the impact he had\non the wider economy and also on the future growth and development of American\nIndustry. We emphasize two of his contributions to the financial industry that\nhave grown in importance over the years. One was the impetus given to the\nPrivate Equity industry and the use of LBOs. The second was the realization\nthat thorough research was the key to success, financial and otherwise. Perhaps\nan unintended consequence of the growth in junk bonds and tailored financing\nwas the growth of Silicon valley and technology powerhouses in the California\nbay area. Investors witnessed that there was a possibility for significant\nreturns and that financial success could be had due to the risk mitigation that\nMilken demonstrated by investing in portfolios of so called high risk and low\nprofile companies. We point out the current trend in many regions of the world,\nwhich is the birth of financial and technology firms and we suggest that\nfinding innovative ways of financing could be the key to the sustained growth\nof these eco-systems.", "category": ["q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/1910.13882v1", "link": "http://dx.doi.org/10.3905/jpe.2019.1.095"}, {"title": "Joint multifractal analysis based on the partition function approach:\n  Analytical analysis, numerical simulation and empirical application", "summary": "Many complex systems generate multifractal time series which are long-range\ncross-correlated. Numerous methods have been proposed to characterize the\nmultifractal nature of these long-range cross correlations. However, several\nimportant issues about these methods are not well understood and most methods\nconsider only one moment order. We study the joint multifractal analysis based\non partition function with two moment orders, which was initially invented to\ninvestigate fluid fields, and derive analytically several important properties.\nWe apply the method numerically to binomial measures with multifractal cross\ncorrelations and bivariate fractional Brownian motions without multifractal\ncross correlations. For binomial multifractal measures, the explicit\nexpressions of mass function, singularity strength and multifractal spectrum of\nthe cross correlations are derived, which agree excellently with the numerical\nresults. We also apply the method to stock market indexes and unveil intriguing\nmultifractality in the cross correlations of index volatilities.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1509.05952v1", "link": "http://dx.doi.org/10.1088/1367-2630/17/10/103020"}, {"title": "Time will tell - Recovering Preferences when Choices are Noisy", "summary": "The ability to uncover preferences from choices is fundamental for both\npositive economics and welfare analysis. Overwhelming evidence shows that\nchoice is stochastic, which has given rise to random utility models as the\ndominant paradigm in applied microeconomics. However, as is well known, it is\nnot possible to infer the structure of preferences in the absence of\nassumptions on the structure of noise. This makes it impossible to empirically\ntest the structure of noise independently from the structure of preferences.\nHere, we show that the difficulty can be bypassed if data sets are enlarged to\ninclude response times. A simple condition on response time distributions (a\nweaker version of first order stochastic dominance) ensures that choices reveal\npreferences without assumptions on the structure of utility noise. Sharper\nresults are obtained if the analysis is restricted to specific classes of\nmodels. Under symmetric noise, response times allow to uncover preferences for\nchoice pairs outside the data set, and if noise is Fechnerian, even choice\nprobabilities can be forecast out of sample. We conclude by showing that\nstandard random utility models from economics and standard drift-diffusion\nmodels from psychology necessarily generate data sets fulfilling our sufficient\ncondition on response time distributions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.02497v1", "link": "http://arxiv.org/abs/1811.02497v1"}, {"title": "Growth and Allocation of Resources in Economics: The Agent-Based\n  Approach", "summary": "Some agent-based models for growth and allocation of resources are described.\nThe first class considered consists of conservative models, where the number of\nagents and the size of resources are constant during time evolution. The second\nclass is made up of multiplicative noise models and some of their extensions to\ncontinuous-time.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0608221v1", "link": "http://dx.doi.org/10.1016/j.physa.2006.04.038"}, {"title": "Two approaches to modeling the interaction of small and medium\n  price-taking traders with a stock exchange by mathematical programming\n  techniques", "summary": "The paper presents two new approaches to modeling the interaction of small\nand medium pricetaking traders with a stock exchange. In the framework of these\napproaches, the traders can form and manage their portfolios of financial\ninstruments traded on a stock exchange with the use of linear, integer, and\nmixed programming techniques. Unlike previous authors publications on the\nsubject, besides standard securities, the present publication considers\nderivative financial instruments such as futures and options contracts.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1610.05703v1", "link": "http://arxiv.org/abs/1610.05703v1"}, {"title": "Limit order market analysis and modelling: on an universal cause for\n  over-diffusive prices", "summary": "We briefly review data analysis of the Island order book, part of NASDAQ,\nwhich suggests a framework to which all limit order markets should comply.\nUsing a simple exclusion particle model, we argue that short-time price\nover-diffusion in limit order markets is due to the non-equilibrium of order\nplacement, cancellation and execution rates, which is an inherent feature of\nreal limit order markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0211082v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01895-2"}, {"title": "The Futures Premium and Rice Market Efficiency in Prewar Japan", "summary": "This paper studies the interrelation between spot and futures prices in the\ntwo major rice markets in prewar Japan from the perspective of market\nefficiency. Applying a non-Bayesian time-varying model approach to the\nfundamental equation for spot returns and the futures premium, we detect when\nefficiency reductions in the two major rice markets occurred. We also examine\nhow government interventions affected the rice markets in Japan, which\ncolonized Taiwan and Korea before World War II, and argue that the function of\nrice futures markets crucially depended on the differences in rice spot\nmarket's structure. The increased volume of imported rice of a different\nvariety from domestic rice first disrupted the rice futures. Then, government\nintervention in the rice futures markets failed to improve the disruption.\nChanges in colonial rice cropping successfully improved the disruption, and\ncolonial rice was promoted to unify the different varieties of inland and\ncolonial rice.", "category": ["q-fin.ST", "q-fin.GN"], "id": "http://arxiv.org/abs/1404.5381v5", "link": "http://arxiv.org/abs/1404.5381v5"}, {"title": "Weak and strong no-arbitrage conditions for continuous financial markets", "summary": "We propose a unified analysis of a whole spectrum of no-arbitrage conditions\nfor financial market models based on continuous semimartingales. In particular,\nwe focus on no-arbitrage conditions weaker than the classical notions of No\nArbitrage and No Free Lunch with Vanishing Risk. We provide a complete\ncharacterisation of the considered no-arbitrage conditions, linking their\nvalidity to the characteristics of the discounted asset price process and to\nthe existence and the properties of (weak) martingale deflators, and review\nclassical as well as recent results.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1302.7192v2", "link": "http://dx.doi.org/10.1142/S0219024915500053"}, {"title": "A non-equilibrium formulation of food security resilience", "summary": "Resilience, the ability to recover from adverse events (\"shocks\"), is of\nfundamental importance to food security. This is especially true in poor\ncountries, where basic needs are frequently threatened by economic,\nenvironmental, and health shocks. An empirically sound formalization of the\nconcept of food security resilience, however, is lacking. Here we introduce a\ngeneral framework for quantifying resilience based on a simple definition: a\nunit is resilient if $(a)$ its long-term food security trend is not\ndeteriorating and $(b)$ the effects of shocks on this trend do not persist over\ntime. Our approach can be applied to any food security variable for which\nhigh-frequency time-series data is available, can accommodate any unit of\nanalysis (e.g., individuals, households, countries), and is especially useful\nin rapidly changing contexts wherein standard equilibrium-based economic models\nare ineffective. We illustrate our method with an analysis of per capita\nkilocalorie availability for 161 countries between 1961 and 2011. We find that\nresilient countries are not necessarily those that are characterized by high\nlevels or less volatile fluctuations of kilocalorie intake. Accordingly, food\nsecurity policies and programs will need to be tailored not only to welfare\nlevels at any one time, but also to long-run welfare dynamics.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1606.02783v1", "link": "http://dx.doi.org/10.1098/rsos.160874"}, {"title": "Multi-frequency-band tests for white noise under heteroskedasticity", "summary": "This paper proposes a new family of multi-frequency-band (MFB) tests for the\nwhite noise hypothesis by using the maximum overlap discrete wavelet packet\ntransform (MODWPT). The MODWPT allows the variance of a process to be\ndecomposed into the variance of its components on different equal-length\nfrequency sub-bands, and the MFB tests then measure the distance between the\nMODWPT-based variance ratio and its theoretical null value jointly over several\nfrequency sub-bands. The resulting MFB tests have the chi-squared asymptotic\nnull distributions under mild conditions, which allow the data to be\nheteroskedastic. The MFB tests are shown to have the desirable size and power\nperformance by simulation studies, and their usefulness is further illustrated\nby two applications.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.09161v1", "link": "http://arxiv.org/abs/2004.09161v1"}, {"title": "The survival of start-ups in time of crisis. A machine learning approach\n  to measure innovation", "summary": "This paper shows how data science can contribute to improving empirical\nresearch in economics by leveraging on large datasets and extracting\ninformation otherwise unsuitable for a traditional econometric approach. As a\ntest-bed for our framework, machine learning algorithms allow us to create a\nnew holistic measure of innovation built on a 2012 Italian Law aimed at\nboosting new high-tech firms. We adopt this measure to analyse the impact of\ninnovativeness on a large population of Italian firms which entered the market\nat the beginning of the 2008 global crisis. The methodological contribution is\norganised in different steps. First, we train seven supervised learning\nalgorithms to recognise innovative firms on 2013 firmographics data and select\na combination of those with best predicting power. Second, we apply the former\non the 2008 dataset and predict which firms would have been labelled as\ninnovative according to the definition of the law. Finally, we adopt this new\nindicator as regressor in a survival model to explain firms' ability to remain\nin the market after 2008. Results suggest that the group of innovative firms\nare more likely to survive than the rest of the sample, but the survival\npremium is likely to depend on location.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.01073v1", "link": "http://arxiv.org/abs/1911.01073v1"}, {"title": "A variation of the Canadisation algorithm for the pricing of American\n  options driven by L\u00e9vy processes", "summary": "We introduce an algorithm for the pricing of finite expiry American options\ndriven by L\\'evy processes. The idea is to tweak Carr's `Canadisation' method,\ncf. Carr [9] (see also Bouchard et al [5]), in such a way that the adjusted\nalgorithm is viable for any L\\'evy process whose law at an independent,\nexponentially distributed time consists of a (possibly infinite) mixture of\nexponentials. This includes Brownian motion plus (hyper)exponential jumps, but\nalso the recently introduced rich class of so-called meromorphic L\\'evy\nprocesses, cf. Kyprianou et al [16]. This class contains all L\\'evy processes\nwhose L\\'evy measure is an infinite mixture of exponentials which can generate\nboth finite and infinite jump activity. L\\'evy processes well known in\nmathematical finance can in a straightforward way be obtained as a limit of\nmeromorphic L\\'evy processes. We work out the algorithm in detail for the\nclassic example of the American put, and we illustrate the results with some\nnumerics.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1304.4534v1", "link": "http://arxiv.org/abs/1304.4534v1"}, {"title": "A Comparative Study of Stochastic Volatility Models", "summary": "The correlated stochastic volatility models constitute a natural extension of\nthe Black and Scholes-Merton framework: here the volatility is not a constant,\nbut a stochastic process correlated with the price log-return one. At present,\nseveral stochastic volatility models are discussed in the literature, differing\nin the dynamics attached to the volatility. The aim of the present work is to\ncompare the most recent results about three popular models: the Vasicek, Heston\nand exponential Ornstein-Uhlenbeck models. We analyzed for each of them the\ntheoretical results known in the literature (volatility and return\ndistribution, higher-order moments and different-time correlations) in order to\ntest their predictive effectiveness on the outcomes of original numerical\nsimulations, paying particular attention to their ability to reproduce\nempirical statistical properties of prices. The numerical results demonstrate\nthat these models can be implemented maintaining all their features, especially\nin view of financial applications like market risk management or option\npricing. In order to critically compare the models, we also perform an\nempirical analysis of financial time series from the Italian stock market,\nshowing the exponential Ornstein-Uhlenbeck model's ability to capture the\nstylized facts of volatility and log-return probability distributions.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0709.0810v1", "link": "http://arxiv.org/abs/0709.0810v1"}, {"title": "Universal price impact functions of individual trades in an order-driven\n  market", "summary": "The trade size $\\omega$ has direct impact on the price formation of the stock\ntraded. Econophysical analyses of transaction data for the US and Australian\nstock markets have uncovered market-specific scaling laws, where a master curve\nof price impact can be obtained in each market when stock capitalization $C$ is\nincluded as an argument in the scaling relation. However, the rationale of\nintroducing stock capitalization in the scaling is unclear and the anomalous\nnegative correlation between price change $r$ and trade size $\\omega$ for small\ntrades is unexplained. Here we show that these issues can be addressed by\ntaking into account the aggressiveness of orders that result in trades together\nwith a proper normalization technique. Using order book data from the Chinese\nmarket, we show that trades from filled and partially filled limit orders have\nvery different price impact. The price impact of trades from partially filled\norders is constant when the volume is not too large, while that of filled\norders shows power-law behavior $r\\sim \\omega^\\alpha$ with $\\alpha\\approx2/3$.\nWhen returns and volumes are normalized by stock-dependent averages,\ncapitalization-independent scaling laws emerge for both types of trades.\nHowever, no scaling relation in terms of stock capitalization can be\nconstructed. In addition, the relation $\\alpha=\\alpha_\\omega/\\alpha_r$ is\nverified, where $\\alpha_\\omega$ and $\\alpha_r$ are the tail exponents of trade\nsizes and returns. These observations also enable us to explain the anomalous\nnegative correlation between $r$ and $\\omega$ for small-size trades. We\nanticipate that these regularities may hold in other order-driven markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0708.3198v2", "link": "http://dx.doi.org/10.1080/14697688.2010.504733"}, {"title": "Systemic risk assessment through high order clustering coefficient", "summary": "In this article we propose a novel measure of systemic risk in the context of\nfinancial networks. To this aim, we provide a definition of systemic risk which\nis based on the structure, developed at different levels, of clustered\nneighbours around the nodes of the network. The proposed measure incorporates\nthe generalized concept of clustering coefficient of order $l$ of a node $i$\nintroduced in Cerqueti et al. (2018). Its properties are also explored in terms\nof systemic risk assessment. Empirical experiments on the time-varying global\nbanking network show the effectiveness of the presented systemic risk measure\nand provide insights on how systemic risk has changed over the last years, also\nin the light of the recent financial crisis and the subsequent more stringent\nregulation for globally systemically important banks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1810.13250v1", "link": "http://dx.doi.org/10.1007/s10479-020-03525-8"}, {"title": "Pump and Dumps in the Bitcoin Era: Real Time Detection of Cryptocurrency\n  Market Manipulations", "summary": "In the last years, cryptocurrencies are increasingly popular. Even people who\nare not experts have started to invest in these securities and nowadays\ncryptocurrency exchanges process transactions for over 100 billion US dollars\nper month. However, many cryptocurrencies have low liquidity and therefore they\nare highly prone to market manipulation schemes. In this paper, we perform an\nin-depth analysis of pump and dump schemes organized by communities over the\nInternet. We observe how these communities are organized and how they carry out\nthe fraud. Then, we report on two case studies related to pump and dump groups.\nLastly, we introduce an approach to detect the fraud in real time that\noutperforms the current state of the art, so to help investors stay out of the\nmarket when a pump and dump scheme is in action.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2005.06610v1", "link": "http://arxiv.org/abs/2005.06610v1"}, {"title": "Credit risk premia and quadratic BSDEs with a single jump", "summary": "This paper is concerned with the determination of credit risk premia of\ndefaultable contingent claims by means of indifference valuation principles.\nAssuming exponential utility preferences we derive representations of\nindifference premia of credit risk in terms of solutions of Backward Stochastic\nDifferential Equations (BSDE). The class of BSDEs needed for that\nrepresentation allows for quadratic growth generators and jumps at random\ntimes. Since the existence and uniqueness theory for this class of BSDEs has\nnot yet been developed to the required generality, the first part of the paper\nis devoted to fill that gap. By using a simple constructive algorithm, and\nknown results on continuous quadratic BSDEs, we provide sufficient conditions\nfor the existence and uniqueness of quadratic BSDEs with discontinuities at\nrandom times.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0907.1221v3", "link": "http://dx.doi.org/10.1142/10.1142/S0219024910006133"}, {"title": "Parallelizing Computation of Expected Values in Recombinant Binomial\n  Trees", "summary": "Recombinant binomial trees are binary trees where each non-leaf node has two\nchild nodes, but adjacent parents share a common child node. Such trees arise\nin finance when pricing an option. For example, valuation of a European option\ncan be carried out by evaluating the expected value of asset payoffs with\nrespect to random paths in the tree. In many variants of the option valuation\nproblem, a closed form solution cannot be obtained and computational methods\nare needed. The cost to exactly compute expected values over random paths grows\nexponentially in the depth of the tree, rendering a serial computation of one\nbranch at a time impractical. We propose a parallelization method that\ntransforms the calculation of the expected value into an \"embarrassingly\nparallel\" problem by mapping the branches of the binomial tree to the processes\nin a multiprocessor computing environment. We also propose a parallel Monte\nCarlo method which takes advantage of the mapping to achieve a reduced variance\nover the basic Monte Carlo estimator. Performance results from R and Julia\nimplementations of the parallelization method on a distributed computing\ncluster indicate that both the implementations are scalable, but Julia is\nsignificantly faster than a similarly written R code. A simulation study is\ncarried out to verify the convergence and the variance reduction behavior in\nthe proposed Monte Carlo method.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1701.03512v2", "link": "http://dx.doi.org/10.1080/00949655.2017.1402898"}, {"title": "Outgroup Homogeneity Bias Causes Ingroup Favoritism", "summary": "Ingroup favoritism, the tendency to favor ingroup over outgroup, is often\nexplained as a product of intergroup conflict, or correlations between group\ntags and behavior. Such accounts assume that group membership is meaningful,\nwhereas human data show that ingroup favoritism occurs even when it confers no\nadvantage and groups are transparently arbitrary. Another possibility is that\ningroup favoritism arises due to perceptual biases like outgroup homogeneity,\nthe tendency for humans to have greater difficulty distinguishing outgroup\nmembers than ingroup ones. We present a prisoner's dilemma model, where\nindividuals use Bayesian inference to learn how likely others are to cooperate,\nand then act rationally to maximize expected utility. We show that, when such\nindividuals exhibit outgroup homogeneity bias, ingroup favoritism between\narbitrary groups arises through direct reciprocity. However, this outcome may\nbe mitigated by: (1) raising the benefits of cooperation, (2) increasing\npopulation diversity, and (3) imposing a more restrictive social structure.", "category": [], "id": "http://arxiv.org/abs/1908.08203v1", "link": "http://arxiv.org/abs/1908.08203v1"}, {"title": "Gambling in contests with random initial law", "summary": "This paper studies a variant of the contest model introduced in Seel and\nStrack [J. Econom. Theory 148 (2013) 2033-2048]. In the Seel-Strack contest,\neach agent or contestant privately observes a Brownian motion, absorbed at\nzero, and chooses when to stop it. The winner of the contest is the agent who\nstops at the highest value. The model assumes that all the processes start from\na common value $x_0>0$ and the symmetric Nash equilibrium is for each agent to\nutilise a stopping rule which yields a randomised value for the stopped\nprocess. In the two-player contest, this randomised value has a uniform\ndistribution on $[0,2x_0]$. In this paper, we consider a variant of the problem\nwhereby the starting values of the Brownian motions are independent,\nnonnegative random variables that have a common law $\\mu$. We consider a\ntwo-player contest and prove the existence and uniqueness of a symmetric Nash\nequilibrium for the problem. The solution is that each agent should aim for the\ntarget law $\\nu$, where $\\nu$ is greater than or equal to $\\mu$ in convex\norder; $\\nu$ has an atom at zero of the same size as any atom of $\\mu$ at zero,\nand otherwise is atom free; on $(0,\\infty)$ $\\nu$ has a decreasing density; and\nthe density of $\\nu$ only decreases at points where the convex order constraint\nis binding.", "category": ["q-fin.EC", "math.PR"], "id": "http://arxiv.org/abs/1405.7801v2", "link": "http://dx.doi.org/10.1214/14-AAP1088"}, {"title": "Far from equilibrium: Wealth reallocation in the United States", "summary": "Studies of wealth inequality often assume that an observed wealth\ndistribution reflects a system in equilibrium. This constraint is rarely tested\nempirically. We introduce a simple model that allows equilibrium but does not\nassume it. To geometric Brownian motion (GBM) we add reallocation: all\nindividuals contribute in proportion to their wealth and receive equal shares\nof the amount collected. We fit the reallocation rate parameter required for\nthe model to reproduce observed wealth inequality in the United States from\n1917 to 2012. We find that this rate was positive until the 1980s, after which\nit became negative and of increasing magnitude. With negative reallocation, the\nsystem cannot equilibrate. Even with the positive reallocation rates observed,\nequilibration is too slow to be practically relevant. Therefore, studies which\nassume equilibrium must be treated skeptically. By design they are unable to\ndetect the dramatic conditions found here when data are analysed without this\nconstraint.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1605.05631v1", "link": "http://arxiv.org/abs/1605.05631v1"}, {"title": "Statistical inference for the doubly stochastic self-exciting process", "summary": "We introduce and show the existence of a Hawkes self-exciting point process\nwith exponentially-decreasing kernel and where parameters are time-varying. The\nquantity of interest is defined as the integrated parameter\n$T^{-1}\\int_0^T\\theta_t^*dt$, where $\\theta_t^*$ is the time-varying parameter,\nand we consider the high-frequency asymptotics. To estimate it na\\\"ively, we\nchop the data into several blocks, compute the maximum likelihood estimator\n(MLE) on each block, and take the average of the local estimates. The\nasymptotic bias explodes asymptotically, thus we provide a non-na\\\"ive\nestimator which is constructed as the na\\\"ive one when applying a first-order\nbias reduction to the local MLE. We show the associated central limit theorem.\nMonte Carlo simulations show the importance of the bias correction and that the\nmethod performs well in finite sample, whereas the empirical study discusses\nthe implementation in practice and documents the stochastic behavior of the\nparameters.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1607.05831v3", "link": "http://arxiv.org/abs/1607.05831v3"}, {"title": "A closed formula for illiquid corporate bonds and an application to the\n  European market", "summary": "We propose an option approach for pricing bond illiquidity that is\nreminiscent of the celebrated work of Longstaff (1995) on the non-marketability\nof some non-dividend-paying shares in IPOs. This approach describes a quite\ncommon situation in the fixed income market: it is rather usual to find issuers\nthat, besides liquid benchmark bonds, issue some other bonds that either are\nplaced to a small number of investors in private placements or have a limited\nissue size.\n  We model interest rate and credit risks via a convenient reduced-form\napproach. We deduce a simple closed formula for illiquid corporate coupon bond\nprices when liquid bonds with similar characteristics (e.g. maturity) are\npresent in the market for the same issuer. The key model parameter is the\ntime-to-liquidate a position, i.e. the time that an experienced bond trader\ntakes to liquidate a given position on a corporate coupon bond. We show that\nilliquid bonds present an additional liquidity spread that depends on the\ntime-to-liquidate aside from bond volatility.\n  We provide a detailed application for two issuers in the European market.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1901.06855v2", "link": "http://arxiv.org/abs/1901.06855v2"}, {"title": "Modeling Nelson-Siegel Yield Curve using Bayesian Approach", "summary": "Yield curve modeling is an essential problem in finance. In this work, we\nexplore the use of Bayesian statistical methods in conjunction with\nNelson-Siegel model. We present the hierarchical Bayesian model for the\nparameters of the Nelson-Siegel yield function. We implement the MAP estimates\nvia BFGS algorithm in rstan. The Bayesian analysis relies on the Monte Carlo\nsimulation method. We perform the Hamiltonian Monte Carlo (HMC), using the\nrstan package. As a by-product of the HMC, we can simulate the Monte Carlo\nprice of a Bond, and it helps us to identify if the bond is over-valued or\nunder-valued. We demonstrate the process with an experiment and US Treasury's\nyield curve data. One of the interesting observation of the experiment is that\nthere is a strong negative correlation between the price and long-term effect\nof yield. However, the relationship between the short-term interest rate effect\nand the value of the bond is weakly positive. This is because posterior\nanalysis shows that the short-term effect and the long-term effect are\nnegatively correlated.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1809.06077v2", "link": "http://arxiv.org/abs/1809.06077v2"}, {"title": "A la Carte of Correlation Models: Which One to Choose?", "summary": "In this paper we propose a copula contagion mixture model for correlated\ndefault times. The model includes the well known factor, copula, and contagion\nmodels as its special cases. The key advantage of such a model is that we can\nstudy the interaction of different models and their pricing impact.\nSpecifically, we model the marginal default times to follow some contagion\nintensity processes coupled with copula dependence structure. We apply the\ntotal hazard construction method to generate ordered default times and\nnumerically compare the pricing impact of different models on basket CDSs and\nCDOs in the presence of exponential decay and counterparty risk.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1010.4053v1", "link": "http://arxiv.org/abs/1010.4053v1"}, {"title": "Leave-One-Out Least Square Monte Carlo Algorithm for Pricing American\n  Options", "summary": "The least square Monte Carlo (LSM) algorithm proposed by Longstaff and\nSchwartz [2001] is widely used for pricing American options. The LSM estimator\ncontains undesirable look-ahead bias, and the conventional technique of\nremoving it necessitates doubling simulations. We present the leave-one-out LSM\n(LOOLSM) algorithm for efficiently eliminating look-ahead bias. We validate the\nmethod with several option examples, including the multi-asset cases that the\nLSM algorithm significantly overvalues. We also obtain the convergence rates of\nlook-ahead bias by measuring it using the LOOLSM method. The analysis and\ncomputational evidence support our findings.", "category": ["q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1810.02071v2", "link": "http://arxiv.org/abs/1810.02071v2"}, {"title": "Panel Data Quantile Regression with Grouped Fixed Effects", "summary": "This paper introduces estimation methods for grouped latent heterogeneity in\npanel data quantile regression. We assume that the observed individuals come\nfrom a heterogeneous population with a finite number of types. The number of\ntypes and group membership is not assumed to be known in advance and is\nestimated by means of a convex optimization problem. We provide conditions\nunder which group membership is estimated consistently and establish asymptotic\nnormality of the resulting estimators. Simulations show that the method works\nwell in finite samples when T is reasonably large. To illustrate the proposed\nmethodology we study the effects of the adoption of Right-to-Carry concealed\nweapon laws on violent crime rates using panel data of 51 U.S. states from 1977\n- 2010.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.05041v2", "link": "http://arxiv.org/abs/1801.05041v2"}, {"title": "Quantile hedging for basket derivatives", "summary": "The problem of quantile hedging for basket derivatives in the Black-Scholes\nmodel with correlation is considered. Explicit formulas for the probability\nmaximizing function and the cost reduction function are derived. Applicability\nof the results for the widely traded derivatives as digital, quantos,\noutperformance and spread options is shown.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1010.5810v3", "link": "http://arxiv.org/abs/1010.5810v3"}, {"title": "Black Magic Investigation Made Simple: Monte Carlo Simulations and\n  Historical Back Testing of Momentum Cross-Over Strategies Using FRACTI\n  Patterns", "summary": "To promote economic stability, finance should be studied as a hard science,\nwhere scientific methods apply. When a trading strategy is proposed, the\nunderlying model should be transparent and defined robustly to allow other\nresearchers to understand and examine it thoroughly. Like any hard sciences,\nresults must be repeatable to allow researchers to collaborate, and build upon\neach other's results. Large-scale collaboration, when applying the steps of\nscientific investigation, is an efficient way to leverage \"crowd science\" to\naccelerate research in finance. In this paper, we demonstrate how a real world\nproblem in economics, an old problem still subject to a lot of debate, can be\nsolved by the application of a crowd-powered, collaborative scientific\ncomputational framework, fully supporting the process of investigation dictated\nby the modern scientific method. This paper provides a real end-to-end example\nof investigation to illustrate the use of the framework. We intentionally\nselected an example that is self-contained, complete, simple, accessible, and\nof constant debate in both academia and the industry: the performance of a\ntrading strategy used commonly in technical analysis. Claims of efficiency in\ntechnical analysis, referred derisively by some sources as \"Black Magic\", are\nof widespread use in mainstream media and usually met with a lot of\ncontroversy. In this paper we show that different researchers assess this\nstrategy differently, and the subsequent debate is due more to the lack of\nmethod than purpose. Most results reported are not repeatable by other\nresearchers. This is not satisfactory if we intend to approach finance as a\nhard science. To counterweight the status quo, we demonstrate what one could do\nby using collaborative and investigative features of contributions and\nleveraging the power of crowds.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1808.07949v1", "link": "http://arxiv.org/abs/1808.07949v1"}, {"title": "Microscopic Study Reveals the Singular Origins of Growth", "summary": "P.W. Anderson proposed the concept of complexity in order to describe the\nemergence and growth of macroscopic collective patterns out of the simple\ninteractions of many microscopic agents. In the physical sciences this paradigm\nwas implemented systematically and confirmed repeatedly by successful\nconfrontation with reality. In the social sciences however, the possibilities\nto stage experiments to validate it are limited. During the 90's a series of\ndramatic political and economic events have provided the opportunity to do so.\nWe exploit the resulting empirical evidence to validate a simple agent based\nalternative to the classical logistic dynamics. The post-liberalization\nempirical data from Poland confirm the theoretical prediction that the dynamics\nis dominated by singular rare events which insure the resilience and\nadaptability of the system. We have shown that growth is led by few singular\n\"growth centers\" (Figure 1), that initially developed at a tremendous rate\n(Figure3), followed by a diffusion process to the rest of the country and\nleading to a positive growth rate uniform across the counties. In addition to\nthe interdisciplinary unifying potential of our generic formal approach, the\npresent work reveals the strong causal ties between the \"softer\" social\nconditions and their \"hard\" economic consequences.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0803.2201v1", "link": "http://dx.doi.org/10.1140/epjb/e2008-00189-6"}, {"title": "Inflation and speculation in a dynamic macroeconomic model", "summary": "We study a monetary version of the Keen model by merging two alternative\nextensions, namely the addition of a dynamic price level and the introduction\nof speculation. We recall and study old and new equilibria, together with their\nlocal stability analysis. This includes a state of recession associated with a\ndeflationary regime and characterized by falling employment but constant wage\nshares, with or without an accompanying debt crisis. We also emphasize some new\nqualitative behavior of the extended model, in particular its ability to\nproduce and describe repeated financial crises as a natural pace of the\neconomy, and its suitability to describe the relationship between economic\ngrowth and financial activities.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1412.7500v1", "link": "http://arxiv.org/abs/1412.7500v1"}, {"title": "Indifference pricing and hedging in stochastic volatility models", "summary": "We apply the concepts of utility based pricing and hedging of derivatives in\nstochastic volatility markets and introduce a new class of \"reciprocal affine\"\nmodels for which the indifference price and optimal hedge portfolio for pure\nvolatility claims are efficiently computable. We obtain a general formula for\nthe market price of volatility risk in these models and calculate it explicitly\nfor the case of an exponential utility.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0404447v1", "link": "http://arxiv.org/abs/math/0404447v1"}, {"title": "On non-existence of a one factor interest rate model for volatility\n  averaged generalized Fong-Vasicek term structures", "summary": "The purpose of this paper is to study the generalized Fong--Vasicek\ntwo-factor interest rate model with stochastic volatility. In this model the\ndispersion of the stochastic short rate (square of volatility) is assumed to be\nstochastic as well and it follows a non-negative process with volatility\nproportional to the square root of dispersion. The drift of the stochastic\nprocess for the dispersion is assumed to be in a rather general form including,\nin particular, linear function having one root (yielding the original\nFong--Vasicek model or a cubic like function having three roots (yielding a\ngeneralized Fong--Vasicek model for description of the volatility clustering).\nWe consider averaged bond prices with respect to the limiting distribution of\nstochastic dispersion. The averaged bond prices depend on time and current\nlevel of the short rate like it is the case in many popular one-factor interest\nrate model including in particular the Vasicek and Cox--Ingersoll-Ross model.\nHowever, as a main result of this paper we show that there is no such\none-factor model yielding the same bond prices as the averaged values described\nabove.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0811.0473v1", "link": "http://arxiv.org/abs/0811.0473v1"}, {"title": "Quasi Maximum Likelihood Estimation and Inference of Large Approximate\n  Dynamic Factor Models via the EM algorithm", "summary": "This paper studies Quasi Maximum Likelihood estimation of dynamic factor\nmodels for large panels of time series. Specifically, we consider the case in\nwhich the autocorrelation of the factors is explicitly accounted for and\ntherefore the factor model has a state-space form. Estimation of the factors\nand their loadings is implemented by means of the Expectation Maximization\nalgorithm, jointly with the Kalman smoother. We prove that, as both the\ndimension of the panel $n$ and the sample size $T$ diverge to infinity, the\nestimated loadings, factors, and common components are $\\min(\\sqrt n,\\sqrt\nT)$-consistent and asymptotically normal. Although the model is estimated under\nthe unrealistic constraint of independent idiosyncratic errors, this\nmis-specification does not affect consistency. Moreover, we give conditions\nunder which the derived asymptotic distribution can still be used for inference\neven in case of mis-specifications. Our results are confirmed by a MonteCarlo\nsimulation exercise where we compare the performance of our estimators with\nPrincipal Components.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.03821v1", "link": "http://arxiv.org/abs/1910.03821v1"}, {"title": "Time-scale dependence of correlations among foreign currencies", "summary": "For the purpose of elucidating the correlation among currencies, we analyze\ndaily and high-resolution data of foreign exchange rates. There is strong\ncorrelation for pairs of currencies of geographically near countries. We show\nthat there is a time delay of order less than a minute between two currency\nmarkets having a strong cross-correlation. The cross-correlation between\nexchange rates is lower in shorter time scale in any case. As a corollary we\nnotice a kind of contradiction that the direct Yen-Dollar rate significantly\ndiffers from the indirect Yen-Dollar rate through Euro in short time scales.\nThis result shows the existence of arbitrage opportunity among currency\nexchange markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0303306v1", "link": "http://arxiv.org/abs/cond-mat/0303306v1"}, {"title": "Scaling and Multi-scaling in Financial Markets", "summary": "This paper reviews some of the phenomenological models which have been\nintroduced to incorporate the scaling properties of financial data. It also\nillustrates a microscopic model, based on heterogeneous interacting agents,\nwhich provides a possible explanation for the complex dynamics of markets'\nreturns. Scaling and multi-scaling analysis performed on the simulated data is\nin good quantitative agreement with the empirical results.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0007385v1", "link": "http://dx.doi.org/10.1063/1.1358199"}, {"title": "Learned Sectors: A fundamentals-driven sector reclassification project", "summary": "Market sectors play a key role in the efficient flow of capital through the\nmodern Global economy. We analyze existing sectorization heuristics, and\nobserve that the most popular - the GICS (which informs the S&P 500), and the\nNAICS (published by the U.S. Government) - are not entirely quantitatively\ndriven, but rather appear to be highly subjective and rooted in dogma. Building\non inferences from analysis of the capital structure irrelevance principle and\nthe Modigliani-Miller theoretic universe conditions, we postulate that\ncorporation fundamentals - particularly those components specific to the\nModigliani-Miller universe conditions - would be optimal descriptors of the\ntrue economic domain of operation of a company. We generate a set of potential\ncandidate learned sector universes by varying the linkage method of a\nhierarchical clustering algorithm, and the number of resulting sectors derived\nfrom the model (ranging from 5 to 19), resulting in a total of 60 candidate\nlearned sector universes. We then introduce reIndexer, a backtest-driven sector\nuniverse evaluation research tool, to rank the candidate sector universes\nproduced by our learned sector classification heuristic. This rank was utilized\nto identify the risk-adjusted return optimal learned sector universe as being\nthe universe generated under CLINK (i.e. complete linkage), with 17 sectors.\nThe optimal learned sector universe was tested against the benchmark GICS\nclassification universe with reIndexer, outperforming on both absolute\nportfolio value, and risk-adjusted return over the backtest period. We conclude\nthat our fundamentals-driven Learned Sector classification heuristic provides a\nsuperior risk-diversification profile than the status quo classification\nheuristic.", "category": ["q-fin.GN", "econ.EM"], "id": "http://arxiv.org/abs/1906.03935v1", "link": "http://arxiv.org/abs/1906.03935v1"}, {"title": "The Network of Inter-Regional Direct Investment Stocks across Europe", "summary": "We propose a methodological framework to study the dynamics of inter-regional\ninvestment flow in Europe from a Complex Networks perspective, an approach with\nrecent proven success in many fields including economics. In this work we study\nthe network of investment stocks in Europe at two different levels: first, we\ncompute the inward-outward investment stocks at the level of firms, based on\nownership shares and number of employees; then we estimate the inward-outward\ninvestment stock at the level of regions in Europe, by aggregating the\nownership network of firms, based on their headquarter location. Despite the\nintuitive value of this approach for EU policy making in economic development,\nto our knowledge there are no similar works in the literature yet. In this\npaper we focus on statistical distributions and scaling laws of activity,\ninvestment stock and connectivity degree both at the level of firms and at the\nlevel of regions. In particular we find that investment stock of firms is power\nlaw distributed with an exponent very close to the one found for firm activity.\nOn the other hand investment stock and activity of regions turn out to be\nlog-normal distributed. At both levels we find scaling laws relating investment\nto activity and connectivity. In particular, we find that investment stock\nscales with connectivity in a similar way as has been previously found for\nstock market data, calling for further investigations on a possible general\nscaling law holding true in economical networks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0508206v1", "link": "http://arxiv.org/abs/physics/0508206v1"}, {"title": "The Perception of Time, Risk and Return During Periods of Speculation", "summary": "What return should you expect when you take on a given amount of risk? How\nshould that return depend upon other people's behavior? What principles can you\nuse to answer these questions? In this paper, we approach these topics by\nexploring the consequences of two simple hypotheses about risk.\n  The first is a common-sense invariance principle: assets with the same\nperceived risk must have the same expected return. The second hypothesis\nconcerns the perception of time. We conjecture that in times of speculative\nexcitement, short-term investors may instinctively imagine stock prices to be\nevolving in a time measure different from that of calendar time. They may\ninstead perceive and experience the risk and return of a stock in intrinsic\ntime, a dimensionless time scale that counts the number of trading\nopportunities that occur.\n  The most noteworthy result is that, in the short-term, a stock's trading\nfrequency affects its expected return. We show that short-term stock\nspeculators will expect returns proportional to the temperature of a stock,\nwhere temperature is defined as the product of the stock's traditional\nvolatility and the square root of its trading frequency. We hope that this\nmodel will have some relevance to the behavior of investors expecting\ninordinate returns in highly speculative markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0201345v1", "link": "http://arxiv.org/abs/cond-mat/0201345v1"}, {"title": "Representation of Exchange Option Prices under Stochastic Volatility\n  Jump-Diffusion Dynamics", "summary": "In this article, we provide representations of European and American exchange\noption prices under stochastic volatility jump-diffusion (SVJD) dynamics\nfollowing models by Merton (1976), Heston (1993), and Bates (1996). A\nRadon-Nikodym derivative process is also introduced to facilitate the shift\nfrom the objective market measure to other equivalent probability measures,\nincluding the equivalent martingale measure. Under the equivalent martingale\nmeasure, we derive the integro-partial differential equation that characterizes\nthe exchange option prices. We also derive representations of the European\nexchange option price using the change-of-numeraire technique proposed by Geman\net al. (1995) and the Fourier inversion formula derived by Caldana and Fusai\n(2013), and show that these two representations are comparable. Lastly, we show\nthat the American exchange option price can be decomposed into the price of the\nEuropean exchange option and an early exercise premium.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/2002.10202v1", "link": "http://dx.doi.org/10.1080/14697688.2019.1655785"}, {"title": "Mixing Kohonen Algorithm, Markov Switching Model and Detection of\n  Multiple Change-Points: An Application to Monetary History", "summary": "The present paper aims at locating the breakings of the integration process\nof an international system observed during about 50 years in the 19th century.\nA historical study could link them to special events, which operated as\nexogenous shocks on this process. The indicator of integration used is the\nspread between the highest and the lowest among the London, Hamburg and Paris\ngold-silver prices. Three algorithms are combined to study this integration: a\nperiodization obtained with the SOM algorithm is confronted to the estimation\nof a two-regime Markov switching model, in order to give an interpretation of\nthe changes of regime; in the same time change-points are identified over the\nwhole period providing a more precise interpretation of the various types of\nregulation.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0710.0745v1", "link": "http://arxiv.org/abs/0710.0745v1"}, {"title": "Super Generalized Central Limit Theorem: Limit distributions for sums of\n  non-identical random variables with power-laws", "summary": "In nature or societies, the power-law is present ubiquitously, and then it is\nimportant to investigate the mathematical characteristics of power-laws in the\nrecent era of big data. In this paper we prove the superposition of\nnon-identical stochastic processes with power-laws converges in density to a\nunique stable distribution. This property can be used to explain the\nuniversality of stable laws such that the sums of the logarithmic return of\nnon-identical stock price fluctuations follow stable distributions.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1702.02826v6", "link": "http://dx.doi.org/10.7566/JPSJ.87.043003"}, {"title": "Small-maturity asymptotics for the at-the-money implied volatility slope\n  in L\u00e9vy models", "summary": "We consider the at-the-money strike derivative of implied volatility as the\nmaturity tends to zero. Our main results quantify the behavior of the slope for\ninfinite activity exponential L\\'evy models including a Brownian component. As\nauxiliary results, we obtain asymptotic expansions of short maturity\nat-the-money digital call options, using Mellin transform asymptotics. Finally,\nwe discuss when the at-the-money slope is consistent with the steepness of the\nsmile wings, as given by Lee's moment formula.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1310.3061v3", "link": "http://arxiv.org/abs/1310.3061v3"}, {"title": "Non-linear Time Series and Artificial Neural Networks of Red Hat\n  Volatility", "summary": "We extend the empirical results published in article \"Empirical Evidence on\nArbitrage by Changing the Stock Exchange\" by means of machine learning and\nadvanced econometric methodologies based on Smooth Transition Regression models\nand Artificial Neural Networks.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1806.01070v1", "link": "http://arxiv.org/abs/1806.01070v1"}, {"title": "A Systematic and Analytical Review of the Socioeconomic and\n  Environmental Impact of the Deployed High-Speed Rail (HSR) Systems on the\n  World", "summary": "The installation of high-speed rail in the world during the last two decades\nresulted in significant socioeconomic and environmental changes. The U.S. has\nthe longest rail network in the world, but the focus is on carrying a wide\nvariety of loads including coal, farm crops, industrial products, commercial\ngoods, and miscellaneous mixed shipments. Freight and passenger services in the\nU.S. dates to 1970, with both carried out by private railway companies.\nRailways were the main means of transport between cities from the late 19th\ncentury through the middle of the 20th century. However, rapid growth in\nproduction and improvements in technologies changed those dynamics. The fierce\ncompetition for comfortability and pleasantness in passenger travel and the\nproliferation of aviation services in the U.S. channeled federal and state\nbudgets towards motor vehicle infrastructure, which brought demand for\nrailroads to a halt in the 1950s. Presently, the U.S. has no high-speed trains,\naside from sections of Amtrak s Acela line in the Northeast Corridor that can\nreach 150 mph for only 34 miles of its 457-mile span. The average speed between\nNew York and Boston is about 65 mph. On the other hand, China has the world s\nfastest and largest high-speed rail network, with more than 19,000 miles, of\nwhich the vast majority was built in the past decade. Japan s bullet trains can\nreach nearly 200 miles per hour and dates to the 1960s. That system moved more\nthan 9 billion people without a single passenger casualty. In this systematic\nreview, we studied the effect of High-Speed Rail (HSR) on the U.S. and other\ncountries including France, Japan, Germany, Italy, and China in terms of energy\nconsumption, land use, economic development, travel behavior, time use, human\nhealth, and quality of life.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.04452v2", "link": "http://arxiv.org/abs/2003.04452v2"}, {"title": "Equity Default Clawback Swaps to Implement Venture Banking", "summary": "I propose creation of a venture bank, able to multiply the capital of a\nventure capital firm by at least 47 times, without requiring access to the\nFederal Reserve or other central bank apart from settlement. This concept rests\non obtaining default swap instruments on loans in order to create the capital\nrequired, and expand Tier 1 and 2 base capital. Profitability depends on\noverall portfolio performance, availability of equity default swaps, cost of\ndefault swap, and the multiple of original capital (MOC) adopted by the venture\nbank. A new derivative financial instrument, the equity default swap (EDS), to\ncover loans made as venture investments. An EDS is similar to a credit default\nswap (CDS) but with some unique features. The features and operation of these\nnew derivative instruments are outlined along with audit requirements. This\ninstrument would be traded on open-outcry exchanges with special features to\nensure orderly operation of the market. It is the creation of public markets\nfor EDSs that makes possible the use of public market pricing to indirectly\nprovide a potential market capitalization for the underlying venture-bank\ninvestment. Full coverage insulates the venture-bank from losses in most\nsituations, and multiplies profitability quite dramatically in all scenarios.\nTen year returns above 20X are attainable. Further, a new feature for EDS\nderivatives, a clawback lien, closes out the equity default swap. Here it is\noptimized at 77%, and is to be paid back to the underwriter at a future date to\nprevent perverse incentive to deliberately fail. This new feature creates an\nEquity Default Clawback Swap (EDCS) which can be used safely. This proposal\nalso solves an old problem in banking, because it matches the term of the loan\nwith the term of the investment. I show that the venture-bank investment and\nthe EDCS underwriting business are profitable.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1707.08078v8", "link": "http://arxiv.org/abs/1707.08078v8"}, {"title": "Identifying Bid Leakage In Procurement Auctions: Machine Learning\n  Approach", "summary": "We propose a novel machine-learning-based approach to detect bid leakage in\nfirst-price sealed-bid auctions. We extract and analyze the data on more than\n1.4 million Russian procurement auctions between 2014 and 2018. As bid leakage\nin each particular auction is tacit, the direct classification is impossible.\nInstead, we reduce the problem of bid leakage detection to Positive-Unlabeled\nClassification. The key idea is to regard the losing participants as fair and\nthe winners as possibly corrupted. This allows us to estimate the prior\nprobability of bid leakage in the sample, as well as the posterior probability\nof bid leakage for each specific auction. We find that at least 16\\% of\nauctions are exposed to bid leakage. Bid leakage is more likely in auctions\nwith a higher reserve price, lower number of bidders and lower price fall, and\nwhere the winning bid is received in the last hour before the deadline.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.00261v1", "link": "http://arxiv.org/abs/1903.00261v1"}, {"title": "Volatility Effects on the Escape Time in Financial Market Models", "summary": "We shortly review the statistical properties of the escape times, or hitting\ntimes, for stock price returns by using different models which describe the\nstock market evolution. We compare the probability function (PF) of these\nescape times with that obtained from real market data. Afterwards we analyze in\ndetail the effect both of noise and different initial conditions on the escape\ntime in a market model with stochastic volatility and a cubic nonlinearity. For\nthis model we compare the PF of the stock price returns, the PF of the\nvolatility and the return correlation with the same statistical characteristics\nobtained from real market data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0810.1625v1", "link": "http://dx.doi.org/10.1142/S0218127408022007"}, {"title": "Hierarchical financial structures with money cascade", "summary": "In this paper we show similarities between turbulence and financial systems.\nMotivated by similarities between the two systems, we construct a multiscale\nmodel for hierarchical financial structures that exhibits a constant cascade of\nwealth from large financial entities to small financial entities. According to\nour model, large and intermediate scale financial institutions have a power law\ndistribution. However, the wealth distribution is Maxwellian at individual\nscales.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1903.00313v1", "link": "http://arxiv.org/abs/1903.00313v1"}, {"title": "Affine HJM Framework on $S_{d}^{+}$ and Long-Term Yield", "summary": "We develop the HJM framework for forward rates driven by affine processes on\nthe state space of symmetric positive matrices. In this setting we find a\nrepresentation for the long-term yield and investigate the yield's asymptotic\nbehaviour.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1311.0688v2", "link": "http://arxiv.org/abs/1311.0688v2"}, {"title": "Semi-parametric Bayesian Partially Identified Models based on Support\n  Function", "summary": "We provide a comprehensive semi-parametric study of Bayesian partially\nidentified econometric models. While the existing literature on Bayesian\npartial identification has mostly focused on the structural parameter, our\nprimary focus is on Bayesian credible sets (BCS's) of the unknown identified\nset and the posterior distribution of its support function. We construct a\n(two-sided) BCS based on the support function of the identified set. We prove\nthe Bernstein-von Mises theorem for the posterior distribution of the support\nfunction. This powerful result in turn infers that, while the BCS and the\nfrequentist confidence set for the partially identified parameter are\nasymptotically different, our constructed BCS for the identified set has an\nasymptotically correct frequentist coverage probability. Importantly, we\nillustrate that the constructed BCS for the identified set does not require a\nprior on the structural parameter. It can be computed efficiently for subset\ninference, especially when the target of interest is a sub-vector of the\npartially identified parameter, where projecting to a low-dimensional subset is\noften required. Hence, the proposed methods are useful in many applications.\n  The Bayesian partial identification literature has been assuming a known\nparametric likelihood function. However, econometric models usually only\nidentify a set of moment inequalities, and therefore using an incorrect\nlikelihood function may result in misleading inferences. In contrast, with a\nnonparametric prior on the unknown likelihood function, our proposed Bayesian\nprocedure only requires a set of moment conditions, and can efficiently make\ninference about both the partially identified parameter and its identified set.\nThis makes it widely applicable in general moment inequality models. Finally,\nthe proposed method is illustrated in a financial asset pricing problem.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1212.3267v2", "link": "http://arxiv.org/abs/1212.3267v2"}, {"title": "Executive stock option exercise with full and partial information on a\n  drift change point", "summary": "We analyse the optimal exercise of an American call ESO written on a stock\nwhose drift parameter falls to a lower value at a \\emph{change point}, an\nexponentially distributed random time independent of the Brownian motion\ndriving the stock. Two agents, who do not trade the stock, have differing\ninformation on the change point, and seek to optimally exercise the option by\nmaximising its discounted payoff under the physical measure. The first agent\nhas full information, and observes the change point. The second agent has\npartial information and filters the change point from price observations. This\nscenario is designed to mimic the positions of two employees of varying\nseniority, a fully informed executive and a partially informed less senior\nemployee, each of whom receives an ESO. The partial information scenario yields\na model under the observation filtration $\\widehat{\\mathbb{F}}$ in which the\nstock drift becomes a diffusion driven by the innovations process, an\n$\\widehat{\\mathbb{F}}$-Brownian motion also driving the stock under\n$\\widehat{\\mathbb{F}}$, and the partial information optimal stopping value\nfunction has two spatial dimensions. We rigorously characterise the free\nboundary PDEs for both agents, establish shape and regularity properties of the\nassociated optimal exercise boundaries, and prove the smooth pasting property\nin both information scenarios, exploiting some stochastic flow ideas to do so\nin the partial information case. We develop finite difference algorithms to\nnumerically solve both agents' exercise and valuation problems and illustrate\nthat the additional information of the fully informed agent can result in\nexercise patterns which exploit the information on the change point, lending\ncredence to empirical studies which suggest that privileged information of bad\nnews is a factor leading to early exercise of ESOs prior to poor stock price\nperformance.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1709.10141v3", "link": "http://arxiv.org/abs/1709.10141v3"}, {"title": "Applications of the Deep Galerkin Method to Solving Partial\n  Integro-Differential and Hamilton-Jacobi-Bellman Equations", "summary": "We extend the Deep Galerkin Method (DGM) introduced in Sirignano and\nSpiliopoulos (2018) to solve a number of partial differential equations (PDEs)\nthat arise in the context of optimal stochastic control and mean field games.\nFirst, we consider PDEs where the function is constrained to be positive and\nintegrate to unity, as is the case with Fokker-Planck equations. Our approach\ninvolves reparameterizing the solution as the exponential of a neural network\nappropriately normalized to ensure both requirements are satisfied. This then\ngives rise to a partial integro-differential equation (PIDE) where the integral\nappearing in the equation is handled using importance sampling. Secondly, we\ntackle a number of Hamilton-Jacobi-Bellman (HJB) equations that appear in\nstochastic optimal control problems. The key contribution is that these\nequations are approached in their unsimplified primal form which includes an\noptimization problem as part of the equation. We extend the DGM algorithm to\nsolve for the value function and the optimal control simultaneously by\ncharacterizing both as deep neural networks. Training the networks is performed\nby taking alternating stochastic gradient descent steps for the two functions,\na technique similar in spirit to policy improvement algorithms.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1912.01455v2", "link": "http://arxiv.org/abs/1912.01455v2"}, {"title": "A note on the Nelson Cao inequality constraints in the GJR-GARCH model:\n  Is there a leverage effect?", "summary": "The majority of stylized facts of financial time series and several\nValue-at-Risk measures are modeled via univariate or multivariate GARCH\nprocesses. It is not rare that advanced GARCH models fail to converge for\ncomputational reasons, and a usual parsimonious approach is the GJR-GARCH\nmodel. There is a disagreement in the literature and the specialized\neconometric software, on which constraints should be used for the parameters,\nintroducing indirectly the distinction between asymmetry and leverage. We show\nthat the approach used by various software packages is not consistent with the\nNelson-Cao inequality constraints. Implementing Monte Carlo simulations,\ndespite of the results being empirically correct, the estimated parameters are\nnot theoretically coherent with the Nelson-Cao constraints for ensuring\npositivity of conditional variances. On the other hand ruling out the leverage\nhypothesis, the asymmetry term in the GJR model can take negative values when\ntypical constraints like the condition for the existence of the second and\nfourth moments, are imposed.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1705.00535v1", "link": "http://arxiv.org/abs/1705.00535v1"}, {"title": "Holder-extendible European option: corrections and extensions", "summary": "Financial contracts with options that allow the holder to extend the contract\nmaturity by paying an additional fixed amount found many applications in\nfinance. Closed-form solutions for the price of these options have appeared in\nthe literature for the case when the contract underlying asset follows a\ngeometric Brownian motion with the constant interest rate, volatility, and\nnon-negative \"dividend\" yield. In this paper, the option price is derived for\nthe case of the underlying asset that follows a geometric Brownian motion with\nthe time-dependent drift and volatility which is important to use the solutions\nin real life applications. The formulas are derived for the drift that may\ninclude non-negative or negative \"dividend\" yield. The latter case results in a\nnew solution type that has not been studied in the literature. Several\ntypographical errors in the formula for the holder-extendible put, typically\nrepeated in textbooks and software, are corrected.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/1010.0090v2", "link": "http://dx.doi.org/10.1017/S1446181115000097"}, {"title": "Non-Gaussianity of the Intraday Returns Distribution: its evolution in\n  time", "summary": "We find a remarkable time persistence of various proxies for the kurtosis\n(p-kurtosis) of the intraday returns distribution for the S&P500 index and this\npermits a significant measure of their evolution from 1983 to 2004. There\nappears a long time scale dramatic variation of the p-kurtosis uncorrelated\nwith the variation of the volatility thus falsifying any hypothesis of a\nuniversal shape for the probability distribution of the returns. A large\nincrease in the kurtosis anticipates the October 87 crash. During the years\n1991-2003 it continuously decreases even when the volatility grows during the\ndot-com bubble. We propose some speculative interpretations of these results.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1112.0770v2", "link": "http://arxiv.org/abs/1112.0770v2"}, {"title": "Entropy Balancing for Continuous Treatments", "summary": "This paper introduces entropy balancing for continuous treatments (EBCT) by\nextending the original entropy balancing methodology of Hainm\\\"uller (2012). In\norder to estimate balancing weights, the proposed approach solves a globally\nconvex constrained optimization problem. EBCT weights reliably eradicate\nPearson correlations between covariates and the continuous treatment variable.\nThis is the case even when other methods based on the generalized propensity\nscore tend to yield insufficient balance due to strong selection into different\ntreatment intensities. Moreover, the optimization procedure is more successful\nin avoiding extreme weights attached to a single unit. Extensive Monte-Carlo\nsimulations show that treatment effect estimates using EBCT display similar or\nlower bias and uniformly lower root mean squared error. These properties make\nEBCT an attractive method for the evaluation of continuous treatments.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2001.06281v1", "link": "http://arxiv.org/abs/2001.06281v1"}, {"title": "Liability-driven investment in longevity risk management", "summary": "This paper studies optimal investment from the point of view of an investor\nwith longevity-linked liabilities. The relevant optimization problems rarely\nare analytically tractable, but we are able to show numerically that liability\ndriven investment can significantly outperform common strategies that do not\ntake the liabilities into account. In problems without liabilities the\nadvantage disappears, which suggests that the superiority of the proposed\nstrategies is indeed based on connections between liabilities and asset\nreturns.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1307.8261v1", "link": "http://arxiv.org/abs/1307.8261v1"}, {"title": "Evolution of worldwide stock markets, correlation structure and\n  correlation based graphs", "summary": "We investigate the daily correlation present among market indices of stock\nexchanges located all over the world in the time period Jan 1996 - Jul 2009. We\ndiscover that the correlation among market indices presents both a fast and a\nslow dynamics. The slow dynamics reflects the development and consolidation of\nglobalization. The fast dynamics is associated with critical events that\noriginate in a specific country or region of the world and rapidly affect the\nglobal system. We provide evidence that the short term timescale of correlation\namong market indices is less than 3 trading months (about 60 trading days). The\naverage values of the non diagonal elements of the correlation matrix,\ncorrelation based graphs and the spectral properties of the largest eigenvalues\nand eigenvectors of the correlation matrix are carrying information about the\nfast and slow dynamics of correlation of market indices. We introduce a measure\nof mutual information based on link co-occurrence in networks, in order to\ndetect the fast dynamics of successive changes of correlation based graphs in a\nquantitative way.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1103.5555v1", "link": "http://dx.doi.org/10.1103/PhysRevE.84.026108"}, {"title": "Information, Impact, Ignorance, Illegality, Investing, and Inequality", "summary": "We note a simple mechanism that may at least partially resolve several\noutstanding economic puzzles, including why the cyclically adjusted price to\nearnings ratio of the S&P 500 index has been oddly high for the past two\ndecades, why gains to capital have outpaced gains to wages, and the persistence\nof the equity premium.", "category": ["q-fin.EC", "q-fin.TR"], "id": "http://arxiv.org/abs/1612.06855v3", "link": "http://arxiv.org/abs/1612.06855v3"}, {"title": "The Leland-Toft optimal capital structure model under Poisson\n  observations", "summary": "We revisit the optimal capital structure model with endogenous bankruptcy\nfirst studied by Leland \\cite{Leland94} and Leland and Toft \\cite{Leland96}.\nDifferently from the standard case, where shareholders observe continuously the\nasset value and bankruptcy is executed instantaneously without delay, we assume\nthat the information of the asset value is updated only at intervals, modeled\nby the jump times of an independent Poisson process. Under the spectrally\nnegative L\\'evy model, we obtain the optimal bankruptcy strategy and the\ncorresponding capital structure. A series of numerical studies are given to\nanalyze the sensitivity of observation frequency on the optimal solutions, the\noptimal leverage and the credit spreads.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1904.03356v2", "link": "http://arxiv.org/abs/1904.03356v2"}, {"title": "Multistep Bayesian strategy in coin-tossing games and its application to\n  asset trading games in continuous time", "summary": "We study multistep Bayesian betting strategies in coin-tossing games in the\nframework of game-theoretic probability of Shafer and Vovk (2001). We show that\nby a countable mixture of these strategies, a gambler or an investor can\nexploit arbitrary patterns of deviations of nature's moves from independent\nBernoulli trials. We then apply our scheme to asset trading games in continuous\ntime and derive the exponential growth rate of the investor's capital when the\nvariation exponent of the asset price path deviates from two.", "category": ["q-fin.TR", "math.PR"], "id": "http://arxiv.org/abs/0802.4311v2", "link": "http://arxiv.org/abs/0802.4311v2"}, {"title": "Multinomial method for option pricing under Variance Gamma", "summary": "This paper presents a multinomial method for option pricing when the\nunderlying asset follows an exponential Variance Gamma process. The continuous\ntime Variance Gamma process is approximated by a discrete time Markov chain\nwith the same firsts four cumulants. This approach is particularly convenient\nfor pricing American and Bermudan options, which can be exercised at any time\nup to expiration date. Numerical computations of European and American options\nare presented, and compared with results obtained with finite differences\nmethods and with the Black Scholes model.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1701.00112v4", "link": "http://dx.doi.org/10.1080/00207160.2018.1427853"}, {"title": "Anatomy of a Bail-In", "summary": "To mitigate potential contagion from future banking crises, the European\nCommission recently proposed a framework which would provide for the\n$\\textit{bail-in}$ of bank creditors in the event of failure. In this study, we\nexamine this framework retrospectively in the context of failed European banks\nduring the global financial crisis. Empirical findings suggest that equity and\nsubordinated bond holders would have been the main losers from the 535 billion\neuro impairment losses realized by failed European banks. Losses attributed to\nsenior debt holders would, on aggregate, have been proportionally small, while\nno losses would have been imposed on depositors. Cross-country analysis,\nincorporating stress-tests, reveals a divergence of outcomes with subordinated\ndebt holders wiped out in a number of countries, while senior debt holders of\nGreek, Austrian and Irish banks would have required bail-in.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1403.7628v1", "link": "http://arxiv.org/abs/1403.7628v1"}, {"title": "An Apology for Money", "summary": "This review is about the convenience, the benefits, as well as the\ndestructive capacities of money. It deals with various aspects of money\ncreation, with its value, and its appropriation. All sorts of money tend to get\ncorrupted by eventually creating too much of them. In the long run, this\nrenders money worthless and deprives people holding it. This misuse of money\ncreation is inevitable and should come as no surprise. Abusive money creation\ncomes in various forms. In the present fiat money system \"suspended in free\nthought\" and sustained merely by our belief in and our conditioning to it,\nmoney is conveniently created out of \"thin air\" by excessive government\nspending and speculative credit creation. Alas, any too tight money supply\ncould ruin an economy by inviting all sorts of unfriendly takeovers, including\nwars or competition. Therefore the ambivalence of money as benefactor and\ndestroyer should be accepted as destiny.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0811.3130v6", "link": "http://arxiv.org/abs/0811.3130v6"}, {"title": "Multivariate transient price impact and matrix-valued positive definite\n  functions", "summary": "We consider a model for linear transient price impact for multiple assets\nthat takes cross-asset impact into account. Our main goal is to single out\nproperties that need to be imposed on the decay kernel so that the model admits\nwell-behaved optimal trade execution strategies. We first show that the\nexistence of such strategies is guaranteed by assuming that the decay kernel\ncorresponds to a matrix-valued positive definite function. An example\nillustrates, however, that positive definiteness alone does not guarantee that\noptimal strategies are well-behaved. Building on previous results from the\none-dimensional case, we investigate a class of nonincreasing, nonnegative and\nconvex decay kernels with values in the symmetric $K\\times K$ matrices. We show\nthat these decay kernels are always positive definite and characterize when\nthey are even strictly positive definite, a result that may be of independent\ninterest. Optimal strategies for kernels from this class are well-behaved when\none requires that the decay kernel is also commuting. We show how such decay\nkernels can be constructed by means of matrix functions and provide a number of\nexamples. In particular we completely solve the case of matrix exponential\ndecay.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1310.4471v6", "link": "http://arxiv.org/abs/1310.4471v6"}, {"title": "Smart network based portfolios", "summary": "In this article we deal with the problem of portfolio allocation by enhancing\nnetwork theory tools. We use the dependence structure of the correlations\nnetwork in constructing some well-known risk-based models in which the\nestimation of correlation matrix is a building block in the portfolio\noptimization. We formulate and solve all these portfolio allocation problems\nusing both the standard approach and the network-based approach. Moreover, in\nconstructing the network-based portfolios we propose the use of two different\nestimators for the covariance matrix: the sample estimator and the shrinkage\ntoward constant correlation one. All the strategies under analysis are\nimplemented on two high-dimensional portfolios having different\ncharacteristics, covering the period from January $2001$ to December $2017$. We\nfind that the network-based portfolio consistently better performs and has\nlower risk compared to the corresponding standard portfolio in an out-of-sample\nperspective.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1907.01274v1", "link": "http://arxiv.org/abs/1907.01274v1"}, {"title": "An interim core for normal form games and exchange economies with\n  incomplete information: a correction", "summary": "We consider the interim core of normal form cooperative games and exchange\neconomies with incomplete information based on the partition model. We develop\na solution concept that we can situate roughly between Wilson's coarse core and\nYannelis's private core. We investigate the interim negotiation of contracts\nand address the two situations of contract delivery: interim and ex post. Our\nsolution differs from Wilson's concept because the measurability of strategies\nin our solution is postponed until the consumption date (assumed with respect\nto the information that will be known by the players at the consumption date).\nFor interim consumption, our concept differs from Yannelis's private core\nbecause players can negotiate conditional on proper common knowledge events in\nour solution, which strengthens the interim aspect of the game, as we will\nillustrate with examples.", "category": [], "id": "http://arxiv.org/abs/1903.09867v1", "link": "http://dx.doi.org/10.1016/j.jmateco.2015.03.005"}, {"title": "Chebyshev Interpolation for Parametric Option Pricing", "summary": "Recurrent tasks such as pricing, calibration and risk assessment need to be\nexecuted accurately and in real-time. Simultaneously we observe an increase in\nmodel sophistication on the one hand and growing demands on the quality of risk\nmanagement on the other. To address the resulting computational challenges, it\nis natural to exploit the recurrent nature of these tasks. We concentrate on\nParametric Option Pricing (POP) and show that polynomial interpolation in the\nparameter space promises to reduce run-times while maintaining accuracy. The\nattractive properties of Chebyshev interpolation and its tensorized extension\nenable us to identify criteria for (sub)exponential convergence and explicit\nerror bounds. We show that these results apply to a variety of European\n(basket) options and affine asset models. Numerical experiments confirm our\nfindings. Exploring the potential of the method further, we empirically\ninvestigate the efficiency of the Chebyshev method for multivariate and\npath-dependent options.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1505.04648v2", "link": "http://arxiv.org/abs/1505.04648v2"}, {"title": "Herding interactions as an opportunity to prevent extreme events in\n  financial markets", "summary": "A characteristic feature of complex systems in general is a tight coupling\nbetween their constituent parts. In complex socio-economic systems this kind of\nbehavior leads to self-organization, which may be both desirable (e.g. social\ncooperation) and undesirable (e.g. mass panic, financial \"bubbles\" or\n\"crashes\"). Abundance of the empirical data as well as general insights into\nthe trading behavior enables the creation of simple agent-based models\nreproducing sophisticated statistical features of the financial markets. In\nthis contribution we consider a possibility to prevent self-organized extreme\nevents in artificial financial market setup built upon a simple agent-based\nherding model. We show that introduction of agents with predefined\nfundamentalist trading behavior helps to significantly reduce the probability\nof the extreme price fluctuations events. We also test random trading control\nstrategy, which was previously found to be promising, and find that its impact\non the market is rather ambiguous. Though some of the results indicate that it\nmight actually stabilize financial fluctuations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1409.8024v5", "link": "http://dx.doi.org/10.1140/epjb/e2015-60160-0"}, {"title": "Near-Optimal Dynamic Asset Allocation in Financial Markets with Trading\n  Constraints", "summary": "We develop a dual-control method for approximating investment strategies in\nincomplete environments that emerge from the presence of trading constraints.\nConvex duality enables the approximate technology to generate lower and upper\nbounds on the optimal value function. The mechanism rests on closed-form\nexpressions pertaining to the portfolio composition, from which we are able to\nderive the near-optimal asset allocation explicitly. In a real financial\nmarket, we illustrate the accuracy of our approximate method on a dual CRRA\nutility function that characterises the preferences of a finite-horizon\ninvestor. Negligible duality gaps and insignificant annual welfare losses\nsubstantiate accuracy of the technique.", "category": ["q-fin.MF", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1906.12317v3", "link": "http://arxiv.org/abs/1906.12317v3"}, {"title": "De Facto Control: Applying Game Theory to the Law on Corporate\n  Nationality", "summary": "One unexamined assumption in foreign ownership regulation is the notion that\nmajority voting rights translate to 'effective control'. This assumption is so\ndeeply entrenched in foreign investments law that possession of majority voting\nrights can determine the nationality of a corporation and its capacity to\nengage in partially nationalized economic activities. The fact, however, is\nthat minority stockholders can possess a degree of voting power higher than\nwhat their shareholding size might suggest. Voting power is not the same as\nvoting weight and is not measured simply by the proportion or number of votes a\nstockholder may cast in a stockholder meeting. This paper proposes and\ndemonstrates a method for calculating 'effective control' based on given voting\nthresholds and voting weights. It also shows instances where the 'effective\ncontrol' of a foreign minority stockholder appears to comply with foreign\nequity limitations, but has a 'real' voting power grossly beyond the allowable\nthreshold.", "category": ["q-fin.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1712.09969v1", "link": "http://arxiv.org/abs/1712.09969v1"}, {"title": "Planning Fallacy or Hiding Hand: Which Is the Better Explanation?", "summary": "This paper asks and answers the question of whether Kahneman's planning\nfallacy or Hirschman's Hiding Hand best explain performance in capital\ninvestment projects. I agree with my critics that the Hiding Hand exists, i.e.,\nsometimes benefit overruns outweigh cost overruns in project planning and\ndelivery. Specifically, I show this happens in one fifth of projects, based on\nthe best and largest dataset that exists. But that was not the main question I\nset out to answer. My main question was whether the Hiding Hand is \"typical,\"\nas claimed by Hirschman. I show this is not the case, with 80 percent of\nprojects not displaying Hiding Hand behavior. Finally, I agree it would be\nimportant to better understand the circumstances where the Hiding Hand actually\nworks. However, if you want to understand how projects \"typically\" work, as\nHirschman said he did, then the theories of the planning fallacy, optimism\nbias, and strategic misrepresentation - according to which cost overruns and\nbenefit shortfalls are the norm - will serve you significantly better than the\nprinciple of the Hiding Hand. The latter will lead you astray, because it is a\nspecial case instead of a typical one.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1802.09999v1", "link": "http://dx.doi.org/10.1016/j.worlddev.2017.10.002"}, {"title": "Utility Maximization with a Stochastic Clock and an Unbounded Random\n  Endowment", "summary": "We introduce a linear space of finitely additive measures to treat the\nproblem of optimal expected utility from consumption under a stochastic clock\nand an unbounded random endowment process. In this way we establish existence\nand uniqueness for a large class of utility-maximization problems including the\nclassical ones of terminal wealth or consumption, as well as the problems that\ndepend on a random time horizon or multiple consumption instances. As an\nexample we explicitly treat the problem of maximizing the logarithmic utility\nof a consumption stream, where the local time of an Ornstein-Uhlenbeck process\nacts as a stochastic clock.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/math/0503516v1", "link": "http://dx.doi.org/10.1214/105051604000000738"}, {"title": "A nonparametric copula approach to conditional Value-at-Risk", "summary": "Value-at-Risk and its conditional allegory, which takes into account the\navailable information about the economic environment, form the centrepiece of\nthe Basel framework for the evaluation of market risk in the banking sector. In\nthis paper, a new nonparametric framework for estimating this conditional\nValue-at-Risk is presented. A nonparametric approach is particularly pertinent\nas the traditionally used parametric distributions have been shown to be\ninsufficiently robust and flexible in most of the equity-return data sets\nobserved in practice. The method extracts the quantile of the conditional\ndistribution of interest, whose estimation is based on a novel estimator of the\ndensity of the copula describing the dynamic dependence observed in the series\nof returns. Real-world back-testing analyses demonstrate the potential of the\napproach, whose performance may be superior to its industry counterparts.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1712.05527v2", "link": "http://arxiv.org/abs/1712.05527v2"}, {"title": "Behavioural and Dynamical Scenarios for Contingent Claims Valuation in\n  Incomplete Markets", "summary": "We study the problem of determination of asset prices in an incomplete market\nproposing three different but related scenarios. One scenario uses a market\ngame approach whereas the other two are based on risk sharing or regret\nminimizing considerations. Dynamical schemes modeling the convergence of the\nbuyer's and of the seller's prices to a unique price are proposed.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/0903.3657v1", "link": "http://arxiv.org/abs/0903.3657v1"}, {"title": "Inference in Linear Regression Models with Many Covariates and\n  Heteroskedasticity", "summary": "The linear regression model is widely used in empirical work in Economics,\nStatistics, and many other disciplines. Researchers often include many\ncovariates in their linear model specification in an attempt to control for\nconfounders. We give inference methods that allow for many covariates and\nheteroskedasticity. Our results are obtained using high-dimensional\napproximations, where the number of included covariates are allowed to grow as\nfast as the sample size. We find that all of the usual versions of Eicker-White\nheteroskedasticity consistent standard error estimators for linear models are\ninconsistent under this asymptotics. We then propose a new heteroskedasticity\nconsistent standard error formula that is fully automatic and robust to both\n(conditional)\\ heteroskedasticity of unknown form and the inclusion of possibly\nmany covariates. We apply our findings to three settings: parametric linear\nmodels with many covariates, linear panel models with many fixed effects, and\nsemiparametric semi-linear models with many technical regressors. Simulation\nevidence consistent with our theoretical results is also provided. The proposed\nmethods are also illustrated with an empirical application.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1507.02493v2", "link": "http://arxiv.org/abs/1507.02493v2"}, {"title": "Hedging of equity-linked with maximal success factor", "summary": "We consider an equity-linked contract whose payoff depends on the lifetime of\npolicy holder and the stock price. We assume the limited capital for hedging\nand we provide with the best strategy for an insurance company in the meaning\nof so called succes factor $\\IE^\\IP\\left[{\\mathbf 1}_{\\{V_T \\geq D)}+{\\mathbf\n1}_{\\{V_T < D\\}}\\frac{V_T}{D}\\right ]$, where $V_T$ denotes the end value of\nstrategy and $D$ is the payoff of the contract. The work is a genaralisation of\nthe work of F\\\"{o}llmer and Schied \\cite{FS2004} and Klusik and Palmowski\n\\cite{KluPal}, but it considers much more general \"incompletness\" of the\nmarket, among others midterm nonmarket information signals and infitite\nnonmarket scenarios.", "category": ["q-fin.RM", "q-fin.PR"], "id": "http://arxiv.org/abs/1405.0732v1", "link": "http://arxiv.org/abs/1405.0732v1"}, {"title": "A theoretical approach for Pareto-Zipf law", "summary": "We suggest an analytical approach for Pareto-Zipf law, where we assume random\nmultiplicative noise and fragmentation processes for the growth of the number\nof citizens of each city and the number of the cities, respectively.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0802.4064v1", "link": "http://dx.doi.org/10.1142/S0129183108012790"}, {"title": "Transaction Cost Analytics for Corporate Bonds", "summary": "With the rise of the electronic trading, corporate bond traders have access\nto data information of past trades. As a first step to automation, they have to\nstart monitoring their own trades, and using past data to build a benchmark for\nthe expected transaction costs with given bond characteristics and market\nconditions. Given the limited liquidity of corporate bonds which are traded few\ntimes daily, a statistical model is the only way to benchmark effective costs.\nIt brings focused attention of the dealing desk of an institutional investor on\nthe most costly trades, and enables identifying and improving business\npractices such as the market timing for selection counterparties.\n  Unlike existing literature which focuses on general measurements using OLS,\nthis paper takes the viewpoint of a given investor, and provides an analytical\napproach to establish a benchmark for transaction cost analysis in corporate\nbond tradings. Regularized methods are used to improve the selection of\nexplanatory variables, as fewer variables provide easier analytics from a\nbusiness perspective. This benchmark is constructed in two steps. The first\nstep is the regression analysis with cross validation to identify abnormal\ntrades. Three regression approaches, OLS, two-step Lasso and Elastic Net, are\nadopted to identify key features for the bid-ask spread of corporate bonds. The\nsecond step is to use the non-parametric approach to estimate the amplitude and\ndecay pattern of price impact. A key discovery is the price impact asymmetry\nbetween customer-buy orders and consumer-sell orders. This benchmark can aid\ndecision makings for retail investors when requesting quotes on the electronic\nplatform.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1903.09140v2", "link": "http://arxiv.org/abs/1903.09140v2"}, {"title": "Distribution Of Wealth In A Network Model Of The Economy", "summary": "We show, analytically and numerically, that wealth distribution in the\nBouchaud-M\\'ezard network model of the economy is described by a\nthree-parameter generalized inverse gamma distribution. In the mean-field limit\nof a network with any two agents linked, it reduces to the inverse gamma\ndistribution.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1208.2696v1", "link": "http://dx.doi.org/10.1016/j.physa.2013.01.045"}, {"title": "A Prototype Model of Stock Exchange", "summary": "A prototype model of stock market is introduced and studied numerically. In\nthis self-organized system, we consider only the interaction among traders\nwithout external influences. Agents trade according to their own strategy, to\naccumulate his assets by speculating on the price's fluctuations which are\nproduced by themselves. The model reproduced rather realistic price histories\nwhose statistical properties are also similar to those observed in real\nmarkets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/9709118v1", "link": "http://dx.doi.org/10.1209/epl/i1997-00491-5"}, {"title": "Regulation Simulation", "summary": "A deterministic trading strategy by a representative investor on a single\nmarket asset, which generates complex and realistic returns with its first four\nmoments similar to the empirical values of European stock indices, is used to\nsimulate the effects of financial regulation that either pricks bubbles, props\nup crashes, or both. The results suggest that regulation makes the market\nprocess appear more Gaussian and less complex, with the difference more\npronounced for more frequent intervention, though particular periods can be\nworse than the non-regulated version, and that pricking bubbles and propping up\ncrashes are not symmetrical.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1002.2281v1", "link": "http://arxiv.org/abs/1002.2281v1"}, {"title": "Utility based pricing and hedging of jump diffusion processes with a\n  view to applications", "summary": "We discuss utility based pricing and hedging of jump diffusion processes with\nemphasis on the practical applicability of the framework. We point out two\ndifficulties that seem to limit this applicability, namely drift dependence and\nessential risk aversion independence. We suggest to solve these by a\nre-interpretation of the framework. This leads to the notion of an implied\ndrift. We also present a heuristic derivation of the marginal indifference\nprice and the marginal optimal hedge that might be useful in numerical\ncomputations.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1106.1395v2", "link": "http://dx.doi.org/10.1142/S0219024912500525"}, {"title": "On a gap between rational annuitization price for producer and price for\n  customer", "summary": "The paper studies pricing of insurance products focusing on the pricing of\nannuities under uncertainty. This pricing problem is crucial for financial\ndecision making and was studied intensively, however, many open questions still\nremain. In particular, there is a so-called \"annuity puzzle\" related to certain\ninconsistency of existing financial theory with the empirical observations for\nthe annuities market. The paper suggests a pricing method based on the risk\nminimization such that both producer and customer seek to minimize the mean\nsquare hedging error accepted as a measure of risk. This leads to two different\nversions of the pricing problem: the selection of the annuity price given the\nrate of regular payments, and the selection of the rate of payments given the\nannuity price. It appears that solutions of these two problems are different.\nThis can contribute to explanation for the \"annuity puzzle\".", "category": ["econ.GN", "q-fin.EC", "q-fin.MF"], "id": "http://arxiv.org/abs/1809.08960v1", "link": "http://arxiv.org/abs/1809.08960v1"}, {"title": "Time-independent pricing of options in range bound markets", "summary": "Assuming that price of the underlying stock is moving in range bound, the\nBlack-Scholes formula for options pricing supports a separation of variables.\nThe resulting time-independent equation is solved employing different behavior\nof the option price function and three significant results are deduced. The\nfirst is the probability of stock price penetration through support or\nresistance level, called transmission coefficient. The second is the distance\nthat price will go through once stock price penetrates out of the range bound.\nThe last one is a predicted short time dramatic fall in the stock volatility\nright ahead of price tunneling. All three results are useful tools that give\nmarket practitioners valuable insights in choosing the right time to get\ninvolved in an option contract, about how far the price will go in case of a\nbreakout, and how to correctly interpret volatility downfalls.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1304.6846v2", "link": "http://arxiv.org/abs/1304.6846v2"}, {"title": "Targeting Customers under Response-Dependent Costs", "summary": "This study provides a formal analysis of the customer targeting decision\nproblem in settings where the cost for marketing action is stochastic and\nproposes a framework to efficiently estimate the decision variables for\ncampaign profit optimization. Targeting a customer is profitable if the\npositive impact of the marketing treatment on the customer and the associated\nprofit to the company is higher than the cost of the treatment. While there is\na growing literature on developing causal or uplift models to identify the\ncustomers who are impacted most strongly by the marketing action, no research\nhas investigated optimal targeting when the costs of the action are uncertain\nat the time of the targeting decision. Because marketing incentives are\nroutinely conditioned on a positive response by the customer, e.g. a purchase\nor contract renewal, stochastic costs are ubiquitous in direct marketing and\ncustomer retention campaigns. This study makes two contributions to the\nliterature, which are evaluated on a coupon targeting campaign in an e-commerce\nsetting. First, the authors formally analyze the targeting decision problem\nunder response-dependent costs. Profit-optimal targeting requires an estimate\nof the treatment effect on the customer and an estimate of the customer\nresponse probability under treatment. The empirical results demonstrate that\nthe consideration of treatment cost substantially increases campaign profit\nwhen used for customer targeting in combination with the estimation of the\naverage or customer-level treatment effect. Second, the authors propose a\nframework to jointly estimate the treatment effect and the response probability\ncombining methods for causal inference with a hurdle mixture model. The\nproposed causal hurdle model achieves competitive campaign profit while\nstreamlining model building. The code for the empirical analysis is available\non Github.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.06271v1", "link": "http://arxiv.org/abs/2003.06271v1"}, {"title": "Modeling Stock Price Dynamics with Fuzzy Opinion Networks", "summary": "We propose a mathematical model for the word-of-mouth communications among\nstock investors through social networks and explore how the changes of the\ninvestors' social networks influence the stock price dynamics and vice versa.\nAn investor is modeled as a Gaussian fuzzy set (a fuzzy opinion) with the\ncenter and standard deviation as inputs and the fuzzy set itself as output.\nInvestors are connected in the following fashion: the center input of an\ninvestor is taken as the average of the neighbors' outputs, where two investors\nare neighbors if their fuzzy opinions are close enough to each other, and the\nstandard deviation (uncertainty) input is taken with local, global or external\nreference schemes to model different scenarios of how investors define\nuncertainties. The centers and standard deviations of the fuzzy opinions are\nthe expected prices and their uncertainties, respectively, that are used as\ninputs to the price dynamic equation. We prove that with the local reference\nscheme the investors converge to different groups in finite time, while with\nthe global or external reference schemes all investors converge to a consensus\nwithin finite time and the consensus may change with time in the external\nreference case. We show how to model trend followers, contrarians and\nmanipulators within this mathematical framework and prove that the biggest\nenemy of a manipulator is the other manipulators. We perform Monte Carlo\nsimulations to show how the model parameters influence the price dynamics, and\nwe apply a modified version of the model to the daily closing prices of fifteen\ntop banking and real estate stocks in Hong Kong for the recent two years from\nDec. 5, 2013 to Dec. 4, 2015 and discover that a sharp increase of the combined\nuncertainty is a reliable signal to predict the reversal of the current price\ntrend.", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1602.06213v1", "link": "http://arxiv.org/abs/1602.06213v1"}, {"title": "Gaussian Process Regression Networks", "summary": "We introduce a new regression framework, Gaussian process regression networks\n(GPRN), which combines the structural properties of Bayesian neural networks\nwith the non-parametric flexibility of Gaussian processes. This model\naccommodates input dependent signal and noise correlations between multiple\nresponse variables, input dependent length-scales and amplitudes, and\nheavy-tailed predictive distributions. We derive both efficient Markov chain\nMonte Carlo and variational Bayes inference procedures for this model. We apply\nGPRN as a multiple output regression and multivariate volatility model,\ndemonstrating substantially improved performance over eight popular multiple\noutput (multi-task) Gaussian process models and three multivariate volatility\nmodels on benchmark datasets, including a 1000 dimensional gene expression\ndataset.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1110.4411v1", "link": "http://arxiv.org/abs/1110.4411v1"}, {"title": "Optimal execution with nonlinear transient market impact", "summary": "We study the problem of the optimal execution of a large trade in the\npresence of nonlinear transient impact. We propose an approach based on\nhomotopy analysis, whereby a well behaved initial strategy is continuously\ndeformed to lower the expected execution cost. We find that the optimal\nsolution is front loaded for concave impact and that its expected cost is\nsignificantly lower than that of conventional strategies. We then consider\nbrute force numerical optimization of the cost functional; we find that the\noptimal solution for a buy program typically features a few short intense\nbuying periods separated by long periods of weak selling. Indeed, in some cases\nwe find negative expected cost. We show that this undesirable characteristic of\nthe nonlinear transient impact model may be mitigated either by introducing a\nbid-ask spread cost or by imposing convexity of the instantaneous market impact\nfunction for large trading rates.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1412.4839v1", "link": "http://arxiv.org/abs/1412.4839v1"}, {"title": "Mapping markets to the statistical mechanics: the derivatives act\n  against the self-regulation of stock market", "summary": "Mapping the economy to the some statistical physics models we get strong\nindications that, in contrary to the pure stock market, the stock market with\nderivatives could not self-regulate.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0903.3254v1", "link": "http://arxiv.org/abs/0903.3254v1"}, {"title": "On the Basel Liquidity Formula for Elliptical Distributions", "summary": "A justification of the Basel liquidity formula for risk capital in the\ntrading book is given under the assumption that market risk-factor changes form\na Gaussian white noise process over 10-day time steps and changes to P&L are\nlinear in the risk-factor changes. A generalization of the formula is derived\nunder the more general assumption that risk-factor changes are multivariate\nelliptical. It is shown that the Basel formula tends to be conservative when\nthe elliptical distributions are from the heavier-tailed generalized hyperbolic\nfamily. As a by-product of the analysis a Fourier approach to calculating\nexpected shortfall for general symmetric loss distributions is developed.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1803.07590v1", "link": "http://arxiv.org/abs/1803.07590v1"}, {"title": "Pareto index induced from the scale of companies", "summary": "Employing profits data of Japanese companies in 2002 and 2003, we confirm\nthat Pareto's law and the Pareto index are derived from the law of detailed\nbalance and Gibrat's law. The last two laws are observed beyond the region\nwhere Pareto's law holds. By classifying companies into job categories, we find\nthat companies in a small scale job category have more possibilities of growing\nthan those in a large scale job category. This kinematically explains that the\nPareto index for the companies in the small scale job class is larger than that\nfor the companies in the large scale job class.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0506066v1", "link": "http://dx.doi.org/10.1016/j.physa.2005.08.038"}, {"title": "Elicitability and backtesting: Perspectives for banking regulation", "summary": "Conditional forecasts of risk measures play an important role in internal\nrisk management of financial institutions as well as in regulatory capital\ncalculations. In order to assess forecasting performance of a risk measurement\nprocedure, risk measure forecasts are compared to the realized financial losses\nover a period of time and a statistical test of correctness of the procedure is\nconducted. This process is known as backtesting. Such traditional backtests are\nconcerned with assessing some optimality property of a set of risk measure\nestimates. However, they are not suited to compare different risk estimation\nprocedures. We investigate the proposal of comparative backtests, which are\nbetter suited for method comparisons on the basis of forecasting accuracy, but\nnecessitate an elicitable risk measure. We argue that supplementing traditional\nbacktests with comparative backtests will enhance the existing trading book\nregulatory framework for banks by providing the correct incentive for accuracy\nof risk measure forecasts. In addition, the comparative backtesting framework\ncould be used by banks internally as well as by researchers to guide selection\nof forecasting methods. The discussion focuses on three risk measures,\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\nsimulation study and data analysis.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1608.05498v2", "link": "http://arxiv.org/abs/1608.05498v2"}, {"title": "Multi-scale analysis of lead-lag relationships in high-frequency\n  financial markets", "summary": "We propose a novel estimation procedure for scale-by-scale lead-lag\nrelationships of financial assets observed at high-frequency in a\nnon-synchronous manner. The proposed estimation procedure does not require any\ninterpolation processing of original datasets and is applicable to those with\nhighest time resolution available. Consistency of the proposed estimators is\nshown under the continuous-time framework that has been developed in our\nprevious work Hayashi and Koike (2018). An empirical application to a quote\ndataset of the NASDAQ-100 assets identifies two types of lead-lag relationships\nat different time scales.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1708.03992v4", "link": "http://arxiv.org/abs/1708.03992v4"}, {"title": "Measuring wage inequality under right censoring", "summary": "In this paper we investigate potential changes which may have occurred over\nthe last two decades in the probability mass of the right tail of the wage\ndistribution, through the analysis of the corresponding tail index. In\nspecific, a conditional tail index estimator is introduced which explicitly\nallows for right tail censoring (top-coding), which is a feature of the widely\nused current population survey (CPS), as well as of other surveys. Ignoring the\ntop-coding may lead to inconsistent estimates of the tail index and to under or\nover statements of inequality and of its evolution over time. Thus, having a\ntail index estimator that explicitly accounts for this sample characteristic is\nof importance to better understand and compute the tail index dynamics in the\ncensored right tail of the wage distribution. The contribution of this paper is\nthreefold: i) we introduce a conditional tail index estimator that explicitly\nhandles the top-coding problem, and evaluate its finite sample performance and\ncompare it with competing methods; ii) we highlight that the factor values used\nto adjust the top-coded wage have changed over time and depend on the\ncharacteristics of individuals, occupations and industries, and propose\nsuitable values; and iii) we provide an in-depth empirical analysis of the\ndynamics of the US wage distribution's right tail using the public-use CPS\ndatabase from 1992 to 2017.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.12856v1", "link": "http://arxiv.org/abs/2004.12856v1"}, {"title": "Ergodic transition in a simple model of the continuous double auction", "summary": "We study a phenomenological model for the continuous double auction,\nequivalent to two independent $M/M/1$ queues. The continuous double auction\ndefines a continuous-time random walk for trade prices. The conditions for\nergodicity of the auction are derived and, as a consequence, three possible\nregimes in the behavior of prices and logarithmic returns are observed. In the\nergodic regime, prices are unstable and one can observe an intermittent\nbehavior in the logarithmic returns. On the contrary, non-ergodicity triggers\nstability of prices, even if two different regimes can be seen.", "category": ["q-fin.TR", "math.PR"], "id": "http://arxiv.org/abs/1305.2716v1", "link": "http://dx.doi.org/10.1371/journal.pone.0088095"}, {"title": "Robustification of Elliott's on-line EM algorithm for HMMs", "summary": "In this paper, we establish a robustification of an on-line algorithm for\nmodelling asset prices within a hidden Markov model (HMM). In this HMM\nframework, parameters of the model are guided by a Markov chain in discrete\ntime, parameters of the asset returns are therefore able to switch between\ndifferent regimes. The parameters are estimated through an on-line algorithm,\nwhich utilizes incoming information from the market and leads to adaptive\noptimal estimates. We robustify this algorithm step by step against additive\noutliers appearing in the observed asset prices with the rationale to better\nhandle possible peaks or missings in asset returns.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1304.2069v1", "link": "http://arxiv.org/abs/1304.2069v1"}, {"title": "Mortgage Contracts and Selective Default", "summary": "We analyze recently proposed mortgage contracts which aim to eliminate\nselective borrower default when the loan balance exceeds the house price (the\n\"underwater\" effect). We show that contracts which automatically reduce the\noutstanding balance in the event of local house price decline remove the\ndefault incentive, but may induce prepayment in low price states. However, low\nstate prepayments vanish if borrower utility from home ownership, or outside\noptions such as rental costs, are too high. We also show that capital gain\nsharing features, through prepayment penalties in high house price states, are\nineffective, as they virtually eliminate prepayment in such states. For typical\nforeclosure costs, we find that contracts with automatic balance adjustments\nbecome preferable to the traditional fixed rate mortgage at contract rate\nspreads of approximately 50-100 basis points, depending on how far prices must\nfall before adjustments are made. Furthermore, these spreads rapidly decrease\nwith the borrower utility from home ownership. Our results are obtained using\nAmerican options pricing methods, in a model with diffusive home prices, and\neither diffusive or constant interest rates. We determine the contract, default\nand prepayment option values with optimal decision rules. We provide explicit\nsolutions in the perpetual case with constant interest rates; and numerically\ncompute the prepayment and default boundaries in the general case.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/2005.03554v1", "link": "http://arxiv.org/abs/2005.03554v1"}, {"title": "Humans of Simulated New York (HOSNY): an exploratory comprehensive model\n  of city life", "summary": "The model presented in this paper experiments with a comprehensive simulant\nagent in order to provide an exploratory platform in which simulation modelers\nmay try alternative scenarios and participation in policy decision-making. The\nframework is built in a computationally distributed online format in which\nusers can join in and visually explore the results. Modeled activity involves\ndaily routine errands, such as shopping, visiting the doctor or engaging in the\nlabor market. Further, agents make everyday decisions based on individual\nbehavioral attributes and minimal requirements, according to social and\ncontagion networks. Fully developed firms and governments are also included in\nthe model allowing for taxes collection, production decisions, bankruptcy and\nchange in ownership. The contributions to the literature are multifold. They\ninclude (a) a comprehensive model with detailing of the agents and firms'\nactivities and processes and original use of simultaneously (b) reinforcement\nlearning for firm pricing and demand allocation; (c) social contagion for\ndisease spreading and social network for hiring opportunities; and (d) Bayesian\nnetworks for demographic-like generation of agents. All of that within a (e)\nvisually rich environment and multiple use of databases. Hence, the model\nprovides a comprehensive framework from where interactions among citizens,\nfirms and governments can be easily explored allowing for learning and\nvisualization of policies and scenarios.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1703.05240v2", "link": "http://arxiv.org/abs/1703.05240v2"}, {"title": "CDO term structure modelling with Levy processes and the relation to\n  market models", "summary": "This paper considers the modelling of collateralized debt obligations (CDOs).\nWe propose a top-down model via forward rates generalizing Filipovi\\'c,\nOverbeck and Schmidt (2009) to the case where the forward rates are driven by a\nfinite dimensional L\\'evy process. The contribution of this work is twofold: we\nprovide conditions for absence of arbitrage in this generalized framework.\nFurthermore, we study the relation to market models by embedding them in the\nforward rate framework in spirit of Brace, Gatarek and Musiela (1997).", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1007.1706v2", "link": "http://dx.doi.org/10.1142/S0219024911006462"}, {"title": "Complexity, Stability Properties of Mixed Games and Dynamic Algorithms,\n  and Learning in the Sharing Economy", "summary": "The Sharing Economy (which includes Airbnb, Apple, Alibaba, Uber, WeWork,\nEbay, Didi Chuxing, Amazon) blossomed across the world, triggered structural\nchanges in industries and significantly affected international capital flows\nprimarily by disobeying a wide variety of statutes and laws in many countries.\nThey also illegally reduced and changing the nature of competition in many\nindustries often to the detriment of social welfare. This article develops new\ndynamic pricing models for the SEOs and derives some stability properties of\nmixed games and dynamic algorithms which eliminate antitrust liability and also\nreduce deadweight losses, greed, Regret and GPS manipulation. The new dynamic\npricing models contravene the Myerson Satterthwaite Impossibility Theorem.", "category": [], "id": "http://arxiv.org/abs/2001.08192v1", "link": "http://arxiv.org/abs/2001.08192v1"}, {"title": "Re-examination of the size distribution of firms", "summary": "In this paper we address the question of the size distribution of firms. To\nthis aim, we use the Bloomberg database comprising multinational firms within\nthe years 1995-2003, and analyze the data of the sales and the total assets of\nthe separate financial statement of the Japanese and the US companies, and make\na comparison of the size distributions between the Japanese companies and the\nUS companies. We find that (i) the size distribution of the US firms is\napproximately log-normal, in agreement with Gibrat's observation (Gibrat 1931),\nand in contrast (ii) the size distribution of the Japanese firms is clearly not\nlog-normal, and the upper tail of the size distribution follows the Pareto law.\nIt agree with the predictions of the Simon model (Simon 1955). Key words: the\nsize distribution of firms, the Gibrat's law, and the Pareto law", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0512124v2", "link": "http://arxiv.org/abs/physics/0512124v2"}, {"title": "The F\u00f6llmer-Schweizer decomposition under incomplete information", "summary": "In this paper we study the F\\\"ollmer-Schweizer decomposition of a square\nintegrable random variable $\\xi$ with respect to a given semimartingale $S$\nunder restricted information. Thanks to the relationship between this\ndecomposition and that of the projection of $\\xi$ with respect to the given\ninformation flow, we characterize the integrand appearing in the\nF\\\"ollmer-Schweizer decomposition under partial information in the general case\nwhere $\\xi$ is not necessarily adapted to the available information level. For\npartially observable Markovian models where the dynamics of $S$ depends on an\nunobservable stochastic factor $X$, we show how to compute the decomposition by\nmeans of filtering problems involving functions defined on an\ninfinite-dimensional space. Moreover, in the case of a partially observed\njump-diffusion model where $X$ is described by a pure jump process taking\nvalues in a finite dimensional space, we compute explicitly the integrand in\nthe F\\\"ollmer-Schweizer decomposition by working with finite dimensional\nfilters.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1511.05465v2", "link": "http://dx.doi.org/10.1080/17442508.2017.1290094"}, {"title": "The commuting phenomenon as a complex network: The case of Greece", "summary": "This article studies the Greek interregional commuting network (GRN) by using\nmeasures and methods of complex network analysis and empirical techniques. The\nstudy aims to detect structural characteristics of the commuting phenomenon,\nwhich are configured by the functionality of the land transport\ninfrastructures, and to interpret how this network serves and promotes the\nregional development. In the empirical analysis, a multiple linear regression\nmodel for the number of commuters is constructed, which is based on the\nconceptual framework of the term network, in effort to promote the\ninterdisciplinary dialogue. The analysis highlights the effect of the spatial\nconstraints on the network's structure, provides information on the major road\ntransport infrastructure projects that constructed recently and influenced the\ncountry capacity, and outlines a gravity pattern describing the commuting\nphenomenon, which expresses that cities of high population attract large\nvolumes of commuting activity within their boundaries, a fact that contributes\nto the reduction of their outgoing commuting and consequently to the increase\nof their inbound productivity. Overall, this paper highlights the effectiveness\nof complex network analysis in the modeling of spatial and particularly of\ntransportation network and promotes the use of the network paradigm in the\nspatial and regional research.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.08096v1", "link": "http://arxiv.org/abs/2003.08096v1"}, {"title": "Nonlinear Stochastic Model of Return matching to the data of New York\n  and Vilnius Stock Exchanges", "summary": "We scale and analyze the empirical data of return from New York and Vilnius\nstock exchanges matching it to the same nonlinear double stochastic model of\nreturn in financial market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1003.5356v1", "link": "http://arxiv.org/abs/1003.5356v1"}, {"title": "Asymptotic analysis for a simple explicit estimator in Barndorff-Nielsen\n  and Shephard stochastic volatility models", "summary": "We provide a simple explicit estimator for discretely observed\nBarndorff-Nielsen and Shephard models, prove rigorously consistency and\nasymptotic normality based on the single assumption that all moments of the\nstationary distribution of the variance process are finite, and give explicit\nexpressions for the asymptotic covariance matrix.\n  We develop in detail the martingale estimating function approach for a\nbivariate model, that is not a diffusion, but admits jumps. We do not use\nergodicity arguments.\n  We assume that both, logarithmic returns and instantaneous variance are\nobserved on a discrete grid of fixed width, and the observation horizon tends\nto infinity. As the instantaneous variance is not observable in practice, our\nresults cannot be applied immediately. Our purpose is to provide a theoretical\nanalysis as a starting point and benchmark for further developments concerning\noptimal martingale estimating functions, and for theoretical and empirical\ninvestigations, that replace the variance process with a substitute, such as\nnumber or volume of trades or implied variance from option data.", "category": ["q-fin.ST", "math.PR"], "id": "http://arxiv.org/abs/0807.3479v1", "link": "http://arxiv.org/abs/0807.3479v1"}, {"title": "J. S. Mill's Liberal Principle and Unanimity", "summary": "The broad concept of an individual's welfare is actually a cluster of related\nspecific concepts that bear a \"family resemblance\" to one another. One might\ncare about how a policy will affect people both in terms of their subjective\npreferences and also in terms of some notion of their objective interests. This\npaper provides a framework for evaluation of policies in terms of welfare\ncriteria that combine these two considerations. Sufficient conditions are\nprovided for such a criterion to imply the same ranking of social states as\ndoes Pareto's unanimity criterion. Sufficiency is proved via study of a\ncommunity of agents with interdependent ordinal preferences.", "category": [], "id": "http://arxiv.org/abs/1903.07769v1", "link": "http://arxiv.org/abs/1903.07769v1"}, {"title": "A Multi-Entity Input Output (MEIO) Approach to Sustainability -\n  Water-Energy-GHG (WEG) Footprint Statements in Use Cases from Auto and Telco\n  Industries", "summary": "A new Input-Output model, called the Multi-Entity Input-Output (MEIO) model,\nis introduced to estimate the responsibility of entities of an ecosystem on the\nfootprint of each other. It assumed that the ecosystem is comprised of end\nusers, service providers, and utilities. The proposed MEIO modeling approach\ncan be seen as a realization of the Everybody-in-the-Loop (EitL) framework,\nwhich promotes a sustainable future using behaviors and actions that are aware\nof their ubiquitous eco-socio-environment impacts. In this vision, the\nbehavioral changes could be initiated by providing all actors with their\nfootprint statement, which would be estimated using the MEIO models. First, a\nnaive MEIO model is proposed in the form of a graph of actions and\nresponsibility by considering interactions and goods transfers among the\nentities and actors along four channels. Then, the unnormalized responsibility\nand also the final responsibility among the actors are introduced, and then are\nused to re-allocate immediate footprint of actors among themselves. The\nfootprint in the current model is limited to three major impacts: Water,\nEnergy, and GHG emissions. The naive model is then generalized to\nProvider-perspective (P-perspective) and End User-perspective (E-perspective)\nMEIO models in order to make it more suitable to cases where a large number of\nend users are served by a provider. The E-perspective modeling approach\nparticularly allows estimating the footprint associated to a specific end user.\nIn two use cases from the auto and Telco industries, it has been observed that\nthe proposed MEIO models are practical and dependable in allocating footprint\nto the provider and also to the end user, while i) avoiding footprint leakage\nto the end users and ii) handling the large numbers end users. In addition, it\nwill be shown that the MEIO models could be sued to integrate Scope-3 and LCA\napproaches.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1404.6227v2", "link": "http://arxiv.org/abs/1404.6227v2"}, {"title": "Noisy independent component analysis of auto-correlated components", "summary": "We present a new method for the separation of superimposed, independent,\nauto-correlated components from noisy multi-channel measurement. The presented\nmethod simultaneously reconstructs and separates the components, taking all\nchannels into account and thereby increases the effective signal-to-noise ratio\nconsiderably, allowing separations even in the high noise regime.\nCharacteristics of the measurement instruments can be included, allowing for\napplication in complex measurement situations. Independent posterior samples\ncan be provided, permitting error estimates on all desired quantities. Using\nthe concept of information field theory, the algorithm is not restricted to any\ndimensionality of the underlying space or discretization scheme thereof.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1705.02344v2", "link": "http://dx.doi.org/10.1103/PhysRevE.96.042114"}, {"title": "Weak and Strong Taylor methods for numerical solutions of stochastic\n  differential equations", "summary": "We apply results of Malliavin-Thalmaier-Watanabe for strong and weak Taylor\nexpansions of solutions of perturbed stochastic differential equations (SDEs).\nIn particular, we work out weight expressions for the Taylor coefficients of\nthe expansion. The results are applied to LIBOR market models in order to deal\nwith the typical stochastic drift and with stochastic volatility. In contrast\nto other accurate methods like numerical schemes for the full SDE, we obtain\neasily tractable expressions for accurate pricing. In particular, we present an\neasily tractable alternative to ``freezing the drift'' in LIBOR market models,\nwhich has an accuracy similar to the full numerical scheme. Numerical examples\nunderline the results.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/0704.0745v1", "link": "http://arxiv.org/abs/0704.0745v1"}, {"title": "Profitable forecast of prices of stock options on real market data via\n  the solution of an ill-posed problem for the Black-Scholes equation", "summary": "A new mathematical model for the Black-Scholes equation is proposed to\nforecast option prices. This model includes new interval for the price of the\nunderlying stock as well as new initial and boundary conditions. Conventional\nnotions of maturity time and strike prices are not used. The Black-Scholes\nequation is solved as a parabolic equation with the reversed time, which is an\nill-posed problem. Thus, a regularization method is used to solve it. This idea\nis verified on real market data for twenty liquid options. A trading strategy\nis proposed. This strategy indicates that our method is profitable on at least\nthose twenty options. We conjecture that our method might lead to significant\nprofits of those financial institutions which trade large amounts of options.\nWe caution, however, that detailed further studies are necessary to verify this\nconjecture.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1503.03567v1", "link": "http://arxiv.org/abs/1503.03567v1"}, {"title": "KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using\n  Twitter Sentiments", "summary": "Cryptocurrencies, such as Bitcoin, are becoming increasingly popular, having\nbeen widely used as an exchange medium in areas such as financial transaction\nand asset transfer verification. However, there has been a lack of solutions\nthat can support real-time price prediction to cope with high currency\nvolatility, handle massive heterogeneous data volumes, including social media\nsentiments, while supporting fault tolerance and persistence in real time, and\nprovide real-time adaptation of learning algorithms to cope with new price and\nsentiment data. In this paper we introduce KryptoOracle, a novel real-time and\nadaptive cryptocurrency price prediction platform based on Twitter sentiments.\nThe integrative and modular platform is based on (i) a Spark-based architecture\nwhich handles the large volume of incoming data in a persistent and fault\ntolerant way; (ii) an approach that supports sentiment analysis which can\nrespond to large amounts of natural language processing queries in real time;\nand (iii) a predictive method grounded on online learning in which a model\nadapts its weights to cope with new prices and sentiments. Besides providing an\narchitectural design, the paper also describes the KryptoOracle platform\nimplementation and experimental evaluation. Overall, the proposed platform can\nhelp accelerate decision-making, uncover new opportunities and provide more\ntimely insights based on the available and ever-larger financial data volume\nand variety.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2003.04967v1", "link": "http://arxiv.org/abs/2003.04967v1"}, {"title": "Pareto's Law for Income of Individuals and Debt of Bankrupt Companies", "summary": "We analyze the distribution of income and income tax of individuals in Japan\nfor the fiscal year 1998. From the rank-size plots we find that the accumulated\nprobability distribution of both data obey a power law with a Pareto exponent\nvery close to -2. We also present an analysis of the distribution of the debts\nowed by bankrupt companies from 1997 to March, 2000, which is consistent with a\npower law behavior with a Pareto exponent equal to -1. This power law is the\nsame as that of the income distribution of companies. Possible implications of\nthese findings for model building are discussed.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0006038v1", "link": "http://arxiv.org/abs/cond-mat/0006038v1"}, {"title": "A filtering approach to tracking volatility from prices observed at\n  random times", "summary": "This paper is concerned with nonlinear filtering of the coefficients in asset\nprice models with stochastic volatility. More specifically, we assume that the\nasset price process $ S=(S_{t})_{t\\geq0} $ is given by \\[\ndS_{t}=r(\\theta_{t})S_{t}dt+v(\\theta_{t})S_{t}dB_{t}, \\] where\n$B=(B_{t})_{t\\geq0}$ is a Brownian motion, $v$ is a positive function, and\n$\\theta=(\\theta_{t})_{t\\geq0}$ is a c\\'{a}dl\\'{a}g strong Markov process. The\nrandom process $\\theta$ is unobservable. We assume also that the asset price\n$S_{t}$ is observed only at random times $0<\\tau_{1}<\\tau_{2}<....$ This is an\nappropriate assumption when modelling high frequency financial data (e.g.,\ntick-by-tick stock prices).\n  In the above setting the problem of estimation of $\\theta$ can be approached\nas a special nonlinear filtering problem with measurements generated by a\nmultivariate point process $(\\tau_{k},\\log S_{\\tau_{k}})$. While quite natural,\nthis problem does not fit into the standard diffusion or simple point process\nfiltering frameworks and requires more technical tools. We derive a closed form\noptimal recursive Bayesian filter for $\\theta_{t}$, based on the observations\nof $(\\tau_{k},\\log S_{\\tau_{k}})_{k\\geq1}$. It turns out that the filter is\ngiven by a recursive system that involves only deterministic Kolmogorov-type\nequations, which should make the numerical implementation relatively easy.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/math/0509503v1", "link": "http://arxiv.org/abs/math/0509503v1"}, {"title": "Modern tontine with bequest: innovation in pooled annuity products", "summary": "We introduce a new pension product that offers retirees the opportunity for a\nlifelong income and a bequest for their estate. Based on a tontine mechanism,\nthe product divides pension savings between a tontine account and a bequest\naccount. The tontine account is given up to a tontine pool upon death while the\nbequest account value is paid to the retiree's estate. The values of these two\naccounts are continuously re-balanced to the same proportion, which is the key\nfeature of our new product. Our main research question about the new product is\nwhat proportion of pension savings should a retiree allocate to the tontine\naccount. Under a power utility function, we show that more risk averse retirees\nallocate a fairly stable proportion of their pension savings to the tontine\naccount, regardless of the strength of their bequest motive. The proportion\ndeclines as the retiree becomes less risk averse for a while. However, for the\nleast risk averse retirees, a high proportion of their pension savings is\noptimally allocated to the tontine account. This surprising result is explained\nby the least risk averse retirees seeking the potentially high value of the\nbequest account at very old ages.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1903.05990v1", "link": "http://dx.doi.org/10.1016/j.insmatheco.2019.03.002"}, {"title": "The long memory of the efficient market", "summary": "For the London Stock Exchange we demonstrate that the signs of orders obey a\nlong-memory process. The autocorrelation function decays roughly as\n$\\tau^{-\\alpha}$ with $\\alpha \\approx 0.6$, corresponding to a Hurst exponent\n$H \\approx 0.7$. This implies that the signs of future orders are quite\npredictable from the signs of past orders; all else being equal, this would\nsuggest a very strong market inefficiency. We demonstrate, however, that\nfluctuations in order signs are compensated for by anti-correlated fluctuations\nin transaction size and liquidity, which are also long-memory processes. This\ntends to make the returns whiter. We show that some institutions display\nlong-range memory and others don't.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0311053v2", "link": "http://arxiv.org/abs/cond-mat/0311053v2"}, {"title": "Polynomial Diffusion Models for Life Insurance Liabilities", "summary": "In this paper we study the pricing and hedging problem of a portfolio of life\ninsurance products under the benchmark approach, where the reference market is\nmodelled as driven by a state variable following a polynomial diffusion on a\ncompact state space. Such a model guarantees not only the positivity of the OIS\nshort rate and the mortality intensity, but also the possibility of\napproximating both pricing formula and hedging strategy of a large class of\nlife insurance products by explicit formulas.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1602.07910v4", "link": "http://dx.doi.org/10.1016/j.insmatheco.2016.08.008"}, {"title": "Spatial firm competition in two dimensions with linear transportation\n  costs: simulations and analytical results", "summary": "Models of spatial firm competition assume that customers are distributed in\nspace and transportation costs are associated with their purchases of products\nfrom a small number of firms that are also placed at definite locations. It has\nbeen long known that the competition equilibrium is not guaranteed to exist if\nthe most straightforward linear transportation costs are assumed. We show by\nsimulations and also analytically that if periodic boundary conditions in two\ndimensions are assumed, the equilibrium exists for a pair of firms at any\ndistance. When a larger number of firms is considered, we find that their total\nequilibrium profit is inversely proportional to the square root of the number\nof firms. We end with a numerical investigation of the system's behavior for a\ngeneral transportation cost exponent.", "category": ["q-fin.TR", "q-fin.GN"], "id": "http://arxiv.org/abs/1609.04944v1", "link": "http://dx.doi.org/10.1140/epjb/e2016-70148-9"}, {"title": "Measuring Differences in Stochastic Network Structure", "summary": "How can one determine whether a community-level treatment, such as the\nintroduction of a social program or trade shock, alters agents' incentives to\nform links in a network? This paper proposes analogues of a two-sample\nKolmogorov-Smirnov test, widely used in the literature to test the null\nhypothesis of \"no treatment effects\", for network data. It first specifies a\ntesting problem in which the null hypothesis is that two networks are drawn\nfrom the same random graph model. It then describes two randomization tests\nbased on the magnitude of the difference between the networks' adjacency\nmatrices as measured by the $2\\to2$ and $\\infty\\to1$ operator norms. Power\nproperties of the tests are examined analytically, in simulation, and through\ntwo real-world applications. A key finding is that the test based on the\n$\\infty\\to1$ norm can be substantially more powerful than that based on the\n$2\\to2$ norm for the kinds of sparse and degree-heterogeneous networks common\nin economics.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.11117v4", "link": "http://arxiv.org/abs/1903.11117v4"}, {"title": "Nonparametric Estimation of the Random Coefficients Model: An Elastic\n  Net Approach", "summary": "This paper investigates and extends the computationally attractive\nnonparametric random coefficients estimator of Fox, Kim, Ryan, and Bajari\n(2011). We show that their estimator is a special case of the nonnegative\nLASSO, explaining its sparse nature observed in many applications. Recognizing\nthis link, we extend the estimator, transforming it to a special case of the\nnonnegative elastic net. The extension improves the estimator's recovery of the\ntrue support and allows for more accurate estimates of the random coefficients'\ndistribution. Our estimator is a generalization of the original estimator and\ntherefore, is guaranteed to have a model fit at least as good as the original\none. A theoretical analysis of both estimators' properties shows that, under\nconditions, our generalized estimator approximates the true distribution more\naccurately. Two Monte Carlo experiments and an application to a travel mode\ndata set illustrate the improved performance of the generalized estimator.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.08434v2", "link": "http://arxiv.org/abs/1909.08434v2"}, {"title": "Stationary states of a spherical Minority Game with ergodicity breaking", "summary": "Using generating functional and replica techniques, respectively, we study\nthe dynamics and statics of a spherical Minority Game (MG), which in contrast\nwith a spherical MG previously presented in J.Phys A: Math. Gen. 36 11159\n(2003) displays a phase with broken ergodicity and dependence of the\nmacroscopic stationary state on initial conditions. The model thus bears more\nsimilarity with the original MG. Still, all order parameters including the\nvolatility can computed in the ergodic phases without making any\napproximations. We also study the effects of market impact correction on the\nphase diagram. Finally we discuss a continuous-time version of the model as\nwell as the differences between on-line and batch update rules. Our analytical\nresults are confirmed convincingly by comparison with numerical simulations. In\nan appendix we extend the analysis of the earlier spherical MG to a model with\ngeneral time-step, and compare the dynamics and statics of the two spherical\nmodels.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0508413v2", "link": "http://dx.doi.org/10.1088/1742-5468/2005/10/P10009"}, {"title": "Which factor dominates the industry evolution? A synergy analysis based\n  on China's ICT industry", "summary": "Industry evolution caused by various reasons, among which technology progress\ndriving industry development has been approved, but with the new trend of\nindustry convergence, inter-industry convergence also plays an increasing\nimportant role. This paper plans to probe the industry synergetic evolution\nmechanism based on industry convergence and technology progress. Firstly, we\nuse self-organization method and Haken Model to establish synergetic evolution\nequations, select technology progress and industry convergence as the key\nvariables of industry evolution system; then use patent licensing data of\nchina's listed ICT companies to measure industry convergence rate and apply DEA\nMalmquist index method to calculate technology progress level; furthermore\napply simultaneous equation estimation method to investigate the synergetic\nindustry evolution process. From 2002 to 2012, China's ICT industry develops\nrapidly; it has the most obvious convergence and powerful technology progress\ncompared with other industries. We choose china's listed ICT industry to make\nempirical analysis. Our main findings are: a) technology progress is the order\nparameter which dominates industry system evolution. Moreover, industry\nconvergence is the control parameter which is influenced by technology\nprogress; b) Development of technology progress is the core factor for causing\nevolution of industry system, and industry convergence is the outcome of\ntechnology progress; c) Especially, it is important that the dominated role of\ntechnology progress will be sustained, even though in the environment of\nconvergence, companies also need focus on self-innovation, rather than only\nadapt to the new industry evolution trend.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1403.4305v1", "link": "http://arxiv.org/abs/1403.4305v1"}, {"title": "Moment Explosions in the Rough Heston Model", "summary": "We show that the moment explosion time in the rough Heston model [El Euch,\nRosenbaum 2016, arxiv:1609.02108] is finite if and only if it is finite for the\nclassical Heston model. Upper and lower bounds for the explosion time are\nestablished, as well as an algorithm to compute the explosion time (under some\nrestrictions). We show that the critical moments are finite for all maturities.\nFor negative correlation, we apply our algorithm for the moment explosion time\nto compute the lower critical moment.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1801.09458v3", "link": "http://arxiv.org/abs/1801.09458v3"}, {"title": "Essay on the State of Research and Innovation in France and the European\n  Union", "summary": "Innovation in the economy is an important engine of growth and no economy,\nwhatever its complexity and degree of advancement, whether it is based on\nindustry, agriculture, high tech or the providing of services, can be truly\nhealthy without innovating actors within it. The aim of this work, done by an\napplied mathematician working in finance, not by an economist or a lawyer,\nisn't to provide an exhaustive view of the all the mechanisms in France and in\nEurope that aim at fostering innovation in the economy and to offer solutions\nfor removing all the roadblocks that still hinder innovation; indeed such a\nstudy would go far beyond the scope of this study. What I modestly attempted to\nachieve in this study was firstly to draw a panorama of what is working and\nwhat needs to perfected as far as innovation is concerned in France and Europe,\nthen secondly to offer some solutions and personal thoughts to boost\ninnovation.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1601.00679v2", "link": "http://arxiv.org/abs/1601.00679v2"}, {"title": "No-arbitrage conditions and absolutely continuous changes of measure", "summary": "We study the stability of several no-arbitrage conditions with respect to\nabsolutely continuous, but not necessarily equivalent, changes of measure. We\nfirst consider models based on continuous semimartingales and show that\nno-arbitrage conditions weaker than NA and NFLVR are always stable. Then, in\nthe context of general semimartingale models, we show that an absolutely\ncontinuous change of measure does never introduce arbitrages of the first kind\nas long as the change of measure density process can reach zero only\ncontinuously.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1312.4296v2", "link": "http://arxiv.org/abs/1312.4296v2"}, {"title": "Change of numeraire in the two-marginals martingale transport problem", "summary": "In this paper we apply change of numeraire techniques to the optimal\ntransport approach for computing model-free prices of derivatives in a two\nperiods model. In particular, we consider the optimal transport plan\nconstructed in \\cite{HobsonKlimmek2013} as well as the one introduced in\n\\cite{BeiglJuil} and further studied in \\cite{BrenierMartingale}. We show that,\nin the case of positive martingales, a suitable change of numeraire applied to\n\\cite{HobsonKlimmek2013} exchanges forward start straddles of type I and type\nII, so that the optimal transport plan in the subhedging problems is the same\nfor both types of options. Moreover, for \\cite{BrenierMartingale}'s\nconstruction, the right monotone transference plan can be viewed as a mirror\ncoupling of its left counterpart under the change of numeraire. An application\nto stochastic volatility models is also provided.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1406.6951v4", "link": "http://arxiv.org/abs/1406.6951v4"}, {"title": "Characteristic time scales of tick quotes on foreign currency markets:\n  an empirical study and agent-based model", "summary": "Power spectrum densities for the number of tick quotes per minute (market\nactivity) on three currency markets (USD/JPY, EUR/USD, and JPY/EUR) for periods\nfrom January 1999 to December 2000 are analyzed. We find some peaks on the\npower spectrum densities at a few minutes. We develop the double-threshold\nagent model and confirm that stochastic resonance occurs for the market\nactivity of this model. We propose a hypothesis that the periodicities found on\nthe power spectrum densities can be observed due to stochastic resonance.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0512163v2", "link": "http://dx.doi.org/10.1140/epjb/e2006-00125-x"}, {"title": "Convenient liquidity measure for Financial markets", "summary": "A liquidity measure based on consideration and price range is proposed.\nInitially defined for daily data, Liquidity Index (LIX) can also be estimated\nvia intraday data by using a time scaling mechanism. The link between LIX and\nthe liquidity measure based on weighted average bid-ask spread is established.\nUsing this liquidity measure, an elementary liquidity algebra is possible: from\nthe estimation of the execution cost, the liquidity of a basket of instruments\nis obtained. A formula for the liquidity of an ETF, from the liquidity of its\nconstituencies and the liquidity of ETF shares, is derived.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1412.5072v1", "link": "http://arxiv.org/abs/1412.5072v1"}, {"title": "Merton problem with one additional indivisible asset", "summary": "In this paper we consider a modification of the classical Merton portfolio\noptimization problem. Namely, an investor can trade in financial asset and\nconsume his capital. He is additionally endowed with a one unit of an\nindivisible asset which he can sell at any time. We give a numerical example of\ncalculating the optimal time to sale the indivisible asset, the optimal\nconsumption rate and the value function.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1403.3223v1", "link": "http://dx.doi.org/10.4467/20843828AM.15.005.3909"}, {"title": "Uncertain Volatility Models with Stochastic Bounds", "summary": "In this paper, we propose the uncertain volatility models with stochastic\nbounds. Like the regular uncertain volatility models, we know only that the\ntrue model lies in a family of progressively measurable and bounded processes,\nbut instead of using two deterministic bounds, the uncertain volatility\nfluctuates between two stochastic bounds generated by its inherent stochastic\nvolatility process. This brings better accuracy and is consistent with the\nobserved volatility path such as for the VIX as a proxy for instance. We apply\nthe regular perturbation analysis upon the worst case scenario price, and\nderive the first order approximation in the regime of slowly varying stochastic\nbounds. The original problem which involves solving a fully nonlinear PDE in\ndimension two for the worst case scenario price, is reduced to solving a\nnonlinear PDE in dimension one and a linear PDE with source, which gives a\ntremendous computational advantage. Numerical experiments show that this\napproximation procedure performs very well, even in the regime of moderately\nslow varying stochastic bounds.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1702.05036v1", "link": "http://arxiv.org/abs/1702.05036v1"}, {"title": "Growth, Industrial Externality, Prospect Dynamics and Well-being on\n  Markets", "summary": "Functions or 'functionings' enable to give a structure to any economic\nactivity whether they are used to describe a good or a service that is\nexchanged on a market or they constitute the capability of an agent to provide\nthe labor market with specific work and skills. That structure encompasses the\nbasic law of supply and demand and the conditions of growth within a\ntransaction and of the inflation control. Functional requirements can be\nfollowed from the design of a product to the delivery of a solution to a\ncustomer needs with different levels of externalities while value is created\nintegrating organizational and technical constraints whereas a budget is\nallocated to the various entities of the firm involved in the production.\nEntering the market through that structure leads to designing basic equations\nof its dynamics and to finding canonical solutions out of particular\nequilibria. This approach enables to tackle behavioral foundations of Prospect\nTheory within a generalization of its probability weighting function turned\ninto an operator which applies to Western, Educated, Industrialized, Rich, and\nDemocratic societies as well as to the poorest ones. The nature of reality and\nwell-being appears then as closely related to the relative satisfaction reached\non the market, as it can be conceived by an agent, according to business\ncycles. This reality being the result of the complementary systems that govern\nhuman mind as structured by rational psychologists.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1812.09302v3", "link": "http://arxiv.org/abs/1812.09302v3"}, {"title": "The Laws of Motion of the Broker Call Rate in the United States", "summary": "In this paper, which is the third installment of the author's trilogy on\nmargin loan pricing, we analyze $1,367$ monthly observations of the U.S. broker\ncall money rate, which is the interest rate at which stock brokers can borrow\nto fund their margin loans to retail clients. We describe the basic features\nand mean-reverting behavior of this series and juxtapose the\nempirically-derived laws of motion with the author's prior theories of margin\nloan pricing (Garivaltis 2019a-b). This allows us to derive stochastic\ndifferential equations that govern the evolution of the margin loan interest\nrate and the leverage ratios of sophisticated brokerage clients (namely,\ncontinuous time Kelly gamblers). Finally, we apply Merton's (1974) arbitrage\ntheory of corporate liability pricing to study theoretical constraints on the\nrisk premia that could be generated in the market for call money. Apparently,\nif there is no arbitrage in the U.S. financial markets, the implication is that\nthe total volume of call loans must constitute north of $70\\%$ of the value of\nall leveraged portfolios.", "category": ["econ.EM", "econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1906.00946v1", "link": "http://dx.doi.org/10.3390/ijfs7040056"}, {"title": "Should we opt for the Black Friday discounted price or wait until the\n  Boxing Day?", "summary": "We derive an optimal strategy for minimizing the expected loss in the\ntwo-period economy when a pivotal decision needs to be made during the first\ntime period and cannot be subsequently reversed. Our interest in the problem\nhas been motivated by the classical shopper's dilemma during the Black Friday\npromotion period, and our solution crucially relies on the pioneering work of\nMcDonnell and Abbott on the two-envelope paradox.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1612.05855v1", "link": "http://arxiv.org/abs/1612.05855v1"}, {"title": "Dynamic Portfolio Optimization with Looping Contagion Risk", "summary": "In this paper we consider a utility maximization problem with defaultable\nstocks and looping contagion risk. We assume that the default intensity of one\ncompany depends on the stock prices of itself and other companies, and the\ndefault of the company induces immediate drops in the stock prices of the\nsurviving companies. We prove that the value function is the unique viscosity\nsolution of the HJB equation. We also perform some numerical tests to compare\nand analyse the statistical distributions of the terminal wealth of log utility\nand power utility based on two strategies, one using the full information of\nintensity process and the other a proxy constant intensity process.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1710.05168v3", "link": "http://arxiv.org/abs/1710.05168v3"}, {"title": "An Ordinal Pattern Approach to Detect and to Model Leverage Effects and\n  Dependence Structures Between Financial Time Series", "summary": "We introduce two types of ordinal pattern dependence between time series.\nPositive (resp. negative) ordinal pattern dependence can be seen as a\nnon-paramatric and in particular non-linear counterpart to positive (resp.\nnegative) correlation. We show in an explorative study that both types of this\ndependence show up in real world financial data.", "category": ["q-fin.ST", "math.PR"], "id": "http://arxiv.org/abs/1502.07321v1", "link": "http://arxiv.org/abs/1502.07321v1"}, {"title": "Short Maturity Asian Options in Local Volatility Models", "summary": "We present a rigorous study of the short maturity asymptotics for Asian\noptions with continuous-time averaging, under the assumption that the\nunderlying asset follows a local volatility model. The asymptotics for\nout-of-the-money, in-the-money, and at-the-money cases are derived, considering\nboth fixed strike and floating strike Asian options. The asymptotics for the\nout-of-the-money case involves a non-trivial variational problem which is\nsolved completely. We present an analytical approximation for Asian options\nprices, and demonstrate good numerical agreement of the asymptotic results with\nthe results of Monte Carlo simulations and benchmark test cases in the\nBlack-Scholes model for option parameters relevant in practical applications.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1609.07559v1", "link": "http://arxiv.org/abs/1609.07559v1"}, {"title": "Optimal market making with persistent order flow", "summary": "We address the issue of market making on electronic markets when taking into\naccount the self exciting property of market order flow. We consider a market\nwith order flows driven by Hawkes processes where one market maker operates,\naiming at optimizing its profit. We characterize an optimal control solving\nthis problem by proving existence and uniqueness of a viscosity solution to the\nassociated Hamilton Jacobi Bellman equation. Finally we propose a methodology\nto approximate the optimal strategy.", "category": ["q-fin.TR", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/2003.05958v1", "link": "http://arxiv.org/abs/2003.05958v1"}, {"title": "Effects of income redistribution on the evolution of cooperation in\n  spatial public goods games", "summary": "Income redistribution is the transfer of income from some individuals to\nothers directly or indirectly by means of social mechanisms, such as taxation,\npublic services and so on. Employing a spatial public goods game, we study the\ninfluence of income redistribution on the evolution of cooperation. Two kinds\nof evolutionary models are constructed, which describe local and global\nredistribution of income respectively. In the local model, players have to pay\npart of their income after each PGG and the accumulated income is redistributed\nto the members. While in the global model, all the players pay part of their\nincome after engaging in all the local PGGs, which are centered on himself and\nhis nearest neighbours, and the accumulated income is redistributed to the\nwhole population. We show that the cooperation prospers significantly with\nincreasing income expenditure proportion in the local redistribution of income,\nwhile in the global model the situation is opposite. Furthermore, the\ncooperation drops dramatically from the maximum curvature point of income\nexpenditure proportion. In particular, the intermediate critical points are\nclosely related to the renormalized enhancement factors.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1611.01531v1", "link": "http://dx.doi.org/10.1088/1367-2630/aa5666"}, {"title": "Approaches to Asian Option Pricing with Discrete Dividends", "summary": "The method and characteristics of several approaches to the pricing of\ndiscretely monitored arithmetic Asian options on stocks with discrete, absolute\ndividends are described. The contrast between method behaviors for options with\nan Asian tail and those with monitoring throughout their lifespan is\nemphasized. Rates of convergence are confirmed, but greater focus is put on\nactual performance in regions of accuracy which are realistic for use by\npractitioners. A hybrid approach combining Curran's analytical approximation\nwith a two-dimensional finite difference method is examined with respect to the\nerrors caused by the approximating assumptions. For Asian tails of equidistant\nmonitoring dates, this method performs very well, but as the scenario deviates\nfrom the method's ideal conditions, the errors in the approximation grow\nunfeasible. For general monitoring straightforward solution of the full\nthree-dimensional partial differential equation by finite differences is highly\naccurate but suffers from rapid degradation in performance as the monitoring\ninterval increases. For options with long monitoring intervals a randomized\nquasi-Monte Carlo method with control variate variance reduction stands out as\na powerful alternative.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1702.00994v1", "link": "http://arxiv.org/abs/1702.00994v1"}, {"title": "A Hedonic Metric Approach to Estimating the Demand for Differentiated\n  Products: An Application to Retail Milk Demand", "summary": "This article introduces the Hedonic Metric (HM) approach as an original\nmethod to model the demand for differentiated products. Using this approach,\ninitially, we create an n-dimensional hedonic space based on the characteristic\ninformation available to consumers. Next, we allocate products into this space\nand estimate the elasticities using distances. Our model makes it possible to\nestimate a large number of differentiated products in a single demand system.\nWe applied our model to estimate the retail demand for fluid milk products.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.07197v1", "link": "http://arxiv.org/abs/2003.07197v1"}, {"title": "On Infectious Model for Dependent Defaults", "summary": "In this paper, we propose a two-sector Markovian infectious model, which is\nan extension of Greenwood's model. The central idea of this model is that the\ncausality of defaults of two sectors is in both direction, which enrich\ndependence dynamics. The Bayesian Information Criterion is adopted to compare\nthe proposed model with the two-sector model in credit literature using the\nreal data. We find that the newly proposed model is statistically better than\nthe model in past literature. We also introduce two measures: CRES and CRVaR to\ngive risk evaluation of our model.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1301.0186v1", "link": "http://arxiv.org/abs/1301.0186v1"}, {"title": "Endogenous crisis waves: a stochastic model with synchronized collective\n  behavior", "summary": "We propose a simple framework to understand commonly observed crisis waves in\nmacroeconomic Agent Based models, that is also relevant to a variety of other\nphysical or biological situations where synchronization occurs. We compute\nexactly the phase diagram of the model and the location of the synchronization\ntransition in parameter space. Many modifications and extensions can be\nstudied, confirming that the synchronization transition is extremely robust\nagainst various sources of noise or imperfections.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1409.3296v1", "link": "http://dx.doi.org/10.1103/PhysRevLett.114.088701"}, {"title": "Anticipating cryptocurrency prices using machine learning", "summary": "Machine learning and AI-assisted trading have attracted growing interest for\nthe past few years. Here, we use this approach to test the hypothesis that the\ninefficiency of the cryptocurrency market can be exploited to generate abnormal\nprofits. We analyse daily data for $1,681$ cryptocurrencies for the period\nbetween Nov. 2015 and Apr. 2018. We show that simple trading strategies\nassisted by state-of-the-art machine learning algorithms outperform standard\nbenchmarks. Our results show that nontrivial, but ultimately simple,\nalgorithmic mechanisms can help anticipate the short-term evolution of the\ncryptocurrency market.", "category": ["q-fin.GN", "q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1805.08550v4", "link": "http://dx.doi.org/10.1155/2018/8983590"}, {"title": "A sparse grid approach to balance sheet risk measurement", "summary": "In this work, we present a numerical method based on a sparse grid\napproximation to compute the loss distribution of the balance sheet of a\nfinancial or an insurance company. We first describe, in a stylised way, the\nassets and liabilities dynamics that are used for the numerical estimation of\nthe balance sheet distribution. For the pricing and hedging model, we chose a\nclassical Black & Scholes model with a stochastic interest rate following a\nHull & White model. The risk management model describing the evolution of the\nparameters of the pricing and hedging model is a Gaussian model. The new\nnumerical method is compared with the traditional nested simulation approach.\nWe review the convergence of both methods to estimate the risk indicators under\nconsideration. Finally, we provide numerical results showing that the sparse\ngrid approach is extremely competitive for models with moderate dimension.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1811.08706v1", "link": "http://arxiv.org/abs/1811.08706v1"}, {"title": "A mixed Monte Carlo and PDE variance reduction method for foreign\n  exchange options under the Heston-CIR model", "summary": "In this paper, the valuation of European and path-dependent options in\nforeign exchange (FX) markets is considered when the currency exchange rate\nevolves according to the Heston model combined with the Cox-Ingersoll-Ross\ndynamics for the stochastic domestic and foreign short interest rates. The\nmixed Monte Carlo/PDE method requires that we simulate only the paths of the\nsquared volatility and the two interest rates, while an \"inner\"\nBlack-Scholes-type expectation is evaluated by means of a PDE. This can lead to\na substantial variance reduction and complexity improvements under certain\ncircumstances depending on the contract and the model parameters. In this work,\nwe establish the uniform boundedness of moments of the exchange rate process\nand its approximation, and prove strong convergence in $L^p$ ($p\\geq1$) of the\nlatter. Then, we carry out a variance reduction analysis and obtain accurate\napproximations for quantities of interest. All theoretical contributions can be\nextended to multi-factor short rates in a straightforward manner. Finally, we\nillustrate the efficiency of the method for the four-factor Heston-CIR model\nthrough a detailed quantitative assessment.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1509.01479v3", "link": "http://arxiv.org/abs/1509.01479v3"}, {"title": "A note on optimal expected utility of dividend payments with\n  proportional reinsurance", "summary": "In this paper, we consider the problem of maximizing the expected discounted\nutility of dividend payments for an insurance company that controls risk\nexposure by purchasing proportional reinsurance. We assume the preference of\nthe insurer is of CRRA form. By solving the corresponding\nHamilton-Jacobi-Bellman equation, we identify the value function and the\ncorresponding optimal strategy. We also analyze the asymptotic behavior of the\nvalue function for large initial reserves. Finally, we provide some numerical\nexamples to illustrate the results and analyze the sensitivity of the\nparameters.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1605.06849v2", "link": "http://arxiv.org/abs/1605.06849v2"}, {"title": "Scaling Analysis on Indian Foreign Exchange Market", "summary": "In this paper we investigate the scaling behavior of the average daily\nexchange rate returns of the Indian Rupee against four foreign currencies\nnamely US Dollar, Euro, Great Britain Pound and Japanese Yen. Average daily\nexchange rate return of the Indian Rupee against US Dollar is found to exhibit\na persistent scaling behavior and follow Levy stable distribution. On the\ncontrary the average daily exchange rate returns of the other three foreign\ncurrencies do not show persistency or antipersistency and follow Gaussian\ndistribution.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0504038v2", "link": "http://dx.doi.org/10.1016/j.physa.2005.09.044"}, {"title": "The ecology of social interactions in online and offline environments", "summary": "The rise in online social networking has brought about a revolution in social\nrelations. However, its effects on offline interactions and its implications\nfor collective well-being are still not clear and are under-investigated. We\nstudy the ecology of online and offline interaction in an evolutionary game\nframework where individuals can adopt different strategies of socialization.\nOur main result is that the spreading of self-protective behaviors to cope with\nhostile social environments can lead the economy to non-socially optimal\nstationary states.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1601.07776v1", "link": "http://arxiv.org/abs/1601.07776v1"}, {"title": "On the drawdown of completely asymmetric Levy processes", "summary": "The {\\em drawdown} process $Y$ of a completely asymmetric L\\'{e}vy process\n$X$ is equal to $X$ reflected at its running supremum $\\bar{X}$: $Y = \\bar{X} -\nX$. In this paper we explicitly express in terms of the scale function and the\nL\\'{e}vy measure of $X$ the law of the sextuple of the first-passage time of\n$Y$ over the level $a>0$, the time $\\bar{G}_{\\tau_a}$ of the last supremum of\n$X$ prior to $\\tau_a$, the infimum $\\unl X_{\\tau_a}$ and supremum $\\ovl\nX_{\\tau_a}$ of $X$ at $\\tau_a$ and the undershoot $a - Y_{\\tau_a-}$ and\novershoot $Y_{\\tau_a}-a$ of $Y$ at $\\tau_a$. As application we obtain explicit\nexpressions for the laws of a number of functionals of drawdowns and rallies in\na completely asymmetric exponential L\\'{e}vy model.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1103.1460v2", "link": "http://arxiv.org/abs/1103.1460v2"}, {"title": "Model-free Superhedging Duality", "summary": "In a model free discrete time financial market, we prove the superhedging\nduality theorem, where trading is allowed with dynamic and semi-static\nstrategies. We also show that the initial cost of the cheapest portfolio that\ndominates a contingent claim on every possible path $\\omega \\in \\Omega$, might\nbe strictly greater than the upper bound of the no-arbitrage prices. We\ntherefore characterize the subset of trajectories on which this duality gap\ndisappears and prove that it is an analytic set.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1506.06608v3", "link": "http://arxiv.org/abs/1506.06608v3"}, {"title": "Self-Organized Criticality and Stock Market Dynamics: an Empirical Study", "summary": "The Stock Market is a complex self-interacting system, characterized by an\nintermittent behaviour. Periods of high activity alternate with periods of\nrelative calm. In the present work we investigate empirically about the\npossibility that the market is in a self-organized critical state (SOC). A\nwavelet transform method is used in order to separate high activity periods,\nrelated to the avalanches of sandpile models, from quiescent.\n  A statistical analysis of the filtered data show a power law behaviour in the\navalanche size, duration and laminar times. The memory process, implied by the\npower law distribution, of the laminar times is not consistent with classical\nconservative models for self-organized criticality. We argue that a\n``near-SOC'' state or a time dependence in the driver, which may be chaotic,\ncan explain this behaviour.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0405257v2", "link": "http://dx.doi.org/10.1016/j.physa.2004.11.061"}, {"title": "Culture and the disposition effect", "summary": "We study the relationship between national culture and the disposition effect\nby investigating international differences in the degree of investors'\ndisposition effect. We utilize brokerage data of 387,993 traders from 83\ncountries and find great variation in the degree of the disposition effect\nacross the world. We find that the cultural dimensions of long-term orientation\nand indulgence help to explain why certain nationalities are more prone to the\ndisposition effect. We also find support on an international level for the role\nof age and gender in explaining the disposition effect.", "category": ["q-fin.GN", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.11492v1", "link": "http://dx.doi.org/10.1016/j.econlet.2019.108653"}, {"title": "A note on essential smoothness in the Heston model", "summary": "This note studies an issue relating to essential smoothness that can arise\nwhen the theory of large deviations is applied to a certain option pricing\nformula in the Heston model. The note identifies a gap, based on this issue, in\nthe proof of Corollary 2.4 in \\cite{FordeJacquier10} and describes how to\ncircumvent it. This completes the proof of Corollary 2.4 in\n\\cite{FordeJacquier10} and hence of the main result in \\cite{FordeJacquier10},\nwhich describes the limiting behaviour of the implied volatility smile in the\nHeston model far from maturity.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1107.4881v1", "link": "http://arxiv.org/abs/1107.4881v1"}, {"title": "On the Financial Crisis 2008 from a Physicist's viewpoint: A Spin-Glass\n  Interpretation", "summary": "In an informal way, a number of thoughts on the financial crisis 2008 are\npresented from a physicist's viewpoint, considering the problem as a\nnonergodicity transition of a spin-glass type of system. Some tentative\nsuggestions concerning the way out of the crisis are also discussed, concerning\nKeynesian \"deficit spending\" methods, tax reductions, and finally the method\n\"ruin and recreate\" known from optimization theory. Also the de\nAlmeida-Thouless instability line of spin-glass theory is given a financial\ninterpretation.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0812.3378v3", "link": "http://arxiv.org/abs/0812.3378v3"}, {"title": "Are the contemporary financial fluctuations sooner converging to normal?", "summary": "Based on the tick-by-tick price changes of the companies from the U.S. and\nfrom the German stock markets over the period 1998-99 we reanalyse several\ncharacteristics established by the Boston Group for the U.S. market in the\nperiod 1994-95, which serves to verify their space and time-translational\ninvariance. By increasing the time scales we find a significantly more\naccelerated crossover from the power-law (alpha approximately 3) asymptotic\nbehaviour of the distribution of returns towards a Gaussian, both for the U.S.\nas well as for the German stock markets. In the latter case the crossover is\neven faster. Consistently, the corresponding autocorrelation functions of\nreturns and of the time averaged volatility also indicate a faster loss of\nmemory with increasing time. This route towards efficiency may reflect a\nsystematic increase of the information processing when going from past to\npresent.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0208240v2", "link": "http://arxiv.org/abs/cond-mat/0208240v2"}, {"title": "Analyses of Statistical Structures in Economic Indices", "summary": "The complex, time-dependent statistical structures observed in the Dow Jones\nIndustrial Average on a typical trading day are modeled with Lorentzian\nfunctions. The resonant-like structures are characterized by the values of the\nbasic ratio: the average lifetime of the individual states associated with a\ngiven structural form divided by the average interval between adjacent states.\nValues of the ratio are determined for three structural forms characterized by\nthe average intervals: 50 to 100 seconds (the fine structure), approximately10\nminutes, and approximately1 hour (the intermediate structures I and II). During\nthe trading day the values of the basic ratio associated with the fine\nstructure of the index are found to lie in the narrow range from 0.49 to 0.52.\nThis finding is characteristic of the highly statistical nature of many-body\nsystems typified by daily trading. It is therefore proposed that the value of\nthis ratio, determined in the first hour-or-so on a given day, be used to\nprovide information concerning the likely performance of the fine, statistical\ncomponent of the index for the remainder of the trading day. For the\nintermediate structures the basic ratios are approximately 0.6 and therefore\nthey too can be analyzed as individual states.\n  Keywords: Analytical economics; Lorentzian analyses of statistical structures\nin the Dow Jones Industrial Average; basic parameters of economic indices.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1501.02216v1", "link": "http://arxiv.org/abs/1501.02216v1"}, {"title": "Observing Each Other's Observations in the Electronic Mail Game", "summary": "We study a Bayesian coordination game where agents receive private\ninformation on the game's payoff structure. In addition, agents receive private\nsignals on each other's private information. We show that once agents possess\nthese different types of information, there exists a coordination game in the\nevaluation of this information. And even though the precisions of both signal\ntypes is exogenous, the precision with which agents predict each other's\nactions at equilibrium turns out to be endogenous. As a consequence, we find\nthat there exist multiple equilibria if the private signals' precision is high.\nThese equilibria differ with regard to the way that agents weight their private\ninformation to reason about each other's actions.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1501.00882v1", "link": "http://arxiv.org/abs/1501.00882v1"}, {"title": "An introduction to flexible methods for policy evaluation", "summary": "This chapter covers different approaches to policy evaluation for assessing\nthe causal effect of a treatment or intervention on an outcome of interest. As\nan introduction to causal inference, the discussion starts with the\nexperimental evaluation of a randomized treatment. It then reviews evaluation\nmethods based on selection on observables (assuming a quasi-random treatment\ngiven observed covariates), instrumental variables (inducing a quasi-random\nshift in the treatment), difference-in-differences and changes-in-changes\n(exploiting changes in outcomes over time), as well as regression\ndiscontinuities and kinks (using changes in the treatment assignment at some\nthreshold of a running variable). The chapter discusses methods particularly\nsuited for data with many observations for a flexible (i.e. semi- or\nnonparametric) modeling of treatment effects, and/or many (i.e. high\ndimensional) observed covariates by applying machine learning to select and\ncontrol for covariates in a data-driven way. This is not only useful for\ntackling confounding by controlling for instance for factors jointly affecting\nthe treatment and the outcome, but also for learning effect heterogeneities\nacross subgroups defined upon observable covariates and optimally targeting\nthose groups for which the treatment is most effective.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.00641v1", "link": "http://arxiv.org/abs/1910.00641v1"}, {"title": "Utility maximization in the large markets", "summary": "In the large financial market, which is described by a model with countably\nmany traded assets, we formulate the problem of the expected utility\nmaximization. Assuming that the preferences of an economic agent are modeled\nwith a stochastic utility and that the consumption occurs according to a\nstochastic clock, we obtain the \"usual\" conclusions of the utility maximization\ntheory. We also give a characterization of the value function in the large\nmarket in terms of a sequence of the value functions in the finite-dimensional\nmodels.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1403.6175v2", "link": "http://arxiv.org/abs/1403.6175v2"}, {"title": "De Finetti's dividend problem and impulse control for a two-dimensional\n  insurance risk process", "summary": "Consider two insurance companies (or two branches of the same company) that\nreceive premiums at different rates and then split the amount they pay in fixed\nproportions for each claim (for simplicity we assume that they are equal). We\nmodel the occurrence of claims according to a Poisson process. The ruin is\nachieved when the corresponding two-dimensional risk process first leaves the\npositive quadrant. We will consider two scenarios of the controlled process:\nrefraction and impulse control. In the first case the dividends are payed out\nwhen the two-dimensional risk process exits the fixed region. In the second\nscenario, whenever the process hits the horizontal line, it is reduced by\npaying dividends to some fixed point in the positive quadrant where it waits\nfor the next claim to arrive. In both models we calculate the discounted\ncumulative dividend payments until the ruin. This paper is the first attempt to\nunderstand the effect of dependencies of two portfolios on the joint optimal\nstrategy of paying dividends. For example in case of proportional reinsurance\none can observe the interesting phenomenon that choice of the optimal barrier\ndepends on the initial reserves. This is in contrast with the one-dimensional\nCram\\'{e}r-Lundberg model where the optimal choice of the barrier is uniform\nfor all initial reserves.", "category": ["q-fin.GN", "math.PR"], "id": "http://arxiv.org/abs/0906.2100v3", "link": "http://arxiv.org/abs/0906.2100v3"}, {"title": "C^{1,1} regularity for degenerate elliptic obstacle problems", "summary": "The Heston stochastic volatility process is a degenerate diffusion process\nwhere the degeneracy in the diffusion coefficient is proportional to the square\nroot of the distance to the boundary of the half-plane. The generator of this\nprocess with killing, called the elliptic Heston operator, is a second-order,\ndegenerate-elliptic partial differential operator, where the degeneracy in the\noperator symbol is proportional to the distance to the boundary of the\nhalf-plane. In mathematical finance, solutions to the obstacle problem for the\nelliptic Heston operator correspond to value functions for perpetual\nAmerican-style options on the underlying asset. With the aid of weighted\nSobolev spaces and weighted Holder spaces, we establish the optimal $C^{1,1}$\nregularity (up to the boundary of the half-plane) for solutions to obstacle\nproblems for the elliptic Heston operator when the obstacle functions are\nsufficiently smooth.", "category": ["math.PR", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1206.0831v5", "link": "http://dx.doi.org/10.1016/j.jde.2015.11.037"}, {"title": "Reciprocity as the foundation of Financial Economics", "summary": "This paper argues that the fundamental principle of contemporary financial\neconomics is balanced reciprocity, not the principle of utility maximisation\nthat is important in economics more generally. The argument is developed by\nanalysing the mathematical Fundamental Theory of Asset Pricing with reference\nto the emergence of mathematical probability in the seventeenth century in the\ncontext of the ethical assessment of commercial contracts. This analysis is\nundertaken within a framework of Pragmatic philosophy and Virtue Ethics. The\npurpose of the paper is to mitigate future financial crises by reorienting\nfinancial economics to emphasise the objectives of market stability and social\ncohesion rather than individual utility maximisation.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1310.2798v1", "link": "http://arxiv.org/abs/1310.2798v1"}, {"title": "Inflation, unemployment, and labour force. Phillips curves and long-term\n  projections for Austria", "summary": "We model the rate of inflation and unemployment in Austria since the early\n1960s within the Phillips/Fisher framework. The change in labour force is the\ndriving force representing economic activity in the Phillips curve. For\nAustria, this macroeconomic variable was first tested as a predictor of\ninflation and unemployment in 2005 with the involved time series ended in 2003.\nHere we extend all series by nine new readings available since 2003 and\nre-estimate the previously estimated relationships between inflation,\nunemployment, and labour force. As before, a structural break is allowed in\nthese relationships, which is related to numerous changes in definitions in the\n1980s. The break year is estimated together with other model parameters by the\nBoundary Element Method with the LSQ fitting between observed and predicted\nintegral curves. The precision of inflation prediction, as described by the\nroot-mean-square (forecasting) error is by 20% to 70% better than that\nestimated by AR(1) model. The estimates of model forecasting error are\navailable for those time series where the change in labour force leads by one\n(the GDP deflator) or two (CPI) years. For the whole period between 1965 and\n2012 as well as for the intervals before and after the structural break (1986\nfor all inflation models) separately, our model is superior to the na\\\"ive\nforecasting, which in turn, is not worse than any other forecasting model. The\nlevel of statistical reliability and the predictive power of the link between\ninflation and labour force imply that the National Bank of Austria does not\ncontrol inflation and unemployment beyond revisions to definitions. The labour\nforce projection provided by Statistic Austria allows foreseeing inflation at a\nforty-year horizon: the rate of CPI inflation will hover around 1.3% and the\nGDP deflator will likely sink below zero between 2018 and 2034.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1310.1786v1", "link": "http://arxiv.org/abs/1310.1786v1"}, {"title": "The Use of Binary Choice Forests to Model and Estimate Discrete Choices", "summary": "We show the equivalence of discrete choice models and the class of binary\nchoice forests, which are random forests based on binary choice trees. This\nsuggests that standard machine learning techniques based on random forests can\nserve to estimate discrete choice models with an interpretable output. This is\nconfirmed by our data-driven theoretical results which show that random forests\ncan predict the choice probability of any discrete choice model consistently,\nwith its splitting criterion capable of recovering preference rank lists. The\nframework has unique advantages: it can capture behavioral patterns such as\nirrationality or sequential searches; it handles nonstandard formats of\ntraining data that result from aggregation; it can measure product importance\nbased on how frequently a random customer would make decisions depending on the\npresence of the product; it can also incorporate price information and customer\nfeatures. Our numerical results show that using random forests to estimate\ncustomer choices represented by binary choice forests can outperform the best\nparametric models in synthetic and real datasets.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.01109v3", "link": "http://arxiv.org/abs/1908.01109v3"}, {"title": "Feature quantization for parsimonious and interpretable predictive\n  models", "summary": "For regulatory and interpretability reasons, logistic regression is still\nwidely used. To improve prediction accuracy and interpretability, a\npreprocessing step quantizing both continuous and categorical data is usually\nperformed: continuous features are discretized and, if numerous, levels of\ncategorical features are grouped. An even better predictive accuracy can be\nreached by embedding this quantization estimation step directly into the\npredictive estimation step itself. But doing so, the predictive loss has to be\noptimized on a huge set. To overcome this difficulty, we introduce a specific\ntwo-step optimization strategy: first, the optimization problem is relaxed by\napproximating discontinuous quantization functions by smooth functions; second,\nthe resulting relaxed optimization problem is solved via a particular neural\nnetwork. The good performances of this approach, which we call glmdisc, are\nillustrated on simulated and real data from the UCI library and Cr\\'edit\nAgricole Consumer Finance (a major European historic player in the consumer\ncredit market).", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.08920v1", "link": "http://arxiv.org/abs/1903.08920v1"}, {"title": "What are the main drivers of the Bitcoin price? Evidence from wavelet\n  coherence analysis", "summary": "Bitcoin has emerged as a fascinating phenomenon of the financial markets.\nWithout any central authority issuing the currency, it has been associated with\ncontroversy ever since its popularity and public interest reached high levels.\nHere, we contribute to the discussion by examining potential drivers of Bitcoin\nprices ranging from fundamental to speculative and technical sources as well as\na potential influence of the Chinese market. The evolution of the relationships\nis examined in both time and frequency domains utilizing the continuous\nwavelets framework so that we comment on development of the interconnections in\ntime but we can also distinguish between short-term and long-term connections.", "category": ["q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/1406.0268v1", "link": "http://dx.doi.org/10.1371/journal.pone.0123923"}, {"title": "Levels of structural change: An analysis of China's development push\n  1998-2014", "summary": "We investigate structural change in the PR China during a period of\nparticularly rapid growth 1998-2014. For this, we utilize sectoral data from\nthe World Input-Output Database and firm-level data from the Chinese Industrial\nEnterprise Database. Starting with correlation laws known from the literature\n(Fabricant's laws), we investigate which empirical regularities hold at the\nsectoral level and show that many of these correlations cannot be recovered at\nthe firm level. For a more detailed analysis, we propose a multi-level\nframework, which is validated with empirically. For this, we perform a robust\nregression, since various input variables at the firm-level as well as the\nresiduals of exploratory OLS regressions are found to be heavy-tailed. We\nconclude that Fabricant's laws and other regularities are primarily\ncharacteristics of the sectoral level which rely on aspects like\ninfrastructure, technology level, innovation capabilities, and the knowledge\nbase of the relevant labor force. We illustrate our analysis by showing the\ndevelopment of some of the larger sectors in detail and offer some policy\nimplications in the context of development economics, evolutionary economics,\nand industrial organization.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.01882v1", "link": "http://arxiv.org/abs/2005.01882v1"}, {"title": "Predictable Forward Performance Processes: The Binomial Case", "summary": "We introduce a new class of forward performance processes that are endogenous\nand predictable with regards to an underlying market information set and,\nfurthermore, are updated at discrete times. We analyze in detail a binomial\nmodel whose parameters are random and updated dynamically as the market\nevolves. We show that the key step in the construction of the associated\npredictable forward performance process is to solve a single-period inverse\ninvestment problem, namely, to determine, period-by-period and conditionally on\nthe current market information, the end-time utility function from a given\ninitial-time value function. We reduce this inverse problem to solving a\nfunctional equation and establish conditions for the existence and uniqueness\nof its solutions in the class of inverse marginal functions.", "category": ["q-fin.MF", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1611.04494v4", "link": "http://arxiv.org/abs/1611.04494v4"}, {"title": "The Topology of African Exports: emerging patterns on spanning trees", "summary": "This paper is a contribution to interweaving two lines of research that have\nprogressed in separate ways: network analyses of international trade and the\nliterature on African trade and development. Gathering empirical data on\nAfrican countries has important limitations and so does the space occupied by\nAfrican countries in the analyses of trade networks. Here, these limitations\nare dealt with by the definition of two independent bipartite networks: a\ndestination share network and\\ a\\ commodity share network. These networks -\ntogether with their corresponding minimal spanning trees - allow to uncover\nsome ordering emerging from African exports in the broader context of\ninternational trade. The emerging patterns help to understand important\ncharacteristics of African exports and its binding relations to other economic,\ngeographic and organizational concerns as the recent literature on African\ntrade, development and growth has shown.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1604.03522v1", "link": "http://dx.doi.org/10.1016/j.physa.2016.06.044"}, {"title": "Communication impacting financial markets", "summary": "Behavioral finance has become an increasingly important subfield of finance.\nHowever the main parts of behavioral finance, prospect theory included,\nunderstand financial markets through individual investment behavior. Behavioral\nfinance thereby ignores any interaction between participants. We introduce a\nsocio-financial model that studies the impact of communication on the pricing\nin financial markets. Considering the simplest possible case where each market\nparticipant has either a positive (bullish) or negative (bearish) sentiment\nwith respect to the market, we model the evolution of the sentiment in the\npopulation due to communication in subgroups of different sizes. Nonlinear\nfeedback effects between the market performance and changes in sentiments are\ntaking into account by assuming that the market performance is dependent on\nchanges in sentiments (e.g. a large sudden positive change in bullishness would\nlead to more buying). The market performance in turn has an impact on the\nsentiment through the transition probabilities to change an opinion in a group\nof a given size. The idea is that if for example the market has observed a\nrecent downturn, it will be easier for even a bearish minority to convince a\nbullish majority to change opinion compared to the case where the meeting takes\nplace in a bullish upturn of the market. Within the framework of our proposed\nmodel, financial markets stylized facts such as volatility clustering and\nextreme events may be perceived as arising due to abrupt sentiment changes via\nongoing communication of the market participants. The model introduces a new\nvolatility measure which is apt of capturing volatility clustering and from\nmaximum likelihood analysis we are able to apply the model to real data and\ngive additional long term insight into where a market is heading.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1410.2550v1", "link": "http://dx.doi.org/10.1209/0295-5075/108/28007"}, {"title": "On Gerber-Shiu functions and optimal dividend distribution for a\n  L\u00e9vy risk process in the presence of a penalty function", "summary": "This paper concerns an optimal dividend distribution problem for an insurance\ncompany whose risk process evolves as a spectrally negative L\\'{e}vy process\n(in the absence of dividend payments). The management of the company is assumed\nto control timing and size of dividend payments. The objective is to maximize\nthe sum of the expected cumulative discounted dividend payments received until\nthe moment of ruin and a penalty payment at the moment of ruin, which is an\nincreasing function of the size of the shortfall at ruin; in addition, there\nmay be a fixed cost for taking out dividends. A complete solution is presented\nto the corresponding stochastic control problem. It is established that the\nvalue-function is the unique stochastic solution and the pointwise smallest\nstochastic supersolution of the associated HJB equation. Furthermore, a\nnecessary and sufficient condition is identified for optimality of a single\ndividend-band strategy, in terms of a particular Gerber-Shiu function. A number\nof concrete examples are analyzed.", "category": ["math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1110.4965v5", "link": "http://dx.doi.org/10.1214/14-AAP1038"}, {"title": "The impact of proportional transaction costs on systematically generated\n  portfolios", "summary": "The effect of proportional transaction costs on systematically generated\nportfolios is studied empirically. The performance of several portfolios (the\nindex tracking portfolio, the equally-weighted portfolio, the entropy-weighted\nportfolio, and the diversity-weighted portfolio) in the presence of dividends\nand transaction costs is examined under different configurations involving the\ntrading frequency, constituent list size, and renewing frequency. Moreover, a\nmethod to smooth transaction costs is proposed.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1904.08925v1", "link": "http://arxiv.org/abs/1904.08925v1"}, {"title": "A Generalized Fourier Transform Approach to Risk Measures", "summary": "We introduce the formalism of generalized Fourier transforms in the context\nof risk management. We develop a general framework to efficiently compute the\nmost popular risk measures, Value-at-Risk and Expected Shortfall (also known as\nConditional Value-at-Risk). The only ingredient required by our approach is the\nknowledge of the characteristic function describing the financial data in use.\nThis allows to extend risk analysis to those non-Gaussian models defined in the\nFourier space, such as Levy noise driven processes and stochastic volatility\nmodels. We test our analytical results on data sets coming from various\nfinancial indexes, finding that our predictions outperform those provided by\nthe standard Log-Normal dynamics and are in remarkable agreement with those of\nthe benchmark historical approach.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/0909.3978v2", "link": "http://dx.doi.org/10.1088/1742-5468/2010/01/P01005"}, {"title": "Contagious disruptions and complexity traps in economic development", "summary": "Poor economies not only produce less; they typically produce things that\ninvolve fewer inputs and fewer intermediate steps. Yet the supply chains of\npoor countries face more frequent disruptions---delivery failures, faulty\nparts, delays, power outages, theft, government failures---that systematically\nthwart the production process. To understand how these disruptions affect\neconomic development, we model an evolving input--output network in which\ndisruptions spread contagiously among optimizing agents. The key finding is\nthat a poverty trap can emerge: agents adapt to frequent disruptions by\nproducing simpler, less valuable goods, yet disruptions persist. Growing out of\npoverty requires that agents invest in buffers to disruptions. These buffers\nrise and then fall as the economy produces more complex goods, a prediction\nconsistent with global patterns of input inventories. Large jumps in economic\ncomplexity can backfire. This result suggests why \"big push\" policies can fail,\nand it underscores the importance of reliability and of gradual increases in\ntechnological complexity.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1707.05914v1", "link": "http://dx.doi.org/10.1038/s41562-017-0190-6"}, {"title": "A unifying approach to constrained and unconstrained optimal reinsurance", "summary": "In this paper, we study two classes of optimal reinsurance models from\nperspectives of both insurers and reinsurers by minimizing their convex\ncombination where the risk is measured by a distortion risk measure and the\npremium is given by a distortion premium principle. Firstly, we show that how\noptimal reinsurance models for the unconstrained optimization problem and\nconstrained optimization problems can be formulated in a unified way. Secondly,\nwe propose a geometric approach to solve optimal reinsurance problems directly.\nThis paper considers a class of increasing convex ceded loss functions and\nderives the explicit solutions of the optimal reinsurance which can be in forms\nof quota-share, stop-loss, change-loss, the combination of quota-share and\nchange-loss or the combination of change-loss and change-loss with different\nretentions. Finally, we consider two specific cases: Value at Risk (VaR) and\nTail Value at Risk (TVaR).", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1807.06892v1", "link": "http://arxiv.org/abs/1807.06892v1"}, {"title": "Inference for High-Dimensional Sparse Econometric Models", "summary": "This article is about estimation and inference methods for high dimensional\nsparse (HDS) regression models in econometrics. High dimensional sparse models\narise in situations where many regressors (or series terms) are available and\nthe regression function is well-approximated by a parsimonious, yet unknown set\nof regressors. The latter condition makes it possible to estimate the entire\nregression function effectively by searching for approximately the right set of\nregressors. We discuss methods for identifying this set of regressors and\nestimating their coefficients based on $\\ell_1$-penalization and describe key\ntheoretical results. In order to capture realistic practical situations, we\nexpressly allow for imperfect selection of regressors and study the impact of\nthis imperfect selection on estimation and inference results. We focus the main\npart of the article on the use of HDS models and methods in the instrumental\nvariables model and the partially linear model. We present a set of novel\ninference results for these models and illustrate their use with applications\nto returns to schooling and growth regression.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1201.0220v1", "link": "http://arxiv.org/abs/1201.0220v1"}, {"title": "Optimal trading with online parameters revisions", "summary": "The aim of this paper is to explain how parameters adjustments can be\nintegrated in the design or the control of automates of trading. Typically, we\nare interested by the online estimation of the market impacts generated by\nrobots or single orders, and how they/the controller should react in an optimal\nway to the informations generated by the observation of the realized impacts.\nThis can be formulated as an optimal impulse control problem with unknown\nparameters, on which a prior is given. We explain how a mix of the classical\nBayesian updating rule and of optimal control techniques allows one to derive\nthe dynamic programming equation satisfied by the corresponding value function,\nfrom which the optimal policy can be inferred. We provide an example of\nconvergent finite difference scheme and consider typical examples of\napplications.", "category": ["q-fin.CP", "q-fin.TR"], "id": "http://arxiv.org/abs/1604.06342v1", "link": "http://arxiv.org/abs/1604.06342v1"}, {"title": "Is There a Real-Estate Bubble in the US?", "summary": "We analyze the quarterly average sale prices of new houses sold in the USA as\na whole, in the northeast, midwest, south, and west of the USA, in each of the\n50 states and the District of Columbia of the USA, to determine whether they\nhave grown faster-than-exponential which we take as the diagnostic of a bubble.\nWe find that 22 states (mostly Northeast and West) exhibit clear-cut signatures\nof a fast growing bubble. From the analysis of the S&P 500 Home Index, we\nconclude that the turning point of the bubble will probably occur around\nmid-2006.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0506027v1", "link": "http://dx.doi.org/10.1016/j.physa.2005.06.098"}, {"title": "Don't stay local - extrapolation analytics for Dupire's local volatility", "summary": "A robust implementation of a Dupire type local volatility model is an\nimportant issue for every option trading floor. Typically, this (inverse)\nproblem is solved in a two step procedure : (i) a smooth parametrization of the\nimplied volatility surface; (ii) computation of the local volatility based on\nthe resulting call price surface. Point (i), and in particular how to\nextrapolate the implied volatility in extreme strike regimes not seen in the\nmarket, has been the subject of numerous articles, starting with Lee (Math.\nFinance, 2004). In the present paper we give direct analytic insights into the\nasymptotic behavior of local volatility at extreme strikes.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1105.1267v1", "link": "http://arxiv.org/abs/1105.1267v1"}, {"title": "Nonlinear price impact from linear models", "summary": "The impact of trades on asset prices is a crucial aspect of market dynamics\nfor academics, regulators and practitioners alike. Recently, universal and\nhighly nonlinear master curves were observed for price impacts aggregated on\nall intra-day scales [1]. Here we investigate how well these curves, their\nscaling, and the underlying return dynamics are captured by linear \"propagator\"\nmodels. We find that the classification of trades as price-changing versus\nnon-price-changing can explain the price impact nonlinearities and short-term\nreturn dynamics to a very high degree. The explanatory power provided by the\nchange indicator in addition to the order sign history increases with\nincreasing tick size. To obtain these results, several long-standing technical\nissues for model calibration and -testing are addressed. We present new\nspectral estimators for two- and three-point cross-correlations, removing the\nneed for previously used approximations. We also show when calibration is\nunbiased and how to accurately reveal previously overlooked biases. Therefore,\nour results contribute significantly to understanding both recent empirical\nresults and the properties of a popular class of impact models.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1708.02411v1", "link": "http://dx.doi.org/10.1088/1742-5468/aa9335"}, {"title": "New copulas based on general partitions-of-unity (part III) - the\n  continuous case (extended version)", "summary": "In this paper we discuss a natural extension of infinite discrete\npartition-of-unity copulas which were recently introduced in the literature to\ncontinuous partition of copulas with possible applications in risk management\nand other fields. We present a general simple algorithm to generate such\ncopulas on the basis of the empirical copula from high-dimensional data sets.\nIn particular, our constructions also allow for an implementation of positive\ntail dependence which sometimes is a desirable property of copula modelling, in\nparticular for internal models under Solvency II.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1803.00957v4", "link": "http://arxiv.org/abs/1803.00957v4"}, {"title": "Origin of Crashes in 3 US stock markets: Shocks and Bubbles", "summary": "This paper presents an exclusive classification of the largest crashes in Dow\nJones Industrial Average (DJIA), SP500 and NASDAQ in the past century. Crashes\nare objectively defined as the top-rank filtered drawdowns (loss from the last\nlocal maximum to the next local minimum disregarding noise fluctuations), where\nthe size of the filter is determined by the historical volatility of the index.\nIt is shown that {\\it all} crashes can be linked to either an external shock,\n{\\it e.g.}, outbreak of war, {\\it or} a log-periodic power law (LPPL) bubble\nwith an empirically well-defined complex value of the exponent. Conversely,\nwith one sole exception {\\it all} previously identified LPPL bubbles are\nfollowed by a top-rank drawdown. As a consequence, the analysis presented\nsuggest a one-to-one correspondence between market crashes defined as top-rank\nfiltered drawdowns on one hand and surprising news and LPPL bubbles on the\nother. We attribute this correspondence to the Efficient Market Hypothesis\neffective on two quite different time scales depending on whether the market\ninstability the crash represent is internally or externally generated.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0401210v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.02.035"}, {"title": "Stability properties of Haezendonck-Goovaerts premium principles", "summary": "We investigate a variety of stability properties of Haezendonck-Goovaerts\npremium principles on their natural domain, namely Orlicz spaces. We show that\nsuch principles always satisfy the Fatou property. This allows to establish a\ntractable dual representation without imposing any condition on the reference\nOrlicz function. In addition, we show that Haezendonck-Goovaerts principles\nsatisfy the stronger Lebesgue property if and only if the reference Orlicz\nfunction fulfills the so-called $\\Delta_2$ condition.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1909.10735v1", "link": "http://arxiv.org/abs/1909.10735v1"}, {"title": "A multilayer approach for price dynamics in financial markets", "summary": "We introduce a new Self-Organized Criticality (SOC) model for simulating\nprice evolution in an artificial financial market, based on a multilayer\nnetwork of traders. The model also implements, in a quite realistic way with\nrespect to previous studies, the order book dy- namics, by considering two\nassets with variable fundamental prices. Fat tails in the probability\ndistributions of normalized returns are observed, together with other features\nof real financial markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1606.09194v1", "link": "http://arxiv.org/abs/1606.09194v1"}, {"title": "Comparison of various risk measures for an optimal portfolio", "summary": "In this paper, we search for optimal portfolio strategies in the presence of\nvarious risk measure that are common in financial applications. Particularly,\nwe deal with the static optimization problem with respect to Value at Risk,\nExpected Loss and Expected Utility Loss measures. To do so, under the Black-\nScholes model for the financial market, Martingale method is applied to give\nclosed-form solutions for the optimal terminal wealths; then via representation\nproblem the optimal portfolio strategies are achieved. We compare the\nperformances of these measures on the terminal wealths and optimal strategies\nof such constrained investors. Finally, we present some numerical results to\ncompare them in several respects to give light to further studies.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1912.09573v1", "link": "http://arxiv.org/abs/1912.09573v1"}, {"title": "Anomalous Scaling of Stochastic Processes and the Moses Effect", "summary": "The state of a stochastic process evolving over a time $t$ is typically\nassumed to lie on a normal distribution whose width scales like $t^{1/2}$.\nHowever, processes where the probability distribution is not normal and the\nscaling exponent differs from $\\frac{1}{2}$ are known. The search for possible\norigins of such \"anomalous\" scaling and approaches to quantify them are the\nmotivations for the work reported here. In processes with stationary\nincrements, where the stochastic process is time-independent, auto-correlations\nbetween increments and infinite variance of increments can cause anomalous\nscaling. These sources have been referred to as the $\\it{Joseph}$ $\\it{effect}$\nthe $\\it{Noah}$ $\\it{effect}$, respectively. If the increments are\nnon-stationary, then scaling of increments with $t$ can also lead to anomalous\nscaling, a mechanism we refer to as the $\\it{Moses}$ $\\it{effect}$. Scaling\nexponents quantifying the three effects are defined and related to the Hurst\nexponent that characterizes the overall scaling of the stochastic process.\nMethods of time series analysis that enable accurate independent measurement of\neach exponent are presented. Simple stochastic processes are used to illustrate\neach effect. Intraday Financial time series data is analyzed, revealing that\nits anomalous scaling is due only to the Moses effect. In the context of\nfinancial market data, we reiterate that the Joseph exponent, not the Hurst\nexponent, is the appropriate measure to test the efficient market hypothesis.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1704.05818v1", "link": "http://dx.doi.org/10.1103/PhysRevE.95.042141"}, {"title": "A Dynamical Systems Approach to Cryptocurrency Stability", "summary": "Recently, the notion of cryptocurrencies has come to the fore of public\ninterest. These assets that exist only in electronic form, with no underlying\nvalue, offer the owners some protection from tracking or seizure by government\nor creditors. We model these assets from the perspective of asset flow\nequations developed by Caginalp and Balenovich, and investigate their stability\nunder various parameters, as classical finance methodology is inapplicable. By\nutilizing the concept of liquidity price and analyzing stability of the\nresulting system of ordinary differential equations, we obtain conditions under\nwhich the system is linearly stable. We find that trend-based motivations and\nadditional liquidity arising from an uptrend are destabilizing forces, while\nanchoring through value assumed to be fairly recent price history tends to be\nstabilizing.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1805.03143v1", "link": "http://arxiv.org/abs/1805.03143v1"}, {"title": "Market inefficiency identified by both single and multiple currency\n  trends", "summary": "Many studies have shown that there are good reasons to claim very low\npredictability of currency nevertheless, the deviations from true randomness\nexist which have potential predictive and prognostic power [J.James,\nQuantitative finance 3 (2003) C75-C77]. We analyze the local trends which are\nof the main focus of the technical analysis. In this article we introduced\nvarious statistical quantities examining role of single temporal discretized\ntrend or multitude of trends corresponding to different time delays. Our\nspecific analysis based on Euro-dollar currency pair data at the one minute\nfrequency suggests the importance of cumulative nonrandom effect of trends on\nthe forecasting performance.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1110.2612v1", "link": "http://dx.doi.org/10.1016/j.physa.2012.06.038"}, {"title": "Power, Levy, Exponential and Gaussian Regimes in Autocatalytic Financial\n  Systems", "summary": "We study by theoretical analysis and by direct numerical simulation the\ndynamics of a wide class of asynchronous stochastic systems composed of many\nautocatalytic degrees of freedom. We describe the generic emergence of\ntruncated power laws in the size distribution of their individual elements. The\nexponents $\\alpha$ of these power laws are time independent and depend only on\nthe way the elements with very small values are treated. These truncated power\nlaws determine the collective time evolution of the system. In particular the\nglobal stochastic fluctuations of the system differ from the normal Gaussian\nnoise according to the time and size scales at which these fluctuations are\nconsidered. We describe the ranges in which these fluctuations are\nparameterized respectively by: the Levy regime $\\alpha < 2$, the power law\ndecay with large exponent ($\\alpha > 2$), and the exponential decay. Finally we\nrelate these results to the large exponent power laws found in the actual\nbehavior of the stock markets and to the exponential cut-off detected in\ncertain recent measurement.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0008026v1", "link": "http://dx.doi.org/10.1007/PL00011114"}, {"title": "Randomisation and recursion methods for mixed-exponential Levy models,\n  with financial applications", "summary": "We develop a new Monte Carlo variance reduction method to estimate the\nexpectation of two commonly encountered path-dependent functionals:\nfirst-passage times and occupation times of sets. The method is based on a\nrecursive approximation of the first-passage time probability and expected\noccupation time of sets of a Levy bridge process that relies in part on a\nrandomisation of the time parameter. We establish this recursion for general\nLevy processes and derive its explicit form for mixed-exponential\njump-diffusions, a dense subclass (in the sense of weak approximation) of Levy\nprocesses, which includes Brownian motion with drift, Kou's double-exponential\nmodel and hyper-exponential jump-diffusion models. We present a highly accurate\nnumerical realisation and derive error estimates. By way of illustration the\nmethod is applied to the valuation of range accruals and barrier options under\nexponential Levy models and Bates-type stochastic volatility models with\nexponential jumps. Compared with standard Monte Carlo methods, we find that the\nmethod is significantly more efficient.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1410.7316v1", "link": "http://arxiv.org/abs/1410.7316v1"}, {"title": "Optimal Portfolio Liquidation in Target Zone Models and Catalytic\n  Superprocesses", "summary": "We study optimal buying and selling strategies in target zone models. In\nthese models the price is modeled by a diffusion process which is reflected at\none or more barriers. Such models arise for example when a currency exchange\nrate is kept above a certain threshold due to central bank intervention. We\nconsider the optimal portfolio liquidation problem for an investor for whom\nprices are optimal at the barrier and who creates temporary price impact. This\nproblem will be formulated as the minimization of a cost-risk functional over\nstrategies that only trade when the price process is located at the barrier. We\nsolve the corresponding singular stochastic control problem by means of a\nscaling limit of critical branching particle systems, which is known as a\ncatalytic superprocess. In this setting the catalyst is a set of points which\nis given by the barriers of the price process. For the cases in which the\nunaffected price process is a reflected arithmetic or geometric Brownian motion\nwith drift, we moreover give a detailed financial justification of our cost\nfunctional by means of an approximation with discrete-time models.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1504.06031v2", "link": "http://arxiv.org/abs/1504.06031v2"}, {"title": "Asymmetric matrices in an analysis of financial correlations", "summary": "Financial markets are highly correlated systems that reveal both the\ninter-market dependencies and the correlations among their different\ncomponents. Standard analyzing techniques include correlation coefficients for\npairs of signals and correlation matrices for rich multivariate data. In the\nlatter case one constructs a real symmetric matrix with real non-negative\neigenvalues describing the correlation structure of the data. However, if one\nperforms a correlation-function-like analysis of multivariate data, when a\nstress is put on investigation of delayed dependencies among different types of\nsignals, one can calculate an asymmetric correlation matrix with complex\neigenspectrum. From the Random Matrix Theory point of view this kind of\nmatrices is closely related to Ginibre Orthogonal Ensemble (GinOE). We present\nan example of practical application of such matrices in correlation analyses of\nempirical data. By introducing the time lag, we are able to identify temporal\nstructure of the inter-market correlations. Our results show that the American\nand German stock markets evolve almost simultaneously without a significant\ntime lag so that it is hard to find imprints of information transfer between\nthese markets. There is only an extremely subtle indication that the German\nmarket advances the American one by a few seconds.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0605115v1", "link": "http://arxiv.org/abs/physics/0605115v1"}, {"title": "Publish and Perish: Creative Destruction and Macroeconomic Theory", "summary": "A number of macroeconomic theories, very popular in the 1980s, seem to have\ncompletely disappeared and been replaced by the dynamic stochastic general\nequilibrium (DSGE) approach. We will argue that this replacement is due to a\ntacit agreement on a number of assumptions, previously seen as mutually\nexclusive, and not due to a settlement by 'nature'. As opposed to econometrics\nand microeconomics and despite massive progress in the access to data and the\nuse of statistical software, macroeconomic theory appears not to be a\ncumulative science so far. Observational equivalence of different models and\nthe problem of identification of parameters of the models persist as will be\nhighlighted by examining two examples: one in growth theory and a second in\ntesting inflation persistence.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.10680v1", "link": "http://dx.doi.org/10.19272/201806102004"}, {"title": "Initial Crypto-asset Offerings (ICOs), tokenization and corporate\n  governance", "summary": "This paper discusses the potential impacts of the so-called `initial coin\nofferings', and of several developments based on distributed ledger technology\n(`DLT'), on corporate governance. While many academic papers focus mainly on\nthe legal qualification of DLT and crypto-assets, and most notably in relation\nto the potential definition of the latter as securities/financial instruments,\nthe authors analyze some of the use cases based on DLT technology and their\npotential for significant changes of the corporate governance analyses. This\narticle studies the consequences due to the emergence of new kinds of firm\nstakeholders, i.e. the crypto-assets holders, on the governance of small and\nmedium-sized enterprises (`SMEs') as well as of publicly traded companies.\nSince early 2016, a new way of raising funds has rapidly emerged as a major\nissue for FinTech founders and financial regulators. Frequently referred to as\ninitial coin offerings, Initial Token Offerings (`ITO'), Token Generation\nEvents (`TGE') or simply `token sales', we use in our paper the terminology\nInitial Crypto-asset Offerings (`ICO'), as it describes more effectively than\n`initial coin offerings' the vast diversity of assets that could be created and\nwhich goes far beyond the payment instrument issue.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1905.03340v1", "link": "http://arxiv.org/abs/1905.03340v1"}, {"title": "Ordinal Classification Method for the Evaluation Of Thai Non-life\n  Insurance Companies", "summary": "This paper proposes a use of an ordinal classifier to evaluate the financial\nsolidity of non-life insurance companies as strong, moderate, weak, and\ninsolvency. This study constructed an efficient classification model that can\nbe used by regulators to evaluate the financial solidity and to determine the\npriority of further examination as an early warning system. The proposed model\nis beneficial to policy-makers to create guidelines for the solvency\nregulations and roles of the government in protecting the public against\ninsolvency.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1203.6424v2", "link": "http://arxiv.org/abs/1203.6424v2"}, {"title": "Trading-profit attribution for the size factor", "summary": "An algorithm was recently introduced by INTECH for the purposes of estimating\nthe trading-profit contribution of systematic rebalancing to the relative\nreturn of rules-based investment strategies. We apply this methodology to\nanalyze the size factor through the use of equal-weighted portfolios. These\nstrategies combine a natural exposure to the size factor with a simple\nunderstanding within the framework of Stochastic Portfolio Theory, furnishing a\nnatural test subject for the attribution algorithm.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1601.07626v1", "link": "http://arxiv.org/abs/1601.07626v1"}, {"title": "The mitigating role of regulation on the concentric patterns of\n  broadband diffusion. The case of Finland", "summary": "This article analyzes the role of Finnish regulation in achieving the\nbroadband penetration goals defined by the National Regulatory Authority. It is\nwell known that in the absence of regulatory mitigation the population density\nhas a positive effect on broadband diffusion. Hence, we measure the effect of\nthe population density on the determinants of broadband diffusion throughout\nthe postal codes of Finland via Geographically Weighted Regression. We suggest\nthat the main determinants of broadband diffusion and the population density\nfollow a spatial pattern that is either concentric with a weak/medium/strong\nstrength or non-concentric convex/concave. Based on 10 patterns, we argue that\nthe Finnish spectrum policy encouraged Mobile Network Operators to satisfy\nambitious Universal Service Obligations without the need for a Universal\nService Fund. Spectrum auctions facilitated infrastructure-based competition\nvia equitable spectrum allocation and coverage obligation delivery via low-fee\nlicenses. However, state subsidies for fiber deployment did not attract\ninvestment from nationwide operators due to mobile preference. These subsidies\nencouraged demand-driven investment, leading to the emergence of fiber consumer\ncooperatives. To explain this emergence, we show that when population density\ndecreases, the level of mobile service quality decreases and community\ncommitment increases. Hence, we recommend regulators implementing market-driven\nstrategies for 5G to stimulate local investment. For example, by allocating the\n3.5 GHz and higher bands partly through local light licensing.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.03002v2", "link": "http://dx.doi.org/10.1016/j.tele.2019.04.008"}, {"title": "Continuous-time trading and the emergence of probability", "summary": "This paper establishes a non-stochastic analogue of the celebrated result by\nDubins and Schwarz about reduction of continuous martingales to Brownian motion\nvia time change. We consider an idealized financial security with continuous\nprice path, without making any stochastic assumptions. It is shown that typical\nprice paths possess quadratic variation, where \"typical\" is understood in the\nfollowing game-theoretic sense: there exists a trading strategy that earns\ninfinite capital without risking more than one monetary unit if the process of\nquadratic variation does not exist. Replacing time by the quadratic variation\nprocess, we show that the price path becomes Brownian motion. This is\nessentially the same conclusion as in the Dubins-Schwarz result, except that\nthe probabilities (constituting the Wiener measure) emerge instead of being\npostulated. We also give an elegant statement, inspired by Peter McCullagh's\nunpublished work, of this result in terms of game-theoretic probability theory.", "category": ["math.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/0904.4364v4", "link": "http://dx.doi.org/10.1007/s00780-012-0180-5"}, {"title": "Least-Squares Prices of Games", "summary": "What are the prices of random variables? In this paper, we define the\nleast-squares prices of coin-flipping games, which are proved to be minimal,\npositive linear, and arbitrage-free. These prices depend both on a set of games\nthat are available for investing simultaneously and on a risk-free interest\nrate. In addition, we show a case where the mean-variance portfolio theory is\ninappropriate.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/math/0703079v3", "link": "http://arxiv.org/abs/math/0703079v3"}, {"title": "Robust Multi-product Pricing under General Extreme Value Models", "summary": "We study robust versions of pricing problems where customers choose products\naccording to a general extreme value (GEV) choice model, and the choice\nparameters are not given exactly but lie in an uncertainty set. We show that,\nwhen the robust problem is unconstrained and the price sensitivity parameters\nare homogeneous, the robust optimal prices have a constant markup over products\nand we provide formulas that allow to compute this constant markup by binary\nsearch. We also show that, in the case that the price sensitivity parameters\nare only homogeneous in each subset of the products and the uncertainty set is\nrectangular, the robust problem can be converted into a deterministic pricing\nproblem and the robust optimal prices have a constant markup in each subset,\nand we also provide explicit formulas to compute them. For constrained pricing\nproblems, we propose a formulation where, instead of requiring that the\nexpected sale constraints be satisfied, we add a penalty cost to the objective\nfunction for violated constraints. We then show that the robust pricing problem\nwith over-expected-sale penalties can be reformulated as a convex optimization\nprogram where the purchase probabilities are the decision variables. We provide\nnumerical results for the logit and nested logit model to illustrate the\nadvantages of our approach. Our results generally hold for any arbitrary GEV\nmodel, including the multinomial logit, nested or cross-nested logit.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.09552v1", "link": "http://arxiv.org/abs/1912.09552v1"}, {"title": "Geometric Arbitrage Theory and Market Dynamics", "summary": "We have embedded the classical theory of stochastic finance into a\ndifferential geometric framework called Geometric Arbitrage Theory and show\nthat it is possible to:\n  --Write arbitrage as curvature of a principal fibre bundle.\n  --Parameterize arbitrage strategies by its holonomy.\n  --Give the Fundamental Theorem of Asset Pricing a differential homotopic\ncharacterization.\n  --Characterize Geometric Arbitrage Theory by five principles and show they\nthey are consistent with the classical theory of stochastic finance.\n  --Derive for a closed market the equilibrium solution for market portfolio\nand dynamics in the cases where:\n  -->Arbitrage is allowed but minimized.\n  -->Arbitrage is not allowed.\n  --Prove that the no-free-lunch-with-vanishing-risk condition implies the zero\ncurvature condition. The converse is in general not true and additionally\nrequires the Novikov condition for the instantaneous Sharpe Ratio Dynamics to\nbe satisfied.", "category": ["q-fin.CP", "math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/0910.1671v5", "link": "http://arxiv.org/abs/0910.1671v5"}, {"title": "Multiple time scales and the empirical models for stochastic volatility", "summary": "The most common stochastic volatility models such as the Ornstein-Uhlenbeck\n(OU), the Heston, the exponential OU (ExpOU) and Hull-White models define\nvolatility as a Markovian process. In this work we check of the applicability\nof the Markovian approximation at separate times scales and will try to answer\nthe question which of the stochastic volatility models indicated above is the\nmost realistic. To this end we consider the volatility at both short (a few\ndays) and long (a few months)time scales as a Markovian process and estimate\nfor it the coefficients of the Kramers-Moyal expansion using the data for\nDow-Jones Index. It has been found that the empirical data allow to take only\nthe first two coefficients of expansion to be non zero that define form of the\nvolatility stochastic differential equation of Ito. It proved to be that for\nthe long time scale the empirical data support the ExpOU model. At the short\ntime scale the empirical model coincides with ExpOU model for the small\nvolatility quantities only.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0611048v1", "link": "http://dx.doi.org/10.1016/j.physa.2006.12.015"}, {"title": "Econophysics of adaptive power markets: When a market does not dampen\n  fluctuations but amplifies them", "summary": "The average economic agent is often used to model the dynamics of simple\nmarkets, based on the assumption that the dynamics of many agents can be\naveraged over in time and space. A popular idea that is based on this seemingly\nintuitive notion is to dampen electric power fluctuations from fluctuating\nsources (as e.g. wind or solar) via a market mechanism, namely by variable\npower prices that adapt demand to supply. The standard model of an average\neconomic agent predicts that fluctuations are reduced by such an adaptive\npricing mechanism.\n  However, the underlying assumption that the actions of all agents average out\non the time axis is not always true in a market of many agents. We numerically\nstudy an econophysics agent model of an adaptive power market that does not\nassume averaging a priori. We find that when agents are exposed to source noise\nvia correlated price fluctuations (as adaptive pricing schemes suggest), the\nmarket may amplify those fluctuations. In particular, small price changes may\ntranslate to large load fluctuations through catastrophic consumer\nsynchronization. As a result, an adaptive power market may cause the opposite\neffect than intended: Power fluctuations are not dampened but amplified\ninstead.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1303.2110v1", "link": "http://dx.doi.org/10.1103/PhysRevE.92.012815"}, {"title": "The double role of GDP in shaping the structure of the International\n  Trade Network", "summary": "The International Trade Network (ITN) is the network formed by trade\nrelationships between world countries. The complex structure of the ITN impacts\nimportant economic processes such as globalization, competitiveness, and the\npropagation of instabilities. Modeling the structure of the ITN in terms of\nsimple macroeconomic quantities is therefore of paramount importance. While\ntraditional macroeconomics has mainly used the Gravity Model to characterize\nthe magnitude of trade volumes, modern network theory has predominantly focused\non modeling the topology of the ITN. Combining these two complementary\napproaches is still an open problem. Here we review these approaches and\nemphasize the double role played by GDP in empirically determining both the\nexistence and the volume of trade linkages. Moreover, we discuss a unified\nmodel that exploits these patterns and uses only the GDP as the relevant\nmacroeconomic factor for reproducing both the topology and the link weights of\nthe ITN.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1512.02454v2", "link": "http://dx.doi.org/10.1504/IJCEE.2017.10003511"}, {"title": "Exit times in non-Markovian drifting continuous-time random walk\n  processes", "summary": "By appealing to renewal theory we determine the equations that the mean exit\ntime of a continuous-time random walk with drift satisfies both when the\npresent coincides with a jump instant or when it does not. Particular attention\nis paid to the corrections ensuing from the non-Markovian nature of the\nprocess. We show that when drift and jumps have the same sign the relevant\nintegral equations can be solved in closed form. The case when holding times\nhave the classical Erlang distribution is considered in detail.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1002.0571v2", "link": "http://dx.doi.org/10.1103/PhysRevE.82.021102"}, {"title": "How to Increase Global Wealth Inequality for Fun and Profit", "summary": "We point out a simple equities trading strategy that allows a sufficiently\nlarge, market-neutral, quantitative hedge fund to achieve outsized returns\nwhile simultaneously contributing significantly to increasing global wealth\ninequality. Overnight and intraday return distributions in major equity indices\nin the United States, Canada, France, Germany, and Japan suggest a few such\nfirms have been implementing this strategy successfully for more than\ntwenty-five years.", "category": ["econ.GN", "q-fin.EC", "q-fin.TR"], "id": "http://arxiv.org/abs/1811.04994v1", "link": "http://arxiv.org/abs/1811.04994v1"}, {"title": "Price-Setting of Market Makers: A Filtering Problem with an Endogenous\n  Filtration", "summary": "We study the price-setting problem of market makers under risk neutrality and\nperfect competition in continuous time. Thereby we follow the classic\nGlosten-Milgrom model that defines bid and ask prices as expectations of a true\nvalue of the asset given the market makers' partial information that includes\nthe customers trading decisions. The true value is modeled as a Markov process\nthat can be observed by the customers with some noise at Poisson times.\n  We analyze the price-setting problem in a mathematically rigorous way by\nsolving a filtering problem with an endogenous filtration that depends on the\nbid and ask price process quoted by the market maker. Under some conditions we\nshow existence and uniqueness of the price processes.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1210.4000v1", "link": "http://arxiv.org/abs/1210.4000v1"}, {"title": "About the decomposition of pricing formulas under stochastic volatility\n  models", "summary": "We obtain a decomposition of the call option price for a very general\nstochastic volatility diffusion model extending the decomposition obtained by\nE. Al\\`os in [2] for the Heston model. We realize that a new term arises when\nthe stock price does not follow an exponential model. The techniques used are\nnon anticipative. In particular, we see also that equivalent results can be\nobtained using Functional It\\^o Calculus. Using the same generalizing ideas we\nalso extend to non exponential models the alternative call option price\ndecompostion formula obtained in [1] and [3] written in terms of the Malliavin\nderivative of the volatility process. Finally, we give a general expression for\nthe derivative of the implied volatility under both, the anticipative and the\nnon anticipative case.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1503.08119v1", "link": "http://arxiv.org/abs/1503.08119v1"}, {"title": "Regulatory Medicine Against Financial Market Instability: What Helps And\n  What Hurts?", "summary": "Do we know if a short selling ban or a Tobin Tax result in more stable asset\nprices? Or do they in fact make things worse? Just like medicine regulatory\nmeasures in financial markets aim at improving an already complex system. And\njust like medicine these interventions can cause side effects which are even\nharder to assess when taking the interplay with other measures into account. In\nthis paper an agent based stock market model is built that tries to find\nanswers to the questions above. In a stepwise procedure regulatory measures are\nintroduced and their implications on market liquidity and stability examined.\nParticularly, the effects of (i) a ban of short selling (ii) a mandatory risk\nlimit, i.e. a Value-at-Risk limit, (iii) an introduction of a Tobin Tax, i.e.\ntransaction tax on trading, and (iv) any arbitrary combination of the measures\nare observed and discussed. The model is set up to incorporate non-linear\nfeedback effects of leverage and liquidity constraints leading to fire sales\nand escape dynamics. In its unregulated version the model outcome is capable of\nreproducing stylised facts of asset returns like fat tails and clustered\nvolatility. Introducing regulatory measures shows that only a mandatory risk\nlimit is beneficial from every perspective, while a short selling ban - though\nreducing volatility - increases tail risk. The contrary holds true for a Tobin\nTax: it reduces the occurrence of crashes but increases volatility.\nFurthermore, the interplay of measures is not negligible: measures block each\nother and a well chosen combination can mitigate unforeseen side effects.\nConcerning the Tobin Tax the findings indicate that an overdose can do severe\nharm.", "category": ["q-fin.TR", "q-fin.RM"], "id": "http://arxiv.org/abs/1011.6284v2", "link": "http://arxiv.org/abs/1011.6284v2"}, {"title": "Risk measures and Margining control", "summary": "This document constitutes the final report of the contractual activity\nbetween Directa SIM and Dipartimento di Automatica e Informatica, Politecnico\ndi Torino, on the research topic titled \"quantificazione del rischio di un\nportafoglio di strumenti finanziari per trading online su device fissi e\nmobili.\"", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1608.08283v1", "link": "http://arxiv.org/abs/1608.08283v1"}, {"title": "Dual representations for systemic risk measures based on acceptance sets", "summary": "We establish dual representations for systemic risk measures based on\nacceptance sets in a general setting. We deal with systemic risk measures of\nboth \"first allocate, then aggregate\" and \"first aggregate, then allocate\"\ntype. In both cases, we provide a detailed analysis of the corresponding\nsystemic acceptance sets and their support functions. The same approach\ndelivers a simple and self-contained proof of the dual representation of\nutility-based risk measures for univariate positions.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1906.10933v2", "link": "http://arxiv.org/abs/1906.10933v2"}, {"title": "Optimum thresholding using mean and conditional mean square error", "summary": "We consider a univariate semimartingale model for (the logarithm of) an asset\nprice, containing jumps having possibly infinite activity (IA). The\nnonparametric threshold estimator of the integrated variance IV proposed in\nMancini 2009 is constructed using observations on a discrete time grid, and\nprecisely it sums up the squared increments of the process when they are below\na threshold, a deterministic function of the observation step and possibly of\nthe coefficients of X. All the threshold functions satisfying given conditions\nallow asymptotically consistent estimates of IV, however the finite sample\nproperties of the truncated realized variation can depend on the specific\nchoice of the threshold. We aim here at optimally selecting the threshold by\nminimizing either the estimation mean square error (MSE) or the conditional\nmean square error (cMSE). The last criterion allows to reach a threshold which\nis optimal not in mean but for the specific volatility (and jumps paths) at\nhand. A parsimonious characterization of the optimum is established, which\nturns out to be asymptotically proportional to the L\\'evy's modulus of\ncontinuity of the underlying Brownian motion. Moreover, minimizing the cMSE\nenables us to propose a novel implementation scheme for approximating the\noptimal threshold. Monte Carlo simulations illustrate the superior performance\nof the proposed method.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1708.04339v1", "link": "http://arxiv.org/abs/1708.04339v1"}, {"title": "Fractional calculus and continuous-time finance", "summary": "In this paper we present a rather general phenomenological theory of\ntick-by-tick dynamics in financial markets. Many well-known aspects, such as\nthe L\\'evy scaling form, follow as particular cases of the theory. The theory\nfully takes into account the non-Markovian and non-local character of financial\ntime series. Predictions on the long-time behaviour of the waiting-time\nprobability density are presented. Finally, a general scaling form is given,\nbased on the solution of the fractional diffusion equation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0001120v1", "link": "http://dx.doi.org/10.1016/S0378-4371(00)00255-7"}, {"title": "Dissecting Ethereum Blockchain Analytics: What We Learn from Topology\n  and Geometry of Ethereum Graph", "summary": "Blockchain technology and, in particular, blockchain-based cryptocurrencies\noffer us information that has never been seen before in the financial world. In\ncontrast to fiat currencies, all transactions of crypto-currencies and\ncrypto-tokens are permanently recorded on distributed ledgers and are publicly\navailable. As a result, this allows us to construct a transaction graph and to\nassess not only its organization but to glean relationships between transaction\ngraph properties and crypto price dynamics. The ultimate goal of this paper is\nto facilitate our understanding on horizons and limitations of what can be\nlearned on crypto-tokens from local topology and geometry of the Ethereum\ntransaction network whose even global network properties remain scarcely\nexplored. By introducing novel tools based on topological data analysis and\nfunctional data depth into Blockchain Data Analytics, we show that Ethereum\nnetwork (one of the most popular blockchains for creating new crypto-tokens)\ncan provide critical insights on price strikes of crypto-tokens that are\notherwise largely inaccessible with conventional data sources and traditional\nanalytic methods.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1912.10105v1", "link": "http://arxiv.org/abs/1912.10105v1"}, {"title": "Overspend? Late? Failure? What the Data Say About IT Project Risk in the\n  Public Sector", "summary": "Implementing large-scale information and communication technology (IT)\nprojects carries large risks and easily might disrupt operations, waste\ntaxpayers' money, and create negative publicity. Because of the high risks it\nis important that government leaders manage the attendant risks. We analysed a\nsample of 1,355 public sector IT projects. The sample included large-scale\nprojects, on average the actual expenditure was $130 million and the average\nduration was 35 months. Our findings showed that the typical project had no\ncost overruns and took on average 24% longer than initially expected. However,\ncomparing the risk distribution with the normative model of a thin-tailed\ndistribution, projects' actual costs should fall within -30% and +25% of the\nbudget in nearly 99 out of 100 projects. The data showed, however, that a\nstaggering 18% of all projects are outliers with cost overruns >25%. Tests\nshowed that the risk of outliers is even higher for standard software (24%) as\nwell as in certain project types, e.g., data management (41%), office\nmanagement (23%), eGovernment (21%) and management information systems (20%).\nAnalysis showed also that projects duration adds risk: every additional year of\nproject duration increases the average cost risk by 4.2 percentage points.\nLastly, we suggest four solutions that public sector organization can take: (1)\nbenchmark your organization to know where you are, (2) de-bias your IT project\ndecision-making, (3) reduce the complexities of your IT projects, and (4)\ndevelop Masterbuilders to learn from the best in the field.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1304.4525v3", "link": "http://arxiv.org/abs/1304.4525v3"}, {"title": "Model independent hedging strategies for variance swaps", "summary": "A variance swap is a derivative with a path-dependent payoff which allows\ninvestors to take positions on the future variability of an asset. In the\nidealised setting of a continuously monitored variance swap written on an asset\nwith continuous paths it is well known that the variance swap payoff can be\nreplicated exactly using a portfolio of puts and calls and a dynamic position\nin the asset. This fact forms the basis of the VIX contract.\n  But what if we are in the more realistic setting where the contract is based\non discrete monitoring, and the underlying asset may have jumps? We show that\nit is possible to derive model-independent, no-arbitrage bounds on the price of\nthe variance swap, and corresponding sub- and super-replicating strategies.\nFurther, we characterise the optimal bounds. The form of the hedges depends\ncrucially on the kernel used to define the variance swap.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1104.4010v2", "link": "http://arxiv.org/abs/1104.4010v2"}, {"title": "Current log-periodic view on future world market development", "summary": "Applicability of the concept of financial log-periodicity is discussed and\nencouragingly verified for various phases of the world stock markets\ndevelopment in the period 2000-2010. In particular, a speculative forecasting\nscenario designed in the end of 2004, that properly predicted the world stock\nmarket increases in 2007, is updated by setting some more precise constraints\non the time of duration of the present long-term equity market bullish phase. A\ntermination of this phase is evaluated to occur in around November 2009. In\nparticular, on the way towards this dead-line, a Spring-Summer 2008 increase is\nexpected. On the precious metals market a forthcoming critical time signal is\ndetected at the turn of March/April 2008 which marks a tendency for at least a\nserious correction to begin.\n  In the present extended version some predictions for the future oil price are\nincorporated. In particular a serious correction on this market is expected to\nstart in the coming days.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0802.4043v2", "link": "http://arxiv.org/abs/0802.4043v2"}, {"title": "Serial correlation and heterogeneous volatility in financial markets:\n  beyond the LeBaron effect", "summary": "We study the relation between serial correlation of financial returns and\nvolatility at intraday level for the S&P500 stock index. At daily and weekly\nlevel, serial correlation and volatility are known to be negatively correlated\n(LeBaron effect). While confirming that the LeBaron effect holds also at\nintraday level, we go beyond it and, complementing the efficient market\nhyphotesis (for returns) with the heterogenous market hyphotesis (for\nvolatility), we test the impact of unexpected volatility, defined as the part\nof volatility which cannot be forecasted, on the presence of serial\ncorrelations in the time series. We show that unexpected volatility is instead\npositively correlated with intraday serial correlation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0810.4912v1", "link": "http://arxiv.org/abs/0810.4912v1"}, {"title": "Stochastic volatility and leverage effect", "summary": "We prove that a wide class of correlated stochastic volatility models exactly\nmeasure an empirical fact in which past returns are anticorrelated with future\nvolatilities: the so-called ``leverage effect''. This quantitative measure\nallows us to fully estimate all parameters involved and it will entail a deeper\nstudy on correlated stochastic volatility models with practical applications on\noption pricing and risk management.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0202203v1", "link": "http://dx.doi.org/10.1103/PhysRevE.67.037102"}, {"title": "Bayesian Dividend Optimization and Finite Time Ruin Probabilities", "summary": "We consider the valuation problem of an (insurance) company under partial\ninformation. Therefore we use the concept of maximizing discounted future\ndividend payments. The firm value process is described by a diffusion model\nwith constant and observable volatility and constant but unknown drift\nparameter. For transforming the problem to a problem with complete information,\nwe derive a suitable filter. The optimal value function is characterized as the\nunique viscosity solution of the associated Hamilton-Jacobi-Bellman equation.\nWe state a numerical procedure for approximating both the optimal dividend\nstrategy and the corresponding value function. Furthermore, threshold\nstrategies are discussed in some detail. Finally, we calculate the probability\nof ruin in the uncontrolled and controlled situation.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1602.04660v1", "link": "http://dx.doi.org/10.1080/15326349.2014.900390"}, {"title": "Zipf's law in city size from a resource utilization model", "summary": "We study a resource utilization scenario characterized by intrinsic fitness.\nTo describe the growth and organization of different cities, we consider a\nmodel for resource utilization where many restaurants compete, as in a game, to\nattract customers using an iterative learning process. Results for the case of\nrestaurants with uniform fitness are reported. When fitness is uniformly\ndistributed, it gives rise to a Zipf law for the number of customers. We\nperform an exact calculation for the utilization fraction for the case when\nchoices are made independent of fitness. A variant of the model is also\nintroduced where the fitness can be treated as an ability to stay in the\nbusiness. When a restaurant loses customers, its fitness is replaced by a\nrandom fitness. The steady state fitness distribution is characterized by a\npower law, while the distribution of the number of customers still follows the\nZipf law, implying the robustness of the model. Our model serves as a paradigm\nfor the emergence of Zipf law in city size distribution.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1403.1822v3", "link": "http://dx.doi.org/10.1103/PhysRevE.90.042815"}, {"title": "Publish and Perish: Creative Destruction and Macroeconomic Theory", "summary": "A number of macroeconomic theories, very popular in the 1980s, seem to have\ncompletely disappeared and been replaced by the dynamic stochastic general\nequilibrium (DSGE) approach. We will argue that this replacement is due to a\ntacit agreement on a number of assumptions, previously seen as mutually\nexclusive, and not due to a settlement by 'nature'. As opposed to econometrics\nand microeconomics and despite massive progress in the access to data and the\nuse of statistical software, macroeconomic theory appears not to be a\ncumulative science so far. Observational equivalence of different models and\nthe problem of identification of parameters of the models persist as will be\nhighlighted by examining two examples: one in growth theory and a second in\ntesting inflation persistence.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.10680v1", "link": "http://dx.doi.org/10.19272/201806102004"}, {"title": "Agent-based Model Construction In Financial Economic System", "summary": "The paper gives picture of enrichment to economic and financial system\nanalysis using agent-based models as a form of advanced study for financial\neconomic data post-statistical-data analysis and micro-simulation analysis.\nTheoretical exploration is carried out by using comparisons of some usual\nfinancial economy system models frequently and popularly used in econophysics\nand computational finance. Primitive model, which consists of agent\nmicrosimulation with fundamentalist strategy, chartist, and noise, was\nestablished with an expectation of adjusting micro-simulation analysis upon\nstock market in Indonesia. The result of simulation showing how financial\neconomy data resulted analysis using statistical tools such as data\ndistribution and central limit theorem, and several other macro-financial\nanalysis tools previously shown (Situngkir & Surya, 2003b). This paper is ended\nwith several further possible advancements from the model built.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/nlin/0403041v1", "link": "http://arxiv.org/abs/nlin/0403041v1"}, {"title": "Mean Reversion Pays, but Costs", "summary": "A mean-reverting financial instrument is optimally traded by buying it when\nit is sufficiently below the estimated `mean level' and selling it when it is\nabove. In the presence of linear transaction costs, a large amount of value is\npaid away crossing bid-offers unless one devises a `buffer' through which the\nprice must move before a trade is done. In this paper, Richard Martin and\nTorsten Sch\\\"oneborn derive the optimal strategy and conclude that for low\ncosts the buffer width is proportional to the cube root of the transaction\ncost, determining the proportionality constant explicitly.", "category": ["q-fin.TR", "math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1103.4934v1", "link": "http://arxiv.org/abs/1103.4934v1"}, {"title": "The continuous time random walk formalism in financial markets", "summary": "We adapt continuous time random walk (CTRW) formalism to describe asset price\nevolution and discuss some of the problems that can be treated using this\napproach. We basically focus on two aspects: (i) the derivation of the price\ndistribution from high-frequency data, and (ii) the inverse problem, obtaining\ninformation on the market microstructure as reflected by high-frequency data\nknowing only the daily volatility. We apply the formalism to financial data to\nshow that the CTRW offers alternative tools to deal with several complex issues\nof financial markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0611138v1", "link": "http://dx.doi.org/10.1016/j.jebo.2004.07.015"}, {"title": "Uncovering networks amongst stocks returns by studying nonlinear\n  interactions in high frequency data of the Indian Stock Market using mutual\n  information", "summary": "In this paper, we explore the detection of clusters of stocks that are in\nsynergy in the Indian Stock Market and understand their behaviour in different\ncircumstances. We have based our study on high frequency data for the year\n2014. This was a year when general elections were held in India, keeping this\nin mind our data set was divided into 3 subsets, pre-election period: Jan-Feb\n2014; election period: Mar-May 2014 and :post-election period: Jun-Dec 2014. On\nanalysing the spectrum of the correlation matrix, quite a few deviations were\nobserved from RMT indicating a correlation across all the stocks. We then used\nmutual information to capture the non-linearity of the data and compared our\nresults with widely used correlation technique using minimum spanning tree\nmethod. With a larger value of power law exponent {\\alpha}, corresponding to\ndistribution of degrees in a network, the nonlinear method of mutual\ninformation succeeds in establishing effective network in comparison to the\ncorrelation method. Of the two prominent clusters detected by our analysis, one\ncorresponds to the financial sector and another to the energy sector. The\nfinancial sector emerged as an isolated, standalone cluster, which remain\nunaffected even during the election periods.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1903.03407v1", "link": "http://arxiv.org/abs/1903.03407v1"}, {"title": "Testing for time-varying properties under misspecified conditional mean\n  and variance", "summary": "This study examines statistical performance of tests for time-varying\nproperties under misspecified conditional mean and variance. When we test for\ntime-varying properties of the conditional mean in the case in which data have\nno time-varying mean but have time-varying variance, asymptotic tests have size\ndistortions. This is improved by the use of a bootstrap method. Similarly, when\nwe test for time-varying properties of the conditional variance in the case in\nwhich data have time-varying mean but no time-varying variance, asymptotic\ntests have large size distortions. This is not improved even by the use of\nbootstrap methods. We show that tests for time-varying properties of the\nconditional mean by the bootstrap are robust regardless of the time-varying\nvariance model, whereas tests for time-varying properties of the conditional\nvariance do not perform well in the presence of misspecified time-varying mean.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.12107v2", "link": "http://arxiv.org/abs/1907.12107v2"}, {"title": "Stock Selection as a Problem in Phylogenetics -- Evidence from the ASX", "summary": "We report the results of fifteen sets of portfolio selection simulations\nusing stocks in the ASX200 index for the period May 2000 to December 2013. We\ninvestigated five portfolio selection methods, randomly and from within\nindustrial groups, and three based on neighbor-Net phylogenetic networks. We\nreport that using random, industrial groups, or neighbor-Net phylogenetic\nnetworks alone rarely produced statistically significant reduction in risk,\nthough in four out of the five cases in which it did so, the portfolios\nselected using the phylogenetic networks had the lowest risk. However, we\nreport that when using the neighbor-Net phylogenetic networks in combination\nwith industry group selection that substantial reductions in portfolio return\nspread were achieved.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1603.02354v1", "link": "http://arxiv.org/abs/1603.02354v1"}, {"title": "Forecasting Implied Volatility Smile Surface via Deep Learning and\n  Attention Mechanism", "summary": "The implied volatility smile surface is the basis of option pricing, and the\ndynamic evolution of the option volatility smile surface is difficult to\npredict. In this paper, attention mechanism is introduced into LSTM, and a\nvolatility surface prediction method combining deep learning and attention\nmechanism is pioneeringly established. LSTM's forgetting gate makes it have\nstrong generalization ability, and its feedback structure enables it to\ncharacterize the long memory of financial volatility. The application of\nattention mechanism in LSTM networks can significantly enhance the ability of\nLSTM networks to select input features. The experimental results show that the\ntwo strategies constructed using the predicted implied volatility surfaces have\nhigher returns and Sharpe ratios than that the volatility surfaces are not\npredicted. This paper confirms that the use of AI to predict the implied\nvolatility surface has theoretical and economic value. The research method\nprovides a new reference for option pricing and strategy.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1912.11059v1", "link": "http://arxiv.org/abs/1912.11059v1"}, {"title": "Growth-Optimal Portfolio Selection under CVaR Constraints", "summary": "Online portfolio selection research has so far focused mainly on minimizing\nregret defined in terms of wealth growth. Practical financial decision making,\nhowever, is deeply concerned with both wealth and risk. We consider online\nlearning of portfolios of stocks whose prices are governed by arbitrary\n(unknown) stationary and ergodic processes, where the goal is to maximize\nwealth while keeping the conditional value at risk (CVaR) below a desired\nthreshold. We characterize the asymptomatically optimal risk-adjusted\nperformance and present an investment strategy whose portfolios are guaranteed\nto achieve the asymptotic optimal solution while fulfilling the desired risk\nconstraint. We also numerically demonstrate and validate the viability of our\nmethod on standard datasets.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1705.09800v1", "link": "http://arxiv.org/abs/1705.09800v1"}, {"title": "Comparisons for backward stochastic differential equations on Markov\n  chains and related no-arbitrage conditions", "summary": "Most previous contributions to BSDEs, and the related theories of nonlinear\nexpectation and dynamic risk measures, have been in the framework of continuous\ntime diffusions or jump diffusions. Using solutions of BSDEs on spaces related\nto finite state, continuous time Markov chains, we develop a theory of\nnonlinear expectations in the spirit of [Dynamically consistent nonlinear\nevaluations and expectations (2005) Shandong Univ.]. We prove basic properties\nof these expectations and show their applications to dynamic risk measures on\nsuch spaces. In particular, we prove comparison theorems for scalar and vector\nvalued solutions to BSDEs, and discuss arbitrage and risk measures in the\nscalar case.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/0810.0055v2", "link": "http://dx.doi.org/10.1214/09-AAP619"}, {"title": "A nested factor model for non-linear dependences in stock returns", "summary": "The aim of our work is to propose a natural framework to account for all the\nempirically known properties of the multivariate distribution of stock returns.\nWe define and study a \"nested factor model\", where the linear factors part is\nstandard, but where the log-volatility of the linear factors and of the\nresiduals are themselves endowed with a factor structure and residuals. We\npropose a calibration procedure to estimate these log-vol factors and the\nresiduals. We find that whereas the number of relevant linear factors is\nrelatively large (10 or more), only two or three log-vol factors emerge in our\nanalysis of the data. In fact, a minimal model where only one log-vol factor is\nconsidered is already very satisfactory, as it accurately reproduces the\nproperties of bivariate copulas, in particular the dependence of the\nmedial-point on the linear correlation coefficient, as reported in\nChicheportiche and Bouchaud (2012). We have tested the ability of the model to\npredict Out-of-Sample the risk of non-linear portfolios, and found that it\nperforms significantly better than other schemes.", "category": ["q-fin.RM", "q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1309.3102v1", "link": "http://dx.doi.org/10.1080/14697688.2014.994668"}, {"title": "Value matters: Predictability of Stock Index Returns", "summary": "We present a simple dynamical model of stock index returns which is grounded\non the ability of the Cyclically Adjusted Price Earning (CAPE) valuation ratio\ndevised by Robert Shiller to predict long-horizon performances of the market.\nMore precisely, we discuss a discrete time dynamics in which the return growth\ndepends on three components: i) a momentum component, naturally justified in\nterms of agents' belief that expected returns are higher in bullish markets\nthan in bearish ones, ii) a fundamental component proportional to the\nlogarithmic CAPE at time zero. The initial value of the ratio determines the\nreference growth level, from which the actual stock price may deviate as an\neffect of random external disturbances, and iii) a driving component which\nensures the diffusive behaviour of stock prices. Under these assumptions, we\nprove that for a sufficiently large horizon the expected rate of return and the\nexpected gross return are linear in the initial logarithmic CAPE, and their\nvariance goes to zero with a rate of convergence consistent with the diffusive\nbehaviour. Eventually this means that the momentum component may generate\nbubbles and crashes in the short and medium run, nevertheless the valuation\nratio remains a good reference point of future long-run returns.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1204.5055v2", "link": "http://arxiv.org/abs/1204.5055v2"}, {"title": "Predicting crypto-currencies using sparse non-Gaussian state space\n  models", "summary": "In this paper we forecast daily returns of crypto-currencies using a wide\nvariety of different econometric models. To capture salient features commonly\nobserved in financial time series like rapid changes in the conditional\nvariance, non-normality of the measurement errors and sharply increasing\ntrends, we develop a time-varying parameter VAR with t-distributed measurement\nerrors and stochastic volatility. To control for overparameterization, we rely\non the Bayesian literature on shrinkage priors that enables us to shrink\ncoefficients associated with irrelevant predictors and/or perform model\nspecification in a flexible manner. Using around one year of daily data we\nperform a real-time forecasting exercise and investigate whether any of the\nproposed models is able to outperform the naive random walk benchmark. To\nassess the economic relevance of the forecasting gains produced by the proposed\nmodels we moreover run a simple trading exercise.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1801.06373v2", "link": "http://arxiv.org/abs/1801.06373v2"}, {"title": "Subspace Clustering for Panel Data with Interactive Effects", "summary": "In this paper, a statistical model for panel data with unobservable grouped\nfactor structures which are correlated with the regressors and the group\nmembership can be unknown. The factor loadings are assumed to be in different\nsubspaces and the subspace clustering for factor loadings are considered. A\nmethod called least squares subspace clustering estimate (LSSC) is proposed to\nestimate the model parameters by minimizing the least-square criterion and to\nperform the subspace clustering simultaneously. The consistency of the proposed\nsubspace clustering is proved and the asymptotic properties of the estimation\nprocedure are studied under certain conditions. A Monte Carlo simulation study\nis used to illustrate the advantages of the proposed method. Further\nconsiderations for the situations that the number of subspaces for factors, the\ndimension of factors and the dimension of subspaces are unknown are also\ndiscussed. For illustrative purposes, the proposed method is applied to study\nthe linkage between income and democracy across countries while subspace\npatterns of unobserved factors and factor loadings are allowed.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.09928v1", "link": "http://arxiv.org/abs/1909.09928v1"}, {"title": "Consistency of extended Nelson-Siegel curve families with the Ho-Lee and\n  Hull and White short rate models", "summary": "Nelson and Siegel curves are widely used to fit the observed term structure\nof interest rates in a particular date. By the other hand, several interest\nrate models have been developed such their initial forward rate curve can be\nadjusted to any observed data, as the Ho-Lee and the Hull and White one factor\nmodels. In this work we study the evolution of the forward curve process for\neach of this models assuming that the initial curve is of Nelson-Siegel type.\nWe conclude that the forward curve process produces curves belonging to a\nparametric family of curves that can be seen as extended Nelson and Siegel\ncurves.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1707.02496v1", "link": "http://arxiv.org/abs/1707.02496v1"}, {"title": "Diagnosis and Prediction of Market Rebounds in Financial Markets", "summary": "We introduce the concept of \"negative bubbles\" as the mirror image of\nstandard financial bubbles, in which positive feedback mechanisms may lead to\ntransient accelerating price falls. To model these negative bubbles, we adapt\nthe Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles with a\nhazard rate describing the collective buying pressure of noise traders. The\nprice fall occurring during a transient negative bubble can be interpreted as\nan effective random downpayment that rational agents accept to pay in the hope\nof profiting from the expected occurrence of a possible rally. We validate the\nmodel by showing that it has significant predictive power in identifying the\ntimes of major market rebounds. This result is obtained by using a general\npattern recognition method which combines the information obtained at multiple\ntimes from a dynamical calibration of the JLS model. Error diagrams, Bayesian\ninference and trading strategies suggest that one can extract genuine\ninformation and obtain real skill from the calibration of negative bubbles with\nthe JLS model. We conclude that negative bubbles are in general predictably\nassociated with large rebounds or rallies, which are the mirror images of the\ncrashes terminating standard bubbles.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1003.5926v2", "link": "http://arxiv.org/abs/1003.5926v2"}, {"title": "Semiparametrically efficient estimation of the average linear regression\n  function", "summary": "Let Y be an outcome of interest, X a vector of treatment measures, and W a\nvector of pre-treatment control variables. Here X may include (combinations of)\ncontinuous, discrete, and/or non-mutually exclusive \"treatments\". Consider the\nlinear regression of Y onto X in a subpopulation homogenous in W = w (formally\na conditional linear predictor). Let b0(w) be the coefficient vector on X in\nthis regression. We introduce a semiparametrically efficient estimate of the\naverage beta0 = E[b0(W)]. When X is binary-valued (multi-valued) our procedure\nrecovers the (a vector of) average treatment effect(s). When X is\ncontinuously-valued, or consists of multiple non-exclusive treatments, our\nestimand coincides with the average partial effect (APE) of X on Y when the\nunderlying potential response function is linear in X, but otherwise\nheterogenous across agents. When the potential response function takes a\ngeneral nonlinear/heterogenous form, and X is continuously-valued, our\nprocedure recovers a weighted average of the gradient of this response across\nindividuals and values of X. We provide a simple, and semiparametrically\nefficient, method of covariate adjustment for settings with complicated\ntreatment regimes. Our method generalizes familiar methods of covariate\nadjustment used for program evaluation as well as methods of semiparametric\nregression (e.g., the partially linear regression model).", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.12511v1", "link": "http://arxiv.org/abs/1810.12511v1"}, {"title": "Volatility, Persistence, and Survival in Financial Markets", "summary": "We study the temporal fluctuations in time-dependent stock prices (both\nindividual and composite) as a stochastic phenomenon using general techniques\nand methods of nonequilibrium statistical mechanics. In particular, we analyze\nstock price fluctuations as a non-Markovian stochastic process using the\nfirst-passage statistical concepts of persistence and survival. We report the\nresults of empirical measurements of the normalized $q$-order correlation\nfunctions $f_q(t)$, survival probability $S(t)$, and persistence probability\n$P(t)$ for several stock market dynamical sets. We analyze both\nminute-to-minute and higher frequency stock market recordings (i.e., with the\nsampling time $\\delta t$ of the order of days). We find that the fluctuating\nstock price is multifractal and the choice of $\\delta t$ has no effect on the\nqualitative multifractal behavior displayed by the $1/q$-dependence of the\ngeneralized Hurst exponent $H_q$ associated with the power-law evolution of the\ncorrelation function $f_q(t)\\sim t^{H_q}$. The probability $S(t)$ of the stock\nprice remaining above the average up to time $t$ is very sensitive to the total\nmeasurement time $t_m$ and the sampling time. The probability $P(t)$ of the\nstock not returning to the initial value within an interval $t$ has a universal\npower-law behavior, $P(t)\\sim t^{-\\theta}$, with a persistence exponent\n$\\theta$ close to 0.5 that agrees with the prediction $\\theta=1-H_2$. The\nempirical financial stocks also present an interesting feature found in\nturbulent fluids, the extended self-similarity.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0507020v2", "link": "http://dx.doi.org/10.1103/PhysRevE.72.051106"}, {"title": "Diversification Return, Portfolio Rebalancing, and the Commodity Return\n  Puzzle", "summary": "Diversification return is an incremental return earned by a rebalanced\nportfolio of assets. The diversification return of a rebalanced portfolio is\noften incorrectly ascribed to a reduction in variance. We argue that the\nunderlying source of the diversification return is the rebalancing, which\nforces the investor to sell assets that have appreciated in relative value and\nbuy assets that have declined in relative value, as measured by their weights\nin the portfolio. In contrast, the incremental return of a buy-and-hold\nportfolio is driven by the fact that the assets that perform the best become a\ngreater fraction of the portfolio. We use these results to resolve two puzzles\nassociated with the Gorton and Rouwenhorst index of commodity futures, and\nthereby obtain a clear understanding of the source of the return of that index.\nDiversification return can be a significant source of return for any rebalanced\nportfolio of volatile assets.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1109.1256v1", "link": "http://arxiv.org/abs/1109.1256v1"}, {"title": "Reward-risk momentum strategies using classical tempered stable\n  distribution", "summary": "We implement momentum strategies using reward-risk measures as ranking\ncriteria based on classical tempered stable distribution. Performances and risk\ncharacteristics for the alternative portfolios are obtained in various asset\nclasses and markets. The reward-risk momentum strategies with lower volatility\nlevels outperform the traditional momentum strategy regardless of asset class\nand market. Additionally, the alternative portfolios are not only less riskier\nin risk measures such as VaR, CVaR and maximum drawdown but also characterized\nby thinner downside tails. Similar patterns in performance and risk profile are\nalso found at the level of each ranking basket in the reward-risk portfolios.\nHigher factor-neutral returns achieved by the reward-risk momentum strategies\nare statistically significant and large portions of the performances are not\nexplained by the Carhart four-factor model.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1403.6093v4", "link": "http://arxiv.org/abs/1403.6093v4"}, {"title": "Dynamic and granular loss reserving with copulae", "summary": "An intensive research sprang up for stochastic methods in insurance during\nthe past years. To meet all future claims rising from policies, it is requisite\nto quantify the outstanding loss liabilities. Loss reserving methods based on\naggregated data from run-off triangles are predominantly used to calculate the\nclaims reserves. Conventional reserving techniques have some disadvantages:\nloss of information from the policy and the claim's development due to the\naggregation, zero or negative cells in the triangle; usually small number of\nobservations in the triangle; only few observations for recent accident years;\nand sensitivity to the most recent paid claims.\n  To overcome these dilemmas, granular loss reserving methods for individual\nclaim-by-claim data will be derived. Reserves' estimation is a crucial part of\nthe risk valuation process, which is now a front burner in economics. Since\nthere is a growing demand for prediction of total reserves for different types\nof claims or even multiple lines of business, a time-varying copula framework\nfor granular reserving will be established.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.01792v1", "link": "http://arxiv.org/abs/1801.01792v1"}, {"title": "Increasing market efficiency: Evolution of cross-correlations of stock\n  returns", "summary": "We analyse the temporal changes in the cross correlations of returns on the\nNew York Stock Exchange. We show that lead-lag relationships between daily\nreturns of stocks vanished in less than twenty years. We have found that even\nfor high frequency data the asymmetry of time dependent cross-correlation\nfunctions has a decreasing tendency, the position of their peaks are shifted\ntowards the origin while these peaks become sharper and higher, resulting in a\ndiminution of the Epps effect. All these findings indicate that the market\nbecomes increasingly efficient.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0506071v2", "link": "http://dx.doi.org/10.1016/j.physa.2005.06.058"}, {"title": "Mean Field Games and Systemic Risk", "summary": "We propose a simple model of inter-bank borrowing and lending where the\nevolution of the log-monetary reserves of $N$ banks is described by a system of\ndiffusion processes coupled through their drifts in such a way that stability\nof the system depends on the rate of inter-bank borrowing and lending. Systemic\nrisk is characterized by a large number of banks reaching a default threshold\nby a given time horizon.\n  Our model incorporates a game feature where each bank controls its rate of\nborrowing/lending to a central bank. The optimization reflects the desire of\neach bank to borrow from the central bank when its monetary reserve falls below\na critical level or lend if it rises above this critical level which is chosen\nhere as the average monetary reserve. Borrowing from or lending to the central\nbank is also subject to a quadratic cost at a rate which can be fixed by the\nregulator. We solve explicitly for Nash equilibria with finitely many players,\nand we show that in this model the central bank acts as a clearing house,\nadding liquidity to the system without affecting its systemic risk. We also\nstudy the corresponding Mean Field Game in the limit of large number of banks\nin the presence of a common noise.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1308.2172v1", "link": "http://arxiv.org/abs/1308.2172v1"}, {"title": "Periodic attractor in the discrete time best-response dynamics of the\n  Rock-Paper-Scissors game", "summary": "The Rock-Paper-Scissors (RPS) game is a classic non-cooperative game widely\nstudied in terms of its theoretical analysis as well as in its applications,\nranging from sociology and biology to economics. Many experimental results of\nthe RPS game indicate that this game is better modelled by the discretized\nbest-response dynamics rather than continuous time dynamics. In this work we\nshow that the attractor of the discrete time best-response dynamics of the RPS\ngame is finite and periodic. Moreover we also describe the bifurcations of the\nattractor and determine the exact number, period and location of the periodic\nstrategies.", "category": [], "id": "http://arxiv.org/abs/1912.06831v1", "link": "http://arxiv.org/abs/1912.06831v1"}, {"title": "Mortgages and Refinancing", "summary": "In general, homeowners refinance in response to a decrease in interest rates,\nas their borrowing costs are lowered. However, it is worth investigating the\neffects of refinancing after taking the underlying costs into consideration.\nHere we develop a synthetic mortgage calculator that sufficiently accounts for\nsuch costs and the implications on new monthly payments. To confirm the\naccuracy of the calculator, we simulate the effects of refinancing over 15 and\n30 year periods. We then model the effects of refinancing as risk to the issuer\nof the mortgage, as there is negative duration associated with shifts in the\ninterest rate. Furthermore, we investigate the effects on the swap market as\nwell as the treasury bond market. We model stochastic interest rates using the\nVasicek model.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1605.04941v1", "link": "http://arxiv.org/abs/1605.04941v1"}, {"title": "A Comparison of First-Difference and Forward Orthogonal Deviations GMM", "summary": "This paper provides a necessary and sufficient instruments condition assuring\ntwo-step generalized method of moments (GMM) based on the forward orthogonal\ndeviations transformation is numerically equivalent to two-step GMM based on\nthe first-difference transformation. The condition also tells us when system\nGMM, based on differencing, can be computed using forward orthogonal\ndeviations. Additionally, it tells us when forward orthogonal deviations and\ndifferencing do not lead to the same GMM estimator. When estimators based on\nthese two transformations differ, Monte Carlo simulations indicate that\nestimators based on forward orthogonal deviations have better finite sample\nproperties than estimators based on differencing.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.12880v1", "link": "http://arxiv.org/abs/1907.12880v1"}, {"title": "Principal Component Analysis: A Generalized Gini Approach", "summary": "A principal component analysis based on the generalized Gini correlation\nindex is proposed (Gini PCA). The Gini PCA generalizes the standard PCA based\non the variance. It is shown, in the Gaussian case, that the standard PCA is\nequivalent to the Gini PCA. It is also proven that the dimensionality reduction\nbased on the generalized Gini correlation matrix, that relies on city-block\ndistances, is robust to outliers. Monte Carlo simulations and an application on\ncars data (with outliers) show the robustness of the Gini PCA and provide\ndifferent interpretations of the results compared with the variance PCA.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.10133v1", "link": "http://arxiv.org/abs/1910.10133v1"}, {"title": "A Time Series Analysis-Based Stock Price Prediction Using Machine\n  Learning and Deep Learning Models", "summary": "Prediction of future movement of stock prices has always been a challenging\ntask for the researchers. While the advocates of the efficient market\nhypothesis (EMH) believe that it is impossible to design any predictive\nframework that can accurately predict the movement of stock prices, there are\nseminal work in the literature that have clearly demonstrated that the\nseemingly random movement patterns in the time series of a stock price can be\npredicted with a high level of accuracy. Design of such predictive models\nrequires choice of appropriate variables, right transformation methods of the\nvariables, and tuning of the parameters of the models. In this work, we present\na very robust and accurate framework of stock price prediction that consists of\nan agglomeration of statistical, machine learning and deep learning models. We\nuse the daily stock price data, collected at five minutes interval of time, of\na very well known company that is listed in the National Stock Exchange (NSE)\nof India. The granular data is aggregated into three slots in a day, and the\naggregated data is used for building and training the forecasting models. We\ncontend that the agglomerative approach of model building that uses a\ncombination of statistical, machine learning, and deep learning approaches, can\nvery effectively learn from the volatile and random movement patterns in a\nstock price data. We build eight classification and eight regression models\nbased on statistical and machine learning approaches. In addition to these\nmodels, a deep learning regression model using a long-and-short-term memory\n(LSTM) network is also built. Extensive results have been presented on the\nperformance of these models, and the results are critically analyzed.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2004.11697v1", "link": "http://dx.doi.org/10.13140/RG.2.2.14022.22085/2"}, {"title": "Measuring the \"non-stopping timeness\" of ends of previsible sets", "summary": "In this paper, we propose several \"measurements\" of the \"non-stopping\ntimeness\" of ends g of previsible sets, such that g avoids stopping times, in\nan ambiant filtration. We then study several explicit examples, involving last\npassage times of some remarkable martingales.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/0810.1059v1", "link": "http://arxiv.org/abs/0810.1059v1"}, {"title": "Solving Nonlinear and High-Dimensional Partial Differential Equations\n  via Deep Learning", "summary": "In this work we apply the Deep Galerkin Method (DGM) described in Sirignano\nand Spiliopoulos (2018) to solve a number of partial differential equations\nthat arise in quantitative finance applications including option pricing,\noptimal execution, mean field games, etc. The main idea behind DGM is to\nrepresent the unknown function of interest using a deep neural network. A key\nfeature of this approach is the fact that, unlike other commonly used numerical\napproaches such as finite difference methods, it is mesh-free. As such, it does\nnot suffer (as much as other numerical methods) from the curse of\ndimensionality associated with highdimensional PDEs and PDE systems. The main\ngoals of this paper are to elucidate the features, capabilities and limitations\nof DGM by analyzing aspects of its implementation for a number of different\nPDEs and PDE systems. Additionally, we present: (1) a brief overview of PDEs in\nquantitative finance along with numerical methods for solving them; (2) a brief\noverview of deep learning and, in particular, the notion of neural networks;\n(3) a discussion of the theoretical foundations of DGM with a focus on the\njustification of why this method is expected to perform well.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1811.08782v1", "link": "http://arxiv.org/abs/1811.08782v1"}, {"title": "Interest Rates and Inflation", "summary": "This article is an extension of the work of one of us (Coopersmith, 2011) in\nderiving the relationship between certain interest rates and the inflation rate\nof a two component economic system. We use the well-known Fisher relation\nbetween the difference of the nominal interest rate and its inflation adjusted\nvalue to eliminate the inflation rate and obtain a delay differential equation.\nWe provide computer simulated solutions for this equation over regimes of\ninterest. This paper could be of interest to three audiences: those in\nEconomics who are interested in interest and inflation; those in Mathematics\nwho are interested in examining a detailed analysis of a delay differential\nequation, which includes a summary of existing results, simulations, and an\nexact solution; and those in Physics who are interested in non-traditional\napplications of traditional methods of modeling.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1603.08311v1", "link": "http://arxiv.org/abs/1603.08311v1"}, {"title": "The Urban Wage Premium: Sorting, agglomeration economies or statistical\n  artifact? The problem of sampling from lognormals", "summary": "Economic explanations for the urban wage premium (UWP) fall in two\ncategories: sorting of more productive individuals into larger cities, or\nagglomeration externalities. We present a third hitherto neglected mechanism: a\nstatistical artifact arising from sampling lognormally distributed wages. We\nshow how this artificial UWP emerges, systematically and predictably, when the\nvariance of log-wages is larger than twice the log-size of workers sampled in\nthe smallest city. We present an analytic derivation of this connection between\nlognormals and increasing returns to scale using extreme value theory. We\nvalidate our results analyzing simulated data and real data on more than six\nmillion real Colombian wages across more than five hundred municipalities. We\nfind that when taking random samples of $1\\%$, or less, of all Colombian\nworkers the estimated real and artificial UWP are both $7\\%$, and become\nstatistically indistinguishable, yet both significantly larger than zero. This\nhighlights the importance of working with large samples of workers. We propose\na method to tell whether an estimate of UWP is real or an artifact.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.09424v2", "link": "http://arxiv.org/abs/1807.09424v2"}, {"title": "Harry Potter and the Goblin Bank of Gringotts", "summary": "Gringotts Wizarding Bank is well known as the only financial institution in\nall of the Wizarding UK as documented in the works recounting the heroics of\nHarry Potter. The concentration of power and wealth in this single bank needs\nto be weighed against the financial stability of the entire Wizarding economy.\nThis study will consider the impact to financial risk of breaking up Gringotts\nWizarding Bank into five component pieces, along the lines of the\nGlass-Steagall Act in the United States. The emphasis of this work is to\ncalibrate and simulate a model of the banking and financial systems within\nWizarding UK under varying stress test scenarios simulating rumors of Lord\nVoldemort's return or the release of magical creatures into an unsuspecting\nmuggle populace. We conclude by comparing the economic fallout from financial\ncrises under the two systems: (i) Gringotts Wizarding Bank as a monopoly and\n(ii) the split-up financial system. We do this comparison on the level of\nminimal system-wide capital injections that would be needed to prevent the\nfinancial crisis from surpassing the damage caused by Lord Voldemort.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1703.10469v1", "link": "http://arxiv.org/abs/1703.10469v1"}, {"title": "A unified approach to pricing and risk management of equity and credit\n  risk", "summary": "We propose a unified framework for equity and credit risk modeling, where the\ndefault time is a doubly stochastic random time with intensity driven by an\nunderlying affine factor process. This approach allows for flexible\ninteractions between the defaultable stock price, its stochastic volatility and\nthe default intensity, while maintaining full analytical tractability. We\ncharacterise all risk-neutral measures which preserve the affine structure of\nthe model and show that risk management as well as pricing problems can be\ndealt with efficiently by shifting to suitable survival measures. As an\nexample, we consider a jump-to-default extension of the Heston stochastic\nvolatility model.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1212.5395v2", "link": "http://dx.doi.org/10.1016/j.cam.2013.04.047"}, {"title": "Adaptive Pricing in Insurance: Generalized Linear Models and Gaussian\n  Process Regression Approaches", "summary": "We study the application of dynamic pricing to insurance. We view this as an\nonline revenue management problem where the insurance company looks to set\nprices to optimize the long-run revenue from selling a new insurance product.\nWe develop two pricing models: an adaptive Generalized Linear Model (GLM) and\nan adaptive Gaussian Process (GP) regression model. Both balance between\nexploration, where we choose prices in order to learn the distribution of\ndemands & claims for the insurance product, and exploitation, where we\nmyopically choose the best price from the information gathered so far. The\nperformance of the pricing policies is measured in terms of regret: the\nexpected revenue loss caused by not using the optimal price. As is commonplace\nin insurance, we model demand and claims by GLMs. In our adaptive GLM design,\nwe use the maximum quasi-likelihood estimation (MQLE) to estimate the unknown\nparameters. We show that, if prices are chosen with suitably decreasing\nvariability, the MQLE parameters eventually exist and converge to the correct\nvalues, which in turn implies that the sequence of chosen prices will also\nconverge to the optimal price. In the adaptive GP regression model, we sample\ndemand and claims from Gaussian Processes and then choose selling prices by the\nupper confidence bound rule. We also analyze these GLM and GP pricing\nalgorithms with delayed claims. Although similar results exist in other\ndomains, this is among the first works to consider dynamic pricing problems in\nthe field of insurance. We also believe this is the first work to consider\nGaussian Process regression in the context of insurance pricing. These initial\nfindings suggest that online machine learning algorithms could be a fruitful\narea of future investigation and application in insurance.", "category": ["econ.EM", "q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1907.05381v1", "link": "http://arxiv.org/abs/1907.05381v1"}, {"title": "Mean-variance hedging based on an incomplete market with external risk\n  factors of non-Gaussian OU processes", "summary": "In this paper, we prove the global risk optimality of the hedging strategy of\ncontingent claim, which is explicitly (or called semi-explicitly) constructed\nfor an incomplete financial market with external risk factors of non-Gaussian\nOrnstein-Uhlenbeck (NGOU) processes. Analytical and numerical examples are both\npresented to illustrate the effectiveness of our optimal strategy. Our study\nestablishes the connection between our financial system and existing general\nsemimartingale based discussions by justifying required conditions. More\nprecisely, there are three steps involved. First, we firmly prove the\nno-arbitrage condition to be true for our financial market, which is used as an\nassumption in existing discussions. In doing so, we explicitly construct the\nsquare-integrable density process of the variance-optimal martingale measure\n(VOMM). Second, we derive a backward stochastic differential equation (BSDE)\nwith jumps for the mean-value process of a given contingent claim. The unique\nexistence of adapted strong solution to the BSDE is proved under suitable\nterminal conditions including both European call and put options as special\ncases. Third, by combining the solution of the BSDE and the VOMM, we reach the\njustification of the global risk optimality for our hedging strategy.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1410.0991v2", "link": "http://arxiv.org/abs/1410.0991v2"}, {"title": "General Properties of Solutions to Inhomogeneous Black-Scholes Equations\n  with Discontinuous Maturity Payoffs and Application", "summary": "We provide representations of solutions to terminal value problems of\ninhomogeneous Black-Scholes equations and studied such general properties as\nmin-max estimates, gradient estimates, monotonicity and convexity of the\nsolutions with respect to the stock price variable, which are important for\nfinancial security pricing. In particular, we focus on finding representation\nof the gradient (with respect to the stock price variable) of solutions to the\nterminal value problems with discontinuous terminal payoffs or inhomogeneous\nterms. Such terminal value problems are often encountered in pricing problems\nof compound-like options such as Bermudan options or defaultable bonds with\ndiscrete default barrier, default intensity and endogenous default recovery.\nOur results are applied in pricing defaultable discrete coupon bonds.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1309.6505v2", "link": "http://dx.doi.org/10.1016/j.jde.2015.08.036"}, {"title": "Modelling Network Interference with Multi-valued Treatments: the Causal\n  Effect of Immigration Policy on Crime Rates", "summary": "Policy evaluation studies, which aim to assess the effect of an intervention,\nimply some statistical challenges: real-world scenarios provide treatments\nwhich have not been assigned randomly and the analysis might be further\ncomplicated by the presence of interference between units. Researchers have\nstarted to develop novel methods that allow to manage spillover mechanisms in\nobservational studies, under binary treatments. But many policy evaluation\nstudies require complex treatments, such as multi-valued treatments. For\ninstance, in political sciences, evaluating the impact of policies implemented\nby administrative entities often implies a multi-valued approach, as the\ngeneral political stance towards a specific issue varies over many dimensions.\nIn this work, we extend the statistical framework about causal inference under\nnetwork interference in observational studies, allowing for a multi-valued\nindividual treatment and an interference structure shaped by a weighted\nnetwork. Under multi-valued treatment, each unit is exposed to all levels of\nthe treatment, due to the influence of his neighbors, according to the network\nweights. The estimation strategy is based on a joint multiple generalized\npropensity score and allows to estimate direct effects, controlling for both\nindividual and network covariates. We follow the proposed methodology to\nanalyze the impact of national immigration policy on crime rates. We define a\nmulti-valued characterization of political attitudes towards migrants and we\nassume that the extent to which each country can be influenced by another is\nmodeled by an appropriate indicator, that we call Interference Compound Index\n(ICI). Results suggest that implementing highly restrictive immigration\npolicies leads to an increase of crime rates and the magnitude of estimated\neffects is stronger if we take into account multi-valued interference.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.10525v1", "link": "http://arxiv.org/abs/2003.10525v1"}, {"title": "Extreme Measures of Agricultural Financial Risk", "summary": "Risk is an inherent feature of agricultural production and marketing and\naccurate measurement of it helps inform more efficient use of resources. This\npaper examines three tail quantile-based risk measures applied to the\nestimation of extreme agricultural financial risk for corn and soybean\nproduction in the US: Value at Risk (VaR), Expected Shortfall (ES) and Spectral\nRisk Measures (SRMs). We use Extreme Value Theory (EVT) to model the tail\nreturns and present results for these three different risk measures using\nagricultural futures market data. We compare the estimated risk measures in\nterms of their size and precision, and find that they are all considerably\nhigher than normal estimates; they are also quite uncertain, and become more\nuncertain as the risks involved become more extreme.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1103.5962v1", "link": "http://arxiv.org/abs/1103.5962v1"}, {"title": "Chain ladder method: Bayesian bootstrap versus classical bootstrap", "summary": "The intention of this paper is to estimate a Bayesian distribution-free chain\nladder (DFCL) model using approximate Bayesian computation (ABC) methodology.\nWe demonstrate how to estimate quantities of interest in claims reserving and\ncompare the estimates to those obtained from classical and credibility\napproaches. In this context, a novel numerical procedure utilising Markov chain\nMonte Carlo (MCMC), ABC and a Bayesian bootstrap procedure was developed in a\ntruly distribution-free setting. The ABC methodology arises because we work in\na distribution-free setting in which we make no parametric assumptions, meaning\nwe can not evaluate the likelihood point-wise or in this case simulate directly\nfrom the likelihood model. The use of a bootstrap procedure allows us to\ngenerate samples from the intractable likelihood without the requirement of\ndistributional assumptions, this is crucial to the ABC framework. The developed\nmethodology is used to obtain the empirical distribution of the DFCL model\nparameters and the predictive distribution of the outstanding loss liabilities\nconditional on the observed claims. We then estimate predictive Bayesian\ncapital estimates, the Value at Risk (VaR) and the mean square error of\nprediction (MSEP). The latter is compared with the classical bootstrap and\ncredibility methods.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1004.2548v1", "link": "http://dx.doi.org/10.1016/j.insmatheco.2010.03.007"}, {"title": "Optimal Data Collection for Randomized Control Trials", "summary": "In a randomized control trial, the precision of an average treatment effect\nestimator can be improved either by collecting data on additional individuals,\nor by collecting additional covariates that predict the outcome variable. We\npropose the use of pre-experimental data such as a census, or a household\nsurvey, to inform the choice of both the sample size and the covariates to be\ncollected. Our procedure seeks to minimize the resulting average treatment\neffect estimator's mean squared error, subject to the researcher's budget\nconstraint. We rely on a modification of an orthogonal greedy algorithm that is\nconceptually simple and easy to implement in the presence of a large number of\npotential covariates, and does not require any tuning parameters. In two\nempirical applications, we show that our procedure can lead to substantial\ngains of up to 58%, measured either in terms of reductions in data collection\ncosts or in terms of improvements in the precision of the treatment effect\nestimator.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1603.03675v4", "link": "http://arxiv.org/abs/1603.03675v4"}, {"title": "Issues In Disintermediation In The Real Estate Brokerage Sector", "summary": "This article introduces new models of disintermediation of the real estate\nbroker by the buyer or the seller. The decision to retain a real estate broker\nis critical in the property purchase/sale process. The existing literature does\nnot contain analysis of: 1) information asymmetry, 2) the conditions under\nwhich it will be optimal to disintermediate the broker, 3) social capital and\nreputation, 4) the impact of different types of real estate brokerage\ncontracts. The article shows that dis-intermediation of the real estate broker\nby the seller or buyer may be optimal in certain conditions.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2005.01710v1", "link": "http://dx.doi.org/10.1016/j.amc.2006.08.053"}, {"title": "The micro-foundations of an open economy money demand: An application to\n  the Central and Eastern European countries", "summary": "This paper investigates and compares currency substitution between the\ncurrencies of Central and Eastern European (CEE) countries and the euro. In\naddition, we develop a model with microeconomic foundations, which identifies\ndifference between currency substitution and money demand sensitivity to\nexchange rate variations. More precisely, we posit that currency substitution\nrelates to money demand sensitivity to the interest rate spread between the CEE\ncountries and the euro area. Moreover, we show how the exchange rate affects\nmoney demand, even absent a currency substitution effect. This model applies to\nany country where an international currency offers liquidity services to\ndomestic agents. The model generates empirical tests of long-run money demand\nusing two complementary cointegrating equations. The opportunity cost of\nholding the money and the scale variable, either household consumption or\noutput, explain the long-run money demand in CEE countries.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1704.01840v1", "link": "http://arxiv.org/abs/1704.01840v1"}, {"title": "Is completeness necessary? Estimation in nonidentified linear models", "summary": "This paper documents the consequences of the identification failures for a\nclass of linear ill-posed inverse models. The Tikhonov-regularized estimator\nconverges to a well-defined limit equal to the best approximation of the\nstructural parameter in the orthogonal complement to the null space of the\noperator. We illustrate that in many cases the best approximation may coincide\nwith the structural parameter or at least may reasonably approximate it. We\ncharacterize the nonasymptotic Hilbert space norm and the uniform norm\nconvergence rates for the best approximation. Nonidentification has important\nimplications for the large sample distribution of the Tikhonov-regularized\nestimator, and we document the transition between the Gaussian and the weighted\nchi-squared limits. The theoretical results are illustrated for the\nnonparametric IV and the functional linear IV regressions and are further\nsupported by the Monte Carlo experiments.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1709.03473v3", "link": "http://arxiv.org/abs/1709.03473v3"}, {"title": "Bootstrapping Structural Change Tests", "summary": "This paper analyses the use of bootstrap methods to test for parameter change\nin linear models estimated via Two Stage Least Squares (2SLS). Two types of\ntest are considered: one where the null hypothesis is of no change and the\nalternative hypothesis involves discrete change at k unknown break-points in\nthe sample; and a second test where the null hypothesis is that there is\ndiscrete parameter change at l break-points in the sample against an\nalternative in which the parameters change at l + 1 break-points. In both\ncases, we consider inferences based on a sup-Wald-type statistic using either\nthe wild recursive bootstrap or the wild fixed bootstrap. We establish the\nasymptotic validity of these bootstrap tests under a set of general conditions\nthat allow the errors to exhibit conditional and/or unconditional\nheteroskedasticity, and report results from a simulation study that indicate\nthe tests yield reliable inferences in the sample sizes often encountered in\nmacroeconomics. The analysis covers the cases where the first-stage estimation\nof 2SLS involves a model whose parameters are either constant or themselves\nsubject to discrete parameter change. If the errors exhibit unconditional\nheteroskedasticity and/or the reduced form is unstable then the bootstrap\nmethods are particularly attractive because the limiting distributions of the\ntest statistics are not pivotal.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1811.04125v1", "link": "http://dx.doi.org/10.1016/j.jeconom.2019.05.019"}, {"title": "Pricing Mechanism in Information Goods", "summary": "We study three pricing mechanisms' performance and their effects on the\nparticipants in the data industry from the data supply chain perspective. A\nwin-win pricing strategy for the players in the data supply chain is proposed.\nWe obtain analytical solutions in each pricing mechanism, including the\ndecentralized and centralized pricing, Nash Bargaining pricing, and revenue\nsharing mechanism.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1803.01530v1", "link": "http://arxiv.org/abs/1803.01530v1"}, {"title": "The Nasdaq crash of April 2000: Yet another example of log-periodicity\n  in a speculative bubble ending in a crash", "summary": "The Nasdaq Composite fell another $\\approx 10 %$ on Friday the 14'th of April\n2000 signaling the end of a remarkable speculative high-tech bubble starting in\nspring 1997. The closing of the Nasdaq Composite at 3321 corresponds to a total\nloss of over 35% since its all-time high of 5133 on the 10'th of March 2000.\nSimilarities to the speculative bubble preceding the infamous crash of October\n1929 are quite striking: the belief in what was coined a ``New Economy'' both\nin 1929 and presently made share-prices of companies with three digits\nprice-earning ratios soar. Furthermore, we show that the largest draw downs of\nthe Nasdaq are outliers with a confidence level better than 99% and that these\ntwo speculative bubbles, as well as others, both nicely fit into the\nquantitative framework proposed by the authors in a series of recent papers.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0004263v4", "link": "http://dx.doi.org/10.1007/s100510070147"}, {"title": "Liquidity Risk, Price Impacts and the Replication Problem", "summary": "We extend a linear version of the liquidity risk model of Cetin et al. (2004)\nto allow for price impacts. We show that the impact of a market order on prices\ndepends on the size of the transaction and the level of liquidity. We obtain a\nsimple characterization of self-financing trading strategies and a sufficient\ncondition for no arbitrage. We consider a stochastic volatility model in which\nthe volatility is partly correlated with the liquidity process and show that,\nwith the use of variance swaps, contingent claims whose payoffs depend on the\nvalue of the asset can be approximately replicated in this setting. The\nreplicating costs of such payoffs are obtained from the solutions of BSDEs with\nquadratic growth and analytical properties of these solutions are investigated.", "category": ["math.PR", "q-fin.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/0812.2440v2", "link": "http://arxiv.org/abs/0812.2440v2"}, {"title": "Data-driven nonlinear expectations for statistical uncertainty in\n  decisions", "summary": "In stochastic decision problems, one often wants to estimate the underlying\nprobability measure statistically, and then to use this estimate as a basis for\ndecisions. We shall consider how the uncertainty in this estimation can be\nexplicitly and consistently incorporated in the valuation of decisions, using\nthe theory of nonlinear expectations.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1609.06545v1", "link": "http://dx.doi.org/10.1214/17-EJS1278"}, {"title": "Precisamos de uma Contabilidade Ambiental para as \"Amaz\u00f4nias\"\n  Paraense?", "summary": "This paper has the following objectives: to understand the concepts of\nEnvironmental Accounting in Brazil; Make criticisms and propositions anchored\nin the reality or demand of environmental accounting for Amazonia Paraense. The\nmethodological strategy was a critical analysis of Ferreira's books (2007);\nRibeiro (2010) and Tinoco and Kraemer (2011) using their correlation with the\nscientific production of authors discussing the Paraense Amazon, besides our\nexperience as researchers of this territory. As a result, we created three\nsections: one for understanding the current constructs of environmental\naccounting, one for criticism and one for propositions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.06499v1", "link": "http://arxiv.org/abs/1910.06499v1"}, {"title": "Trading Strategy with Stochastic Volatility in a Limit Order Book Market", "summary": "In this paper, we employ the Heston stochastic volatility model to describe\nthe stock's volatility and apply the model to derive and analyze the optimal\ntrading strategies for dealers in a security market. We also extend our study\nto option market making for options written on stocks in the presence of\nstochastic volatility. Mathematically, the problem is formulated as a\nstochastic optimal control problem and the controlled state process is the\ndealer's mark-to-market wealth. Dealers in the security market can optimally\ndetermine their ask and bid quotes on the underlying stocks or options\ncontinuously over time. Their objective is to maximize an expected profit from\ntransactions with a penalty proportional to the variance of cumulative\ninventory cost.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1602.00358v1", "link": "http://arxiv.org/abs/1602.00358v1"}, {"title": "Price Optimisation for New Business", "summary": "This contribution is concerned with price optimisation of the new business\nfor a non-life product. Due to high competition in the insurance market,\nnon-life insurers are interested in increasing their conversion rates on new\nbusiness based on some profit level. In this respect, we consider the\ncompetition in the market to model the probability of accepting an offer for a\nspecific customer. We study two optimisation problems relevant for the insurer\nand present some algorithmic solutions for both continuous and discrete case.\nFinally, we provide some applications to a motor insurance dataset.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1711.07753v1", "link": "http://arxiv.org/abs/1711.07753v1"}, {"title": "Variance Dynamics - An empirical journey", "summary": "We investigate the joint dynamics of spot and implied volatility from an\nempirical perspective. We focus on the equity market with the SPX Index our\nunderlying of choice. Using only observable quantities, we extract the\ninstantaneous variance curves implied by the market and study their daily\nvariations jointly with spot returns. We analyze the characteristics of their\nindividual and joint densities, quantify the non-linear relationship between\nspot and volatility, and discuss the modeling implications on the implied\nleverage and the volatility clustering effects. We show that non-linearities\nhave little impact on the dynamics of at-the-money volatilities, but can have a\nsignificant effect on the pricing and hedging of volatility derivatives.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1507.00846v1", "link": "http://arxiv.org/abs/1507.00846v1"}, {"title": "From the Samuelson Volatility Effect to a Samuelson Correlation Effect:\n  Evidence from Crude Oil Calendar Spread Options", "summary": "We introduce a multi-factor stochastic volatility model based on the\nCIR/Heston stochastic volatility process. In order to capture the Samuelson\neffect displayed by commodity futures contracts, we add expiry-dependent\nexponential damping factors to their volatility coefficients. The pricing of\nsingle underlying European options on futures contracts is straightforward and\ncan incorporate the volatility smile or skew observed in the market. We\ncalculate the joint characteristic function of two futures contracts in the\nmodel in analytic form and use the one-dimensional Fourier inversion method of\nCaldana and Fusai (JBF 2013) to price calendar spread options. The model leads\nto stochastic correlation between the returns of two futures contracts. We\nillustrate the distribution of this correlation in an example. We then propose\nanalytical expressions to obtain the copula and copula density directly from\nthe joint characteristic function of a pair of futures. These expressions are\nconvenient to analyze the term-structure of dependence between the two futures\nproduced by the model. In an empirical application we calibrate the proposed\nmodel to volatility surfaces of vanilla options on WTI. In this application we\nprovide evidence that the model is able to produce the desired stylized facts\nin terms of volatility and dependence. In a separate appendix, we give guidance\nfor the implementation of the proposed model and the Fourier inversion results\nby means of one and two-dimensional FFT methods.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1401.7913v3", "link": "http://arxiv.org/abs/1401.7913v3"}, {"title": "Are Stock Markets Integrated? Evidence from a Partially Segmented ICAPM\n  with Asymmetric Effects", "summary": "In this paper, we test a partially segmented ICAPM for two developed markets,\ntwo emerging markets and World market, using an asymmetric extension of the\nmultivariate GARCH process of De Santis and Gerard (1997,1998). We find that\nthis asymmetric process provides a significantly better fit of the data than a\nstandard symmetric process. The evidence obtained from the whole period and\nsub-periods analysis supports the financial integration hypothesis and suggests\nthat domestic risk is not a priced factor.", "category": ["q-fin.ST", "q-fin.PM"], "id": "http://arxiv.org/abs/0905.3875v1", "link": "http://arxiv.org/abs/0905.3875v1"}, {"title": "A dynamic network model with persistent links and node-specific latent\n  variables, with an application to the interbank market", "summary": "We propose a dynamic network model where two mechanisms control the\nprobability of a link between two nodes: (i) the existence or absence of this\nlink in the past, and (ii) node-specific latent variables (dynamic fitnesses)\ndescribing the propensity of each node to create links. Assuming a Markov\ndynamics for both mechanisms, we propose an Expectation-Maximization algorithm\nfor model estimation and inference of the latent variables. The estimated\nparameters and fitnesses can be used to forecast the presence of a link in the\nfuture. We apply our methodology to the e-MID interbank network for which the\ntwo linkage mechanisms are associated with two different trading behaviors in\nthe process of network formation, namely preferential trading and trading\ndriven by node-specific characteristics. The empirical results allow to\nrecognise preferential lending in the interbank market and indicate how a\nmethod that does not account for time-varying network topologies tends to\noverestimate preferential linkage.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1801.00185v1", "link": "http://arxiv.org/abs/1801.00185v1"}, {"title": "PROOF OF VALUE ALIENATION (PoVA) - a concept of a cryptocurrency\n  issuance protocol", "summary": "In this paper, we will describe a concept of a cryptocurrency issuance\nprotocol which supports digital currencies in a Proof-of-Work (< PoW >) like\nmanner. However, the methods assume alternative utilization of assets used for\ncryptocurrency creation (rather than purchasing electricity necessary for <\nmining >).", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1901.04928v1", "link": "http://arxiv.org/abs/1901.04928v1"}, {"title": "Revisiting integral functionals of geometric Brownian motion", "summary": "In this paper we revisit the integral functional of geometric Brownian motion\n$I_t= \\int_0^t e^{-(\\mu s +\\sigma W_s)}ds$, where $\\mu\\in\\mathbb{R}$, $\\sigma >\n0$, and $(W_s )_s>0$ is a standard Brownian motion. Specifically, we calculate\nthe Laplace transform in $t$ of the cumulative distribution function and of the\nprobability density function of this functional.", "category": ["math.PR", "q-fin.CP", "q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/2001.11861v1", "link": "http://arxiv.org/abs/2001.11861v1"}, {"title": "An Unconventional Attempt to Tame Mandelbrot's Grey Swans", "summary": "We suggest an original physical approach to describe the mechanism of market\npricing. The core of our approach is to consider pricing at different time\nscales separately, using independent equations of motion. Such an approach\nleads to a pricing model that not only allows estimating the volatility of\nfuture market prices, but also permits forecasting the direction of the price\nmove. Alongside with that, it is crucial that our model implies no calibration\non historical market data. And last but not least, properties of the model's\nsolution are consistent with those of real markets: it has fat tails, possesses\nscaling and evinces nonlinear market memory. As our model has been derived with\nthe tip of the pen, it may be not a yet another confirmation of the known\nempirical facts, but a theoretical justification thereto. Tests on real\nfinancial instruments prove the competence of our approach.", "category": ["q-fin.ST", "q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1406.5718v1", "link": "http://arxiv.org/abs/1406.5718v1"}, {"title": "Prediction of Shared Bicycle Demand with Wavelet Thresholding", "summary": "Consumers are creatures of habit, often periodic, tied to work, shopping and\nother schedules. We analyzed one month of data from the world's largest\nbike-sharing company to elicit demand behavioral cycles, initially using models\nfrom animal tracking that showed large customers fit an Ornstein-Uhlenbeck\nmodel with demand peaks at periodicities of 7, 12, 24 hour and 7-days. Lorenz\ncurves of bicycle demand showed that the majority of customer usage was\ninfrequent, and demand cycles from time-series models would strongly overfit\nthe data yielding unreliable models. Analysis of thresholded wavelets for the\nspace-time tensor of bike-sharing contracts was able to compress the data into\na 56-coefficient model with little loss of information, suggesting that\nbike-sharing demand behavior is exceptionally strong and regular. Improvements\nto predicted demand could be made by adjusting for 'noise' filtered by our\nmodel from air quality and weather information and demand from infrequent\nriders.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1802.02683v1", "link": "http://arxiv.org/abs/1802.02683v1"}, {"title": "On the optimal dividend problem for a spectrally positive Levy process", "summary": "In this paper we study the optimal dividend problem for a company whose\nsurplus process evolves as a spectrally positive Levy process. This model\nincluding the dual model of the classical risk model and the dual model with\ndiffusion as special cases. We assume that dividends are paid to the\nshareholders according to admissible strategy whose dividend rate is bounded by\na constant. The objective is to find a dividend policy so as to maximize the\nexpected discounted value of dividends which are paid to the shareholders until\nthe company is ruined. We show that the optimal dividend strategy is formed by\na threshold strategy.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1302.2231v4", "link": "http://arxiv.org/abs/1302.2231v4"}, {"title": "How Safe are European Safe Bonds? An Analysis from the Perspective of\n  Modern Portfolio Credit Risk Models", "summary": "Several proposals for the reform of the euro area advocate the creation of a\nmarket in synthetic securities backed by portfolios of sovereign bonds. Most\ndebated are the so-called European Safe Bonds or ESBies proposed by\n(Brunnermeier et al. 2017). This paper provides a comprehensive quantitative\nanalysis of such products. A key component of our contribution is a novel\ndynamic credit risk model which captures salient features of euro area\nsovereign CDS spreads and enables tractable modelling of default dependence\namongst euro members. After successful calibration of our model to CDS spreads,\nwe perform a thorough analysis of ESBies. We provide model-independent price\nbounds; we consider the expected loss as a function of model parameters and\nattachment points; we study the volatility of the credit spread of ESBies; and\nwe discuss several approaches to assess the market risk of ESBies. Our analysis\nprovides a fairly comprehensive picture of the risks associated with ESBies.", "category": ["q-fin.PR", "q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/2001.11249v1", "link": "http://arxiv.org/abs/2001.11249v1"}, {"title": "American Put Option pricing using Least squares Monte Carlo method under\n  Bakshi, Cao and Chen Model Framework (1997) and comparison to alternative\n  regression techniques in Monte Carlo", "summary": "This paper explores alternative regression techniques in pricing American put\noptions and compares to the least-squares method (LSM) in Monte Carlo\nimplemented by Longstaff-Schwartz, 2001 which uses least squares to estimate\nthe conditional expected payoff to the option holder from continuation. The\npricing is done under general model framework of Bakshi, Cao and Chen 1997\nwhich incorporates, stochastic volatility, stochastic interest rate and jumps.\nAlternative regression techniques used are Artificial Neural Network (ANN) and\nGradient Boosted Machine (GBM) Trees. Model calibration is done on American put\noptions on SPY using these three techniques and results are compared on out of\nsample data.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1808.02791v1", "link": "http://arxiv.org/abs/1808.02791v1"}, {"title": "Penalized Sieve GEL for Weighted Average Derivatives of Nonparametric\n  Quantile IV Regressions", "summary": "This paper considers estimation and inference for a weighted average\nderivative (WAD) of a nonparametric quantile instrumental variables regression\n(NPQIV). NPQIV is a non-separable and nonlinear ill-posed inverse problem,\nwhich might be why there is no published work on the asymptotic properties of\nany estimator of its WAD. We first characterize the semiparametric efficiency\nbound for a WAD of a NPQIV, which, unfortunately, depends on an unknown\nconditional derivative operator and hence an unknown degree of ill-posedness,\nmaking it difficult to know if the information bound is singular or not. In\neither case, we propose a penalized sieve generalized empirical likelihood\n(GEL) estimation and inference procedure, which is based on the unconditional\nWAD moment restriction and an increasing number of unconditional moments that\nare implied by the conditional NPQIV restriction, where the unknown quantile\nfunction is approximated by a penalized sieve. Under some regularity\nconditions, we show that the self-normalized penalized sieve GEL estimator of\nthe WAD of a NPQIV is asymptotically standard normal. We also show that the\nquasi likelihood ratio statistic based on the penalized sieve GEL criterion is\nasymptotically chi-square distributed regardless of whether or not the\ninformation bound is singular.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1902.10100v1", "link": "http://arxiv.org/abs/1902.10100v1"}, {"title": "Spurious memory in non-equilibrium stochastic models of imitative\n  behavior", "summary": "The origin of the long-range memory in the non-equilibrium systems is still\nan open problem as the phenomenon can be reproduced using models based on\nMarkov processes. In these cases a notion of spurious memory is introduced. A\ngood example of Markov processes with spurious memory is stochastic process\ndriven by a non-linear stochastic differential equation (SDE). This example is\nat odds with models built using fractional Brownian motion (fBm). We analyze\ndifferences between these two cases seeking to establish possible empirical\ntests of the origin of the observed long-range memory. We investigate\nprobability density functions (PDFs) of burst and inter-burst duration in\nnumerically obtained time series and compare with the results of fBm. Our\nanalysis confirms that the characteristic feature of the processes described by\na one-dimensional SDE is the power-law exponent $3/2$ of the burst or\ninter-burst duration PDF. This property of stochastic processes might be used\nto detect spurious memory in various non-equilibrium systems, where observed\nmacroscopic behavior can be derived from the imitative interactions of agents.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1707.09801v1", "link": "http://dx.doi.org/10.3390/e19080387"}, {"title": "State-Varying Factor Models of Large Dimensions", "summary": "This paper develops an inferential theory for state-varying factor models of\nlarge dimensions. Unlike constant factor models, loadings are general functions\nof some recurrent state process. We develop an estimator for the latent factors\nand state-varying loadings under a large cross-section and time dimension. Our\nestimator combines nonparametric methods with principal component analysis. We\nderive the rate of convergence and limiting normal distribution for the\nfactors, loadings and common components. In addition, we develop a statistical\ntest for a change in the factor structure in different states. We apply the\nestimator to U.S. Treasury yields and S&P500 stock returns. The systematic\nfactor structure in treasury yields differs in times of booms and recessions as\nwell as in periods of high market volatility. State-varying factors based on\nthe VIX capture significantly more variation and pricing information in\nindividual stocks than constant factor models.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1807.02248v3", "link": "http://arxiv.org/abs/1807.02248v3"}, {"title": "An alternative approach on the existence of affine realizations for HJM\n  term structure models", "summary": "We propose an alternative approach on the existence of affine realizations\nfor HJM interest rate models. It is applicable to a wide class of models, and\nsimultaneously it is conceptually rather comprehensible. We also supplement\nsome known existence results for particular volatility structures and provide\nfurther insights into the geometry of term structure models.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1907.03256v1", "link": "http://dx.doi.org/10.1098/rspa.2009.0493"}, {"title": "Interdisciplinarity in Socio-economics, mathematical analysis and\n  predictability of complex systems", "summary": "In this essay, I attempt to provide supporting evidence as well as some\nbalance for the thesis on `Transforming socio-economics with a new\nepistemology' presented by Hollingworth and Mueller (2008). First, I review a\npersonal highlight of my own scientific path that illustrates the power of\ninterdisciplinarity as well as unity of the mathematical description of natural\nand social processes. I also argue against the claim that complex systems are\nin general `not susceptible to mathematical analysis, but must be understood by\nletting them evolve over time or with simulation analysis'. Moreover, I present\nevidence of the limits of the claim that scientists working within Science II\ndo not make predictions about the future because it is too complex. I stress\nthe potentials for a third `Quantum Science' and its associated conceptual and\nphilosophical revolutions, and finally point out some limits of the `new'\ntheory of networks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0807.3814v1", "link": "http://dx.doi.org/10.1093/ser/mwn015"}, {"title": "Consistent Modeling of VIX and Equity Derivatives Using a 3/2 plus Jumps\n  Model", "summary": "The paper demonstrates that a pure-diffusion 3/2 model is able to capture the\nobserved upward-sloping implied volatility skew in VIX options. This\nobservation contradicts a common perception in the literature that jumps are\nrequired for the consistent modelling of equity and VIX derivatives. The\npure-diffusion model, however, struggles to reproduce the smile in the implied\nvolatilities of short-term index options. One remedy to this problem is to\naugment the model by introducing jumps in the index. The resulting 3/2 plus\njumps model turns out to be as tractable as its pure-diffusion counterpart when\nit comes to pricing equity, realized variance and VIX derivatives, but\naccurately captures the smile in implied volatilities of short-term index\noptions.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1203.5903v3", "link": "http://arxiv.org/abs/1203.5903v3"}, {"title": "On the optimal allocation of assets in investment portfolio with\n  application of modern portfolio and nonlinear dynamic chaos theories in\n  investment, commercial and central banks", "summary": "The investment economy is a main characteristic of prosperous society. The\ninvestment portfolio management is a main financial problem, which has to be\nsolved by the investment, commercial and central banks with the application of\nmodern portfolio theory in the investment economy. We use the learning\nanalytics together with the integrative creative imperative intelligent\nconceptual co-lateral adaptive thinking with the purpose to advance our\nscientific knowledge on the diversified investment portfolio management in the\nnonlinear dynamic financial system. We apply the econophysics principles and\nthe econometrics methods with the aim to find the solution to the problem of\nthe optimal allocation of assets in the investment portfolio, using the\nadvanced risk management techniques with the efficient frontier modeling in\nagreement with the modern portfolio theory and using the stability management\ntechniques with the dynamic regimes modeling on the bifurcation diagram in\nagreement with the dynamic chaos theory. We show that the bifurcation diagram,\ncreated with the use of the logistic function in Matlab, can provide some\nvaluable information on the stability of combining risky investments in the\ninvestment portfolio, solving the problem of optimization of assets allocation\nin the investment portfolio. We propose the Ledenyov investment portfolio\ntheorem, based on the Lyapunov stability criteria, with the aim to create the\noptimized investment portfolio with the uncorrelated diversified assets, which\ncan deliver the increased expected returns to the institutional and private\ninvestors in the nonlinear dynamic financial system in the frames of investment\neconomy.", "category": ["q-fin.GN", "q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1301.4881v2", "link": "http://arxiv.org/abs/1301.4881v2"}, {"title": "Geometric Local Variance Gamma model", "summary": "This paper describes another extension of the Local Variance Gamma model\noriginally proposed by P. Carr in 2008, and then further elaborated on by Carr\nand Nadtochiy, 2017 (CN2017), and Carr and Itkin, 2018 (CI2018). As compared\nwith the latest version of the model developed in CI2018 and called the ELVG\n(the Expanded Local Variance Gamma model), here we provide two innovations.\nFirst, in all previous papers the model was constructed based on a Gamma\ntime-changed {\\it arithmetic} Brownian motion: with no drift in CI2017, and\nwith drift in CI2018, and the local variance to be a function of the spot level\nonly. In contrast, here we develop a {\\it geometric} version of this model with\ndrift. Second, in CN2017 the model was calibrated to option smiles assuming the\nlocal variance is a piecewise constant function of strike, while in CI2018 the\nlocal variance is a piecewise linear} function of strike. In this paper we\nconsider 3 piecewise linear models: the local variance as a function of strike,\nthe local variance as function of log-strike, and the local volatility as a\nfunction of strike (so, the local variance is a piecewise quadratic function of\nstrike). We show that for all these new constructions it is still possible to\nderive an ordinary differential equation for the option price, which plays a\nrole of Dupire's equation for the standard local volatility model, and,\nmoreover, it can be solved in closed form. Finally, similar to CI2018, we show\nthat given multiple smiles the whole local variance/volatility surface can be\nrecovered which does not require solving any optimization problem. Instead, it\ncan be done term-by-term by solving a system of non-linear algebraic equations\nfor each maturity which is fast.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1809.07727v2", "link": "http://arxiv.org/abs/1809.07727v2"}, {"title": "Quasi-Experimental Shift-Share Research Designs", "summary": "Many studies use shift-share (or \"Bartik\") instruments, which average a set\nof shocks with exposure share weights. We provide a new econometric framework\nfor such designs in which identification follows from the quasi-random\nassignment of shocks, allowing exposure shares to be endogenous. This framework\nis centered around a numerical equivalence: conventional shift-share\ninstrumental variable (SSIV) regression coefficients are equivalently obtained\nfrom a transformed regression where the shocks are used directly as an\ninstrument. This equivalence implies a shock-level translation of the SSIV\nexclusion restriction, which holds when shocks are as-good-as-randomly assigned\nand large in number, with sufficient dispersion in their average exposure. We\ndiscuss and illustrate several practical insights delivered by this framework.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1806.01221v6", "link": "http://arxiv.org/abs/1806.01221v6"}, {"title": "Bayesian Nonparametric Adaptive Spectral Density Estimation for\n  Financial Time Series", "summary": "Discrimination between non-stationarity and long-range dependency is a\ndifficult and long-standing issue in modelling financial time series. This\npaper uses an adaptive spectral technique which jointly models the\nnon-stationarity and dependency of financial time series in a non-parametric\nfashion assuming that the time series consists of a finite, but unknown number,\nof locally stationary processes, the locations of which are also unknown. The\nmodel allows a non-parametric estimate of the dependency structure by modelling\nthe auto-covariance function in the spectral domain. All our estimates are made\nwithin a Bayesian framework where we use aReversible Jump Markov Chain Monte\nCarlo algorithm for inference. We study the frequentist properties of our\nestimates via a simulation study, and present a novel way of generating time\nseries data from a nonparametric spectrum. Results indicate that our techniques\nperform well across a range of data generating processes. We apply our method\nto a number of real examples and our results indicate that several financial\ntime series exhibit both long-range dependency and non-stationarity.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1902.03350v1", "link": "http://arxiv.org/abs/1902.03350v1"}, {"title": "Conditional heteroskedasticity in crypto-asset returns", "summary": "This paper examines the time series properties of cryptocurrency assets, such\nas Bitcoin, using established econometric inference techniques, namely models\nof the GARCH family. The contribution of this study is twofold. I explore the\ntime series properties of cryptocurrencies, a new type of financial asset on\nwhich there appears to be little or no literature. I suggest an improved\neconometric specification to that which has been recently proposed in Chu et al\n(2017), the first econometric study to examine the price dynamics of the most\npopular cryptocurrencies. Questions regarding the reliability of their study\nstem from the authors mis-diagnosing the distribution of GARCH innovations.\nChecks are performed on whether innovations are Gaussian or GED by using\nKolmogorov type non-parametric tests and Khmaladze's martingale transformation.\nNull of gaussianity is strongly rejected for all GARCH(p,q) models, with $p,q\n\\in \\{1,\\ldots,5 \\}$, for all cryptocurrencies in sample. For tests of\nnormality, I make use of the Gauss-Kronrod quadrature. Parameters of GARCH\nmodels are estimated with generalized error distribution innovations using\nmaximum likelihood. For calculating P-values, the parametric bootstrap method\nis used. Arguing against Chu et al (2017), I show that there is a strong\nempirical argument against modelling innovations under some common assumptions.", "category": ["q-fin.ST", "q-fin.GN"], "id": "http://arxiv.org/abs/1804.07978v2", "link": "http://arxiv.org/abs/1804.07978v2"}, {"title": "Stratification Trees for Adaptive Randomization in Randomized Controlled\n  Trials", "summary": "This paper proposes an adaptive randomization procedure for two-stage\nrandomized controlled trials. The method uses data from a first-wave experiment\nin order to determine how to stratify in a second wave of the experiment, where\nthe objective is to minimize the variance of an estimator for the average\ntreatment effect (ATE). We consider selection from a class of stratified\nrandomization procedures which we call stratification trees: these are\nprocedures whose strata can be represented as decision trees, with differing\ntreatment assignment probabilities across strata. By using the first wave to\nestimate a stratification tree, we simultaneously select which covariates to\nuse for stratification, how to stratify over these covariates, as well as the\nassignment probabilities within these strata. Our main result shows that using\nthis randomization procedure with an appropriate estimator results in an\nasymptotic variance which is minimal in the class of stratification trees.\nMoreover, the results we present are able to accommodate a large class of\nassignment mechanisms within strata, including stratified block randomization.\nIn a simulation study, we find that our method, paired with an appropriate\ncross-validation procedure ,can improve on ad-hoc choices of stratification. We\nconclude by applying our method to the study in Karlan and Wood (2017), where\nwe estimate stratification trees using the first wave of their experiment.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1806.05127v4", "link": "http://arxiv.org/abs/1806.05127v4"}, {"title": "Cyber bonds and their pricing models", "summary": "Motivated by the developments in cyber risk treatment in the finance\nindustry, we propose a general framework of cyber bond, whose main purpose is\nto insure (compensate) losses of a cyber attack. Based on a database of\npublicly available cyber events, we determine cyber loss distribution\nparameters and use them to numerically simulate cyber bond price, yield, and\nother characteristics. We also consider two possible approaches to cyber bond\ncoupon calculation.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1911.06698v1", "link": "http://arxiv.org/abs/1911.06698v1"}, {"title": "Inference for Volatility Functionals of Multivariate It\u00f4\n  Semimartingales Observed with Jump and Noise", "summary": "This paper presents the nonparametric inference for nonlinear volatility\nfunctionals of general multivariate It\\^o semimartingales, in high-frequency\nand noisy setting. Pre-averaging and truncation enable simultaneous handling of\nnoise and jumps. Second-order expansion reveals explicit biases and a pathway\nto bias correction. Estimators based on this framework achieve the optimal\nconvergence rate. A class of stable central limit theorems are attained with\nestimable asymptotic covariance matrices. This paper form a basis for infill\nasymptotic results of, for example, the realized Laplace transform, the\nrealized principal component analysis, the continuous-time linear regression,\nand the generalized method of integrated moments, hence helps to extend the\napplication scopes to more frequently sampled noisy data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1810.04725v2", "link": "http://arxiv.org/abs/1810.04725v2"}, {"title": "Market ecology of active and passive investors", "summary": "We study the role of active and passive investors in an investment market\nwith uncertainties. Active investors concentrate on a single or a few stocks\nwith a given probability of determining the quality of them. Passive investors\nspread their investment uniformly, resembling buying the market index. In this\ntoy market stocks are introduced as good and bad. If a stock receives\nsufficient investment it will survive, otherwise die. Active players exert a\nselective pressure since they can determine to an extent the investment\nquality. We show that the active players provide the driving force whereas the\npassive ones act as free riders. While their gains do not differ too much, we\nshow that the active players enjoy an edge. Their presence also provides better\ngains to the passive players and stocks themselves.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0104337v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00256-4"}, {"title": "Stochastic volatility of financial markets as the fluctuating rate of\n  trading: an empirical study", "summary": "We present an empirical study of the subordination hypothesis for a\nstochastic time series of a stock price. The fluctuating rate of trading is\nidentified with the stochastic variance of the stock price, as in the\ncontinuous-time random walk (CTRW) framework. The probability distribution of\nthe stock price changes (log-returns) for a given number of trades N is found\nto be approximately Gaussian. The probability distribution of N for a given\ntime interval Dt is non-Poissonian and has an exponential tail for large N and\na sharp cutoff for small N. Combining these two distributions produces a\nnontrivial distribution of log-returns for a given time interval Dt, which has\nexponential tails and a Gaussian central part, in agreement with empirical\nobservations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0608299v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.03.051"}, {"title": "A String Model of Liquidity in Financial Markets", "summary": "We consider a dynamic market model of liquidity where unmatched buy and sell\nlimit orders are stored in order books. The resulting net demand surface\nconstitutes the sole input to the model. We prove that generically there is no\narbitrage in the model when the driving noise is a stochastic string. Under the\nequivalent martingale measure, the clearing price is a martingale, and options\ncan be priced under the no-arbitrage hypothesis. We consider several\nparameterized versions of the model, and show some advantages of specifying the\ndemand curve as quantity as a function of price (as opposed to price as a\nfunction of quantity). We calibrate our model to real order book data, compute\noption prices by Monte Carlo simulation, and compare the results to observed\ndata.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1608.05900v3", "link": "http://arxiv.org/abs/1608.05900v3"}, {"title": "Sustainable Business Models: A Review", "summary": "The concept of the sustainable business model describes the rationale of how\nan organization creates, delivers, and captures value, in economic, social,\ncultural, or other contexts, in a sustainable way. The process of sustainable\nbusiness model construction forms an innovative part of a business strategy.\nDifferent industries and businesses have utilized sustainable business models\nconcept to satisfy their economic, environmental, and social goals\nsimultaneously. However, the success, popularity, and progress of sustainable\nbusiness models in different application domains are not clear. To explore this\nissue, this research provides a comprehensive review of sustainable business\nmodels literature in various application areas. Notable sustainable business\nmodels are identified and further classified in fourteen unique categories, and\nin every category, the progress -- either failure or success -- has been\nreviewed, and the research gaps are discussed. Taxonomy of the applications\nincludes innovation, management and marketing, entrepreneurship, energy,\nfashion, healthcare, agri-food, supply chain management, circular economy,\ndeveloping countries, engineering, construction and real estate, mobility and\ntransportation, and hospitality. The key contribution of this study is that it\nprovides an insight into the state of the art of sustainable business models in\nthe various application areas and future research directions. This paper\nconcludes that popularity and the success rate of sustainable business models\nin all application domains have been increased along with the increasing use of\nadvanced technologies.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.10052v1", "link": "http://dx.doi.org/10.3390/su11061663"}, {"title": "Hermitian and non-Hermitian covariance estimators for multivariate\n  Gaussian and non-Gaussian assets from random matrix theory", "summary": "The random matrix theory method of planar Gaussian diagrammatic expansion is\napplied to find the mean spectral density of the Hermitian equal-time and\nnon-Hermitian time-lagged cross-covariance estimators, firstly in the form of\nmaster equations for the most general multivariate Gaussian system, secondly\nfor seven particular toy models of the true covariance function. For the\nsimplest one of these models, the existing result is shown to be incorrect and\nthe right one is presented, moreover its generalizations are accomplished to\nthe exponentially-weighted moving average estimator as well as two non-Gaussian\ndistributions, Student t and free Levy. The paper revolves around applications\nto financial complex systems, and the results constitute a sensitive probe of\nthe true correlations present there.", "category": ["q-fin.ST", "q-fin.RM"], "id": "http://arxiv.org/abs/1010.2981v3", "link": "http://arxiv.org/abs/1010.2981v3"}, {"title": "An interest rates cluster analysis", "summary": "An empirical analysis of interest rates in money and capital markets is\nperformed. We investigate a set of 34 different weekly interest rate time\nseries during a time period of 16 years between 1982 and 1997. Our study is\nfocused on the collective behavior of the stochastic fluctuations of these\ntime-series which is investigated by using a clustering linkage procedure.\nWithout any a priori assumption, we individuate a meaningful separation in 6\nmain clusters organized in a hierarchical structure.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0401443v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.03.041"}, {"title": "Minimizing the Probability of Ruin when Consumption is Ratcheted", "summary": "We assume that an agent's rate of consumption is {\\it ratcheted}; that is, it\nforms a non-decreasing process. Given the rate of consumption, we act as\nfinancial advisers and find the optimal investment strategy for the agent who\nwishes to minimize his probability of ruin.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/0806.2358v1", "link": "http://arxiv.org/abs/0806.2358v1"}, {"title": "Orthogonal Statistical Learning", "summary": "We provide excess risk guarantees for statistical learning in a setting where\nthe population risk with respect to which we evaluate the target model depends\non an unknown model that must be to be estimated from data (a \"nuisance\nmodel\"). We analyze a two-stage sample splitting meta-algorithm that takes as\ninput two arbitrary estimation algorithms: one for the target model and one for\nthe nuisance model. We show that if the population risk satisfies a condition\ncalled Neyman orthogonality, the impact of the nuisance estimation error on the\nexcess risk bound achieved by the meta-algorithm is of second order. Our\ntheorem is agnostic to the particular algorithms used for the target and\nnuisance and only makes an assumption on their individual performance. This\nenables the use of a plethora of existing results from statistical learning and\nmachine learning literature to give new guarantees for learning with a nuisance\ncomponent. Moreover, by focusing on excess risk rather than parameter\nestimation, we can give guarantees under weaker assumptions than in previous\nworks and accommodate the case where the target parameter belongs to a complex\nnonparametric class. We characterize conditions on the metric entropy such that\noracle rates---rates of the same order as if we knew the nuisance model---are\nachieved. We also analyze the rates achieved by specific estimation algorithms\nsuch as variance-penalized empirical risk minimization, neural network\nestimation and sparse high-dimensional linear model estimation. We highlight\nthe applicability of our results in four settings of central importance in the\nliterature: 1) heterogeneous treatment effect estimation, 2) offline policy\noptimization, 3) domain adaptation, and 4) learning with missing data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1901.09036v2", "link": "http://arxiv.org/abs/1901.09036v2"}, {"title": "Computing Greeks for L\u00e9vy Models: The Fourier Transform Approach", "summary": "The computation of Greeks for exponential L\\'evy models are usually\napproached by Malliavin Calculus and other methods, as the Likelihood Ratio and\nthe finite difference method. In this paper we obtain exact formulas for Greeks\nof European options based on the Lewis formula for the option value. Therefore,\nit is possible to obtain accurate approximations using Fast Fourier Transform.\nWe will present an exhaustive development of Greeks for Call options. The error\nis shown for all Greeks in the Black-Scholes model, where Greeks can be exactly\ncomputed. Other models used in the literature are compared, such as the Merton\nand Variance Gamma models. The presented formulas can reach desired accuracy\nbecause our approach generates error only by approximation of the integral.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1407.1343v1", "link": "http://arxiv.org/abs/1407.1343v1"}, {"title": "Fast Quantization of Stochastic Volatility Models", "summary": "Recursive Marginal Quantization (RMQ) allows fast approximation of solutions\nto stochastic differential equations in one-dimension. When applied to two\nfactor models, RMQ is inefficient due to the fact that the optimization problem\nis usually performed using stochastic methods, e.g., Lloyd's algorithm or\nCompetitive Learning Vector Quantization. In this paper, a new algorithm is\nproposed that allows RMQ to be applied to two-factor stochastic volatility\nmodels, which retains the efficiency of gradient-descent techniques. By\nmargining over potential realizations of the volatility process, a significant\ndecrease in computational effort is achieved when compared to current\nquantization methods. Additionally, techniques for modelling the correct\nzero-boundary behaviour are used to allow the new algorithm to be applied to\ncases where the previous methods would fail. The proposed technique is\nillustrated for European options on the Heston and Stein-Stein models, while a\nmore thorough application is considered in the case of the popular SABR model,\nwhere various exotic options are also priced.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1704.06388v1", "link": "http://arxiv.org/abs/1704.06388v1"}, {"title": "Empirical Relevance of Ambiguity in First Price Auction Models", "summary": "We study the identification and estimation of first-price auction models\nwhere bidders have ambiguity about the valuation distribution and their\npreferences are represented by maxmin expected utility. When entry is\nexogenous, the distribution and ambiguity structure are nonparametrically\nidentified, separately from risk aversion (CRRA). We propose a flexible\nBayesian method based on Bernstein polynomials. Monte Carlo experiments show\nthat our method estimates parameters precisely, and chooses reserve prices with\n(nearly) optimal revenues, whether there is ambiguity or not. Furthermore, if\nthe model is misspecified -- incorrectly assuming no ambiguity among bidders --\nit may induce estimation bias with a substantial revenue loss.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1504.02516v1", "link": "http://arxiv.org/abs/1504.02516v1"}, {"title": "A fixed-point policy-iteration-type algorithm for symmetric nonzero-sum\n  stochastic impulse games", "summary": "Nonzero-sum stochastic differential games with impulse controls offer a\nrealistic and far-reaching modelling framework for applications within finance,\nenergy markets and other areas, but the difficulty in solving such problems has\nhindered their proliferation. Semi-analytical approaches make strong\nassumptions pertaining very particular cases. To the author's best knowledge,\nthe only numerical method in the literature is the heuristic one we put forward\nto solve an underlying system of quasi-variational inequalities. Focusing on\nsymmetric games, this paper presents a simpler and more efficient fixed-point\npolicy-iteration-type algorithm which removes the strong dependence on the\ninitial guess and the relaxation scheme of the previous method. A rigorous\nconvergence analysis is undertaken with natural assumptions on the players\nstrategies, which admit graph-theoretic interpretations in the context of\nweakly chained diagonally dominant matrices. A provably convergent\nsingle-player impulse control solver, often outperforming classical policy\niteration, is also provided. The main algorithm is used to compute with high\nprecision equilibrium payoffs and Nash equilibria of otherwise too challenging\nproblems, and even some for which results go beyond the scope of all the\ncurrently available theory.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.03574v1", "link": "http://arxiv.org/abs/1909.03574v1"}, {"title": "Statistical analysis of the price index of Tehran Stock Exchange", "summary": "This paper presents a statistical analysis of Tehran Price Index (TePIx) for\nthe period of 1992 to 2004. The results present asymmetric property of the\nreturn distribution which tends to the right hand of the mean. Also the return\ndistribution can be fitted by a stable Levy distribution and the tails are very\nfatter than the gaussian distribution. We estimate the tail index of the TePIx\nreturns with two different methods and the results are consistent with the\nprevious studies on the stock markets. A strong autocorrelation has been\ndetected in the TePIx time series representing a long memory of several trading\ndays. We have also applied a Zipf analysis on the TePIx data presenting strong\ncorrelations between the TePIx daily fluctuations. We hope that this paper be\nable to give a brief description about the statistical behavior of financial\ndata in Iran stock market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0410289v1", "link": "http://arxiv.org/abs/cond-mat/0410289v1"}, {"title": "Principal-Agent Problem with Common Agency without Communication", "summary": "In this paper, we consider a problem of contract theory in which several\nPrincipals hire a common Agent and we study the model in the continuous time\nsetting. We show that optimal contracts should satisfy some equilibrium\nconditions and we reduce the optimisation problem of the Principals to a system\nof coupled Hamilton-Jacobi-Bellman (HJB) equations. We provide conditions\nensuring that for risk-neutral Principals, the system of coupled HJB equations\nadmits a solution. Further, we apply our study in a more specific\nlinear-quadratic model where two interacting Principals hire one common Agent.\nIn this continuous time model, we extend the result of Bernheim and Whinston\n(1986) in which the authors compare the optimal effort of the Agent in a\nnon-cooperative Principals model and that in the aggregate model, by showing\nthat these two optimisations coincide only in the first best case. We also\nstudy the sensibility of the optimal effort and the optimal remunerations with\nrespect to appetence parameters and the correlation between the projects.", "category": ["math.PR", "q-fin.CP", "q-fin.GN"], "id": "http://arxiv.org/abs/1706.02936v2", "link": "http://arxiv.org/abs/1706.02936v2"}, {"title": "Identifying Network Ties from Panel Data: Theory and an Application to\n  Tax Competition", "summary": "Social interactions determine many economic behaviors, but information on\nsocial ties does not exist in most publicly available and widely used datasets.\nWe present results on the identification of social networks from observational\npanel data that contains no information on social ties between agents. In the\ncontext of a canonical social interactions model, we provide sufficient\nconditions under which the social interactions matrix, endogenous and exogenous\nsocial effect parameters are all globally identified. While this result is\nrelevant across different estimation strategies, we then describe how\nhigh-dimensional estimation techniques can be used to estimate the interactions\nmodel based on the Adaptive Elastic Net GMM method. We employ the method to\nstudy tax competition across US states. We find the identified social\ninteractions matrix implies tax competition differs markedly from the common\nassumption of competition between geographically neighboring states, providing\nfurther insights for the long-standing debate on the relative roles of factor\nmobility and yardstick competition in driving tax setting behavior across\nstates. Most broadly, our identification and application show the analysis of\nsocial interactions can be extended to economic realms where no network data\nexists.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.07452v2", "link": "http://arxiv.org/abs/1910.07452v2"}, {"title": "Consistent Recalibration of Yield Curve Models", "summary": "The analytical tractability of affine (short rate) models, such as the\nVasicek and the Cox-Ingersoll-Ross models, has made them a popular choice for\nmodelling the dynamics of interest rates. However, in order to account properly\nfor the dynamics of real data, these models need to exhibit time-dependent or\neven stochastic parameters. This in turn breaks their tractability, and\nmodelling and simulating becomes an arduous task. We introduce a new class of\nHeath-Jarrow-Morton (HJM) models that both fit the dynamics of real market data\nand remain tractable. We call these models consistent recalibration (CRC)\nmodels. These CRC models appear as limits of concatenations of forward rate\nincrements, each belonging to a Hull-White extended affine factor model with\npossibly different parameters. That is, we construct HJM models from \"tangent\"\naffine models. We develop a theory for a continuous path version of such models\nand discuss their numerical implementations within the Vasicek and\nCox-Ingersoll-Ross frameworks.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1502.02926v2", "link": "http://arxiv.org/abs/1502.02926v2"}, {"title": "Moving averages and markets inefficiency", "summary": "We introduce a stochastic price model where, together with a random\ncomponent, a moving average of logarithmic prices contributes to the price\nformation. Our model is tested against financial datasets, showing an extremely\ngood agreement with them. It suggests how to construct trading strategies which\nimply a capital growth rate larger than the growth rate of the underlying\nasset, with also the effect of reducing the fluctuations. These results are a\nclear evidence that some hidden information is not fully integrated in price\ndynamics, and therefore financial markets are partially inefficient. In simple\nterms, we give a recipe for speculators to make money as long as only few\ninvestors follow it.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0011337v2", "link": "http://arxiv.org/abs/cond-mat/0011337v2"}, {"title": "Viewing Risk Measures as Information", "summary": "Regulation and risk management in banks depend on underlying risk measures.\nIn general this is the only purpose that is seen for risk measures. In this\npaper we suggest that the reporting of risk measures can be used to determine\nthe loss distribution function for a financial entity. We demonstrate that a\nlack of sufficient information can lead to ambiguous risk situations. We give\nexamples, showing the need for the reporting of multiple risk measures in order\nto determine a bank's loss distribution. We conclude by suggesting a regulatory\nrequirement of multiple risk measures being reported by banks, giving specific\nrecommendations.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1111.4417v1", "link": "http://arxiv.org/abs/1111.4417v1"}, {"title": "Inference based on Kotlarski's Identity", "summary": "Kotlarski's identity has been widely used in applied economic research.\nHowever, how to conduct inference based on this popular identification approach\nhas been an open question for two decades. This paper addresses this open\nproblem by constructing a novel confidence band for the density function of a\nlatent variable in repeated measurement error model. The confidence band builds\non our finding that we can rewrite Kotlarski's identity as a system of linear\nmoment restrictions. The confidence band controls the asymptotic size uniformly\nover a class of data generating processes, and it is consistent against all\nfixed alternatives. Simulation studies support our theoretical results.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.09375v3", "link": "http://arxiv.org/abs/1808.09375v3"}, {"title": "The role of low temperature waste heat recovery in achieving 2050 goals:\n  a policy positioning paper", "summary": "Urban waste heat recovery, in which low temperature heat from urban sources\nis recovered for use in a district heat network, has a great deal of potential\nin helping to achieve 2050 climate goals. For example, heat from data centres,\nmetro systems, public sector buildings and waste water treatment plants could\nbe used to supply ten percent of Europe's heat demand. Despite this, at\npresent, urban waste heat recovery is not widespread and is an immature\ntechnology. To help achieve greater uptake, three policy recommendations are\nmade. First, policy raising awareness of waste heat recovery and creating a\nlegal framework is suggested. Second, it is recommended that pilot projects are\npromoted to help demonstrate technical and economic feasibility. Finally, a\npilot credit facility is proposed aimed at bridging the gap between potential\ninvestors and heat recovery projects.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.06558v1", "link": "http://arxiv.org/abs/1912.06558v1"}, {"title": "Asymptotics for rough stochastic volatility models", "summary": "Using the large deviation principle (LDP) for a re-scaled fractional Brownian\nmotion $B^H_t$ where the rate function is defined via the reproducing kernel\nHilbert space, we compute small-time asymptotics for a correlated fractional\nstochastic volatility model of the form $dS_t=S_t\\sigma(Y_t) (\\bar{\\rho} dW_t\n+\\rho dB_t), \\,dY_t=dB^H_t$ where $\\sigma$ is $\\alpha$-H\\\"{o}lder continuous\nfor some $\\alpha\\in(0,1]$; in particular, we show that $t^{H-\\frac{1}{2}} \\log\nS_t $ satisfies the LDP as $t\\to0$ and the model has a well-defined implied\nvolatility smile as $t \\to 0$, when the log-moneyness $k(t)=x\nt^{\\frac{1}{2}-H}$. Thus the smile steepens to infinity or flattens to zero\ndepending on whether $H\\in(0,\\frac{1}{2})$ or $H\\in(\\frac{1}{2},1)$. We also\ncompute large-time asymptotics for a fractional local-stochastic volatility\nmodel of the form: $dS_t= S_t^{\\beta} |Y_t|^p dW_t,dY_t=dB^H_t$, and we\ngeneralize two identities in Matsumoto&Yor05 to show that $\\frac{1}{t^{2H}}\\log\n\\frac{1}{t}\\int_0^t e^{2 B^H_s} ds$ and $\\frac{1}{t^{2H}}(\\log \\int_0^t\ne^{2(\\mu s+B^H_s)} ds-2 \\mu t)$ converge in law to $ 2\\mathrm{max}_{0 \\le s \\le\n1} B^H_{s}$ and $2B_1$ respectively for $H \\in (0,\\frac{1}{2})$ and $\\mu>0$ as\n$t \\to \\infty$.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1610.08878v1", "link": "http://dx.doi.org/10.1137/15M1009330"}, {"title": "Real payoffs and virtual trading in agent based market models", "summary": "The \\$-Game was recently introduced as an extension of the Minority Game. In\nthis paper we compare this model with the well know Minority Game and the\nMajority Game models. Due to the inter-temporal nature of the market payoff, we\nintroduce a two step transaction with single and mixed group of interacting\ntraders. When the population is composed of two different group of \\$-traders,\nthey show an anti-imitative behavior. However, when they interact with minority\nor majority players the \\$-population imitates the usual behavior of these\nplayers. Finally we discuss how these models contribute to clarify the market\nmechanism.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0311257v2", "link": "http://arxiv.org/abs/cond-mat/0311257v2"}, {"title": "Privatizaciones, fusiones y adquisiciones: las grandes empresas en\n  M\u00e9xico", "summary": "The present work has as principal objective analyze the evolution of the\nprocess of privatization, mergers and acquisitions of the big companies in the\ncountry in the last decades, to understand the conductive threads that formed\nthe structural changes of the economy, in order world oligop\\'olicas to insert\nit to the global market characterized by formations of strategic alliances,\nacross the mergers and acquisitions that they favour to the transnational\ncompanies.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1905.03339v1", "link": "http://arxiv.org/abs/1905.03339v1"}, {"title": "Global risk minimization in financial markets", "summary": "Recurring international financial crises have adverse socioeconomic effects\nand demand novel regulatory instruments or strategies for risk management and\nmarket stabilization. However, the complex web of market interactions often\nimpedes rational decisions that would absolutely minimize the risk. Here we\nshow that, for any given expected return, investors can overcome this\ncomplexity and globally minimize their financial risk in portfolio selection\nmodels, which is mathematically equivalent to computing the ground state of\nspin glass models in physics, provided the margin requirement remains below a\ncritical, empirically measurable value. For markets with centrally regulated\nmargin requirements, this result suggests a potentially stabilizing\nintervention strategy.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/0908.0682v1", "link": "http://arxiv.org/abs/0908.0682v1"}, {"title": "Graph-based era segmentation of international financial integration", "summary": "Assessing world-wide financial integration constitutes a recurrent challenge\nin macroeconometrics, often addressed by visual inspections searching for data\npatterns. Econophysics literature enables us to build complementary,\ndata-driven measures of financial integration using graphs. The present\ncontribution investigates the potential and interests of a novel 3-step\napproach that combines several state-of-the-art procedures to i) compute\ngraph-based representations of the multivariate dependence structure of asset\nprices time series representing the financial states of 32 countries world-wide\n(1955-2015); ii) compute time series of 5 graph-based indices that characterize\nthe time evolution of the topologies of the graph; iii) segment these time\nevolutions in piece-wise constant eras, using an optimization framework\nconstructed on a multivariate multi-norm total variation penalized functional.\nThe method shows first that it is possible to find endogenous stable eras of\nworld-wide financial integration. Then, our results suggest that the most\nrelevant globalization eras would be based on the historical patterns of global\ncapital flows, while the major regulatory events of the 1970s would only appear\nas a cause of sub-segmentation.", "category": ["q-fin.GN", "econ.EM"], "id": "http://arxiv.org/abs/1905.11842v1", "link": "http://dx.doi.org/10.1016/j.physa.2019.122877"}, {"title": "Adjusted QMLE for the spatial autoregressive parameter", "summary": "One simple, and often very effective, way to attenuate the impact of nuisance\nparameters on maximum likelihood estimation of a parameter of interest is to\nrecenter the profile score for that parameter. We apply this general principle\nto the quasi-maximum likelihood estimator (QMLE) of the autoregressive\nparameter $\\lambda$ in a spatial autoregression. The resulting estimator for\n$\\lambda$ has better finite sample properties compared to the QMLE for\n$\\lambda$, especially in the presence of a large number of covariates. It can\nalso solve the incidental parameter problem that arises, for example, in social\ninteraction models with network fixed effects, or in spatial panel models with\nindividual or time fixed effects. However, spatial autoregressions present\nspecific challenges for this type of adjustment, because recentering the\nprofile score may cause the adjusted estimate to be outside the usual parameter\nspace for $\\lambda$. Conditions for this to happen are given, and implications\nare discussed. For inference, we propose confidence intervals based on a\nLugannani--Rice approximation to the distribution of the adjusted QMLE of\n$\\lambda$. Based on our simulations, the coverage properties of these intervals\nare excellent even in models with a large number of covariates.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.08141v1", "link": "http://arxiv.org/abs/1909.08141v1"}, {"title": "Deterministic Criteria for the Absence and Existence of Arbitrage in\n  Multi-Dimensional Diffusion Markets", "summary": "We derive deterministic criteria for the existence and non-existence of\nequivalent (local) martingale measures for financial markets driven by\nmulti-dimensional time-inhomogeneous diffusions. Our conditions can be used to\nconstruct financial markets in which the \\emph{no unbounded profit with bounded\nrisk} condition holds, while the classical \\emph{no free lunch with vanishing\nrisk} condition fails.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1609.01621v5", "link": "http://arxiv.org/abs/1609.01621v5"}, {"title": "Locally adaptive estimation methods with application to univariate time\n  series", "summary": "The paper offers a unified approach to the study of three locally adaptive\nestimation methods in the context of univariate time series from both\ntheoretical and empirical points of view. A general procedure for the\ncomputation of critical values is given. The underlying model encompasses all\ndistributions from the exponential family providing for great flexibility. The\nprocedures are applied to simulated and real financial data distributed\naccording to the Gaussian, volatility, Poisson, exponential and Bernoulli\nmodels. Numerical results exhibit a very reasonable performance of the methods.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0812.0449v1", "link": "http://arxiv.org/abs/0812.0449v1"}, {"title": "A new $\u03ba$-deformed parametric model for the size distribution of\n  wealth", "summary": "It has been pointed out by Patriarca et al. (2005) that the power-law tailed\nequilibrium distribution in heterogeneous kinetic exchange models with a\ndistributed saving parameter can be resolved as a mixture of Gamma\ndistributions corresponding to particular subsets of agents. Here, we propose a\nnew four-parameter statistical distribution which is a $\\kappa$-deformation of\nthe Generalized Gamma distribution with a power-law tail, based on the deformed\nexponential and logarithm functions introduced by Kaniadakis(2001). We found\nthat this new distribution is also an extension to the $\\kappa$-Generalized\ndistribution proposed by Clementi et al. (2007), with an additional shape\nparameter $\\nu$, and properly reproduces the whole range of the distribution of\nwealth in such heterogeneous kinetic exchange models. We also provide various\nassociated statistical measures and inequality measures.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1805.06929v1", "link": "http://dx.doi.org/10.1016/j.physa.2018.09.060"}, {"title": "From Ecology to Finance (and Back?): Recent Advancements in the Analysis\n  of Bipartite Networks", "summary": "Bipartite networks provide an insightful representation of many systems,\nranging from mutualistic networks of species interactions to investment\nnetworks in finance. The analysis of their topological structures has revealed\nthe ubiquitous presence of properties which seem to characterize many -\napparently different - systems. Nestedness, for example, has been observed in\nplants-pollinator as well as in country-product trade networks. This has raised\nquestions about the significance of these patterns, which are often believed to\nconstitute a genuine signature of self-organization. Here, we review several\nmethods that have been developed for the analysis of such evidence. Due to the\ninterdisciplinary character of complex networks, tools developed in one field,\nfor example ecology, can greatly enrich other areas of research, such as\neconomy and finance, and vice versa. With this in mind, we briefly review\nseveral entropy-based bipartite null models that have been recently proposed\nand discuss their application to several real-world systems. The focus on these\nmodels is motivated by the fact that they show three very desirable features:\nanalytical character, general applicability and versatility. In this respect,\nentropy-based methods have been proven to perform satisfactorily both in\nproviding benchmarks for testing evidence-based null hypotheses and in\nreconstructing unknown network configurations from partial information. On top\nof that, entropy-based models have been successfully employed to analyze\necological as well as economic systems, thus representing an ideal,\ninterdisciplinary tool to approach the study of bipartite complex systems.\n[...]", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1710.10143v1", "link": "http://dx.doi.org/10.1007/s10955-018-2039-4"}, {"title": "Conditional Asian Options", "summary": "Conditional Asian options are recent market innovations, which offer cheaper\nand long-dated alternatives to regular Asian options. In contrast with payoffs\nfrom regular Asian options which are based on average asset prices, the payoffs\nfrom conditional Asian options are determined only by average prices above\ncertain threshold. Due to the limited inclusion of prices, conditional Asian\noptions further reduce the volatility in the payoffs than their regular\ncounterparts and have been promoted in the market as viable hedging and risk\nmanagement instruments for equity-linked life insurance products. There has\nbeen no previous academic literature on this subject and practitioners have\nonly been known to price these products by simulations. We propose the first\nanalytical approach to computing prices and deltas of conditional Asian options\nin comparison with regular Asian options. In the numerical examples, we put to\nthe test some cost-benefit claims by practitioners. As a by-product, the work\nalso presents some distributional properties of the occupation time and the\ntime-integral of geometric Brownian motion during the occupation time.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1505.06946v1", "link": "http://arxiv.org/abs/1505.06946v1"}, {"title": "Clustering Approaches for Global Minimum Variance Portfolio", "summary": "The only input to attain the portfolio weights of global minimum variance\nportfolio (GMVP) is the covariance matrix of returns of assets being considered\nfor investment. Since the population covariance matrix is not known, investors\nuse historical data to estimate it. Even though sample covariance matrix is an\nunbiased estimator of the population covariance matrix, it includes a great\namount of estimation error especially when the number of observed data is not\nmuch bigger than number of assets. As it is difficult to estimate the\ncovariance matrix with high dimensionality all at once, clustering stocks is\nproposed to come up with covariance matrix in two steps: firstly, within a\ncluster and secondly, between clusters. It decreases the estimation error by\nreducing the number of features in the data matrix. The motivation of this\ndissertation is that the estimation error can still remain high even after\nclustering, if a large amount of stocks is clustered together in a single\ngroup. This research proposes to utilize a bounded clustering method in order\nto limit the maximum cluster size. The result of experiments shows that not\nonly the gap between in-sample volatility and out-of-sample volatility\ndecreases, but also the out-of-sample volatility gets reduced. It implies that\nwe need a bounded clustering algorithm so that maximum clustering size can be\nprecisely controlled to find the best portfolio performance.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/2001.02966v3", "link": "http://arxiv.org/abs/2001.02966v3"}, {"title": "First-Order Asymptotics of Path-Dependent Derivatives in Multiscale\n  Stochastic Volatility Environment", "summary": "In this paper, we extend the first-order asymptotics analysis of Fouque et\nal. to general path-dependent financial derivatives using Dupire's functional\nIto calculus. The main conclusion is that the market group parameters\ncalibrated to vanilla options can be used to price to the same order exotic,\npath-dependent derivatives as well. Under general conditions, the first-order\ncondition is represented by a conditional expectation that could be numerically\nevaluated. Moreover, if the path-dependence is not too severe, we are able to\nfind path-dependent closed-form solutions equivalent to the fist-order\napproximation of path-independent options derived in Fouque et al.\nAdditionally, we exemplify the results with Asian options and options on\nquadratic variation.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1712.07320v1", "link": "http://dx.doi.org/10.1142/S0219024918500243"}, {"title": "A simple model suggesting economically rational sample-size choice\n  drives irreproducibility", "summary": "Several systematic studies have suggested that a large fraction of published\nresearch is not reproducible. One probable reason for low reproducibility is\ninsufficient sample size, resulting in low power and low positive predictive\nvalue. It has been suggested that insufficient sample-size choice is driven by\na combination of scientific competition and 'positive publication bias'. Here\nwe formalize this intuition in a simple model, in which scientists choose\neconomically rational sample sizes, balancing the cost of experimentation with\nincome from publication. Specifically, assuming that a scientist's income\nderives only from 'positive' findings (positive publication bias) and that\nindividual samples cost a fixed amount, allows to leverage basic statistical\nformulas into an economic optimality prediction. We find that if effects have\ni) low base probability, ii) small effect size or iii) low grant income per\npublication, then the rational (economically optimal) sample size is small.\nFurthermore, for plausible distributions of these parameters we find a robust\nemergence of a bimodal distribution of obtained statistical power and low\noverall reproducibility rates, both matching empirical findings. Finally, we\nexplore conditional equivalence testing as a means to align economic incentives\nwith adequate sample sizes. Overall, the model describes a simple mechanism\nexplaining both the prevalence and the persistence of small sample sizes, and\nis well suited for empirical validation. It proposes economic rationality, or\neconomic pressures, as a principal driver of irreproducibility and suggests\nstrategies to change this.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.08702v4", "link": "http://arxiv.org/abs/1908.08702v4"}, {"title": "Market Integration in the Prewar Japanese Rice Markets", "summary": "This paper examines the integration process of the Japanese major rice\nmarkets (Tokyo and Osaka) from 1881 to 1932. Using a non-Bayesian time-varying\nvector error correction model, we argue that the process strongly depended on\nthe government's policy on the network system of the telegram and telephone;\nrice traders with an intention to use modern communication tools were usually\naffected by the changes in policy. We find that (i) the Japanese rice markets\nhad been integrated in the 1910s; (ii) increasing use of telegraphs had\naccelerated rice market integration from the Meiji period in Japan; and (iii)\nlocal telephone system, which reduced the time spent by urban users sending and\nreceiving telegrams, promoted market integration.", "category": ["q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1604.00148v6", "link": "http://arxiv.org/abs/1604.00148v6"}, {"title": "Duality in Robust Utility Maximization with Unbounded Claim via a Robust\n  Extension of Rockafellar's Theorem", "summary": "We study the convex duality method for robust utility maximization in the\npresence of a random endowment. When the underlying price process is a locally\nbounded semimartingale, we show that the fundamental duality relation holds\ntrue for a wide class of utility functions on the whole real line and unbounded\nrandom endowment. To obtain this duality, we prove a robust version of\nRockafellar's theorem on convex integral functionals and apply Fenchel's\ngeneral duality theorem.", "category": ["q-fin.CP", "math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1101.2968v1", "link": "http://arxiv.org/abs/1101.2968v1"}, {"title": "A Model of Presidential Debates", "summary": "Presidential debates are viewed as providing an important public good by\nrevealing information on candidates to voters. We consider an endogenous model\nof presidential debates in which an incumbent and a challenger (who is\nprivately informed about her own quality) publicly announce whether they are\nwilling to participate in a public debate, taking into account that a voter's\nchoice of candidate depends on her beliefs regarding the candidates' qualities\nand on the state of nature.It is found that in equilibrium a debate occurs or\ndoes not occur independently of the challenger's quality and therefore the\ncandidates' announcements are uninformative. This is because opting-out is\nperceived to be worse than losing a debate and therefore the challenger never\nrefuses to participate.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.01362v10", "link": "http://arxiv.org/abs/1907.01362v10"}, {"title": "Demonetization and Its Impact on Employment in India", "summary": "On November 08, the sudden announcement to demonetization the high\ndenomination currency notes sent tremors all across the country. Given the\ntiming, and socioeconomic and political repercussions of the decision, many\ntermed it a financial emergency. Given high proportion of these notes in\ncirculation, over 86 percent, it led to most economic activities, particularly\nemployment, affected in a big way. Political parties, however, seemed divided\non the issue, i.e. those in favor of the decision feel it will help to curb the\ngalloping size of black money, fake currency, cross boarder terrorism, etc. In\nsharp contrast, the others believe it is a purely misleading, decision, based\non no or poor understanding of black economy, and hence is only politically\nmotivated in wake of the assembly elections due in a couple of states.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1702.01686v1", "link": "http://arxiv.org/abs/1702.01686v1"}, {"title": "Machine Learning for Yield Curve Feature Extraction: Application to\n  Illiquid Corporate Bonds", "summary": "This paper studies an application of machine learning in extracting features\nfrom the historical market implied corporate bond yields. We consider an\nexample of a hypothetical illiquid fixed income market. After choosing a\nsurrogate liquid market, we apply the Denoising Autoencoder (DAE) algorithm to\nlearn the features of the missing yield parameters from the historical data of\nthe instruments traded in the chosen liquid market. The DAE algorithm is then\nchallenged by two \"point-in-time\" inpainting algorithms taken from the image\nprocessing and computer vision domain. It is observed that, when tested on\nunobserved rate surfaces, the DAE algorithm exhibits superior performance\nthanks to the features it has learned from the historical shapes of yield\ncurves.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1812.01102v1", "link": "http://arxiv.org/abs/1812.01102v1"}, {"title": "Sequence Classification of the Limit Order Book using Recurrent Neural\n  Networks", "summary": "Recurrent neural networks (RNNs) are types of artificial neural networks\n(ANNs) that are well suited to forecasting and sequence classification. They\nhave been applied extensively to forecasting univariate financial time series,\nhowever their application to high frequency trading has not been previously\nconsidered. This paper solves a sequence classification problem in which a\nshort sequence of observations of limit order book depths and market orders is\nused to predict a next event price-flip. The capability to adjust quotes\naccording to this prediction reduces the likelihood of adverse price selection.\nOur results demonstrate the ability of the RNN to capture the non-linear\nrelationship between the near-term price-flips and a spatio-temporal\nrepresentation of the limit order book. The RNN compares favorably with other\nclassifiers, including a linear Kalman filter, using S&P500 E-mini futures\nlevel II data over the month of August 2016. Further results assess the effect\nof retraining the RNN daily and the sensitivity of the performance to trade\nlatency.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1707.05642v1", "link": "http://arxiv.org/abs/1707.05642v1"}, {"title": "Perfect and partial hedging for swing game options in discrete time", "summary": "The paper introduces and studies hedging for game (Israeli) style extension\nof swing options considered as multiple exercise derivatives. Assuming that the\nunderlying security can be traded without restrictions we derive a formula for\nvaluation of multiple exercise options via classical hedging arguments.\nIntroducing the notion of the shortfall risk for such options we study also\npartial hedging which leads to minimization of this risk.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/0907.2541v1", "link": "http://arxiv.org/abs/0907.2541v1"}, {"title": "Optimization-Based Algorithm for Evolutionarily Stable Strategies\n  against Pure Mutations", "summary": "Evolutionarily stable strategy (ESS) is an important solution concept in game\ntheory which has been applied frequently to biological models. Informally an\nESS is a strategy that if followed by the population cannot be taken over by a\nmutation strategy that is initially rare. Finding such a strategy has been\nshown to be difficult from a theoretical complexity perspective. We present an\nalgorithm for the case where mutations are restricted to pure strategies, and\npresent experiments on several game classes including random and a\nrecently-proposed cancer model. Our algorithm is based on a mixed-integer\nnon-convex feasibility program formulation, which constitutes the first general\noptimization formulation for this problem. It turns out that the vast majority\nof the games included in the experiments contain ESS with small support, and\nour algorithm is outperformed by a support-enumeration based approach. However\nwe suspect our algorithm may be useful in the future as games are studied that\nhave ESS with potentially larger and unknown support size.", "category": [], "id": "http://arxiv.org/abs/1803.00607v2", "link": "http://arxiv.org/abs/1803.00607v2"}, {"title": "Double Debiased Machine Learning Nonparametric Inference with Continuous\n  Treatments", "summary": "We propose a nonparametric inference method for causal effects of continuous\ntreatment variables, under unconfoundedness and in the presence of\nhigh-dimensional or nonparametric nuisance parameters. Our double debiased\nmachine learning (DML) estimators for the average dose-response function (or\nthe average structural function) and the partial effects are asymptotically\nnormal with nonparametric convergence rates. The nuisance estimators for the\nconditional expectation function and the conditional density can be\nnonparametric kernel or series estimators or ML methods. Using a kernel-based\ndoubly robust influence function and cross-fitting, we give tractable primitive\nconditions under which the nuisance estimators do not affect the first-order\nlarge sample distribution of the DML estimators. We justify the use of kernel\nto localize the continuous treatment at a given value by the Gateaux\nderivative. We implement various ML methods in Monte Carlo simulations and an\nempirical application on a job training program evaluation.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.03036v1", "link": "http://arxiv.org/abs/2004.03036v1"}, {"title": "An Economic Bubble Model and Its First Passage Time", "summary": "We introduce a new diffusion process Xt to describe asset prices within an\neconomic bubble cycle. The main feature of the process, which differs from\nexisting models, is the drift term where a mean-reversion is taken based on an\nexponential decay of the scaled price. Our study shows the scaling factor on Xt\nis crucial for modelling economic bubbles as it mitigates the dependence\nstructure between the price and parameters in the model. We prove both the\nprocess and its first passage time are well-defined. An efficient calibration\nscheme, together with the probability density function for the process are\ngiven. Moreover, by employing the perturbation technique, we deduce the\nclosed-form density for the downward first passage time, which therefore can be\nused in estimating the burst time of an economic bubble. The object of this\nstudy is to understand the asset price dynamics when a financial bubble is\nbelieved to form, and correspondingly provide estimates to the bubble crash\ntime. Calibration examples on the US dot-com bubble and the 2007 Chinese stock\nmarket crash verify the effectiveness of the model itself. The example on\nBitCoin prediction confirms that we can provide meaningful estimate on the\ndownward probability for asset prices.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1803.08160v1", "link": "http://arxiv.org/abs/1803.08160v1"}, {"title": "Long-term prediction intervals of economic time series", "summary": "We construct long-term prediction intervals for time-aggregated future values\nof univariate economic time series. We propose computational adjustments of the\nexisting methods to improve coverage probability under a small sample\nconstraint. A pseudo-out-of-sample evaluation shows that our methods perform at\nleast as well as selected alternative methods based on model-implied Bayesian\napproaches and bootstrapping. Our most successful method yields prediction\nintervals for eight macroeconomic indicators over a horizon spanning several\ndecades.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.05384v1", "link": "http://arxiv.org/abs/2002.05384v1"}, {"title": "Stochastic Dynamic Utilities and Inter-Temporal Preferences", "summary": "We propose an axiomatic approach which economically underpins the\nrepresentation of dynamic preferences in terms of a stochastic utility\nfunction, sensitive to the information available to the decision maker. Our\nconstruction is iterative and based on inter-temporal preference relations,\nwhose characterization is inpired by the original intuition given by Debreu's\nState Dependent Utilities (1960).", "category": ["math.PR", "q-fin.EC"], "id": "http://arxiv.org/abs/1803.05244v4", "link": "http://arxiv.org/abs/1803.05244v4"}, {"title": "Fractional processes as models in stochastic finance", "summary": "We survey some new progress on the pricing models driven by fractional\nBrownian motion \\cb{or} mixed fractional Brownian motion. In particular, we\ngive results on arbitrage opportunities, hedging, and option pricing in these\nmodels. We summarize some recent results on fractional Black & Scholes pricing\nmodel with transaction costs. We end the paper by giving some approximation\nresults and indicating some open problems related to the paper.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1004.3106v1", "link": "http://arxiv.org/abs/1004.3106v1"}, {"title": "Multi-Stage Compound Real Options Valuation in Residential PV-Battery\n  Investment", "summary": "Strategic valuation of efficient and well-timed network investments under\nuncertain electricity market environment has become increasingly challenging,\nbecause there generally exist multiple interacting options in these\ninvestments, and failing to systematically consider these options can lead to\ndecisions that undervalue the investment. In our work, a real options valuation\n(ROV) framework is proposed to determine the optimal strategy for executing\nmultiple interacting options within a distribution network investment, to\nmitigate the risk of financial losses in the presence of future uncertainties.\nTo demonstrate the characteristics of the proposed framework, we determine the\noptimal strategy to economically justify the investment in residential\nPV-battery systems for additional grid supply during peak demand periods. The\noptions to defer, and then expand, are considered as multi-stage compound\noptions, since the option to expand is a subsequent option of the former. These\noptions are valued via the least squares Monte Carlo method, incorporating\nuncertainty over growing power demand, varying diesel fuel price, and the\ndeclining cost of PV-battery technology as random variables. Finally, a\nsensitivity analysis is performed to demonstrate how the proposed framework\nresponds to uncertain events. The proposed framework shows that executing the\ninteracting options at the optimal timing increases the investment value.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1910.09132v1", "link": "http://arxiv.org/abs/1910.09132v1"}, {"title": "We've walked a million miles for one of these smiles", "summary": "We derive a new, exact and transparent expansion for option smiles, which\nlends itself both to analytical approximation and, perhaps more importantly, to\ncongenial numerical treatments. We show that the skew and the curvature of the\nsmile can be computed as exotic options, for which the Hedged Monte Carlo\nmethod is particularly well suited. When applied to options on the S&P index,\nwe find that the skew and the curvature of the smile are very poorly reproduced\nby the standard Edgeworth (cumulant) expansion. Most notably, the relation\nbetween the skew and the skewness is inverted at small and large vols, a\nfeature that none of the model studied so far is able to reproduce.\nFurthermore, the around-the-money curvature of the smile is found to be very\nsmall, in stark contrast with the highly kurtic nature of the returns.", "category": ["q-fin.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1203.5703v2", "link": "http://arxiv.org/abs/1203.5703v2"}, {"title": "Anticipatory Systems, Preferences, Averages: Inflation, Uncertain\n  Phenomena, Management", "summary": "Behavior of systems that are functions of anticipated behavior of other\nsystems, whose own behavior is also anticipatory but homeostatic and determined\nby hierarchical ordering, which changes over time, of sets of possible\nenvironments that are not co-possible, is proven to be highly non-linear and\nsensitively dependent on precise parameters. Averages and other kinds of\naggregates cannot be calculated for sets of measurements of behavior of\nsystems, defined in this essay, that are \"index complex\" in this way. This\nincludes many systems, for instance, social behavior, where anticipation of\nbehavior of other individuals plays a central role. Anticipation of preferences\nof economic actors are discussed in this way. Analysis by way of generalized\nfunctions of complex variables is done for these kinds of systems, and\nequations of change of state are formally described. Behavior that comprises of\nresponses to market interest rates is taken for example. Continuity assumptions\nin economics analyzed in this context. Anticipatory responses to inflation in\neconomics are discussed. Applications to theory of production are presented.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1301.4207v2", "link": "http://arxiv.org/abs/1301.4207v2"}, {"title": "The Effect of Growth On Equality in Models of the Economy", "summary": "We investigate the relation between economic growth and equality in a\nmodified version of the agent-based asset exchange model (AEM). The modified\nmodel is a driven system that for a range of parameter space is effectively\nergodic in the limit of an infinite system. We find that the belief that \"a\nrising tide lifts all boats\" does not always apply, but the effect of growth on\nthe wealth distribution depends on the nature of the growth. In particular, we\nfind that the rate of growth, the way the growth is distributed, and the\npercentage of wealth exchange determine the degree of equality. We find strong\nnumerical evidence that there is a phase transition in the modified model, and\nfor a part of parameter space the modified AEM acts like a geometric random\nwalk.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1305.0794v1", "link": "http://arxiv.org/abs/1305.0794v1"}, {"title": "On Policy Evaluation with Aggregate Time-Series Shocks", "summary": "We propose a general strategy for estimating treatment effects, in contexts\nwhere the only source of exogenous variation is a sequence of aggregate\ntime-series shocks. We start by arguing that commonly used estimation\nprocedures tend to ignore the crucial time-series aspects of the data. Next, we\ndevelop a graphical tool and a novel test to illustrate the issues of the\ndesign using data from influential studies in development economics and\nmacroeconomics. Motivated by these studies, we construct a new estimator, which\nis based on the time-series model for the aggregate shock. We analyze the\nstatistical properties of our estimator in the practically relevant case, where\nboth cross-sectional and time-series dimensions are of similar size. Finally,\nto provide causal interpretation for our estimator, we analyze a new causal\nmodel that allows for rich unobserved heterogeneity in potential outcomes and\nunobserved aggregate shocks.", "category": ["econ.EM", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.13660v2", "link": "http://arxiv.org/abs/1905.13660v2"}, {"title": "Specification Tests for the Propensity Score", "summary": "This paper proposes new nonparametric diagnostic tools to assess the\nasymptotic validity of different treatment effects estimators that rely on the\ncorrect specification of the propensity score. We derive a particular\nrestriction relating the propensity score distribution of treated and control\ngroups, and develop specification tests based upon it. The resulting tests do\nnot suffer from the \"curse of dimensionality\" when the vector of covariates is\nhigh-dimensional, are fully data-driven, do not require tuning parameters such\nas bandwidths, and are able to detect a broad class of local alternatives\nconverging to the null at the parametric rate $n^{-1/2}$, with $n$ the sample\nsize. We show that the use of an orthogonal projection on the tangent space of\nnuisance parameters facilitates the simulation of critical values by means of a\nmultiplier bootstrap procedure, and can lead to power gains. The finite sample\nperformance of the tests is examined by means of a Monte Carlo experiment and\nan empirical application. Open-source software is available for implementing\nthe proposed tests.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1611.06217v2", "link": "http://arxiv.org/abs/1611.06217v2"}, {"title": "American Options with Asymmetric Information and Reflected BSDE", "summary": "We consider an American contingent claim on a financial market where the\nbuyer has additional information. Both agents (seller and buyer) observe the\nsame prices, while the information available to them may differ due to some\nextra exogenous knowledge the buyer has. The buyer's information flow is\nmodeled by an initial enlargement of the reference filtration. It seems natural\nto investigate the value of the American contingent claim with asymmetric\ninformation. We provide a representation for the cost of the additional\ninformation relying on some results on reflected backward stochastic\ndifferential equations (RBSDE). This is done by using an interpretation of\nprices of American contingent claims with extra information for the buyer by\nsolutions of appropriate RBSDE.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1505.05046v2", "link": "http://arxiv.org/abs/1505.05046v2"}, {"title": "Portfolio Choice with Transaction Costs: a User's Guide", "summary": "Recent progress in portfolio choice has made a wide class of problems\ninvolving transaction costs tractable. We review the basic approach to these\nproblems, and outline some directions for future research.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1207.7330v1", "link": "http://arxiv.org/abs/1207.7330v1"}, {"title": "Do firms share the same functional form of their growth rate\n  distribution? A new statistical test", "summary": "We introduce a new statistical test of the hypothesis that a balanced panel\nof firms have the same growth rate distribution or, more generally, that they\nshare the same functional form of growth rate distribution. We applied the test\nto European Union and US publicly quoted manufacturing firms data, considering\nfunctional forms belonging to the Subbotin family of distributions. While our\nhypotheses are rejected for the vast majority of sets at the sector level, we\ncannot rejected them at the subsector level, indicating that homogenous panels\nof firms could be described by a common functional form of growth rate\ndistribution.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1103.2234v1", "link": "http://dx.doi.org/10.1016/j.jedc.2013.11.010"}, {"title": "Moment generating functions and Normalized implied volatilities:\n  unification and extension via Fukasawa's pricing formula", "summary": "We extend the model-free formula of [Fukasawa 2012] for $\\mathbb\nE[\\Psi(X_T)]$, where $X_T=\\log S_T/F$ is the log-price of an asset, to\nfunctions $\\Psi$ of exponential growth. The resulting integral representation\nis written in terms of normalized implied volatilities. Just as Fukasawa's work\nprovides rigourous ground for Chriss and Morokoff's (1999) model-free formula\nfor the log-contract (related to the Variance swap implied variance), we prove\nan expression for the moment generating function $\\mathbb E[e^{p X_T}]$ on its\nanalyticity domain, that encompasses (and extends) Matytsin's formula [Matytsin\n2000] for the characteristic function $\\mathbb E[e^{i \\eta X_T}]$ and Bergomi's\nformula [Bergomi 2016] for $\\mathbb E[e^{p X_T}]$, $p \\in [0,1]$. Besides, we\n(i) show that put-call duality transforms the first normalized implied\nvolatility into the second, and (ii) analyze the invertibility of the extended\ntransformation $d(p,\\cdot) = p \\, d_1 + (1-p)d_2$ when $p$ lies outside\n$[0,1]$. As an application of (i), one can generate representations for the MGF\n(or other payoffs) by switching between one normalized implied volatility and\nthe other.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1703.00957v2", "link": "http://arxiv.org/abs/1703.00957v2"}, {"title": "Long-term and blow-up behaviors of exponential moments in\n  multi-dimensional affine diffusions", "summary": "This paper considers multi-dimensional affine processes with continuous\nsample paths. By analyzing the Riccati system, which is associated with affine\nprocesses via the transform formula, we fully characterize the regions of\nexponents in which exponential moments of a given process do not explode at any\ntime or explode at a given time. In these two cases, we also compute the\nlong-term growth rate and the explosion rate for exponential moments. These\nresults provide a handle to study implied volatility asymptotics in models\nwhere returns of stock prices are described by affine processes whose\nexponential moments do not have an explicit formula.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1010.2865v4", "link": "http://arxiv.org/abs/1010.2865v4"}, {"title": "Information-Based Asset Pricing", "summary": "A new framework for asset price dynamics is introduced in which the concept\nof noisy information about future cash flows is used to derive the price\nprocesses. In this framework an asset is defined by its cash-flow structure.\nEach cash flow is modelled by a random variable that can be expressed as a\nfunction of a collection of independent random variables called market factors.\nWith each such \"X-factor\" we associate a market information process, the values\nof which are accessible to market agents. Each information process is a sum of\ntwo terms; one contains true information about the value of the market factor;\nthe other represents \"noise\". The noise term is modelled by an independent\nBrownian bridge. The market filtration is assumed to be that generated by the\naggregate of the independent information processes. The price of an asset is\ngiven by the expectation of the discounted cash flows in the risk-neutral\nmeasure, conditional on the information provided by the market filtration. When\nthe cash flows are the dividend payments associated with equities, an explicit\nmodel is obtained for the share-price, and the prices of options on\ndividend-paying assets are derived. Remarkably, the resulting formula for the\nprice of a European call option is of the Black-Scholes-Merton type. The\ninformation-based framework also generates a natural explanation for the origin\nof stochastic volatility.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0704.1976v1", "link": "http://arxiv.org/abs/0704.1976v1"}, {"title": "Expected utility operators and coinsurance problem", "summary": "The expected utility operators introduced in a previous paper, offer a\nframework for a general risk aversion theory, in which risk is modelled by a\nfuzzy number $A$. In this paper we formulate a coinsurance problem in the\npossibilistic setting defined by an expected utility operator $T$. Some\nproperties of the optimal saving $T$-coinsurance rate are proved and an\napproximate calculation formula of this is established with respect to the\nArrow-Pratt index of the utility function of the policyholder, as well as the\nexpected value and the variance of a fuzzy number $A$. Various formulas of the\noptimal $T$-coinsurance rate are deduced for a few expected utility operators\nin case of a triangular fuzzy number and of some HARA and CRRA-type utility\nfunctions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1908.06927v1", "link": "http://arxiv.org/abs/1908.06927v1"}, {"title": "Numerical analysis of an extended structural default model with mutual\n  liabilities and jump risk", "summary": "We consider a structural default model in an interconnected banking network\nas in Lipton [International Journal of Theoretical and Applied Finance, 19(6),\n2016], with mutual obligations between each pair of banks. We analyse the model\nnumerically for two banks with jumps in their asset value processes.\nSpecifically, we develop a finite difference method for the resulting\ntwo-dimensional partial integro-differential equation, and study its stability\nand consistency. We then compute joint and marginal survival probabilities, as\nwell as prices of credit default swaps (CDS), first-to-default swaps (FTD),\ncredit and debt value adjustments (CVA and DVA). Finally, we calibrate the\nmodel to market data and assess the impact of jump risk.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1701.00030v1", "link": "http://arxiv.org/abs/1701.00030v1"}, {"title": "Fast computation of vanilla prices in time-changed models and implied\n  volatilities using rational approximations", "summary": "We present a new numerical method to price vanilla options quickly in\ntime-changed Brownian motion models. The method is based on rational function\napproximations of the Black-Scholes formula. Detailed numerical results are\ngiven for a number of widely used models. In particular, we use the\nvariance-gamma model, the CGMY model and the Heston model without correlation\nto illustrate our results. Comparison to the standard fast Fourier transform\nmethod with respect to accuracy and speed appears to favour the newly developed\nmethod in the cases considered. We present error estimates for the option\nprices. Additionally, we use this method to derive a procedure to compute, for\na given set of arbitrage-free European call option prices, the corresponding\nBlack-Scholes implied volatility surface. To achieve this, rational function\napproximations of the inverse of the Black-Scholes formula are used. We are\nthus able to work out implied volatilities more efficiently than one can by the\nuse of other common methods. Error estimates are presented for a wide range of\nparameters.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1203.6899v1", "link": "http://arxiv.org/abs/1203.6899v1"}, {"title": "Characterization of Market Models in the Presence of Traded Vanilla and\n  Barrier Options", "summary": "We characterize the set of market models when there are a finite number of\ntraded Vanilla and Barrier options with maturity $T$ written on the asset $S$.\nFrom a probabilistic perspective, our result describes the set of joint\ndistributions for $(S_T, \\sup_{u \\leq T} S_u)$ when a finite number of marginal\nlaw constraints on both $S_T$ and $\\sup_{u \\leq T} S_u$ is imposed. An\nextension to the case of multiple maturities is obtained.\n  Our characterization requires a decomposition of the call price function and\nonce it is obtained, we can explicitly express certain joint probabilities in\nthis model. In order to obtain a fully specified joint distribution we discuss\ninterpolation methods.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1411.4193v1", "link": "http://arxiv.org/abs/1411.4193v1"}, {"title": "Reduced Form Capital Optimization", "summary": "We formulate banks' capital optimization problem as a classic mean variance\noptimization, by leveraging an accurate linear approximation to the Shapely or\nConstrained Aumann-Shapley (CAS) allocation of max or nested max cost\nfunctions. This reduced form formulation admits an analytical solution, to the\noptimal leveraged balance sheet (LBS) and risk weighted assets (RWA) target of\nbanks' business units for achieving the best return on capital.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1905.05911v1", "link": "http://dx.doi.org/10.13140/RG.2.2.15535.18080"}, {"title": "Gittins' theorem under uncertainty", "summary": "We study dynamic allocation problems for discrete time multi-armed bandits\nunder uncertainty, based on the the theory of nonlinear expectations. We show\nthat, under strong independence of the bandits and with some relaxation in the\ndefinition of optimality, a Gittins allocation index gives optimal choices.\nThis involves studying the interaction of our uncertainty with controls which\ndetermine the filtration. We also run a simple numerical example which\nillustrates the interaction between the willingness to explore and uncertainty\naversion of the agent when making decisions.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1907.05689v1", "link": "http://arxiv.org/abs/1907.05689v1"}, {"title": "The effect of a market factor on information flow between stocks using\n  minimal spanning tree", "summary": "We empirically investigated the effects of market factors on the information\nflow created from N(N-1)/2 linkage relationships among stocks. We also examined\nthe possibility of employing the minimal spanning tree (MST) method, which is\ncapable of reducing the number of links to N-1. We determined that market\nfactors carry important information value regarding information flow among\nstocks. Moreover, the information flow among stocks evidenced time-varying\nproperties according to the changes in market status. In particular, we noted\nthat the information flow increased dramatically during periods of market\ncrises. Finally, we confirmed, via the MST method, that the information flow\namong stocks could be assessed effectively with the reduced linkage\nrelationships among all links between stocks from the perspective of the\noverall market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0905.2043v1", "link": "http://dx.doi.org/10.1016/j.physa.2009.12.044"}, {"title": "Noise Fit, Estimation Error and a Sharpe Information Criterion", "summary": "When the in-sample Sharpe ratio is obtained by optimizing over a\nk-dimensional parameter space, it is a biased estimator for what can be\nexpected on unseen data (out-of-sample). We derive (1) an unbiased estimator\nadjusting for both sources of bias: noise fit and estimation error. We then\nshow (2) how to use the adjusted Sharpe ratio as model selection criterion\nanalogously to the Akaike Information Criterion (AIC). Selecting a model with\nthe highest adjusted Sharpe ratio selects the model with the highest estimated\nout-of-sample Sharpe ratio in the same way as selection by AIC does for the\nlog-likelihood as measure of fit.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1602.06186v5", "link": "http://arxiv.org/abs/1602.06186v5"}, {"title": "Hedging problems for Asian options with transactions costs", "summary": "In this paper, we consider the problem of hedging Asian options in financial\nmarkets with transaction costs. For this, we use the asymptotic hedging\napproach. The main task of asymptotic hedging in financial markets with\ntransaction costs is to prove the probability convergence of the terminal value\nof the investment portfolio to the payment function when the number of\nportfolio revisions tends to be $n$ to infinity. In practice, this means that\nthe investor, using such a strategy, is able to compensation payments for all\nfinancial transactions, even if their number increases unlimitedly.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/2001.01443v1", "link": "http://arxiv.org/abs/2001.01443v1"}, {"title": "Dynamics of multivariate default system in random environment", "summary": "We consider a multivariate default system where random environmental\ninformation is available. We study the dynamics of the system in a general\nsetting and adopt the point of view of change of probability measures. We also\nmake a link with the density approach in the credit risk modelling. In the\nparticular case where no environmental information is concerned, we pay a\nspecial attention to the phenomenon of system weakened by failures as in the\nclassical reliability system.", "category": ["q-fin.RM", "math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1509.09133v2", "link": "http://arxiv.org/abs/1509.09133v2"}, {"title": "Optimal Choice under Short Sell Limit with Sharpe Ratio as Criterion\n  among Multiple Assets", "summary": "This article is the term paper of the course Investments. We mainly focus on\nmodeling long-term investment decisions of a typical utility-maximizing\nindividual, with features of Chinese stock market in perspective. We adopt an\nOR based methodology with market information as input parameters to carry out\nthe solution. Two main features of this article are: first, we take the no\nshort-sell constraint in Chinese stock market into consideration and use an\napproach otherwise identical to Markowitz to work out the optimal portfolio\nchoice; this method has critical and practical implication to Chinese\ninvestors. Second, we incorporate the benefits of multiple assets into one\nsingle well-defined utility function and use a MIQP procedure to derive the\noptimal allocation of funds upon each of them along the time-line.", "category": ["q-fin.PM", "q-fin.GN"], "id": "http://arxiv.org/abs/1310.6822v1", "link": "http://arxiv.org/abs/1310.6822v1"}, {"title": "Log-Optimal Portfolio Selection Using the Blackwell Approachability\n  Theorem", "summary": "We present a method for constructing the log-optimal portfolio using the\nwell-calibrated forecasts of market values. Dawid's notion of calibration and\nthe Blackwell approachability theorem are used for computing well-calibrated\nforecasts. We select a portfolio using this \"artificial\" probability\ndistribution of market values. Our portfolio performs asymptotically at least\nas well as any stationary portfolio that redistributes the investment at each\nround using a continuous function of side information. Unlike in classical\nmathematical finance theory, no stochastic assumptions are made about market\nvalues.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1410.5996v2", "link": "http://arxiv.org/abs/1410.5996v2"}, {"title": "Pricing foreign exchange options under stochastic volatility and\n  interest rates using an RBF--FD method", "summary": "This paper proposes a numerical method for pricing foreign exchange (FX)\noptions in a model which deals with stochastic interest rates and stochastic\nvolatility of the FX rate. The model considers four stochastic drivers, each\nrepresented by an It\\^{o}'s diffusion with time--dependent drift, and with a\nfull matrix of correlations. It is known that prices of FX options in this\nmodel can be found by solving an associated backward partial differential\nequation (PDE). However, it contains non--affine terms, which makes its\ndifficult to solve it analytically. Also, a standard approach of solving it\nnumerically by using traditional finite--difference (FD) or finite elements\n(FE) methods suffers from the high computational burden. Therefore, in this\npaper a flavor of a localized radial basis functions (RBFs) method, RBF--FD, is\ndeveloped which allows for a good accuracy at a relatively low computational\ncost. Results of numerical simulations are presented which demonstrate\nefficiency of such an approach in terms of both performance and accuracy for\npricing FX options and computation of the associated Greeks.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1903.00937v1", "link": "http://arxiv.org/abs/1903.00937v1"}, {"title": "Robust Utility Maximization with L\u00e9vy Processes", "summary": "We study a robust portfolio optimization problem under model uncertainty for\nan investor with logarithmic or power utility. The uncertainty is specified by\na set of possible L\\'evy triplets; that is, possible instantaneous drift,\nvolatility and jump characteristics of the price process. We show that an\noptimal investment strategy exists and compute it in semi-closed form.\nMoreover, we provide a saddle point analysis describing a worst-case model.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1502.05920v2", "link": "http://arxiv.org/abs/1502.05920v2"}, {"title": "Determining bottom price-levels after a speculative peak", "summary": "During a stock market peak the price of a given stock ($ i $) jumps from an\ninitial level $ p_1(i) $ to a peak level $ p_2(i) $ before falling back to a\nbottom level $ p_3(i) $. The ratios $ A(i) = p_2(i)/p_1(i) $ and $ B(i)=\np_3(i)/p_1(i) $ are referred to as the peak- and bottom-amplitude respectively.\nThe paper shows that for a sample of stocks there is a linear relationship\nbetween $ A(i) $ and $ B(i) $ of the form: $ B=0.4A+b $. In words, this means\nthat the higher the price of a stock climbs during a bull market the better it\nresists during the subsequent bear market. That rule, which we call the\nresilience pattern, also applies to other speculative markets. It provides a\nuseful guiding line for Monte Carlo simulations.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0009222v1", "link": "http://dx.doi.org/10.1007/s100510070150"}, {"title": "On the emergence of scale-free production networks", "summary": "We propose a simple dynamical model of the formation of production networks\namong monopolistically competitive firms. The model subsumes the standard\ngeneral equilibrium approach \\`a la Arrow-Debreu but displays a wide set of\npotential dynamic behaviors. It robustly reproduces key stylized facts of\nfirms' demographics. Our main result is that competition between intermediate\ngood producers generically leads to the emergence of scale-free production\nnetworks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1509.01483v2", "link": "http://arxiv.org/abs/1509.01483v2"}, {"title": "Continuous Equilibrium in Affine and Information-Based Capital Asset\n  Pricing Models", "summary": "We consider a class of generalized capital asset pricing models in continuous\ntime with a finite number of agents and tradable securities. The securities may\nnot be sufficient to span all sources of uncertainty. If the agents have\nexponential utility functions and the individual endowments are spanned by the\nsecurities, an equilibrium exists and the agents' optimal trading strategies\nare constant. Affine processes, and the theory of information-based asset\npricing are used to model the endogenous asset price dynamics and the terminal\npayoff. The derived semi-explicit pricing formulae are applied to numerically\nanalyze the impact of the agents' risk aversion on the implied volatility of\nsimultaneously-traded European-style options.", "category": ["q-fin.GN", "math.PR"], "id": "http://arxiv.org/abs/1201.1840v2", "link": "http://arxiv.org/abs/1201.1840v2"}, {"title": "Stochastic Switching Games", "summary": "We study nonzero-sum stochastic switching games. Two players compete for\nmarket dominance through controlling (via timing options) the discrete-state\nmarket regime $M$. Switching decisions are driven by a continuous stochastic\nfactor $X$ that modulates instantaneous revenue rates and switching costs. This\ngenerates a competitive feedback between the short-term fluctuations due to $X$\nand the medium-term advantages based on $M$. We construct threshold-type\nFeedback Nash Equilibria which characterize stationary strategies describing\nlong-run dynamic equilibrium market organization. Two sequential approximation\nschemes link the switching equilibrium to (i) constrained optimal switching,\n(ii) multi-stage timing games. We provide illustrations using an\nOrnstein-Uhlenbeck $X$ that leads to a recurrent equilibrium $M^\\ast$ and a\nGeometric Brownian Motion $X$ that makes $M^\\ast$ eventually \"absorbed\" as one\nplayer eventually gains permanent advantage. Explicit computations and\ncomparative statics regarding the emergent macroscopic market equilibrium are\nalso provided.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.03893v1", "link": "http://arxiv.org/abs/1807.03893v1"}, {"title": "The Affine Wealth Model: An agent-based model of asset exchange that\n  allows for negative-wealth agents and its empirical validation", "summary": "We present a stochastic, agent-based, binary-transaction Asset-Exchange Model\n(AEM) for wealth distribution that allows for agents with negative wealth. This\nmodel retains certain features of prior AEMs such as redistribution and\nwealth-attained advantage, but it also allows for shifts as well as scalings of\nthe agent density function. We derive the Fokker-Planck equation describing its\ntime evolution and we describe its numerical solution, including a methodology\nfor solving the inverse problem of finding the model parameters that best match\nempirical data. Using this methodology, we compare the steady-state solutions\nof the Fokker-Planck equation with data from the United States Survey of\nConsumer Finances over a time period of 27 years. In doing so, we demonstrate\nagreement with empirical data of an average error less than 0.16\\% over this\ntime period. We present the model parameters for the US wealth distribution\ndata as a function of time under the assumption that the distribution responds\nto their variation adiabatically. We argue that the time series of model\nparameters thus obtained provides a valuable new diagnostic tool for analyzing\nwealth inequality.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1604.02370v2", "link": "http://arxiv.org/abs/1604.02370v2"}, {"title": "Waiting times between orders and trades in double-auction markets", "summary": "In this paper, the survival function of waiting times between orders and the\ncorresponding trades in a double-auction market is studied both by means of\nexperiments and of empirical data. It turns out that, already at the level of\norder durations, the survival function cannot be represented by a single\nexponential, thus ruling out the hypothesis of constant activity during\ntrading. This fact has direct consequences for market microstructural models.\nThey must include such a non-exponential behaviour to be realistic.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0608273v1", "link": "http://dx.doi.org/10.1016/j.physa.2005.09.047"}, {"title": "Financial factor influence on scaling and memory of trading volume in\n  stock market", "summary": "We study the daily trading volume volatility of 17,197 stocks in the U.S.\nstock markets during the period 1989--2008 and analyze the time return\nintervals $\\tau$ between volume volatilities above a given threshold q. For\ndifferent thresholds q, the probability density function P_q(\\tau) scales with\nmean interval <\\tau> as P_q(\\tau)=<\\tau>^{-1}f(\\tau/<\\tau>) and the tails of\nthe scaling function can be well approximated by a power-law f(x)~x^{-\\gamma}.\nWe also study the relation between the form of the distribution function\nP_q(\\tau) and several financial factors: stock lifetime, market capitalization,\nvolume, and trading value. We find a systematic tendency of P_q(\\tau)\nassociated with these factors, suggesting a multi-scaling feature in the volume\nreturn intervals. We analyze the conditional probability P_q(\\tau|\\tau_0) for\n$\\tau$ following a certain interval \\tau_0, and find that P_q(\\tau|\\tau_0)\ndepends on \\tau_0 such that immediately following a short/long return interval\na second short/long return interval tends to occur. We also find indications\nthat there is a long-term correlation in the daily volume volatility. We\ncompare our results to those found earlier for price volatility.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1106.1415v1", "link": "http://dx.doi.org/10.1103/PhysRevE.84.046112"}, {"title": "Risks of Large Portfolios", "summary": "Estimating and assessing the risk of a large portfolio is an important topic\nin financial econometrics and risk management. The risk is often estimated by a\nsubstitution of a good estimator of the volatility matrix. However, the\naccuracy of such a risk estimator for large portfolios is largely unknown, and\na simple inequality in the previous literature gives an infeasible upper bound\nfor the estimation error. In addition, numerical studies illustrate that this\nupper bound is very crude. In this paper, we propose factor-based risk\nestimators under a large amount of assets, and introduce a high-confidence\nlevel upper bound (H-CLUB) to assess the accuracy of the risk estimation. The\nH-CLUB is constructed based on three different estimates of the volatility\nmatrix: sample covariance, approximate factor model with known factors, and\nunknown factors (POET, Fan, Liao and Mincheva, 2013). For the first time in the\nliterature, we derive the limiting distribution of the estimated risks in high\ndimensionality. Our numerical results demonstrate that the proposed upper\nbounds significantly outperform the traditional crude bounds, and provide\ninsightful assessment of the estimation of the portfolio risks. In addition,\nour simulated results quantify the relative error in the risk estimation, which\nis usually negligible using 3-month daily data. Finally, the proposed methods\nare applied to an empirical study.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1302.0926v1", "link": "http://arxiv.org/abs/1302.0926v1"}, {"title": "Realtime market microstructure analysis: online Transaction Cost\n  Analysis", "summary": "Motivated by the practical challenge in monitoring the performance of a large\nnumber of algorithmic trading orders, this paper provides a methodology that\nleads to automatic discovery of the causes that lie behind a poor trading\nperformance. It also gives theoretical foundations to a generic framework for\nreal-time trading analysis. Academic literature provides different ways to\nformalize these algorithms and show how optimal they can be from a\nmean-variance, a stochastic control, an impulse control or a statistical\nlearning viewpoint. This paper is agnostic about the way the algorithm has been\nbuilt and provides a theoretical formalism to identify in real-time the market\nconditions that influenced its efficiency or inefficiency. For a given set of\ncharacteristics describing the market context, selected by a practitioner, we\nfirst show how a set of additional derived explanatory factors, called anomaly\ndetectors, can be created for each market order. We then will present an online\nmethodology to quantify how this extended set of factors, at any given time,\npredicts which of the orders are underperforming while calculating the\npredictive power of this explanatory factor set. Armed with this information,\nwhich we call influence analysis, we intend to empower the order monitoring\nuser to take appropriate action on any affected orders by re-calibrating the\ntrading algorithms working the order through new parameters, pausing their\nexecution or taking over more direct trading control. Also we intend that use\nof this method in the post trade analysis of algorithms can be taken advantage\nof to automatically adjust their trading action.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1302.6363v2", "link": "http://arxiv.org/abs/1302.6363v2"}, {"title": "Effect of Intellectual Property Policy on the Speed of Technological\n  Advancement", "summary": "In this paper, the agent-based modeling is employed to model the effect of\nintellectual property policy at the speed of technological advancement. Every\nagent has inborn preferences towards investing their capital into independent\ntechnological development, innovation appropriation, and production. The\nrelative cost of appropriation compared to independent development is chosen as\na measure of strictness of intellectual property protection. We vary this\nparameter and look at the performance of agents with different preferences and\noverall technological progress. In general, it is found that in the specific\nsetting considered with stronger intellectual property protection leads to\nfaster progress.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1706.04518v1", "link": "http://arxiv.org/abs/1706.04518v1"}, {"title": "Optimal ETF Selection for Passive Investing", "summary": "This paper considers the problem of isolating a small number of exchange\ntraded funds (ETFs) that suffice to capture the fundamental dimensions of\nvariation in U.S. financial markets. First, the data is fit to a vector-valued\nBayesian regression model, which is a matrix-variate generalization of the well\nknown stochastic search variable selection (SSVS) of George and McCulloch\n(1993). ETF selection is then performed using the decoupled shrinkage and\nselection (DSS) procedure described in Hahn and Carvalho (2015), adapted in two\nways: to the vector-response setting and to incorporate stochastic covariates.\nThe selected set of ETFs is obtained under a number of different penalty and\nmodeling choices. Optimal portfolios are constructed from selected ETFs by\nmaximizing the Sharpe ratio posterior mean, and they are compared to the\n(unknown) optimal portfolio based on the full Bayesian model. We compare our\nselection results to popular ETF advisor Wealthfront.com. Additionally, we\nconsider selecting ETFs by modeling a large set of mutual funds.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1510.03385v2", "link": "http://arxiv.org/abs/1510.03385v2"}, {"title": "Pricing Fixed-Income Securities in an Information-Based Framework", "summary": "In this paper we introduce a class of information-based models for the\npricing of fixed-income securities. We consider a set of continuous- time\ninformation processes that describe the flow of information about market\nfactors in a monetary economy. The nominal pricing kernel is at any given time\nassumed to be given by a function of the values of information processes at\nthat time. By use of a change-of-measure technique we derive explicit\nexpressions for the price processes of nominal discount bonds, and deduce the\nassociated dynamics of the short rate of interest and the market price of risk.\nThe interest rate positivity condition is expressed as a differential\ninequality. We proceed to the modelling of the price-level, which at any given\ntime is also taken to be a function of the values of the information processes\nat that time. A simple model for a stochastic monetary economy is introduced in\nwhich the prices of nominal discount bonds and inflation-linked notes can be\nexpressed in terms of aggregate consumption and the liquidity benefit generated\nby the money supply.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0911.1610v2", "link": "http://arxiv.org/abs/0911.1610v2"}, {"title": "Bartlett's delta in the SABR model", "summary": "We refine the analysis of hedging strategies for options under the SABR model\ncarried out in [2]. In particular, we provide a theoretical justification of\nthe empirical observation made in [2] that the modified delta (\"Bartlett's\ndelta\") introduced there provides a more accurate and robust hedging strategy\nthan the conventional SABR delta hedge.", "category": ["q-fin.CP", "q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1704.03110v2", "link": "http://arxiv.org/abs/1704.03110v2"}, {"title": "Dynamical Trading Mechanism in Limit Order Markets", "summary": "This work's purpose is to understand the dynamics of limit order books in\norder-driven markets. We try to illustrate a dynamical trading mechanism\nattached to the microstructure of limit order markets. We capture the iterative\nnature of trading processes, which is critical in the dynamics of bid-ask pairs\nand the switching laws between different traders' types and their orders. In\ngeneral, after introducing the atomic trading scheme, we study a general\niterated trading process in both combinatorial and stochastic ways, and state a\nfew results on the stability of a dynamical trading system. We also study the\ncontrolled dynamics of the spread and the mid-price in an iterated trading\nsystem, when their movements, generated from the dynamics of bid-ask pairs, are\nassumed to be restricted within some extremely small ranges.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1303.3133v1", "link": "http://dx.doi.org/10.3233/AF-13027"}, {"title": "Keynesian Economics After All", "summary": "It is demonstrated that the US economy has on the long-term in reality been\ngoverned by the Keynesian approach to economics independent of the current\nofficial economical policy. This is done by calculating the two-point\ncorrelation function between the fluctuations of the DJIA and the US public\ndebt. We find that the origin of this condition is mainly related to the wars\nthat the USA has fought during the time period investigated. Wars mean a large\ninflux of public money into the economy, thus as a consequence creating a\nsignificant economical upturn in the DJIA. A reason for this straight-cut\nresult of our analysis, is that very few wars have been fought on US-territory\nand those that have, were in the 18th century, when the partial destruction of\ncities, factories, railways and so on, was more limited and with less effect on\nthe over-all economy.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1107.3095v1", "link": "http://arxiv.org/abs/1107.3095v1"}, {"title": "Reinforcement Learning in Economics and Finance", "summary": "Reinforcement learning algorithms describe how an agent can learn an optimal\naction policy in a sequential decision process, through repeated experience. In\na given environment, the agent policy provides him some running and terminal\nrewards. As in online learning, the agent learns sequentially. As in\nmulti-armed bandit problems, when an agent picks an action, he can not infer\nex-post the rewards induced by other action choices. In reinforcement learning,\nhis actions have consequences: they influence not only rewards, but also future\nstates of the world. The goal of reinforcement learning is to find an optimal\npolicy -- a mapping from the states of the world to the set of actions, in\norder to maximize cumulative reward, which is a long term strategy. Exploring\nmight be sub-optimal on a short-term horizon but could lead to optimal\nlong-term ones. Many problems of optimal control, popular in economics for more\nthan forty years, can be expressed in the reinforcement learning framework, and\nrecent advances in computational science, provided in particular by deep\nlearning algorithms, can be used by economists in order to solve complex\nbehavioral problems. In this article, we propose a state-of-the-art of\nreinforcement learning techniques, and present applications in economics, game\ntheory, operation research and finance.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/2003.10014v1", "link": "http://arxiv.org/abs/2003.10014v1"}, {"title": "Equilibrium pricing under relative performance concerns", "summary": "We investigate the effects of the social interactions of a finite set of\nagents on an equilibrium pricing mechanism. A derivative written on\nnon-tradable underlyings is introduced to the market and priced in an\nequilibrium framework by agents who assess risk using convex dynamic risk\nmeasures expressed by Backward Stochastic Differential Equations (BSDE). Each\nagent is not only exposed to financial and non-financial risk factors, but she\nalso faces performance concerns with respect to the other agents.\n  Within our proposed model we prove the existence and uniqueness of an\nequilibrium whose analysis involves systems of fully coupled multi-dimensional\nquadratic BSDEs. We extend the theory of the representative agent by showing\nthat a non-standard aggregation of risk measures is possible via\nweighted-dilated infimal convolution. We analyze the impact of the problem's\nparameters on the pricing mechanism, in particular how the agents' performance\nconcern rates affect prices and risk perceptions. In extreme situations, we\nfind that the concern rates destroy the equilibrium while the risk measures\nthemselves remain stable.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1511.04218v3", "link": "http://arxiv.org/abs/1511.04218v3"}, {"title": "Extracting the multi-timescale activity patterns of online financial\n  markets", "summary": "Online financial markets can be represented as complex systems where trading\ndynamics can be captured and characterized at different resolutions and time\nscales. In this work, we develop a methodology based on non-negative tensor\nfactorization (NTF) aimed at extracting and revealing the multi-timescale\ntrading dynamics governing online financial systems. We demonstrate the\nadvantage of our strategy first using synthetic data, and then on real-world\ndata capturing all interbank transactions (over a million) occurred in an\nItalian online financial market (e-MID) between 2001 and 2015. Our results\ndemonstrate how NTF can uncover hidden activity patterns that characterize\ngroups of banks exhibiting different trading strategies (normal vs. early vs.\nflash trading, etc.). We further illustrate how our methodology can reveal\n\"crisis modalities\" in trading triggered by endogenous and exogenous system\nshocks: as an example, we reveal and characterize trading anomalies in the\nmidst of the 2008 financial crisis.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1802.07405v2", "link": "http://dx.doi.org/10.1038/s41598-018-29537-w"}, {"title": "G-Doob-Meyer Decomposition and its Application in Bid-Ask Pricing for\n  American Contingent Claim Under Knightian Uncertainty", "summary": "The target of this paper is to establish the bid-ask pricing frame work for\nthe American contingent claims against risky assets with G-asset price systems\n(see \\cite{Chen2013b}) on the financial market under Knight uncertainty. First,\nwe prove G-Dooby-Meyer decomposition for G-supermartingale. Furthermore, we\nconsider bid-ask pricing American contingent claims under Knight uncertain, by\nusing G-Dooby-Meyer decomposition, we construct dynamic superhedge stragies for\nthe optimal stopping problem, and prove that the value functions of the optimal\nstopping problems are the bid and ask prices of the American contingent claims\nunder Knight uncertain. Finally, we consider a free boundary problem, prove the\nstrong solution existence of the free boundary problem, and derive that the\nvalue function of the optimal stopping problem is equivalent to the strong\nsolution to the free boundary problem.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1401.0677v1", "link": "http://arxiv.org/abs/1401.0677v1"}, {"title": "An Investigation of the Structural Characteristics of the Indian IT\n  Sector and the Capital Goods Sector: An Application of the R Programming in\n  Time Series Decomposition and Forecasting", "summary": "Time series analysis and forecasting of stock market prices has been a very\nactive area of research over the last two decades. Availability of extremely\nfast and parallel architecture of computing and sophisticated algorithms has\nmade it possible to extract, store, process and analyze high volume stock\nmarket time series data very efficiently. In this paper, we have used time\nseries data of the two sectors of the Indian economy: Information Technology\nand Capital Goods for the period January 2009 till April 2016 and have studied\nthe relationships of these two time series with the time series of DJIA index,\nNIFTY index and the US Dollar to Indian Rupee exchange rate. We establish by\ngraphical and statistical tests that while the IT sector of India has a strong\nassociation with DJIA index and the Dollar to Rupee exchange rate, the Indian\nCG sector exhibits a strong association with the NIFTY index. We contend that\nthese observations corroborate our hypotheses that the Indian IT sector is\nstrongly coupled with the world economy whereas the CG sector of India reflects\ninternal economic growth of India. We also present several models of regression\nbetween the time series which exhibit strong association among them. The\neffectiveness of these models have been demonstrated by very low values of\ntheir forecasting errors.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1706.07821v1", "link": "http://arxiv.org/abs/1706.07821v1"}, {"title": "Dancing with Donald: Polarity in the 2016 Presidential Election", "summary": "In almost every election cycle, the validity of the United States Electoral\nCollege is brought into question. The 2016 Presidential Election again brought\nup the issue of a candidate winning the popular vote but not winning the\nElectoral College, with Hillary Clinton receiving close to three million more\nvotes than Donald Trump. However, did the popular vote actually determine the\nmost liked candidate in the election? In this paper, we demonstrate that\ndifferent voting policies can alter which candidate is elected. Additionally,\nwe explore the trade-offs between each of these mechanisms. Finally, we\nintroduce two novel mechanisms with the intent of electing the least polarizing\ncandidate.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.07542v1", "link": "http://arxiv.org/abs/1901.07542v1"}, {"title": "A Nonlinear Super-Exponential Rational Model of Speculative Financial\n  Bubbles", "summary": "Keeping a basic tenet of economic theory, rational expectations, we model the\nnonlinear positive feedback between agents in the stock market as an interplay\nbetween nonlinearity and multiplicative noise. The derived hyperbolic\nstochastic finite-time singularity formula transforms a Gaussian white noise\ninto a rich time series possessing all the stylized facts of empirical prices,\nas well as accelerated speculative bubbles preceding crashes. We use the\nformula to invert the two years of price history prior to the recent crash on\nthe Nasdaq (april 2000) and prior to the crash in the Hong Kong market\nassociated with the Asian crisis in early 1994. These complex price dynamics\nare captured using only one exponent controlling the explosion, the variance\nand mean of the underlying random walk. This offers a new and powerful\ndetection tool of speculative bubbles and herding behavior.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0104341v2", "link": "http://dx.doi.org/10.1142/S0129183102003085"}, {"title": "Dynamic portfolio strategy using clustering approach", "summary": "The problem of portfolio optimization is one of the most important issues in\nasset management. This paper proposes a new dynamic portfolio strategy based on\nthe time-varying structures of MST networks in Chinese stock markets, where the\nmarket condition is further considered when using the optimal portfolios for\ninvestment. A portfolio strategy comprises two stages: selecting the portfolios\nby choosing central and peripheral stocks in the selection horizon using five\ntopological parameters, i.e., degree, betweenness centrality, distance on\ndegree criterion, distance on correlation criterion and distance on distance\ncriterion, then using the portfolios for investment in the investment horizon.\nThe optimal portfolio is chosen by comparing central and peripheral portfolios\nunder different combinations of market conditions in the selection and\ninvestment horizons. Market conditions in our paper are identified by the\nratios of the number of trading days with rising index or the sum of the\namplitudes of the trading days with rising index to the total number of trading\ndays. We find that central portfolios outperform peripheral portfolios when the\nmarket is under a drawup condition, or when the market is stable or drawup in\nthe selection horizon and is under a stable condition in the investment\nhorizon. We also find that the peripheral portfolios gain more than central\nportfolios when the market is stable in the selection horizon and is drawdown\nin the investment horizon. Empirical tests are carried out based on the optimal\nportfolio strategy. Among all the possible optimal portfolio strategy based on\ndifferent parameters to select portfolios and different criteria to identify\nmarket conditions, $65\\%$ of our optimal portfolio strategies outperform the\nrandom strategy for the Shanghai A-Share market and the proportion is $70\\%$\nfor the Shenzhen A-Share market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1608.03058v1", "link": "http://dx.doi.org/10.1371/journal.pone.0169299"}, {"title": "Valuation of path-dependent American options using a Monte Carlo\n  approach", "summary": "It is shown how to obtain accurate values for American options using Monte\nCarlo simulation. The main feature of the novel algorithm consists of tracking\nthe boundary between exercise and hold regions via optimization of a certain\npayoff function. We compare estimates from simulation for some types of claims\nwith results from binomial tree calculations and find very good agreement. The\nnovel method allows to calculate so far untractable path-dependent option\nvalues.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/math/9801057v1", "link": "http://arxiv.org/abs/math/9801057v1"}, {"title": "Identification of Conduit Countries and Community Structures in the\n  Withholding Tax Networks", "summary": "Due to economic globalization, each country's economic law, including tax\nlaws and tax treaties, has been forced to work as a single network. However,\neach jurisdiction (country or region) has not made its economic law under the\nassumption that its law functions as an element of one network, so it has\nbrought unexpected results. We thought that the results are exactly\ninternational tax avoidance. To contribute to the solution of international tax\navoidance, we tried to investigate which part of the network is vulnerable.\nSpecifically, focusing on treaty shopping, which is one of international tax\navoidance methods, we attempt to identified which jurisdiction are likely to be\nused for treaty shopping from tax liabilities and the relationship between\njurisdictions which are likely to be used for treaty shopping and others. For\nthat purpose, based on withholding tax rates imposed on dividends, interest,\nand royalties by jurisdictions, we produced weighted multiple directed graphs,\ncomputed the centralities and detected the communities. As a result, we\nclarified the jurisdictions that are likely to be used for treaty shopping and\npointed out that there are community structures. The results of this study\nsuggested that fewer jurisdictions need to introduce more regulations for\nprevention of treaty abuse worldwide.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1806.00799v1", "link": "http://arxiv.org/abs/1806.00799v1"}, {"title": "Risk diversification: a study of persistence with a filtered\n  correlation-network approach", "summary": "The evolution with time of the correlation structure of equity returns is\nstudied by means of a filtered network approach investigating persistences and\nrecurrences and their implications for risk diversification strategies. We\nbuild dynamically Planar Maximally Filtered Graphs from the correlation\nstructure over a rolling window and we study the persistence of the associated\nDirected Bubble Hierarchical Tree (DBHT) clustering structure. We observe that\nthe DBHT clustering structure is quite stable during the early 2000' becoming\ngradually less persistent before the unfolding of the 2007-2008 crisis. The\ncorrelation structure eventually recovers persistence in the aftermath of the\ncrisis settling up a new phase, distinct from the pre-cysts structure, where\nthe market structure is less related to industrial sector activity. Notably, we\nobserve that - presently - the correlation structure is loosing again\npersistence indicating the building-up of another, different, phase. Such\ndynamical changes in persistence and their occurrence at the unfolding of\nfinancial crises rises concerns about the effectiveness of correlation-based\nportfolio management tools for risk diversification.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1410.5621v1", "link": "http://arxiv.org/abs/1410.5621v1"}, {"title": "Happy family of stable marriages", "summary": "Some aspects of the problem of stable marriage are discussed. There are two\ndistinguished marriage plans: the fully transferable case, where money can be\ntransferred between the participants, and the fully non transferable case where\neach participant has its own rigid preference list regarding the other gender.\nWe continue to discuss intermediate partial transferable cases. Partial\ntransferable plans can be approached as either special cases of cooperative\ngames using the notion of a core, or as a generalization of the cyclical\nmonotonicity property of the fully transferable case (fake promises). We shall\nintroduced these two approaches, and prove the existence of stable marriage for\nthe fully transferable and non-transferable plans.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1805.06687v1", "link": "http://arxiv.org/abs/1805.06687v1"}, {"title": "Removing noise from correlations in multivariate stock price data", "summary": "This paper examines the applicability of Random Matrix Theory to portfolio\nmanagement in finance. Starting from a group of normally distributed stochastic\nprocesses with given correlations we devise an algorithm for removing noise\nfrom the estimator of correlations constructed from measured time series. We\nthen apply this algorithm to historical time series for the Standard and Poor's\n500 index. We discuss to what extent the noise can be removed and whether the\nresulting underlying correlations are sufficiently accurate for portfolio\nmanagement purposes.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0403177v1", "link": "http://arxiv.org/abs/cond-mat/0403177v1"}, {"title": "Classifying economics for the common good: Connecting sustainable\n  development goals to JEL codes", "summary": "How does economics research help in solving societal challenges? This brief\nnote sheds additional light on this question by providing ways to connect\nJournal of Economic Literature (JEL) codes and Sustainable Development Goals\n(SDGs) of the United Nations. These simple linkages illustrate that the themes\nof SDGs have corresponding JEL classification codes. As the mappings presented\nhere are necessarily imperfect and incomplete, there is plenty of room for\nimprovements. In an ideal world, there would be a JEL classification system for\nSDGs, a separate JEL code for each of the 17 SDGs.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.04384v1", "link": "http://arxiv.org/abs/2004.04384v1"}, {"title": "The intensity of the random variable intercept in the sector of negative\n  probabilities", "summary": "We consider properties of the measurement intensity $\\rho$ of a random\nvariable for which the probability density function represented by the\ncorresponding Wigner function attains negative values on a part of the domain.\nWe consider a simple economic interpretation of this problem. This model is\nused to present the applicability of the method to the analysis of the negative\nprobability on markets where there are anomalies in the law of supply and\ndemand (e.g. Giffen's goods). It turns out that the new conditions to optimize\nthe intensity $\\rho$ require a new strategy. We propose a strategy (so-called\n$\\grave{a}$ rebours strategy) based on the fixed point method and explore its\neffectiveness.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1503.07495v1", "link": "http://arxiv.org/abs/1503.07495v1"}, {"title": "Momentum universe shrinkage effect in price momentum", "summary": "We test the price momentum effect in the Korean stock markets under the\nmomentum universe shrinkage to subuniverses of the KOSPI 200. Performance of\nthe momentum strategy is not homogeneous with respect to change of the momentum\nuniverse. It is found that some submarkets generate the higher momentum returns\nthan other universes do but large-size companies such as the KOSPI 50\ncomponents hinder the performance of the momentum strategy. The observation is\nalso cross-checked with size portfolios and liquidity portfolios. Transactions\nby investor groups, in particular, the trading patterns by foreign investors\ncan be a source of the momentum universe shrinkage effect in the momentum\nreturns.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1211.6517v1", "link": "http://arxiv.org/abs/1211.6517v1"}, {"title": "Precisamos de uma Contabilidade Ambiental para as \"Amaz\u00f4nias\"\n  Paraense?", "summary": "This paper has the following objectives: to understand the concepts of\nEnvironmental Accounting in Brazil; Make criticisms and propositions anchored\nin the reality or demand of environmental accounting for Amazonia Paraense. The\nmethodological strategy was a critical analysis of Ferreira's books (2007);\nRibeiro (2010) and Tinoco and Kraemer (2011) using their correlation with the\nscientific production of authors discussing the Paraense Amazon, besides our\nexperience as researchers of this territory. As a result, we created three\nsections: one for understanding the current constructs of environmental\naccounting, one for criticism and one for propositions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.06499v1", "link": "http://arxiv.org/abs/1910.06499v1"}, {"title": "From characteristic functions to implied volatility expansions", "summary": "For any strictly positive martingale $S = \\exp(X)$ for which $X$ has a\ncharacteristic function, we provide an expansion for the implied volatility.\nThis expansion is explicit in the sense that it involves no integrals, but only\npolynomials in the log strike. We illustrate the versatility of our expansion\nby computing the approximate implied volatility smile in three well-known\nmartingale models: one finite activity exponential L\\'evy model (Merton), one\ninfinite activity exponential L\\'evy model (Variance Gamma), and one stochastic\nvolatility model (Heston). Finally, we illustrate how our expansion can be used\nto perform a model-free calibration of the empirically observed implied\nvolatility surface.", "category": ["q-fin.CP", "q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1207.0233v5", "link": "http://arxiv.org/abs/1207.0233v5"}, {"title": "On return rate implied by behavioural present value", "summary": "The future value of a security is described as a random variable.\nDistribution of this random variable is the formal image of risk uncertainty.\nOn the other side, any present value is defined as a value equivalent to the\ngiven future value. This equivalence relationship is a subjective. Thus\nfollows, that present value is described as a fuzzy number, which is depend on\nthe investor's susceptibility to behavioural factors. All above reasons imply,\nthat return rate is given as a fuzzy probabilistic set. The basic properties of\nsuch image of return rate are studied. At the last the set of effective\nsecurities is distinguished as a fuzzy set.", "category": ["q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1302.0538v1", "link": "http://arxiv.org/abs/1302.0538v1"}, {"title": "Profitability of simple technical trading rules of Chinese stock\n  exchange indexes", "summary": "Although technical trading rules have been widely used by practitioners in\nfinancial markets, their profitability still remains controversial. We here\ninvestigate the profitability of moving average (MA) and trading range break\n(TRB) rules by using the Shanghai Stock Exchange Composite Index (SHCI) from\nMay 21, 1992 through December 31, 2013 and Shenzhen Stock Exchange Composite\nIndex (SZCI) from April 3, 1991 through December 31, 2013. The $t$-test is\nadopted to check whether the mean returns which are conditioned on the trading\nsignals are significantly different from unconditioned returns and whether the\nmean returns conditioned on the buy signals are significantly different from\nthe mean returns conditioned on the sell signals. We find that TRB rules\noutperform MA rules and short-term variable moving average (VMA) rules\noutperform long-term VMA rules. By applying White's Reality Check test and\naccounting for the data snooping effects, we find that the best trading rule\noutperforms the buy-and-hold strategy when transaction costs are not taken into\nconsideration. Once transaction costs are included, trading profits will be\neliminated completely. Our analysis suggests that simple trading rules like MA\nand TRB cannot beat the standard buy-and-hold strategy for the Chinese stock\nexchange indexes.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1504.04254v1", "link": "http://dx.doi.org/10.1016/j.physa.2015.07.032"}, {"title": "Multiple defaults and contagion risks", "summary": "We study multiple defaults where the global market information is modelled as\nprogressive enlargement of filtrations. We shall provide a general pricing\nformula by establishing a relationship between the enlarged filtration and the\nreference default-free filtration in the random measure framework. On each\ndefault scenario, the formula can be interpreted as a Radon-Nikodym derivative\nof random measures. The contagion risks are studied in the multi-defaults\nsetting where we consider the optimal investment problem in a contagion risk\nmodel and show that the optimization can be effectuated in a recursive manner\nwith respect to the default-free filtration.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/0912.3132v1", "link": "http://arxiv.org/abs/0912.3132v1"}, {"title": "Interpreting Economic Complexity", "summary": "Two network measures known as the Economic Complexity Index (ECI) and Product\nComplexity Index (PCI) have provided important insights into patterns of\neconomic development. We show that the ECI and PCI are equivalent to a spectral\nclustering algorithm that partitions a similarity graph into two parts. The\nmeasures are also related to various dimensionality reduction methods and can\nbe interpreted as vectors that determine distances between nodes based on their\nsimilarity. Our results shed a new light on the ECI's empirical success in\nexplaining cross-country differences in GDP/capita and economic growth, which\nis often linked to the diversity of country export baskets. In fact, countries\nwith high (low) ECI tend to specialize in high (low) PCI products. We also find\nthat the ECI and PCI uncover economically informative specialization patterns\nacross US states and UK regions.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1711.08245v3", "link": "http://arxiv.org/abs/1711.08245v3"}, {"title": "Effects of Tobin Taxes in Minority Game markets", "summary": "We show that the introduction of Tobin taxes in agent-based models of\ncurrency markets can lead to a reduction of speculative trading and reduce the\nmagnitude of exchange rate fluctuations at intermediate tax rates. In this\nregime revenues for the market maker obtained from speculators are maximal. We\nhere focus on Minority Game models of markets, which are accessible by exact\ntechniques from statistical mechanics. Results are supported by computer\nsimulations. Our findings suggest that at finite systems sizes the effect is\nmost pronounced in a critical region around the phase transition of the\ninfinite system, but much weaker if the market is operating far from\ncriticality and does not exhibit anomalous fluctuations.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0603134v1", "link": "http://arxiv.org/abs/cond-mat/0603134v1"}, {"title": "Yield Uncertainty and Strategic Formation of Supply Chain Networks", "summary": "How does supply uncertainty affect the structure of supply chain networks? To\nanswer this question we consider a setting where retailers and suppliers must\nestablish a costly relationship with each other prior to engaging in trade.\nSuppliers, with uncertain yield, announce wholesale prices, while retailers\nmust decide which suppliers to link to based on their wholesale prices.\nSubsequently, retailers compete with each other in Cournot fashion to sell the\nacquired supply to consumers. We find that in equilibrium retailers concentrate\ntheir links among too few suppliers, i.e., there is insufficient\ndiversification of the supply base. We find that either reduction of supply\nvariance or increase of mean supply, increases a supplier's profit. However,\nthese two ways of improving service have qualitatively different effects on\nwelfare: improvement of the expected supply by a supplier makes everyone better\noff, whereas improvement of supply variance lowers consumer surplus.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1907.09943v1", "link": "http://arxiv.org/abs/1907.09943v1"}, {"title": "On Agents and Equilibria", "summary": "This essay discusses the advantages of a probabilistic agent-based approach\nto questions in theoretical economics, from the nature of economic agents, to\nthe nature of the equilibria supported by their interactions. One idea we\npropose is that \"agents\" are meta-individual, hierarchically structured\nobjects, that include as irreducible components groupings of different\ndimensions. We also explore the effects of non-ergodicity, by constructing a\nsimple stochastic model for the contingent nature of economic interactions.", "category": ["q-fin.GN", "math.PR"], "id": "http://arxiv.org/abs/1311.0414v1", "link": "http://arxiv.org/abs/1311.0414v1"}, {"title": "Exploring cities of Central and Eastern Europe within transnational\n  company networks: the core-periphery effect", "summary": "After the Fall of the Berlin Wall, Central Eastern European cities (CEEc)\nintegrated the globalized world, characterized by a core-periphery structure\nand hierarchical interactions between cities. This article gives evidence of\nthe core-periphery effect on CEEc in 2013 in terms of differentiation of their\nurban functions after 1989. We investigate the position of all CEEc in\ntransnational company networks in 2013. We examine the orientations of\nownership links between firms, the spatial patterns of these networks and the\nspecialization of firms in CEEc involved. The major contribution of this paper\nconsists in giving proof of a core-periphery structure within Central Eastern\nEurope itself, but also of the diffusion of innovations theory as not only\nlarge cities, but also medium-sized and small ones are part of the\nmultinational networks of firms. These findings provide significant insights\nfor the targeting of specific regional policies of the European Union.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.14652v1", "link": "http://arxiv.org/abs/1910.14652v1"}, {"title": "Economic information from Smart Meter: Nexus Between Demand Profile and\n  Electricity Retail Price Between Demand Profile and Electricity Retail Price", "summary": "In this paper, we demonstrate that a consumer's marginal system impact is\nonly determined by their demand profile rather than their demand level. Demand\nprofile clustering is identical to cluster consumers according to their\nmarginal impacts on system costs. A profile-based uniform-rate price is\neconomically efficient as real-time pricing. We develop a criteria system to\nevaluate the economic efficiency of an implemented retail price scheme in a\ndistribution system by comparing profile clustering and daily-average\nclustering. Our criteria system can examine the extent of a retail price\nscheme's inefficiency even without information about the distribution system's\ndaily cost structure. We analyze data from a real distribution system in China.\nIn this system, targeting each consumer's high-impact days is more efficient\nthan target high-impact consumers.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1701.02646v1", "link": "http://arxiv.org/abs/1701.02646v1"}, {"title": "Coherence and incoherence collective behavior in financial market", "summary": "Financial markets have been extensively studied as highly complex evolving\nsystems. In this paper, we quantify financial price fluctuations through a\ncoupled dynamical system composed of phase oscillators. We find a Financial\nCoherence and Incoherence (FCI) coexistence collective behavior emerges as the\nsystem evolves into the stable state, in which the stocks split into two\ngroups: one is represented by coherent, phase-locked oscillators, the other is\ncomposed of incoherent, drifting oscillators. It is demonstrated that the size\nof the coherent stock groups fluctuates during the economic periods according\nto real-world financial instabilities or shocks. Further, we introduce the\ncoherent characteristic matrix to characterize the involvement dynamics of\nstocks in the coherent groups. Clustering results on the matrix provides a\nnovel manifestation of the correlations among stocks in the economic periods.\nOur analysis for components of the groups is consistent with the Global\nIndustry Classification Standard (GICS) classification and can also figure out\nfeatures for newly developed industries. These results can provide potentially\nimplications on characterizing inner dynamical structure of financial markets\nand making optimal investment tragedies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1605.02283v1", "link": "http://dx.doi.org/10.1209/0295-5075/112/28002"}, {"title": "Strategic Insights From Playing the Quantum Tic-Tac-Toe", "summary": "In this paper, we perform a minimalistic quantization of the classical game\nof tic-tac-toe, by allowing superpositions of classical moves. In order for the\nquantum game to reduce properly to the classical game, we require legal quantum\nmoves to be orthogonal to all previous moves. We also admit interference\neffects, by squaring the sum of amplitudes over all moves by a player to\ncompute his or her occupation level of a given site. A player wins when the\nsums of occupations along any of the eight straight lines we can draw in the $3\n\\times 3$ grid is greater than three. We play the quantum tic-tac-toe first\nrandomly, and then deterministically, to explore the impact different opening\nmoves, end games, and different combinations of offensive and defensive\nstrategies have on the outcome of the game. In contrast to the classical\ntic-tac-toe, the deterministic quantum game does not always end in a draw. In\ncontrast also to most classical two-player games of no chance, it is possible\nfor Player 2 to win. More interestingly, we find that Player 1 enjoys an\noverwhelming quantum advantage when he opens with a quantum move, but loses\nthis advantage when he opens with a classical move. We also find the quantum\nblocking move, which consists of a weighted superposition of moves that the\nopponent could use to win the game, to be very effective in denying the\nopponent his or her victory. We then speculate what implications these results\nmight have on quantum information transfer and portfolio optimization.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1007.3601v1", "link": "http://dx.doi.org/10.1088/1751-8113/43/45/455304"}, {"title": "A Second Order Cumulant Spectrum Test That a Stochastic Process is\n  Strictly Stationary and a Step Toward a Test for Graph Signal Strict\n  Stationarity", "summary": "This article develops a statistical test for the null hypothesis of strict\nstationarity of a discrete time stochastic process in the frequency domain.\nWhen the null hypothesis is true, the second order cumulant spectrum is zero at\nall the discrete Fourier frequency pairs in the principal domain. The test uses\na window averaged sample estimate of the second order cumulant spectrum to\nbuild a test statistic with an asymptotic complex standard normal distribution.\nWe derive the test statistic, study the properties of the test and demonstrate\nits application using 137Cs gamma ray decay data. Future areas of research\ninclude testing for strict stationarity of graph signals, with applications in\nlearning convolutional neural networks on graphs, denoising, and inpainting.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1801.06727v2", "link": "http://arxiv.org/abs/1801.06727v2"}, {"title": "How volatilities nonlocal in time affect the price dynamics in complex\n  financial systems", "summary": "What is the dominating mechanism of the price dynamics in financial systems\nis of great interest to scientists. The problem whether and how volatilities\naffect the price movement draws much attention. Although many efforts have been\nmade, it remains challenging. Physicists usually apply the concepts and methods\nin statistical physics, such as temporal correlation functions, to study\nfinancial dynamics. However, the usual volatility-return correlation function,\nwhich is local in time, typically fluctuates around zero. Here we construct\ndynamic observables nonlocal in time to explore the volatility-return\ncorrelation, based on the empirical data of hundreds of individual stocks and\n25 stock market indices in different countries. Strikingly, the correlation is\ndiscovered to be non-zero, with an amplitude of a few percent and a duration of\nover two weeks. This result provides compelling evidence that past volatilities\nnonlocal in time affect future returns. Further, we introduce an agent-based\nmodel with a novel mechanism, that is, the asymmetric trading preference in\nvolatile and stable markets, to understand the microscopic origin of the\nvolatility-return correlation nonlocal in time.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1502.00824v1", "link": "http://dx.doi.org/10.1371/journal.pone.0118399"}, {"title": "Worst-Case Expected Shortfall with Univariate and Bivariate Marginals", "summary": "Worst-case bounds on the expected shortfall risk given only limited\ninformation on the distribution of the random variables has been studied\nextensively in the literature. In this paper, we develop a new worst-case bound\non the expected shortfall when the univariate marginals are known exactly and\nadditional expert information is available in terms of bivariate marginals.\nSuch expert information allows for one to choose from among the many possible\nparametric families of bivariate copulas. By considering a neighborhood of\ndistance $\\rho$ around the bivariate marginals with the Kullback-Leibler\ndivergence measure, we model the trade-off between conservatism in the\nworst-case risk measure and confidence in the expert information. Our bound is\ndeveloped when the only information available on the bivariate marginals forms\na tree structure in which case it is efficiently computable using convex\noptimization. For consistent marginals, as $\\rho$ approaches $\\infty$, the\nbound reduces to the comonotonic upper bound and as $\\rho$ approaches $0$, the\nbound reduces to the worst-case bound with bivariates known exactly. We also\ndiscuss extensions to inconsistent marginals and instances where the expert\ninformation which might be captured using other parameters such as\ncorrelations.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1701.04167v1", "link": "http://arxiv.org/abs/1701.04167v1"}, {"title": "Asymptotic arbitrage in the Heston model", "summary": "In the context of the Heston model, we establish a precise link between the\nset of equivalent martingale measures, the ergodicity of the underlying\nvariance process and the concept of asymptotic arbitrage proposed in\nKabanov-Kramkov and in Follmer-Schachermayer.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1302.6491v2", "link": "http://arxiv.org/abs/1302.6491v2"}, {"title": "Quantifying Health Shocks Over the Life Cycle", "summary": "We first show (1) the importance of investigating health expenditure process\nusing the order two Markov chain model, rather than the standard order one\nmodel, which is widely used in the literature. Markov chain of order two is the\nminimal framework that is capable of distinguishing those who experience a\ncertain health expenditure level for the first time from those who have been\nexperiencing that or other levels for some time. In addition, using the model\nwe show (2) that the probability of encountering a health shock first de-\ncreases until around age 10, and then increases with age, particularly, after\nage 40, (3) that health shock distributions among different age groups do not\ndiffer until their percentiles reach the median range, but that above the\nmedian the health shock distributions of older age groups gradually start to\nfirst-order dominate those of younger groups, and (4) that the persistency of\nhealth shocks also shows a U-shape in relation to age.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.08746v1", "link": "http://arxiv.org/abs/1801.08746v1"}, {"title": "Deep Neural Networks for Choice Analysis: Architectural Design with\n  Alternative-Specific Utility Functions", "summary": "Whereas deep neural network (DNN) is increasingly applied to choice analysis,\nit is challenging to reconcile domain-specific behavioral knowledge with\ngeneric-purpose DNN, to improve DNN's interpretability and predictive power,\nand to identify effective regularization methods for specific tasks. This study\ndesigns a particular DNN architecture with alternative-specific utility\nfunctions (ASU-DNN) by using prior behavioral knowledge. Unlike a fully\nconnected DNN (F-DNN), which computes the utility value of an alternative k by\nusing the attributes of all the alternatives, ASU-DNN computes it by using only\nk's own attributes. Theoretically, ASU-DNN can dramatically reduce the\nestimation error of F-DNN because of its lighter architecture and sparser\nconnectivity. Empirically, ASU-DNN has 2-3% higher prediction accuracy than\nF-DNN over the whole hyperparameter space in a private dataset that we\ncollected in Singapore and a public dataset in R mlogit package. The\nalternative-specific connectivity constraint, as a domain-knowledge-based\nregularization method, is more effective than the most popular generic-purpose\nexplicit and implicit regularization methods and architectural hyperparameters.\nASU-DNN is also more interpretable because it provides a more regular\nsubstitution pattern of travel mode choices than F-DNN does. The comparison\nbetween ASU-DNN and F-DNN can also aid in testing the behavioral knowledge. Our\nresults reveal that individuals are more likely to compute utility by using an\nalternative's own attributes, supporting the long-standing practice in choice\nmodeling. Overall, this study demonstrates that prior behavioral knowledge\ncould be used to guide the architecture design of DNN, to function as an\neffective domain-knowledge-based regularization method, and to improve both the\ninterpretability and predictive power of DNN in choice analysis.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.07481v1", "link": "http://arxiv.org/abs/1909.07481v1"}, {"title": "Market Model with Heterogeneous Buyers", "summary": "In market modeling, one often treats buyers as a homogeneous group. In this\npaper we consider buyers with heterogeneous preferences and products available\nin many variants. Such a framework allows us to successfully model various\nmarket phenomena. In particular, we investigate how is the vendor's behavior\ninfluenced by the amount of available information and by the presence of\ncorrelations in the system.", "category": ["q-fin.TR", "q-fin.GN"], "id": "http://arxiv.org/abs/0712.3350v1", "link": "http://dx.doi.org/10.1016/j.physa.2008.01.008"}, {"title": "Dialectical Roots for Interest Prohibition Theory", "summary": "It is argued that arguments for strict prohibition of interests must be based\non the use of arguments from authority. This is carried out by first making a\nsurvey of so-called dialectical roots for interest prohibition and then\ndemonstrating that for at least one important positive interest bearing\nfinancial product, the savings account with interest, its prohibition cannot be\ninferred from a match with any of these root cases.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1105.2900v1", "link": "http://arxiv.org/abs/1105.2900v1"}, {"title": "Conditional survival probabilities under partial information: a\n  recursive quantization approach with applications", "summary": "We consider a structural model where the survival/default state is observed\ntogether with a noisy version of the firm value process. This assumption makes\nthe model more realistic than most of the existing alternatives, but triggers\nimportant challenges related to the computation of conditional default\nprobabilities. In order to deal with general diffusions as firm value process,\nwe derive a numerical procedure based on the recursive quantization method to\napproximate it. Then, we investigate the error approximation induced by our\nprocedure. Eventually, numerical tests are performed to evaluate the\nperformance of the method, and an application is proposed to the pricing of CDS\noptions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1909.01970v1", "link": "http://arxiv.org/abs/1909.01970v1"}, {"title": "Emergent inequality and endogenous dynamics in a simple behavioral\n  macroeconomic model", "summary": "Standard macroeconomic models assume that households are rational in the\nsense that they are perfect utility maximizers, and explain economic dynamics\nin terms of shocks that drive the economy away from the stead-state. Here we\nbuild on a standard macroeconomic model in which a single rational\nrepresentative household makes a savings decision of how much to consume or\ninvest. In our model households are myopic boundedly rational heterogeneous\nagents embedded in a social network. From time to time each household updates\nits savings rate by copying the savings rate of its neighbor with the highest\nconsumption. If the updating time is short, the economy is stuck in a poverty\ntrap, but for longer updating times economic output approaches its optimal\nvalue, and we observe a critical transition to an economy with irregular\nendogenous oscillations in economic output, resembling a business cycle. In\nthis regime households divide into two groups: Poor households with low savings\nrates and rich households with high savings rates. Thus inequality and economic\ndynamics both occur spontaneously as a consequence of imperfect household\ndecision making. Our work here supports an alternative program of research that\nsubstitutes utility maximization for behaviorally grounded decision making.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.02155v1", "link": "http://arxiv.org/abs/1907.02155v1"}, {"title": "Hedging in Field Theory Models of the Term Structure", "summary": "We use path integrals to calculate hedge parameters and efficacy of hedging\nin a quantum field theory generalization of the Heath, Jarrow and Morton (HJM)\nterm structure model which parsimoniously describes the evolution of\nimperfectly correlated forward rates. We also calculate, within the model\nspecification, the effectiveness of hedging over finite periods of time. We use\nempirical estimates for the parameters of the model to show that a low\ndimensional hedge portfolio is quite effective.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/0209343v1", "link": "http://dx.doi.org/10.1103/PhysRevE.69.036130"}, {"title": "Brexit Risk Implied by the SABR Martingale Defect in the EUR-GBP Smile", "summary": "We construct a data-driven statistical indicator for quantifying the tail\nrisk perceived by the EURGBP option market surrounding Brexit-related events.\nWe show that under lognormal SABR dynamics this tail risk is closely related to\nthe so-called martingale defect and provide a closed-form expression for this\ndefect which can be computed by solving an inverse calibration problem. In\norder to cope with the the uncertainty which is inherent to this inverse\nproblem, we adopt a Bayesian statistical parameter estimation perspective. We\nprobe the resulting posterior densities with a combination of optimization and\nadaptive Markov chain Monte Carlo methods, thus providing a careful uncertainty\nestimation for all of the underlying parameters and the martingale defect\nindicator. Finally, to support the feasibility of the proposed method, we\nprovide a Brexit \"fever curve\" for the year 2019.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1912.05773v2", "link": "http://arxiv.org/abs/1912.05773v2"}, {"title": "A Consumer Behavior Based Approach to Multi-Stage EV Charging Station\n  Placement", "summary": "This paper presents a multi-stage approach to the placement of charging\nstations under the scenarios of different electric vehicle (EV) penetration\nrates. The EV charging market is modeled as the oligopoly. A consumer behavior\nbased approach is applied to forecast the charging demand of the charging\nstations using a nested logit model. The impacts of both the urban road network\nand the power grid network on charging station planning are also considered. At\neach planning stage, the optimal station placement strategy is derived through\nsolving a Bayesian game among the service providers. To investigate the\ninterplay of the travel pattern, the consumer behavior, urban road network,\npower grid network, and the charging station placement, a simulation platform\n(The EV Virtual City 1.0) is developed using Java on Repast.We conduct a case\nstudy in the San Pedro District of Los Angeles by importing the geographic and\ndemographic data of that region into the platform. The simulation results\ndemonstrate a strong consistency between the charging station placement and the\ntraffic flow of EVs. The results also reveal an interesting phenomenon that\nservice providers prefer clustering instead of spatial separation in this\noligopoly market.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.02135v1", "link": "http://dx.doi.org/10.1109/VTCSpring.2015.7145593"}, {"title": "Multifractal Flexibly Detrended Fluctuation Analysis", "summary": "Multifractal time series analysis is a approach that shows the possible\ncomplexity of the system. Nowadays, one of the most popular and the best\nmethods for determining multifractal characteristics is Multifractal Detrended\nFluctuation Analysis (MFDFA). However, it has some drawback. One of its core\nelements is detrending of the series. In the classical MFDFA a trend is\nestimated by fitting a polynomial of degree $m$ where $m=const$. We propose\nthat the degree $m$ of a polynomial was not constant ($m\\neq const$) and its\nselection was ruled by an established criterion. Taking into account the above\namendment, we examine the multifractal spectra both for artificial and\nreal-world mono- and the multifractal time series. Unlike classical MFDFA\nmethod, obtained singularity spectra almost perfectly reflects the theoretical\nresults and for real time series we observe a significant right side shift of\nthe spectrum.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1510.05115v1", "link": "http://dx.doi.org/10.5506/APhysPolB.46.1925"}, {"title": "Semi-parametric Dynamic Asymmetric Laplace Models for Tail Risk\n  Forecasting, Incorporating Realized Measures", "summary": "The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression\nmodel of Taylor (2017) is extended via incorporating a realized measure, to\ndrive the tail risk dynamics, as a potentially more efficient driver than daily\nreturns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte\nCarlo method are employed for estimation, whose properties are assessed and\ncompared via a simulation study; results favour the Bayesian approach, which is\nsubsequently employed in a forecasting study of seven market indices and two\nindividual assets. The proposed models are compared to a range of parametric,\nnon-parametric and semi-parametric models, including GARCH, Realized-GARCH and\nthe joint VaR and ES quantile regression models in Taylor (2017). The\ncomparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected\nShortfall forecasts, over a long forecast sample period that includes the\nglobal financial crisis in 2007-2008. The results favor the proposed models\nincorporating a realized measure, especially when employing the sub-sampled\nRealized Variance and the sub-sampled Realized Range.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1805.08653v1", "link": "http://arxiv.org/abs/1805.08653v1"}, {"title": "Dissection of Bitcoin's Multiscale Bubble History from January 2012 to\n  February 2018", "summary": "We present a detailed bubble analysis of the Bitcoin to US Dollar price\ndynamics from January 2012 to February 2018. We introduce a robust automatic\npeak detection method that classifies price time series into periods of\nuninterrupted market growth (drawups) and regimes of uninterrupted market\ndecrease (drawdowns). In combination with the Lagrange Regularisation Method\nfor detecting the beginning of a new market regime, we identify 3 major peaks\nand 10 additional smaller peaks, that have punctuated the dynamics of Bitcoin\nprice during the analyzed time period. We explain this classification of long\nand short bubbles by a number of quantitative metrics and graphs to understand\nthe main socio-economic drivers behind the ascent of Bitcoin over this period.\nThen, a detailed analysis of the growing risks associated with the three long\nbubbles using the Log-Periodic Power Law Singularity (LPPLS) model is based on\nthe LPPLS Confidence Indicators, defined as the fraction of qualified fits of\nthe LPPLS model over multiple time windows. Furthermore, for various fictitious\n'present' times $t_2$ before the crashes, we employ a clustering method to\ngroup the predicted critical times $t_c$ of the LPPLS fits over different time\nscales, where $t_c$ is the most probable time for the ending of the bubble.\nEach cluster is proposed as a plausible scenario for the subsequent Bitcoin\nprice evolution. We present these predictions for the three long bubbles and\nthe four short bubbles that our time scale of analysis was able to resolve.\nOverall, our predictive scheme provides useful information to warn of an\nimminent crash risk.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1804.06261v4", "link": "http://arxiv.org/abs/1804.06261v4"}, {"title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic\n  time series with applications to AR-ARCH type processes", "summary": "Shrinkage algorithms are of great importance in almost every area of\nstatistics due to the increasing impact of big data. Especially time series\nanalysis benefits from efficient and rapid estimation techniques such as the\nlasso. However, currently lasso type estimators for autoregressive time series\nmodels still focus on models with homoscedastic residuals. Therefore, an\niteratively reweighted adaptive lasso algorithm for the estimation of time\nseries models under conditional heteroscedasticity is presented in a\nhigh-dimensional setting. The asymptotic behaviour of the resulting estimator\nis analysed. It is found that the proposed estimation procedure performs\nsubstantially better than its homoscedastic counterpart. A special case of the\nalgorithm is suitable to compute the estimated multivariate AR-ARCH type models\nefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH\nor ARMA-GARCH are discussed. Finally, different simulation results and\napplications to electricity market data and returns of metal prices are shown.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1502.06557v2", "link": "http://dx.doi.org/10.1016/j.csda.2015.11.016"}, {"title": "Information ratio analysis of momentum strategies", "summary": "In the past 20 years, momentum or trend following strategies have become an\nestablished part of the investor toolbox. We introduce a new way of analyzing\nmomentum strategies by looking at the information ratio (IR, average return\ndivided by standard deviation). We calculate the theoretical IR of a momentum\nstrategy, and show that if momentum is mainly due to the positive\nautocorrelation in returns, IR as a function of the portfolio formation period\n(look-back) is very different from momentum due to the drift (average return).\nThe IR shows that for look-back periods of a few months, the investor is more\nlikely to tap into autocorrelation. However, for look-back periods closer to 1\nyear, the investor is more likely to tap into the drift. We compare the\nhistorical data to the theoretical IR by constructing stationary periods. The\nempirical study finds that there are periods/regimes where the autocorrelation\nis more important than the drift in explaining the IR (particularly pre-1975)\nand others where the drift is more important (mostly after 1975). We conclude\nour study by applying our momentum strategy to 100 plus years of the Dow-Jones\nIndustrial Average. We report damped oscillations on the IR for look-back\nperiods of several years and model such oscilations as a reversal to the mean\ngrowth rate.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1402.3030v2", "link": "http://arxiv.org/abs/1402.3030v2"}, {"title": "Power law for the calm-time interval of price changes", "summary": "In this paper, we describe a newly discovered statistical property of time\nseries data for daily price changes. We conducted quantitative investigation of\nthe {\\it calm-time intervals} of price changes for 800 companies listed in the\nTokyo Stock Exchange, and for the Nikkei 225 index over a 27-year period from\nJanuary 4, 1975 to December 28, 2001. A calm-time interval is defined as the\ninterval between two successive price changes above a fixed threshold. We found\nthat the calm-time interval distribution of price changes obeys a power law\ndecay. Furthermore, we show that the power-law exponent decreases monotonically\nwith respect to the threshold.\n  Keyword: econophysics, stock price changes, calm time interval, power-laws;\nPACS: 89.90.+n; 05.40.Df;", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0312560v2", "link": "http://dx.doi.org/10.1016/j.physa.2003.12.054"}, {"title": "Time scales involved in market emergence", "summary": "In addressing the question of the time scales characteristic for the market\nformation, we analyze high frequency tick-by-tick data from the NYSE and from\nthe German market. By using returns on various time scales ranging from seconds\nor minutes up to two days, we compare magnitude of the largest eigenvalue of\nthe correlation matrix for the same set of securities but for different time\nscales. For various sets of stocks of different capitalization (and the average\ntrading frequency), we observe a significant elevation of the largest\neigenvalue with increasing time scale. Our results from the correlation matrix\nstudy go in parallel with the so-called Epps effect. There is no unique\nexplanation of this effect and it seems that many different factors play a role\nhere. One of such factors is randomness in transaction moments for different\nstocks. Another interesting conclusion to be drawn from our results is that in\nthe contemporary markets the emergence of significant correlations occurs on\ntime scales much smaller than in the more distant history.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0311103v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.01.050"}, {"title": "Day of the Week Effect in biotechnology stocks: An Application of the\n  GARCH processes", "summary": "This study examines the presence of the day-of-the-week effect on daily\nreturns of biotechnology stocks over a 16-year period from January 2002 to\nDecember 2015. Using daily returns from the NASDAQ Biotechnology Index (NBI),\nwe find that the stock returns were the lowest on Mondays, and compared to the\nMondays the stock returns were significantly higher on Wednesdays, Thursdays,\nand Fridays. Moreover, the results from using the asymmetric GARCH processes\nreveal that momentum and small-firm effect were positively associated with the\nmarket risk-adjusted returns of the biotechnology stocks during this period.\nThe findings of our study suggest that active portfolio managers need to\nconsider the day of the week, momentum, and small-firm effect when making\ntrading decisions for biotechnology stocks.", "category": ["q-fin.ST", "q-fin.EC"], "id": "http://arxiv.org/abs/1701.07175v1", "link": "http://arxiv.org/abs/1701.07175v1"}, {"title": "Quasi-random Monte Carlo application in CGE systematic sensitivity\n  analysis", "summary": "The uncertainty and robustness of Computable General Equilibrium models can\nbe assessed by conducting a Systematic Sensitivity Analysis. Different methods\nhave been used in the literature for SSA of CGE models such as Gaussian\nQuadrature and Monte Carlo methods. This paper explores the use of Quasi-random\nMonte Carlo methods based on the Halton and Sobol' sequences as means to\nimprove the efficiency over regular Monte Carlo SSA, thus reducing the\ncomputational requirements of the SSA. The findings suggest that by using\nlow-discrepancy sequences, the number of simulations required by the regular MC\nSSA methods can be notably reduced, hence lowering the computational time\nrequired for SSA of CGE models.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1709.09755v1", "link": "http://arxiv.org/abs/1709.09755v1"}, {"title": "Mining the Automotive Industry: A Network Analysis of Corporate\n  Positioning and Technological Trends", "summary": "The digital transformation is driving revolutionary innovations and new\nmarket entrants threaten established sectors of the economy such as the\nautomotive industry. Following the need for monitoring shifting industries, we\npresent a network-centred analysis of car manufacturer web pages. Solely\nexploiting publicly-available information, we construct large networks from web\npages and hyperlinks. The network properties disclose the internal corporate\npositioning of the three largest automotive manufacturers, Toyota, Volkswagen\nand Hyundai with respect to innovative trends and their international outlook.\nWe tag web pages concerned with topics like e-mobility and environment or\nautonomous driving, and investigate their relevance in the network. Sentiment\nanalysis on individual web pages uncovers a relationship between page linking\nand use of positive language, particularly with respect to innovative trends.\nWeb pages of the same country domain form clusters of different size in the\nnetwork that reveal strong correlations with sales market orientation. Our\napproach maintains the web content's hierarchical structure imposed by the web\npage networks. It, thus, presents a method to reveal hierarchical structures of\nunstructured text content obtained from web scraping. It is highly transparent,\nreproducible and data driven, and could be used to gain complementary insights\ninto innovative strategies of firms and competitive landscapes, which would not\nbe detectable by the analysis of web content alone.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.10097v2", "link": "http://arxiv.org/abs/1912.10097v2"}, {"title": "Robust utility maximization in non-dominated models with 2BSDEs", "summary": "The problem of robust utility maximization in an incomplete market with\nvolatility uncertainty is considered, in the sense that the volatility of the\nmarket is only assumed to lie between two given bounds. The set of all possible\nmodels (probability measures) considered here is non-dominated. We propose\nstudying this problem in the framework of second-order backward stochastic\ndifferential equations (2BSDEs for short) with quadratic growth generators. We\nshow for exponential, power and logarithmic utilities that the value function\nof the problem can be written as the initial value of a particular 2BSDE and\nprove existence of an optimal strategy. Finally several examples which shed\nmore light on the problem and its links with the classical utility maximization\none are provided. In particular, we show that in some cases, the upper bound of\nthe volatility interval plays a central role, exactly as in the option pricing\nproblem with uncertain volatility models of [2].", "category": ["math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1201.0769v6", "link": "http://arxiv.org/abs/1201.0769v6"}, {"title": "Emergence of Distributed Coordination in the Kolkata Paise Restaurant\n  Problem with Finite Information", "summary": "In this paper, we study a large-scale distributed coordination problem and\npropose efficient adaptive strategies to solve the problem. The basic problem\nis to allocate finite number of resources to individual agents such that there\nis as little congestion as possible and the fraction of unutilized resources is\nreduced as far as possible. In the absence of a central planner and global\ninformation, agents can employ adaptive strategies that uses only finite\nknowledge about the competitors. In this paper, we show that a combination of\nfinite information sets and reinforcement learning can increase the utilization\nrate of resources substantially.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1702.01017v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.04.171"}, {"title": "Second-order BSDEs with general reflection and game options under\n  uncertainty", "summary": "The aim of this paper is twofold. First, we extend the results of [33]\nconcerning the existence and uniqueness of second-order reflected 2BSDEs to the\ncase of two obstacles. Under some regularity assumptions on one of the\nbarriers, similar to the ones in [10], and when the two barriers are completely\nseparated, we provide a complete wellposedness theory for doubly reflected\nsecond-order BSDEs. We also show that these objects are related to non-standard\noptimal stopping games, thus generalizing the connection between DRBSDEs and\nDynkin games first proved by Cvitanic and Karatzas [11]. More precisely, we\nshow under a technical assumption that the second order DRBSDEs provide\nsolutions of what we call uncertain Dynkin games and that they also allow us to\nobtain super and subhedging prices for American game options (also called\nIsraeli options) in financial markets with volatility uncertainty", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1212.0476v3", "link": "http://arxiv.org/abs/1212.0476v3"}, {"title": "Data Analytics in Operations Management: A Review", "summary": "Research in operations management has traditionally focused on models for\nunderstanding, mostly at a strategic level, how firms should operate. Spurred\nby the growing availability of data and recent advances in machine learning and\noptimization methodologies, there has been an increasing application of data\nanalytics to problems in operations management. In this paper, we review recent\napplications of data analytics to operations management, in three major areas\n-- supply chain management, revenue management and healthcare operations -- and\nhighlight some exciting directions for the future.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.00556v1", "link": "http://arxiv.org/abs/1905.00556v1"}, {"title": "Linkages and systemic risk in the European insurance sector: Some new\n  evidence based on dynamic spanning trees", "summary": "This paper is part of the research on the interlinkages between insurers and\ntheir contribution to systemic risk on the insurance market. Its main purpose\nis to present the results of the analysis of linkage dynamics and systemic risk\nin the European insurance sector which are obtained using correlation networks.\nThese networks are based on dynamic dependence structures modelled using a\ncopula. Then, we determine minimum spanning trees (MST). Finally, the linkage\ndynamics is described by means of selected topological network measures.", "category": ["q-fin.ST", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.01142v2", "link": "http://arxiv.org/abs/1908.01142v2"}, {"title": "Multivariate Modeling of Daily REIT Volatility", "summary": "This paper examines volatility in REITs using a multivariate GARCH based\nmodel. The Multivariate VAR-GARCH technique documents the return and volatility\nlinkages between REIT sub-sectors and also examines the influence of other US\nequity series. The motivation is for investors to incorporate time-varyng\nvolatility and correlations in their portfolio selection. The results\nillustrate the differences in results when higher frequency daily data is\ntested in comparison to the monthly data that has been commonly used in the\nexisting literature. The linkages both within the REIT sector and between REITs\nand related sectors such as value stocks are weaker than commonly found in\nmonthly studies. The broad market would appear to be more influential in the\ndaily case.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1103.5660v1", "link": "http://arxiv.org/abs/1103.5660v1"}, {"title": "How Required Reserve Ratio Affects Distribution and Velocity of Money", "summary": "In this paper the dependence of wealth distribution and the velocity of money\non the required reserve ratio is examined based on a random transfer model of\nmoney and computer simulations. A fractional reserve banking system is\nintroduced to the model where money creation can be achieved by bank loans and\nthe monetary aggregate is determined by the monetary base and the required\nreserve ratio. It is shown that monetary wealth follows asymmetric Laplace\ndistribution and latency time of money follows exponential distribution. The\nexpression of monetary wealth distribution and that of the velocity of money in\nterms of the required reserve ratio are presented in a good agreement with\nsimulation results.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0507160v1", "link": "http://dx.doi.org/10.1016/j.physa.2005.04.014"}, {"title": "Are volatility correlations in financial markets related to Omori\n  processes occurring on all scales?", "summary": "We analyze the memory in volatility by studying volatility return intervals,\ndefined as the time between two consecutive fluctuations larger than a given\nthreshold, in time periods following stock market crashes. Such an aftercrash\nperiod is characterized by the Omori law, which describes the decay in the rate\nof aftershocks of a given size with time t by a power law with exponent close\nto 1. A shock followed by such a power law decay in the rate is here called\nOmori process. Studying several aftercrash time series, we show that the Omori\nlaw holds not only after significant market crashes, but also after\n``intermediate shocks''. Moreover, we find self-similar features in the\nvolatility. Specifically, within the aftercrash period there are smaller shocks\nthat themselves constitute Omori processes on smaller scales, similar to the\nOmori process after the large crash. We call these smaller shocks subcrashes,\nwhich are followed by their own aftershocks. We also find similar Omori\nprocesses after intermediate crashes in time regimes without a large market\ncrash. By appropriate detrending we remove the influence of the crashes and\nsubcrashes from the data, and find that this procedure significantly reduces\nthe memory in the records. Our results are consistent with the hypothesis that\nthe memory in volatility is related to Omori processes present on different\ntime scales.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0611093v1", "link": "http://dx.doi.org/10.1103/PhysRevE.76.016109"}, {"title": "Multilevel Monte Carlo methods for applications in finance", "summary": "Since Giles introduced the multilevel Monte Carlo path simulation method\n[18], there has been rapid development of the technique for a variety of\napplications in computational finance. This paper surveys the progress so far,\nhighlights the key features in achieving a high rate of multilevel variance\nconvergence, and suggests directions for future research.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1212.1377v1", "link": "http://arxiv.org/abs/1212.1377v1"}, {"title": "Income and wealth distribution of the richest Norwegian individuals: An\n  inequality analysis", "summary": "Using the empirical data from the Norwegian tax office, we analyse the wealth\nand income of the richest individuals in Norway during the period 2010--2013.\nWe find that both annual income and wealth level of the richest individuals are\ndescribable using the Pareto law. We find that the robust mean Pareto exponent\nover the four-year period to be $\\approx 2.3$ for income and $\\approx 1.5$ for\nwealth.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1610.08918v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.01.077"}, {"title": "Criticality in a model of banking crises", "summary": "An interbank market lets participants pool the risk arising from the\ncombination of illiquid investments and random withdrawals by depositors. But\nit also creates the potential for one bank's failure to trigger off avalanches\nof further failures. We simulate a model of interbank lending to study the\ninterplay of these two effects. We show that when banks are similar in size and\nexposure to risk, avalanche effects are small so that widening the interbank\nmarket leads to more stability. But as heterogeneity increases, avalanche\neffects become more important. By varying the heterogeneity and connectivity\nacross banks, the system enters a critical regime with a power law distribution\nof avalanche sizes.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0104080v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00297-7"}, {"title": "Semi-parametric Realized Nonlinear Conditional Autoregressive Expectile\n  and Expected Shortfall", "summary": "A joint conditional autoregressive expectile and Expected Shortfall framework\nis proposed. The framework is extended through incorporating a measurement\nequation which models the contemporaneous dependence between the realized\nmeasures and the latent conditional expectile. Nonlinear threshold\nspecification is further incorporated into the proposed framework. A Bayesian\nMarkov Chain Monte Carlo method is adapted for estimation, whose properties are\nassessed and compared with maximum likelihood via a simulation study.\nOne-day-ahead VaR and ES forecasting studies, with seven market indices,\nprovide empirical support to the proposed models.", "category": ["q-fin.RM", "econ.EM"], "id": "http://arxiv.org/abs/1906.09961v1", "link": "http://arxiv.org/abs/1906.09961v1"}, {"title": "The effects of energy and commodity prices on commodity output in a\n  three-factor, two-good general equilibrium trade model", "summary": "We analyze the effects of energy and commodity prices on commodity output\nusing a three-factor, two-good general equilibrium trade model with three\nfactors: capital, labor, and imported energy. We derive a sufficient condition\nfor each sign pattern of each relationship to hold, which no other studies have\nderived. We assume factor-intensity ranking is constant and use the EWS\n(economy-wide substitution)-ratio vector and the Hadamard product in the\nanalysis. The results reveal that the position of the EWS-ratio vector\ndetermines the relationships. Specifically, the strengthening (resp. reduction)\nof import restrictions can increase (resp. decrease) the commodity output of\nexportables, if capital and labor, domestic factors, are economy-wide\ncomplements. This seems paradoxical.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1711.10096v2", "link": "http://arxiv.org/abs/1711.10096v2"}, {"title": "Unified Growth Theory Contradicted by the Economic Growth in Africa", "summary": "One of the fundamental postulates of the Unified Growth Theory is the claimed\nexistence of three distinctly different regimes of economic growth governed by\nthree distinctly different mechanisms of growth. However, Galor also proposed\nthat the timing of these regimes is different for developed countries and for\nless-developed countries. Africa is the perfect example of economic growth in\nless-developed countries. The data used by Galor, but never properly\ninvestigated, are now analysed. They turn out to be in dramatic contradiction\nof this theory.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1512.03164v2", "link": "http://arxiv.org/abs/1512.03164v2"}, {"title": "Outperformance and Tracking: Dynamic Asset Allocation for Active and\n  Passive Portfolio Management", "summary": "Portfolio management problems are often divided into two types: active and\npassive, where the objective is to outperform and track a preselected\nbenchmark, respectively. Here, we formulate and solve a dynamic asset\nallocation problem that combines these two objectives in a unified framework.\nWe look to maximize the expected growth rate differential between the wealth of\nthe investor's portfolio and that of a performance benchmark while penalizing\nrisk-weighted deviations from a given tracking portfolio. Using stochastic\ncontrol techniques, we provide explicit closed-form expressions for the optimal\nallocation and we show how the optimal strategy can be related to the growth\noptimal portfolio. The admissible benchmarks encompass the class of\nfunctionally generated portfolios (FGPs), which include the market portfolio,\nas the only requirement is that they depend only on the prevailing asset\nvalues. Finally, some numerical experiments are presented to illustrate the\nrisk-reward profile of the optimal allocation.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1803.05819v3", "link": "http://arxiv.org/abs/1803.05819v3"}, {"title": "Deep Neural Networks for Estimation and Inference", "summary": "We study deep neural networks and their use in semiparametric inference. We\nestablish novel rates of convergence for deep feedforward neural nets. Our new\nrates are sufficiently fast (in some cases minimax optimal) to allow us to\nestablish valid second-step inference after first-step estimation with deep\nlearning, a result also new to the literature. Our estimation rates and\nsemiparametric inference results handle the current standard architecture:\nfully connected feedforward neural networks (multi-layer perceptrons), with the\nnow-common rectified linear unit activation function and a depth explicitly\ndiverging with the sample size. We discuss other architectures as well,\nincluding fixed-width, very deep networks. We establish nonasymptotic bounds\nfor these deep nets for a general class of nonparametric regression-type loss\nfunctions, which includes as special cases least squares, logistic regression,\nand other generalized linear models. We then apply our theory to develop\nsemiparametric inference, focusing on causal parameters for concreteness, such\nas treatment effects, expected welfare, and decomposition effects. Inference in\nmany other semiparametric contexts can be readily obtained. We demonstrate the\neffectiveness of deep learning with a Monte Carlo analysis and an empirical\napplication to direct mail marketing.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1809.09953v3", "link": "http://arxiv.org/abs/1809.09953v3"}, {"title": "The Stochastic Solution to a Cauchy Problem for Degenerate Parabolic\n  Equations", "summary": "We study the stochastic solution to a Cauchy problem for a degenerate\nparabolic equation arising from option pricing. When the diffusion coefficient\nof the underlying price process is locally H\\\"older continuous with exponent\n$\\delta\\in (0, 1]$, the stochastic solution, which represents the price of a\nEuropean option, is shown to be a classical solution to the Cauchy problem.\nThis improves the standard requirement $\\delta\\ge 1/2$. Uniqueness results,\nincluding a Feynman-Kac formula and a comparison theorem, are established\nwithout assuming the usual linear growth condition on the diffusion\ncoefficient. When the stochastic solution is not smooth, it is characterized as\nthe limit of an approximating smooth stochastic solutions. In deriving the main\nresults, we discover a new, probabilistic proof of Kotani's criterion for\nmartingality of a one-dimensional diffusion in natural scale.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1309.0046v5", "link": "http://arxiv.org/abs/1309.0046v5"}, {"title": "Valuation of asset and volatility derivatives using decoupled\n  time-changed L\u00e9vy processes", "summary": "In this paper we propose a general derivative pricing framework which employs\ndecoupled time-changed (DTC) L\\'evy processes to model the underlying asset of\ncontingent claims. A DTC L\\'evy process is a generalized time-changed L\\'evy\nprocess whose continuous and pure jump parts are allowed to follow separate\nrandom time scalings; we devise the martingale structure for a DTC\nL\\'evy-driven asset and revisit many popular models which fall under this\nframework. Postulating different time changes for the underlying L\\'evy\ndecomposition allows to introduce asset price models consistent with the\nassumption of a correlated pair of continuous and jump market activities; we\nstudy one illustrative DTC model having this property by assuming that the\ninstantaneous activity rates follow the the so-called Wishart process. The\ntheory developed is applied to the problem of pricing claims depending not only\non the price or the volatility of an underlying asset, but also to more\nsophisticated derivatives that pay-off on the joint performance of these two\nfinancial variables, like the target volatility option (TVO). We solve the\npricing problem through a Fourier-inversion method; numerical computations\nvalidating our technique are provided.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1210.5479v4", "link": "http://arxiv.org/abs/1210.5479v4"}, {"title": "To Detect Irregular Trade Behaviors In Stock Market By Using Graph Based\n  Ranking Methods", "summary": "To detect the irregular trade behaviors in the stock market is the important\nproblem in machine learning field. These irregular trade behaviors are\nobviously illegal. To detect these irregular trade behaviors in the stock\nmarket, data scientists normally employ the supervised learning techniques. In\nthis paper, we employ the three graph Laplacian based semi-supervised ranking\nmethods to solve the irregular trade behavior detection problem. Experimental\nresults show that that the un-normalized and symmetric normalized graph\nLaplacian based semi-supervised ranking methods outperform the random walk\nLaplacian based semi-supervised ranking method.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1909.08964v1", "link": "http://arxiv.org/abs/1909.08964v1"}, {"title": "Why Money Trickles Up - Wealth & Income Distributions", "summary": "This paper combines ideas from classical economics and modern finance with\nthe general Lotka-Volterra models of Levy & Solomon to provide straightforward\nexplanations of wealth and income distributions. Using a simple and realistic\neconomic formulation, the distributions of both wealth and income are fully\nexplained. Both the power tail and the log-normal like body are fully captured.\nIt is of note that the full distribution, including the power law tail, is\ncreated via the use of absolutely identical agents. It is further demonstrated\nthat a simple scheme of compulsory saving could eliminate poverty at little\ncost to the taxpayer.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1105.2122v2", "link": "http://arxiv.org/abs/1105.2122v2"}, {"title": "Dominantly Truthful Multi-task Peer Prediction with a Constant Number of\n  Tasks", "summary": "In the setting where participants are asked multiple similar possibly\nsubjective multi-choice questions (e.g. Do you like Panda Express? Y/N; do you\nlike Chick-fil-A? Y/N), a series of peer prediction mechanisms are designed to\nincentivize honest reports and some of them achieve dominantly truthfulness:\ntruth-telling is a dominant strategy and strictly dominate other\n\"non-permutation strategy\" with some mild conditions. However, a major issue\nhinders the practical usage of those mechanisms: they require the participants\nto perform an infinite number of tasks. When the participants perform a finite\nnumber of tasks, these mechanisms only achieve approximated dominant\ntruthfulness. The existence of a dominantly truthful multi-task peer prediction\nmechanism that only requires a finite number of tasks remains to be an open\nquestion that may have a negative result, even with full prior knowledge.\n  This paper answers this open question by proposing a new mechanism,\nDeterminant based Mutual Information Mechanism (DMI-Mechanism), that is\ndominantly truthful when the number of tasks is at least 2C and the number of\nparticipants is at least 2. C is the number of choices for each question (C=2\nfor binary-choice questions). In addition to incentivizing honest reports,\nDMI-Mechanism can also be transferred into an information evaluation rule that\nidentifies high-quality information without verification when there are at\nleast 3 participants. To the best of our knowledge, DMI-Mechanism is the first\ndominantly truthful mechanism that works for a finite number of tasks, not to\nsay a small constant number of tasks.", "category": [], "id": "http://arxiv.org/abs/1911.00272v1", "link": "http://arxiv.org/abs/1911.00272v1"}, {"title": "Time consistency of dynamic risk measures in markets with transaction\n  costs", "summary": "The paper concerns primal and dual representations as well as time\nconsistency of set-valued dynamic risk measures. Set-valued risk measures\nappear naturally when markets with transaction costs are considered and capital\nrequirements can be made in a basket of currencies or assets. Time consistency\nof scalar risk measures can be generalized to set-valued risk measures in\ndifferent ways. The most intuitive generalization is called time consistency.\nWe will show that the equivalence between a recursive form of the risk measure\nand time consistency, which is a central result in the scalar case, does not\nhold in the set-valued framework. Instead, we propose an alternative\ngeneralization, which we will call multi-portfolio time consistency and show in\nthe main result of the paper that this property is indeed equivalent to the\nrecursive form as well as to an additive property for the acceptance sets.\nMulti-portfolio time consistency is a stronger property than time consistency.\nIn the scalar case, both notions coincide.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1201.1483v2", "link": "http://arxiv.org/abs/1201.1483v2"}, {"title": "A unified approach to mortality modelling using state-space framework:\n  characterisation, identification, estimation and forecasting", "summary": "This paper explores and develops alternative statistical representations and\nestimation approaches for dynamic mortality models. The framework we adopt is\nto reinterpret popular mortality models such as the Lee-Carter class of models\nin a general state-space modelling methodology, which allows modelling,\nestimation and forecasting of mortality under a unified framework. Furthermore,\nwe propose an alternative class of model identification constraints which is\nmore suited to statistical inference in filtering and parameter estimation\nsettings based on maximization of the marginalized likelihood or in Bayesian\ninference. We then develop a novel class of Bayesian state-space models which\nincorporate apriori beliefs about the mortality model characteristics as well\nas for more flexible and appropriate assumptions relating to heteroscedasticity\nthat present in observed mortality data. We show that multiple period and\ncohort effect can be cast under a state-space structure. To study long term\nmortality dynamics, we introduce stochastic volatility to the period effect.\nThe estimation of the resulting stochastic volatility model of mortality is\nperformed using a recent class of Monte Carlo procedure specifically designed\nfor state and parameter estimation in Bayesian state-space models, known as the\nclass of particle Markov chain Monte Carlo methods. We illustrate the framework\nwe have developed using Danish male mortality data, and show that incorporating\nheteroscedasticity and stochastic volatility markedly improves model fit\ndespite an increase of model complexity. Forecasting properties of the enhanced\nmodels are examined with long term and short term calibration periods on the\nreconstruction of life tables.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1605.09484v1", "link": "http://arxiv.org/abs/1605.09484v1"}, {"title": "Promoting cooperation by reputation-driven group formation", "summary": "In previous studies of spatial public goods game, each player is able to\nestablish a group. However, in real life, some players cannot successfully\norganize groups for various reasons. In this paper, we propose a mechanism of\nreputation-driven group formation, in which groups can only be organized by\nplayers whose reputation reaches or exceeds a threshold. We define a player's\nreputation as the frequency of cooperation in the last $T$ time steps. We find\nthat the highest cooperation level can be obtained when groups are only\nestablished by pure cooperators who always cooperate in the last $T$ time\nsteps. Effects of the memory length $T$ on cooperation are also studied.", "category": [], "id": "http://arxiv.org/abs/1802.01253v1", "link": "http://arxiv.org/abs/1802.01253v1"}, {"title": "Utility Maximization of an Indivisible Market with Transaction Costs", "summary": "This work takes up the challenges of utility maximization problem when the\nmarket is indivisible and the transaction costs are included. First there is a\nso-called solvency region given by the minimum margin requirement in the\nproblem formulation. Then the associated utility maximization is formulated as\nan optimal switching problem. The diffusion turns out to be degenerate and the\nboundary of domain is an unbounded set. One no longer has the continuity of the\nvalue function without posing further conditions due to the degeneracy and the\ndependence of the random terminal time on the initial data. This paper provides\nsufficient conditions under which the continuity of the value function is\nobtained. The essence of our approach is to find a sequence of continuous\nfunctions locally uniformly converging to the desired value function. Thanks to\ncontinuity, the value function can be characterized by using the notion of\nviscosity solution of certain quasi-variational inequality.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1003.2930v1", "link": "http://arxiv.org/abs/1003.2930v1"}, {"title": "An Insurance-Led Response to Climate Change", "summary": "Climate change is widely expected to increase weather related damage and the\ninsurance claims that result from it. This will increase insurance premiums, in\na way that is independent of a customer's contribution to the causes of climate\nchange. Insurance provides a financial mechanism that mitigates some of the\nconsequences of climate change, allowing damage from increasingly frequent\nevents to be repaired. We observe that the insurance industry could reclaim any\nincrease in claims due to climate change, by increasing the insurance premiums\non energy producers for example, without needing government intervention or a\nnew tax. We argue that this insurance-led levy must acknowledge both present\ncarbon emissions and a modern industry's carbon inheritance, that is, to\nrecognise that fossil-fuel driven industrial growth has provided the\ninnovations and conditions needed for modern civilisation to exist and develop.\nA tax or levy on energy production is one mechanism that would recognise carbon\ninheritance through the increased (energy) costs for manufacturing and using\nmodern technology, and can also provide an incentive to minimise carbon\nemissions, through higher costs for the most polluting industries. The\nnecessary increases in insurance premiums would initially be small, and will\nrequire an event attribution (EA) methodology to determine their size. We\npropose that the levies can be phased in as the science of event attribution\nbecomes sufficiently robust for each claim type, to ultimately provide a global\ninsurance-led response to climate change.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1509.01157v3", "link": "http://arxiv.org/abs/1509.01157v3"}, {"title": "From Glosten-Milgrom to the whole limit order book and applications to\n  financial regulation", "summary": "We build an agent-based model for the order book with three types of market\nparticipants: informed trader, noise trader and competitive market makers.\nUsing a Glosten-Milgrom like approach, we are able to deduce the whole limit\norder book (bid-ask spread and volume available at each price) from the\ninteractions between the different agents. More precisely, we obtain a link\nbetween efficient price dynamic, proportion of trades due to the noise trader,\ntraded volume, bid-ask spread and equilibrium limit order book state. With this\nmodel, we provide a relevant tool for regulators and market platforms. We show\nfor example that it allows us to forecast consequences of a tick size change on\nthe microstructure of an asset. It also enables us to value quantitatively the\nqueue position of a limit order in the book.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1902.10743v1", "link": "http://arxiv.org/abs/1902.10743v1"}, {"title": "The equivalent constant-elasticity-of-variance (CEV) volatility of the\n  stochastic-alpha-beta-rho (SABR) model", "summary": "This study presents new analytic approximations of the\nstochastic-alpha-beta-rho (SABR) model. Unlike existing studies that focus on\nthe equivalent Black-Scholes (BS) volatility, we instead derive the equivalent\nvolatility under the constant-elasticity-of-variance (CEV) model, which is the\nlimit of the SABR model when the volatility of volatility approaches 0.\nNumerical examples demonstrate the accuracy of the CEV volatility approximation\nfor a wide range of parameters. Moreover, in our approach, arbitrage occurs at\na lower strike price than in existing BS-based approximations.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1911.13123v2", "link": "http://arxiv.org/abs/1911.13123v2"}, {"title": "Coupling news sentiment with web browsing data improves prediction of\n  intra-day price dynamics", "summary": "The new digital revolution of big data is deeply changing our capability of\nunderstanding society and forecasting the outcome of many social and economic\nsystems. Unfortunately, information can be very heterogeneous in the\nimportance, relevance, and surprise it conveys, affecting severely the\npredictive power of semantic and statistical methods. Here we show that the\naggregation of web users' behavior can be elicited to overcome this problem in\na hard to predict complex system, namely the financial market. Specifically,\nour in-sample analysis shows that the combined use of sentiment analysis of\nnews and browsing activity of users of Yahoo! Finance greatly helps forecasting\nintra-day and daily price changes of a set of 100 highly capitalized US stocks\ntraded in the period 2012-2013. Sentiment analysis or browsing activity when\ntaken alone have very small or no predictive power. Conversely, when\nconsidering a \"news signal\" where in a given time interval we compute the\naverage sentiment of the clicked news, weighted by the number of clicks, we\nshow that for nearly 50% of the companies such signal Granger-causes hourly\nprice returns. Our result indicates a \"wisdom-of-the-crowd\" effect that allows\nto exploit users' activity to identify and weigh properly the relevant and\nsurprising news, enhancing considerably the forecasting power of the news\nsentiment.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1412.3948v4", "link": "http://arxiv.org/abs/1412.3948v4"}, {"title": "Pricing path-dependent Bermudan options using Wiener chaos expansion: an\n  embarrassingly parallel approach", "summary": "In this work, we propose a new policy iteration algorithm for pricing\nBermudan options when the payoff process cannot be written as a function of a\nlifted Markov process. Our approach is based on a modification of the\nwell-known Longstaff Schwartz algorithm, in which we basically replace the\nstandard least square regression by a Wiener chaos expansion. Not only does it\nallow us to deal with a non Markovian setting, but it also breaks the\nbottleneck induced by the least square regression as the coefficients of the\nchaos expansion are given by scalar products on the L^2 space and can therefore\nbe approximated by independent Monte Carlo computations. This key feature\nenables us to provide an embarrassingly parallel algorithm.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1901.05672v1", "link": "http://arxiv.org/abs/1901.05672v1"}, {"title": "Semi-Closed Form Cubature and Applications to Financial Diffusion Models", "summary": "Cubature methods, a powerful alternative to Monte Carlo due to\nKusuoka~[Adv.~Math.~Econ.~6, 69--83, 2004] and\nLyons--Victoir~[Proc.~R.~Soc.\\\\Lond.~Ser.~A 460, 169--198, 2004], involve the\nsolution to numerous auxiliary ordinary differential equations. With focus on\nthe Ninomiya-Victoir algorithm~[Appl.~Math.~Fin.~15, 107--121, 2008], which\ncorresponds to a concrete level $5$ cubature method, we study some parametric\ndiffusion models motivated from financial applications, and exhibit structural\nconditions under which all involved ODEs can be solved explicitly and\nefficiently. We then enlarge the class of models for which this technique\napplies, by introducing a (model-dependent) variation of the Ninomiya-Victoir\nmethod. Our method remains easy to implement; numerical examples illustrate the\nsavings in computation time.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1009.4818v1", "link": "http://arxiv.org/abs/1009.4818v1"}, {"title": "A Model of a Randomized Experiment with an Application to the PROWESS\n  Clinical Trial", "summary": "I develop a model of a randomized experiment with a binary intervention and a\nbinary outcome. Potential outcomes in the intervention and control groups give\nrise to four types of participants. Fixing ideas such that the outcome is\nmortality, some participants would live regardless, others would be saved,\nothers would be killed, and others would die regardless. These potential\noutcome types are not observable. However, I use the model to develop\nestimators of the number of participants of each type. The model relies on the\nrandomization within the experiment and on deductive reasoning. I apply the\nmodel to an important clinical trial, the PROWESS trial, and I perform a Monte\nCarlo simulation calibrated to estimates from the trial. The reduced form from\nthe trial shows a reduction in mortality, which provided a rationale for FDA\napproval. However, I find that the intervention killed two participants for\nevery three it saved.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.05810v1", "link": "http://arxiv.org/abs/1908.05810v1"}, {"title": "A Regularized Factor-augmented Vector Autoregressive Model", "summary": "We propose a regularized factor-augmented vector autoregressive (FAVAR) model\nthat allows for sparsity in the factor loadings. In this framework, factors may\nonly load on a subset of variables which simplifies the factor identification\nand their economic interpretation. We identify the factors in a data-driven\nmanner without imposing specific relations between the unobserved factors and\nthe underlying time series. Using our approach, the effects of structural\nshocks can be investigated on economically meaningful factors and on all\nobserved time series included in the FAVAR model. We prove consistency for the\nestimators of the factor loadings, the covariance matrix of the idiosyncratic\ncomponent, the factors, as well as the autoregressive parameters in the dynamic\nmodel. In an empirical application, we investigate the effects of a monetary\npolicy shock on a broad range of economically relevant variables. We identify\nthis shock using a joint identification of the factor model and the structural\ninnovations in the VAR model. We find impulse response functions which are in\nline with economic rationale, both on the factor aggregates and observed time\nseries level.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.06049v1", "link": "http://arxiv.org/abs/1912.06049v1"}, {"title": "Boltzmann-Gibbs Distribution of Fortune and Broken Time-Reversible\n  Symmetry in Econodynamics", "summary": "Within the description of stochastic differential equations it is argued that\nthe existence of Boltzmann-Gibbs type distribution in economy is independent of\nthe time reversal symmetry in econodynamics. Both power law and exponential\ndistributions can be accommodated by it. The demonstration is based on a\nmathematical structure discovered during a study in gene regulatory network\ndynamics. Further possible analogy between equilibrium economy and\nthermodynamics is explored.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0506103v1", "link": "http://arxiv.org/abs/physics/0506103v1"}, {"title": "Tail probabilities of random linear functions of regularly varying\n  random vectors", "summary": "We provide a new extension of Breiman's Theorem on computing tail\nprobabilities of a product of random variables to a multivariate setting. In\nparticular, we give a complete characterization of regular variation on cones\nin $[0,\\infty)^d$ under random linear transformations. This allows us to\ncompute probabilities of a variety of tail events, which classical multivariate\nregularly varying models would report to be asymptotically negligible. We\nillustrate our findings with applications to risk assessment in financial\nsystems and reinsurance markets under a bipartite network structure.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1904.06824v1", "link": "http://arxiv.org/abs/1904.06824v1"}, {"title": "Nonparametric Instrumental Variables Estimation Under Misspecification", "summary": "We show that nonparametric instrumental variables (NPIV) estimators are\nhighly sensitive to misspecification: an arbitrarily small deviation from\ninstrumental validity can lead to large asymptotic bias for a broad class of\nestimators. One can mitigate the problem by placing strong restrictions on the\nstructural function in estimation. However, if the true function does not obey\nthe restrictions then imposing them imparts bias. Therefore, there is a\ntrade-off between the sensitivity to invalid instruments and bias from imposing\nexcessive restrictions. In light of this trade-off we propose a partial\nidentification approach to estimation in NPIV models. We provide a point\nestimator that minimizes the worst-case asymptotic bias and error-bounds that\nexplicitly account for some degree of misspecification. We apply our methods to\nthe empirical setting of Blundell et al. (2007) and Horowitz (2011) to estimate\nshape-invariant Engel curves.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1901.01241v6", "link": "http://arxiv.org/abs/1901.01241v6"}, {"title": "Pricing American Call Options by the Black-Scholes Equation with a\n  Nonlinear Volatility Function", "summary": "In this paper we investigate a nonlinear generalization of the Black-Scholes\nequation for pricing American style call options in which the volatility term\nmay depend on the underlying asset price and the Gamma of the option. We\npropose a numerical method for pricing American style call options by means of\ntransformation of the free boundary problem for a nonlinear Black-Scholes\nequation into the so-called Gamma variational inequality with the new variable\ndepending on the Gamma of the option. We apply a modified projective successive\nover relaxation method in order to construct an effective numerical scheme for\ndiscretization of the Gamma variational inequality. Finally, we present several\ncomputational examples for the nonlinear Black-Scholes equation for pricing\nAmerican style call option under presence of variable transaction costs.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1707.00358v3", "link": "http://arxiv.org/abs/1707.00358v3"}, {"title": "Hidden Forces and Fluctuations from Moving Averages: A Test Study", "summary": "The possibility that price dynamics is affected by its distance from a moving\naverage has been recently introduced as new statistical tool. The purpose is to\nidentify the tendency of the price dynamics to be attractive or repulsive with\nrespect to its own moving average. We consider a number of tests for various\nmodels which clarify the advantages and limitations of this new approach. The\nanalysis leads to the identification of an effective potential with respect to\nthe moving average. Its specific implementation requires a detailed\nconsideration of various effects which can alter the statistical methods used.\nHowever, the study of various model systems shows that this approach is indeed\nsuitable to detect hidden forces in the market which go beyond usual\ncorrelations and volatility clustering.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0601089v1", "link": "http://dx.doi.org/10.1016/j.physa.2006.04.113"}, {"title": "Electoral Crime Under Democracy: Information Effects from Judicial\n  Decisions in Brazil", "summary": "This paper examines voters' responses to the disclosure of electoral crime\ninformation in large democracies. I focus on Brazil, where the electoral court\nmakes candidates' criminal records public before every election. Using a sample\nof local candidates running for office between 2004 and 2016, I find that a\nconviction for an electoral crime reduces candidates' probability of election\nand vote share by 10.3 and 12.9 percentage points (p.p.), respectively. These\nresults are not explained by (potential) changes in judge, voter, or candidate\nbehavior over the electoral process. I additionally perform machine\nclassification of court documents to estimate heterogeneous punishment for\nsevere and trivial crimes. I document a larger electoral penalty (6.5 p.p.) if\ncandidates are convicted for severe crimes. These results supplement the\ninformation shortcut literature by examining how judicial information\ninfluences voters' decisions and showing that voters react more strongly to\nmore credible sources of information.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.10958v1", "link": "http://arxiv.org/abs/1912.10958v1"}, {"title": "On Fairness of Systemic Risk Measures", "summary": "In our previous paper, \"A Unified Approach to Systemic Risk Measures via\nAcceptance Set\" (\\textit{Mathematical Finance, 2018}), we have introduced a\ngeneral class of systemic risk measures that allow for random allocations to\nindividual banks before aggregation of their risks. In the present paper, we\nprove the dual representation of a particular subclass of such systemic risk\nmeasures and the existence and uniqueness of the optimal allocation related to\nthem. We also introduce an associated utility maximization problem which has\nthe same optimal solution as the systemic risk measure. In addition, the\noptimizer in the dual formulation provides a \\textit{risk allocation} which is\nfair from the point of view of the individual financial institutions. The case\nwith exponential utilities which allows for explicit computation is treated in\ndetails.", "category": ["q-fin.MF", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1803.09898v4", "link": "http://arxiv.org/abs/1803.09898v4"}, {"title": "Lower Bound Approximation to Basket Option Values for Local Volatility\n  Jump-Diffusion Models", "summary": "In this paper we derive an easily computed approximation to European basket\ncall prices for a local volatility jump-diffusion model. We apply the\nasymptotic expansion method to find the approximate value of the lower bound of\nEuropean basket call prices. If the local volatility function is time\nindependent then there is a closed-form expression for the approximation.\nNumerical tests show that the suggested approximation is fast and accurate in\ncomparison with the Monte Carlo and other approximation methods in the\nliterature.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1212.3147v2", "link": "http://arxiv.org/abs/1212.3147v2"}, {"title": "Filterbased Stochastic Volatility in Continuous-Time Hidden Markov\n  Models", "summary": "Regime-switching models, in particular Hidden Markov Models (HMMs) where the\nswitching is driven by an unobservable Markov chain, are widely-used in\nfinancial applications, due to their tractability and good econometric\nproperties. In this work we consider HMMs in continuous time with both constant\nand switching volatility. In the continuous-time model with switching\nvolatility the underlying Markov chain could be observed due to this stochastic\nvolatility, and no estimation (filtering) of it is needed (in theory), while in\nthe discretized model or the model with constant volatility one has to filter\nfor the underlying Markov chain. The motivations for continuous-time models are\nexplicit computations in finance. To have a realistic model with unobservable\nMarkov chain in continuous time and good econometric properties we introduce a\nregime-switching model where the volatility depends on the filter for the\nunderlying chain and state the filtering equations. We prove an approximation\nresult for a fixed information filtration and further motivate the model by\nconsidering social learning arguments. We analyze its relation to the switching\nvolatility model and present a convergence result for the discretized model. We\nthen illustrate its econometric properties by considering numerical\nsimulations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1602.05323v1", "link": "http://arxiv.org/abs/1602.05323v1"}, {"title": "The Aggregation Property and its Applications to Realised Higher Moments", "summary": "We develop a general multivariate aggregation property which encompasses the\ndistinct versions of the property that were introduced by Neuberger [2012] and\nBondarenko [2014] independently. This way, we classify new types of model-free\nrealised characteristics for which risk premia may be estimated without bias.\nWe focus on the aggregation property for multivariate martingales and log\nmartingales, and then define realised third and fourth moments which allow\nlong-term higher-moment risk premia to be measured, efficiently and without\nbias, using high-frequency returns.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1709.08188v1", "link": "http://arxiv.org/abs/1709.08188v1"}, {"title": "Theory of Weak Identification in Semiparametric Models", "summary": "We provide general formulation of weak identification in semiparametric\nmodels and an efficiency concept. Weak identification occurs when a parameter\nis weakly regular, i.e., when it is locally homogeneous of degree zero. When\nthis happens, consistent or equivariant estimation is shown to be impossible.\nWe then show that there exists an underlying regular parameter that fully\ncharacterizes the weakly regular parameter. While this parameter is not unique,\nconcepts of sufficiency and minimality help pin down a desirable one. If\nestimation of minimal sufficient underlying parameters is inefficient, it\nintroduces noise in the corresponding estimation of weakly regular parameters,\nwhence we can improve the estimators by local asymptotic Rao-Blackwellization.\nWe call an estimator weakly efficient if it does not admit such improvement.\nNew weakly efficient estimators are presented in linear IV and nonlinear\nregression models. Simulation of a linear IV model demonstrates how 2SLS and\noptimal IV estimators are improved.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.10478v2", "link": "http://arxiv.org/abs/1908.10478v2"}, {"title": "Approximate Option Pricing in the L\u00e9vy Libor Model", "summary": "In this paper we consider the pricing of options on interest rates such as\ncaplets and swaptions in the L\\'evy Libor model developed by Eberlein and\n\\\"Ozkan (2005). This model is an extension to L\\'evy driving processes of the\nclassical log-normal Libor market model (LMM) driven by a Brownian motion.\nOption pricing is significantly less tractable in this model than in the LMM\ndue to the appearance of stochastic terms in the jump part of the driving\nprocess when performing the measure changes which are standard in pricing of\ninterest rate derivatives. To obtain explicit approximation for option prices,\nwe propose to treat a given L\\'evy Libor model as a suitable perturbation of\nthe log-normal LMM. The method is inspired by recent works by Cern\\'y, Denkl\nand Kallsen (2013) and M\\'enass\\'e and Tankov (2015). The approximate option\nprices in the L\\'evy Libor model are given as the corresponding LMM prices plus\ncorrection terms which depend on the characteristics of the underlying L\\'evy\nprocess and some additional terms obtained from the LMM model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1511.08466v2", "link": "http://arxiv.org/abs/1511.08466v2"}, {"title": "Multi-scale Representation of High Frequency Market Liquidity", "summary": "We introduce an event based framework of directional changes and overshoots\nto map continuous financial data into the so-called Intrinsic Network - a state\nbased discretisation of intrinsically dissected time series. Defining a method\nfor state contraction of Intrinsic Network, we show that it has a consistent\nhierarchical structure that allows for multi-scale analysis of financial data.\nWe define an information theoretic measurement termed Liquidity that\ncharacterises the unlikeliness of price trajectories and argue that the new\nmetric has the ability to detect and predict stress in financial markets. We\nshow empirical examples within the Foreign Exchange market where the new\nmeasure not only quantifies liquidity but also acts as an early warning signal.", "category": ["q-fin.TR", "q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1402.2198v1", "link": "http://arxiv.org/abs/1402.2198v1"}, {"title": "Can Volatility Solve the Na\u00efve Diversification Puzzle?", "summary": "We investigate whether sophisticated volatility estimation improves the\nout-of-sample performance of mean-variance portfolio strategies relative to the\nna\\\"{\\i}ve 1/N strategy. The portfolio strategies rely solely upon second\nmoments. Using a diverse group of econometric and portfolio models across\nmultiple datasets, we show that a majority of models achieve significantly\nhigher Sharpe ratios and lower portfolio volatility relative to the na\\\"{\\i}ve\nrule, even after controlling for turnover costs. Our results suggest that there\nare benefits to employing more sophisticated econometric models than the sample\ncovariance matrix, and that mean-variance strategies often outperform the\nna\\\"{\\i}ve portfolio across multiple datasets and assessment criteria.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2005.03204v1", "link": "http://arxiv.org/abs/2005.03204v1"}, {"title": "An Introduction to Business Mathematics", "summary": "These lecture notes provide a self-contained introduction to the mathematical\nmethods required in a Bachelor degree programme in Business, Economics, or\nManagement. In particular, the topics covered comprise real-valued vector and\nmatrix algebra, systems of linear algebraic equations, Leontief's stationary\ninput-output matrix model, linear programming, elementary financial\nmathematics, as well as differential and integral calculus of real-valued\nfunctions of one real variable. A special focus is set on applications in\nquantitative economical modelling.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1509.04333v2", "link": "http://arxiv.org/abs/1509.04333v2"}, {"title": "A distribution function analysis of wealth distribution", "summary": "We develop a general framework to analyze the distribution functions of\nwealth and income. Within this framework we study wealth distribution in a\nsociety by using a model which turns on two-party trading for poor people while\nfor rich people interaction with wealthy entities (huge reservoir) is relevant.\nAt equilibrium, the interaction with wealthy entities gives a power law\n(Pareto-like) behavior in the wealth distribution while the two party\ninteraction gives a distribution similar to that reported earlier.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0310343v1", "link": "http://arxiv.org/abs/cond-mat/0310343v1"}, {"title": "Gambling in contests with regret", "summary": "This paper discusses the gambling contest introduced in Seel & Strack\n(Gambling in contests, Discussion Paper Series of SFB/TR 15 Governance and the\nEfficiency of Economic Systems 375, Mar 2012.) and considers the impact of\nadding a penalty associated with failure to follow a winning strategy.\n  The Seel & Strack model consists of $n$-agents each of whom privately\nobserves a transient diffusion process and chooses when to stop it. The player\nwith the highest stopped value wins the contest, and each player's objective is\nto maximise their probability of winning the contest. We give a new derivation\nof the results of Seel & Strack based on a Lagrangian approach. Moreover, we\nconsider an extension of the problem in which in the case when an agent is\npenalised when their strategy is suboptimal, in the sense that they do not win\nthe contest, but there existed an alternative strategy which would have\nresulted in victory.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1301.0719v1", "link": "http://arxiv.org/abs/1301.0719v1"}, {"title": "Anticorrelations, subbrownian stochastic drift, and 1/f-like spectra in\n  stable financial systems", "summary": "Statistic dynamics of financial systems is investigated, basing on a model of\nrandomly coupled equation system driven by stochastic Langevin force. It is\nfound that in stable regime the noise power spectrum of the system is of\n1/f^alpha form, with the exponent alpha=3/2 in case of Hermitian coupling\nmatrices, or slightly larger (up to 5/3) in nonhermitian case. In all cases it\nleads to negative autocorrelation function of the increments of the variables\n(prices, exchange rates), and to subbrownian stochastic drift of the variables.\nThe model can be generalized to arbitrary stable, driven by noise system of\nrandomly coupled components.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0104456v1", "link": "http://arxiv.org/abs/cond-mat/0104456v1"}, {"title": "PDGM: a Neural Network Approach to Solve Path-Dependent Partial\n  Differential Equations", "summary": "In this paper, we propose a novel numerical method for Path-Dependent Partial\nDifferential Equations (PPDEs). These equations firstly appeared in the seminal\nwork of Dupire [2009], where the functional It\\^o calculus was developed to\ndeal with path-dependent financial derivatives contracts. More specificaly, we\ngeneralize the Deep Galerking Method (DGM) of Sirignano and Spiliopoulos [2018]\nto deal with these equations. The method, which we call Path-Dependent DGM\n(PDGM), consists of using a combination of feed-forward and Long Short-Term\nMemory architectures to model the solution of the PPDE. We then analyze several\nnumerical examples, many from the Financial Mathematics literature, that show\nthe capabilities of the method under very different situations.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/2003.02035v2", "link": "http://arxiv.org/abs/2003.02035v2"}, {"title": "Estimating the volatility of Bitcoin using GARCH models", "summary": "In this paper, an application of three GARCH-type models (sGARCH, iGARCH, and\ntGARCH) with Student t-distribution, Generalized Error distribution (GED), and\nNormal Inverse Gaussian (NIG) distribution are examined. The new development\nallows for the modeling of volatility clustering effects, the leptokurtic and\nthe skewed distributions in the return series of Bitcoin. Comparative to the\ntwo distributions, the normal inverse Gaussian distribution captured adequately\nthe fat tails and skewness in all the GARCH type models. The tGARCH model was\nthe best model as it described the asymmetric occurrence of shocks in the\nBitcoin market. That is, the response of investors to the same amount of good\nand bad news are distinct. From the empirical results, it can be concluded that\ntGARCH-NIG was the best model to estimate the volatility in the return series\nof Bitcoin. Generally, it would be optimal to use the NIG distribution in GARCH\ntype models since time series of most cryptocurrency are leptokurtic.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/1909.04903v2", "link": "http://arxiv.org/abs/1909.04903v2"}, {"title": "Currency Based on Time Standard", "summary": "The Total Economic Time Capacity of a Year 525600 minutes is postulated as a\ntime standard for a new Monetary Minute currency in this evaluation study.\nConsequently, the Monetary Minute MonMin is defined as a 1/525600 part of the\nTotal Economic Time Capacity of a Year. The Value CMonMin of the Monetary\nMinute MonMin is equal to a 1/525600 part of the GDP, p.c., expressed in a\nspecific state currency C. There is described how the Monetary Minutes MonMin\nare determined, and how their values CMonMin are calculated based on the GDP\nand all the population in specific economies. The Monetary Minutes trace\ndifferent aggregate productivity, i.e. exploitation of the total time capacity\nof a year for generating of the GDP in economies of different states.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.07859v1", "link": "http://arxiv.org/abs/1910.07859v1"}, {"title": "New Pricing Framework: Options and Bonds", "summary": "A unified analytical pricing framework with involvement of the shot noise\nrandom process has been introduced and elaborated. Two exactly solvable new\nmodels have been developed. The first model has been designed to value options.\nIt is assumed that asset price stochastic dynamics follows a Geometric Shot\nNoise motion. A new arbitrage-free integro-differential option pricing equation\nhas been found and solved. The put-call parity has been proved and the Greeks\nhave been calculated. Three additional new Greeks associated with market model\nparameters have been introduced and evaluated. It has been shown that in\ndiffusion approximation the developed option pricing model incorporates the\nwell-known Black-Scholes equation and its solution. The stochastic dynamic\norigin of the Black-Scholes volatility has been uncovered. The new option\npricing model has been generalized based on asset price dynamics modeled by the\nsuperposition of Geometric Brownian motion and Geometric Shot Noise. To model\nstochastic dynamics of a short term interest rate, the second model has been\nintroduced and developed based on Langevin type equation with shot noise. A new\nbond pricing formula has been obtained. It has been shown that in diffusion\napproximation the developed bond pricing formula goes into the well-known\nVasicek solution. The stochastic dynamic origin of the long-term mean and\ninstantaneous volatility of the Vasicek model has been uncovered. A generalized\nbond pricing model has been introduced and developed based on short term\ninterest rate stochastic dynamics modeled by superposition of a standard Wiener\nprocess and shot noise. Despite the non-Gaussianity of probability\ndistributions involved, all newly elaborated models have the same degree of\nanalytical tractability as the Black-Scholes model and the Vasicek model.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1407.4452v2", "link": "http://arxiv.org/abs/1407.4452v2"}, {"title": "Numerical solution of a semilinear parabolic degenerate\n  Hamilton-Jacobi-Bellman equation with singularity", "summary": "We consider a semilinear parabolic degenerated Hamilton-Jacobi-Bellman (HJB)\nequation with singularity which is related to a stochastic control problem with\nfuel constraint. The fuel constraint translates into a singular initial\ncondition for the HJB equation. We first propose a transformation based on a\nchange of variables that gives rise to an equivalent HJB equation with\nnonsingular initial condition but irregular coefficients. We then construct\nexplicit and implicit numerical schemes for solving the transformed HJB\nequation and prove their convergences by establishing an extension to the\nresult of Barles and Souganidis (1991).", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1609.00702v2", "link": "http://arxiv.org/abs/1609.00702v2"}, {"title": "Correlators of Polynomial Processes", "summary": "In the setting of polynomial jump-diffusion dynamics, we provide a formula\nfor computing correlators, namely, cross-moments of the process at different\ntime points along its path. The formula involves only linear combinations of\nthe exponential of the so-called generator matrix, extending the well-known\nmoment formula for polynomial processes. The developed framework allows to\nreplace costly simulations with more accurate estimates, and it may be used for\nincreasing the accuracy in financial pricing, such as for path-dependent\noptions or in a stochastic volatility models context.", "category": ["math.PR", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1906.11320v2", "link": "http://arxiv.org/abs/1906.11320v2"}, {"title": "Optimization of Financial Instrument Parcels in Stochastic Wavelet Model", "summary": "To define oscillatory movements of securities market, we put in the non-local\nextension of Ito- equation for wavelet-images of random processes. It is\nproposed an algorithm of creation of evolutionary equation and a model of\nprediction of the most probable price movement path. It is carried out\nexperimental validation of findings.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1007.5413v1", "link": "http://arxiv.org/abs/1007.5413v1"}, {"title": "Inverse Gaussian quadrature and finite normal-mixture approximation of\n  generalized hyperbolic distribution", "summary": "In this study, a numerical quadrature for the generalized inverse Gaussian\ndistribution is derived from the Gauss--Hermite quadrature by exploiting its\nrelationship with the normal distribution. Unlike Gaussian quadrature, the\nproposed quadrature exactly evaluates both positive and negative moments, thus\nimproving evaluation accuracy. The generalized hyperbolic distribution is\nefficiently approximated as a finite normal variance-mean mixture with the\nquadrature. Therefore, the expectations under the distribution, such as\ncumulative distribution function and option price, are accurately computed as\nweighted sums of those under normal distributions.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1810.01116v1", "link": "http://arxiv.org/abs/1810.01116v1"}, {"title": "Scale invariant multiplier and multifractality of absolute returns in\n  stock markets", "summary": "The statistical properties of the multipliers of the absolute returns are\ninvestigated using one-minute high-frequency data of financial time series. The\nmultiplier distribution is found to be independent of the box size $s$ when $s$\nis larger than some crossover scale, providing direct evidence of the existence\nof scale invariance in financial data. The multipliers with base $a=2$ are well\napproximated by a normal distribution and the most probable multiplier scales\nas a power law in respect to the base $a$. We unravel that the volatility\nmultipliers possess multifractal nature which is independent of construction of\nthe multipliers, that is, the values of $s$ and $a$.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0609210v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.03.015"}, {"title": "A Uniform Bound of the Operator Norm of Random Element Matrices and\n  Operator Norm Minimizing Estimation", "summary": "In this paper, we derive a uniform stochastic bound of the operator norm (or\nequivalently, the largest singular value) of random matrices whose elements are\nindexed by parameters. As an application, we propose a new estimator that\nminimizes the operator norm of the matrix that consists of the moment\nfunctions. We show the consistency of the estimator.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.01096v1", "link": "http://arxiv.org/abs/1905.01096v1"}, {"title": "Pricing electricity derivatives within a Markov regime-switching model", "summary": "In this paper analytic formulas for electricity derivatives are calculated.\nTo this end, we assume that electricity spot prices follow a 3-regime Markov\nregime-switching model with independent spikes and drops and periodic\ntransition matrix. Since the classical derivatives pricing methodology cannot\nbe used in case of non-storable commodities, we employ the concept of the risk\npremium. The obtained theoretical results are then used for the European Energy\nExchange (EEX) market data. The 3-regime model is calibrated to the spot\nelectricity prices. Next, the risk premium is derived and used to calculate\nprices of European options written on spot, as well as, forward prices.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1203.5442v1", "link": "http://arxiv.org/abs/1203.5442v1"}, {"title": "Marking Systemic Portfolio Risk with Application to the Correlation Skew\n  of Equity Baskets", "summary": "The downside risk of a portfolio of (equity)assets is generally substantially\nhigher than the downside risk of its components. In particular in times of\ncrises when assets tend to have high correlation, the understanding of this\ndifference can be crucial in managing systemic risk of a portfolio. In this\npaper we generalize Merton's option formula in the presence jumps to the\nmulti-asset case. It is shown how common jumps across assets provide an\nintuitive and powerful tool to describe systemic risk that is consistent with\ndata. The methodology provides a new way to mark and risk-manage systemic risk\nof portfolios in a systematic way.", "category": ["q-fin.RM", "q-fin.PR"], "id": "http://arxiv.org/abs/1012.4674v4", "link": "http://arxiv.org/abs/1012.4674v4"}, {"title": "Multilateral Index Number Systems for International Price Comparisons:\n  Properties, Existence and Uniqueness", "summary": "Over the past five decades a number of multilateral index number systems have\nbeen proposed for spatial and cross-country price comparisons. These\nmultilateral indexes are usually expressed as solutions to systems of linear or\nnonlinear equations. In this paper, we provide general theorems that can be\nused to establish necessary and sufficient conditions for the existence and\nuniqueness of the Geary-Khamis, IDB, Neary and Rao indexes as well as potential\nnew systems including two generalized systems of index numbers. One of our main\nresults is that the necessary and sufficient conditions for existence and\nuniqueness of solutions can often be stated in terms of graph-theoretic\nconcepts and a verifiable condition based on observed quantities of\ncommodities.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.04197v2", "link": "http://arxiv.org/abs/1811.04197v2"}, {"title": "A Dynamical Model for Forecasting Operational Losses", "summary": "A novel dynamical model for the study of operational risk in banks and\nsuitable for the calculation of the Value at Risk (VaR) is proposed. The\nequation of motion takes into account the interactions among different bank's\nprocesses, the spontaneous generation of losses via a noise term and the\nefforts made by the bank to avoid their occurrence. Since the model is very\ngeneral, it can be tailored on the internal organizational structure of a\nspecific bank by estimating some of its parameters from historical operational\nlosses. The model is exactly solved in the case in which there are no causal\nloops in the matrix of couplings and it is shown how the solution can be\nexploited to estimate also the parameters of the noise. The forecasting power\nof the model is investigated by using a fraction $f$ of simulated data to\nestimate the parameters, showing that for $f = 0.75$ the VaR can be forecast\nwith an error $\\simeq 10^{-3}$.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1007.0026v6", "link": "http://dx.doi.org/10.1016/j.physa.2011.12.046"}, {"title": "Comparing the forecasting of cryptocurrencies by Bayesian time-varying\n  volatility models", "summary": "This paper studies the forecasting ability of cryptocurrency time series.\nThis study is about the four most capitalized cryptocurrencies: Bitcoin,\nEthereum, Litecoin and Ripple. Different Bayesian models are compared,\nincluding models with constant and time-varying volatility, such as stochastic\nvolatility and GARCH. Moreover, some crypto-predictors are included in the\nanalysis, such as S\\&P 500 and Nikkei 225. In this paper the results show that\nstochastic volatility is significantly outperforming the benchmark of VAR in\nboth point and density forecasting. Using a different type of distribution, for\nthe errors of the stochastic volatility the student-t distribution came out to\nbe outperforming the standard normal approach.", "category": ["econ.EM", "econ.GN", "q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1909.06599v1", "link": "http://arxiv.org/abs/1909.06599v1"}, {"title": "Final Topology for Preference Spaces", "summary": "Most decision problems can be understood as a mapping from a preference space\ninto a set of outcomes. When preferences are representable via utility\nfunctions, this generates a mapping from a space of utility functions into\noutcomes. We say a model is continuous in utilities (resp., preferences) if\nsmall perturbations of utility functions (resp., preferences) generate small\nchanges in outcomes. While similar, these two concepts are equivalent only when\nthe topology satisfies the following universal property: for each continuous\nmapping from preferences to outcomes there is a unique mapping from utilities\nto outcomes that is faithful to the preference map and is continuous. The\ntopologies that satisfy such a universal property are called final topologies.\nIn this paper, we analyze the properties of the final topology for preference\nsets. This is of practical importance since most of the analysis on continuity\nis done via utility functions and not the primitive preference space. Our\nresults allow the researcher to extrapolate continuity in utility to continuity\nin the underlying preferences.", "category": [], "id": "http://arxiv.org/abs/2004.02357v1", "link": "http://arxiv.org/abs/2004.02357v1"}, {"title": "Methods and Concepts in Economic Complexity", "summary": "Knowhow in societies accumulates as it gets transmitted from group to group,\nand from generation to generation. However, we lack of a unified quantitative\nformalism that takes into account the structured process for how this\naccumulation occurs, and this has precluded the development of a unified view\nof human development in the past and in the present. Here, we summarize a\nparadigm to understand and model this process. The paradigm goes under the\ngeneral name of the Theory of Economic Complexity (TEC). Based on it, we\npresent a combination of analytical, numerical and empirical results that\nillustrate how to characterize the process of development, providing measurable\nquantities that can be used to predict future developments. The emphasis is the\nquantification of the collective knowhow an economy has accumulated, and what\nare the directions in which it is likely to expand. As a case study we consider\ndata on trade, which provides consistent data on the technological\ndiversification of 200 countries across more than 50 years. The paradigm\nrepresented by TEC should be relevant for anthropologists, sociologists, and\neconomists interested in the role of collective knowhow as the main determinant\nof the success and welfare of a society.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1809.10781v2", "link": "http://arxiv.org/abs/1809.10781v2"}, {"title": "Decision-making in Livestock Biosecurity Practices amidst Environmental\n  and Social Uncertainty: Evidence from an Experimental Game", "summary": "Livestock industries are vulnerable to disease threats, which can cost\nbillions of dollars and have substantial negative social ramifications. Losses\nare mitigated through increased use of disease-related biosecurity practices,\nmaking increased biosecurity an industry goal. Currently, there is no\nindustry-wide standard for sharing information about disease incidence or\non-site biosecurity strategies, resulting in uncertainty regarding disease\nprevalence and biosecurity strategies employed by industry stakeholders. Using\nan experimental simulation game, we examined human participant's willingness to\ninvest in biosecurity when confronted with disease outbreak scenarios. We\nvaried the scenarios by changing the information provided about 1) disease\nincidence and 2) biosecurity strategy or response by production facilities to\nthe threat of disease. Here we show that willingness to invest in biosecurity\nincreases with increased information about disease incidence, but decreases\nwith increased information about biosecurity practices used by nearby\nfacilities. Thus, the type or context of the uncertainty confronting the\ndecision maker may be a major factor influencing behavior. Our findings suggest\nthat policies and practices that encourage greater sharing of disease incidence\ninformation should have the greatest benefit for protecting herd health.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.01081v2", "link": "http://dx.doi.org/10.1371/journal.pone.0214500"}, {"title": "Sharpe portfolio using a cross-efficiency evaluation", "summary": "The Sharpe ratio is a way to compare the excess returns (over the risk free\nasset) of portfolios for each unit of volatility that is generated by a\nportfolio. In this paper we introduce a robust Sharpe ratio portfolio under the\nassumption that the risk free asset is unknown. We propose a robust portfolio\nthat maximizes the Sharpe ratio when the risk free asset is unknown, but is\nwithin a given interval. To compute the best Sharpe ratio portfolio all the\nSharpe ratios for any risk free asset are considered and compared by using the\nso-called cross-efficiency evaluation. An explicit expression of the\nCross-Eficiency Sharpe ratio portfolio is presented when short selling is\nallowed.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1610.00937v2", "link": "http://arxiv.org/abs/1610.00937v2"}, {"title": "Co-movements in financial fluctuations are anchored to economic\n  fundamentals: A mesoscopic mapping", "summary": "We demonstrate the existence of an empirical linkage between the nominal\nfinancial networks and the underlying economic fundamentals across countries.\nWe construct the nominal return correlation networks from daily data to\nencapsulate sector-level dynamics and figure the relative importance of the\nsectors in the nominal network through a measure of centrality and clustering\nalgorithms. The eigenvector centrality robustly identifies the backbone of the\nminimum spanning tree defined on the return networks as well as the primary\ncluster in the multidimensional scaling map. We show that the sectors that are\nrelatively large in size, defined with the metrics market capitalization,\nrevenue and number of employees, constitute the core of the return networks,\nwhereas the periphery is mostly populated by relatively smaller sectors.\nTherefore, sector-level nominal return dynamics is anchored to the real size\neffect, which ultimately shapes the optimal portfolios for risk management. Our\nresults are reasonably robust across 27 countries of varying degrees of\nprosperity and across periods of market turbulence (2008-09) as well as\nrelative calmness (2015-16).", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1612.05952v2", "link": "http://arxiv.org/abs/1612.05952v2"}, {"title": "General Price Bounds for Guaranteed Annuity Options", "summary": "In this paper, we are concerned with the valuation of Guaranteed Annuity\nOptions (GAOs) under the most generalised modelling framework where both\ninterest and mortality rates are stochastic and correlated. Pricing these type\nof options in the correlated environment is a challenging task and no closed\nform solution exists in the literature. We employ the use of doubly stochastic\nstopping times to incorporate the randomness about the time of death and employ\na suitable change of measure to facilitate the valuation of survival benefit,\nthere by adapting the payoff of the GAO in terms of the payoff of a basket call\noption. We derive general price bounds for GAOs by utilizing a conditioning\napproach for the lower bound and arithmetic-geometric mean inequality for the\nupper bound. The theory is then applied to affine models to present some very\ninteresting formulae for the bounds under the affine set up. Numerical examples\nare furnished and benchmarked against Monte Carlo simulations to estimate the\nprice of a GAO for a variety of affine processes governing the evolution of\nmortality and the interest rate.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1707.00807v1", "link": "http://arxiv.org/abs/1707.00807v1"}, {"title": "Computational Analysis of the structural properties of Economic and\n  Financial Networks", "summary": "In recent years, methods from network science are gaining rapidly interest in\neconomics and finance. A reason for this is that in a globalized world the\ninterconnectedness among economic and financial entities are crucial to\nunderstand and networks provide a natural framework for representing and\nstudying such systems. In this paper, we are surveying the use of networks and\nnetwork-based methods for studying economy related questions. We start with a\nbrief overview of graph theory and basic definitions. Then we discuss\ndescriptive network measures and network complexity measures for quantifying\nstructural properties of economic networks. Finally, we discuss different\nnetwork and tree structures as relevant for applications.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1710.04455v1", "link": "http://arxiv.org/abs/1710.04455v1"}, {"title": "Probabilistic Approach to Mean Field Games and Mean Field Type Control\n  Problems with Multiple Populations", "summary": "In this work, we systematically investigate mean field games and mean field\ntype control problems with multiple populations using a coupled system of\nforward-backward stochastic differential equations of McKean-Vlasov type\nstemming from Pontryagin's stochastic maximum principle. Although the same cost\nfunctions as well as the coefficient functions of the state dynamics are shared\namong the agents within each population, they can be different population by\npopulation. We study the mean field limit for the three different situations;\n(i) every agent is non-cooperative; (ii) the agents within each population are\ncooperative; and (iii) the agents in some populations are cooperative but those\nin the other populations are not. We provide several sets of sufficient\nconditions for the existence of a mean field equilibrium for each of these\ncases. Furthermore, under appropriate conditions, we show that the mean field\nsolution to each of these problems actually provides an approximate Nash\nequilibrium for the corresponding game with a large but finite number of\nagents.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1911.11501v2", "link": "http://arxiv.org/abs/1911.11501v2"}, {"title": "Resolving New Keynesian Anomalies with Wealth in the Utility Function", "summary": "At the zero lower bound, the New Keynesian model predicts that output and\ninflation collapse to implausibly low levels, and that government spending and\nforward guidance have implausibly large effects. To resolve these anomalies, we\nintroduce wealth into the utility function; the justification is that wealth is\na marker of social status, and people value status. Since people partly save to\naccrue social status, the Euler equation is modified. As a result, when the\nmarginal utility of wealth is sufficiently large, the dynamical system\nrepresenting the zero-lower-bound equilibrium transforms from a saddle to a\nsource---which resolves all the anomalies.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.13645v3", "link": "http://arxiv.org/abs/1905.13645v3"}, {"title": "Nonparametric Sample Splitting", "summary": "This paper develops a threshold regression model, where the threshold is\ndetermined by an unknown relation between two variables. The threshold function\nis estimated fully nonparametrically. The observations are allowed to be\ncross-sectionally dependent and the model can be applied to determine an\nunknown spatial border for sample splitting over a random field. The uniform\nrate of convergence and the nonstandard limiting distribution of the\nnonparametric threshold estimator are derived. The root-n consistency and the\nasymptotic normality of the regression slope parameter estimator are also\nobtained. Empirical relevance is illustrated by estimating an economic border\ninduced by the housing price difference between Queens and Brooklyn in New York\nCity, where the economic border deviates substantially from the administrative\none.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.13140v1", "link": "http://arxiv.org/abs/1905.13140v1"}, {"title": "Deep Learning can Replicate Adaptive Traders in a Limit-Order-Book\n  Financial Market", "summary": "We report successful results from using deep learning neural networks (DLNNs)\nto learn, purely by observation, the behavior of profitable traders in an\nelectronic market closely modelled on the limit-order-book (LOB) market\nmechanisms that are commonly found in the real-world global financial markets\nfor equities (stocks & shares), currencies, bonds, commodities, and\nderivatives. Successful real human traders, and advanced automated algorithmic\ntrading systems, learn from experience and adapt over time as market conditions\nchange; our DLNN learns to copy this adaptive trading behavior. A novel aspect\nof our work is that we do not involve the conventional approach of attempting\nto predict time-series of prices of tradeable securities. Instead, we collect\nlarge volumes of training data by observing only the quotes issued by a\nsuccessful sales-trader in the market, details of the orders that trader is\nexecuting, and the data available on the LOB (as would usually be provided by a\ncentralized exchange) over the period that the trader is active. In this paper\nwe demonstrate that suitably configured DLNNs can learn to replicate the\ntrading behavior of a successful adaptive automated trader, an algorithmic\nsystem previously demonstrated to outperform human traders. We also demonstrate\nthat DLNNs can learn to perform better (i.e., more profitably) than the trader\nthat provided the training data. We believe that this is the first ever\ndemonstration that DLNNs can successfully replicate a human-like, or\nsuper-human, adaptive trader operating in a realistic emulation of a real-world\nfinancial market. Our results can be considered as proof-of-concept that a DLNN\ncould, in principle, observe the actions of a human trader in a real financial\nmarket and over time learn to trade equally as well as that human trader, and\npossibly better.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1811.02880v1", "link": "http://arxiv.org/abs/1811.02880v1"}, {"title": "Quotation for the Value Added Assessment during Product Development and\n  Production Processes", "summary": "This communication is based on an original approach linking economical\nfactors to technical and methodological ones. This work is applied to the\ndecision process for mix production. This approach is relevant for costing\ndriving systems. The main interesting point is that the quotation factors\n(linked to time indicators for each step of the industrial process) allow the\ncomplete evaluation and control of, on the one hand, the global balance of the\ncompany for a six-month period and, on the other hand, the reference values for\neach step of the process cycle of the parts. This approach is based on a\ncomplete numerical traceability and control of the processes (design and\nmanufacturing of the parts and tools, mass production). This is possible due to\nnumerical models and to feedback loops for cost indicator analysis at design\nand production levels. Quotation is also the base for the design requirements\nand for the choice and the configuration of the production process. The\nreference values of the quotation generate the base reference parameters of the\nprocess steps and operations. The traceability of real values (real time\nconsuming, real consumable) is mainly used for a statistic feedback to the\nquotation application. The industrial environment is a steel sand casting\ncompany with a wide mix product and the application concerns both design and\nmanufacturing. The production system is fully automated and integrates\ndifferent products at the same time.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1011.5715v1", "link": "http://arxiv.org/abs/1011.5715v1"}, {"title": "Fast, Accurate, Straightforward Extreme Quantiles of Compound Loss\n  Distributions", "summary": "We present an easily implemented, fast, and accurate method for approximating\nextreme quantiles of compound loss distributions (frequency+severity) as are\ncommonly used in insurance and operational risk capital models. The\nInterpolated Single Loss Approximation (ISLA) of Opdyke (2014) is based on the\nwidely used Single Loss Approximation (SLA) of Degen (2010) and maintains two\nimportant advantages over its competitors: first, ISLA correctly accounts for a\ndiscontinuity in SLA that otherwise can systematically and notably bias the\nquantile (capital) approximation under conditions of both finite and infinite\nmean. Secondly, because it is based on a closed-form approximation, ISLA\nmaintains the notable speed advantages of SLA over other methods requiring\nalgorithmic looping (e.g. fast Fourier transform or Panjer recursion). Speed is\nimportant when simulating many quantile (capital) estimates, as is so often\nrequired in practice, and essential when simulations of simulations are needed\n(e.g. some power studies). The modified ISLA (MISLA) presented herein increases\nthe range of application across the severity distributions most commonly used\nin these settings, and it is tested against extensive Monte Carlo simulation\n(one billion years' worth of losses) and the best competing method (the\nperturbative expansion (PE2) of Hernandez et al., 2014) using twelve\nheavy-tailed severity distributions, some of which are truncated. MISLA is\nshown to be comparable to PE2 in terms of both speed and accuracy, and it is\narguably more straightforward to implement for the majority of Advanced\nMeasurement Approaches (AMA) banks that are already using SLA (and failing to\ntake into account its biasing discontinuity).", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1610.03718v7", "link": "http://arxiv.org/abs/1610.03718v7"}, {"title": "Old and new approaches to LIBOR modeling", "summary": "In this article, we review the construction and properties of some popular\napproaches to modeling LIBOR rates. We discuss the following frameworks:\nclassical LIBOR market models, forward price models and Markov-functional\nmodels. We close with the recently developed affine LIBOR models.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0910.4941v3", "link": "http://dx.doi.org/10.1111/j.1467-9574.2010.00458.x"}, {"title": "General Smooth Solutions to the HJB PDE: Applications to Finance", "summary": "We overcome a major obstacle in mathematical optimization. In so doing, we\nprovide a smooth solution to the HJB PDE without assuming the differentiability\nof the value function. We apply our method to financial models.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1211.5816v8", "link": "http://arxiv.org/abs/1211.5816v8"}, {"title": "Estimating Tipping Points in Feedback-Driven Financial Networks", "summary": "Much research has been conducted arguing that tipping points at which complex\nsystems experience phase transitions are difficult to identify. To test the\nexistence of tipping points in financial markets, based on the alternating\noffer strategic model we propose a network of bargaining agents who mutually\neither cooperate or where the feedback mechanism between trading and price\ndynamics is driven by an external \"hidden\" variable R that quantifies the\ndegree of market overpricing. Due to the feedback mechanism, R fluctuates and\noscillates over time, and thus periods when the market is underpriced and\noverpriced occur repeatedly. As the market becomes overpriced, bubbles are\ncreated that ultimately burst in a market crash. The probability that the index\nwill drop in the next year exhibits a strong hysteresis behavior from which we\ncalculate the tipping point. The probability distribution function of R has a\nbimodal shape characteristic of small systems near the tipping point. By\nexamining the S&P500 index we illustrate the applicability of the model and\ndemonstate that the financial data exhibits a hysteresis and a tipping point\nthat agree with the model predictions. We report a cointegration between the\nreturns of the S&P 500 index and its intrinsic value.", "category": ["q-fin.CP", "q-fin.GN"], "id": "http://arxiv.org/abs/1509.04952v1", "link": "http://dx.doi.org/10.1109/JSTSP.2016.2593099"}, {"title": "Behavioral and Game-Theoretic Security Investments in Interdependent\n  Systems Modeled by Attack Graphs", "summary": "We consider a system consisting of multiple interdependent assets, and a set\nof defenders, each responsible for securing a subset of the assets against an\nattacker. The interdependencies between the assets are captured by an attack\ngraph, where an edge from one asset to another indicates that if the former\nasset is compromised, an attack can be launched on the latter asset. Each edge\nhas an associated probability of successful attack, which can be reduced via\nsecurity investments by the defenders. In such scenarios, we investigate the\nsecurity investments that arise under certain features of human decision-making\nthat have been identified in behavioral economics. In particular, humans have\nbeen shown to perceive probabilities in a nonlinear manner, typically\noverweighting low probabilities and underweighting high probabilities. We show\nthat suboptimal investments can arise under such weighting in certain network\ntopologies. We also show that pure strategy Nash equilibria exist in settings\nwith multiple (behavioral) defenders, and study the inefficiency of the\nequilibrium investments by behavioral defenders compared to a centralized\nsocially optimal solution.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.03213v2", "link": "http://arxiv.org/abs/2001.03213v2"}, {"title": "A closed-form representation of mean-variance hedging for additive\n  processes via Malliavin calculus", "summary": "We focus on mean-variance hedging problem for models whose asset price\nfollows an exponential additive process. Some representations of mean-variance\nhedging strategies for jump type models have already been suggested, but none\nis suited to develop numerical methods of the values of strategies for any\ngiven time up to the maturity. In this paper, we aim to derive a new explicit\nclosed-form representation, which enables us to develop an efficient numerical\nmethod using the fast Fourier transforms. Note that our representation is\ndescribed in terms of Malliavin derivatives. In addition, we illustrate\nnumerical results for exponential L\\'evy models.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1702.07556v2", "link": "http://arxiv.org/abs/1702.07556v2"}, {"title": "Analysing Global Fixed Income Markets with Tensors", "summary": "Global fixed income returns span across multiple maturities and economies,\nthat is, they naturally reside on multi-dimensional data structures referred to\nas tensors. In contrast to standard \"flat-view\" multivariate models that are\nagnostic to data structure and only describe linear pairwise relationships, we\nintroduce a tensor-valued approach to model the global risks shared by multiple\ninterest rate curves. In this way, the estimated risk factors can be\nanalytically decomposed into maturity-domain and country-domain constituents,\nwhich allows the investor to devise rigorous and tractable global portfolio\nmanagement and hedging strategies tailored to each risk domain. An empirical\nanalysis confirms the existence of global risk factors shared by eight\ndeveloped economies, and demonstrates their ability to compactly describe the\nglobal macroeconomic environment.", "category": ["q-fin.PM", "econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1908.02101v4", "link": "http://arxiv.org/abs/1908.02101v4"}, {"title": "On the Topological Properties of the World Trade Web: A Weighted Network\n  Analysis", "summary": "This paper studies the topological properties of the World Trade Web (WTW)\nand its evolution over time by employing a weighted network analysis. We show\nthat the WTW, viewed as a weighted network, displays statistical features that\nare very different from those obtained by using a traditional binary-network\napproach. In particular, we find that: (i) the majority of existing links are\nassociated to weak trade relationships; (ii) the weighted WTW is only weakly\ndisassortative; (iii) countries holding more intense trade relationships are\nmore clustered.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0708.4359v1", "link": "http://dx.doi.org/10.1016/j.physa.2008.01.050"}, {"title": "The Influence of Seed Selection on the Solvency II Ratio", "summary": "This article contains the first published example of a real economic balance\nsheet where the Solvency II ratio substantially depends on the seed selected\nfor the random number generator (RNG) used. The theoretical background and the\nmain quality criteria for RNGs are explained in detail. To serve as a gauge for\nRNGs, a definition of true randomness is given. Quality tests that RNGs should\npass in order to generate stable results when used in risk management under\nSolvency II are described.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1801.05409v2", "link": "http://arxiv.org/abs/1801.05409v2"}, {"title": "Semi-analytical solution of a McKean-Vlasov equation with feedback\n  through hitting a boundary", "summary": "In this paper, we study the non-linear diffusion equation associated with a\nparticle system where the common drift depends on the rate of absorption of\nparticles at a boundary. We provide an interpretation as a structural credit\nrisk model with default contagion in a large interconnected banking system.\nUsing the method of heat potentials, we derive a coupled system of Volterra\nintegral equations for the transition density and for the loss through\nabsorption. An approximation by expansion is given for a small interaction\nparameter. We also present a numerical solution algorithm and conduct\ncomputational tests.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1808.05311v2", "link": "http://arxiv.org/abs/1808.05311v2"}, {"title": "Closed-form Solutions for an Explicit Modern Ideal Tontine with Bequest\n  Motive", "summary": "In this paper I extend the work of Bernhardt and Donnelly (2019) dealing with\nmodern explicit tontines, as a way of providing income under a specified\nbequest motive, from a defined contribution pension pot. A key feature of the\npresent paper is that it relaxes the assumption of fixed proportions invested\nin tontine and bequest accounts. In making the bequest proportion an additional\ncontrol function I obtain, hitherto unavailable, closed-form solutions for the\nfractional consumption rate, wealth, bequest amount, and bequest proportion\nunder a constant relative risk averse utility. I show that the optimal bequest\nproportion is the product of the optimum fractional consumption rate and an\nexponentiated bequest parameter. Typical scenarios are explored using UK Office\nof National Statistics life tables, showing the behaviour of these\ncharacteristics under varying degrees of constant relative risk aversion.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2005.00715v1", "link": "http://arxiv.org/abs/2005.00715v1"}, {"title": "Price manipulation in a market impact model with dark pool", "summary": "For a market impact model, price manipulation and related notions play a role\nthat is similar to the role of arbitrage in a derivatives pricing model. Here,\nwe give a systematic investigation into such regularity issues when orders can\nbe executed both at a traditional exchange and in a dark pool. To this end, we\nfocus on a class of dark-pool models whose market impact at the exchange is\ndescribed by an Almgren--Chriss model. Conditions for the absence of price\nmanipulation for all Almgren--Chriss models include the absence of temporary\ncross-venue impact, the presence of full permanent cross-venue impact, and the\nadditional penalization of orders executed in the dark pool. When a particular\nAlmgren--Chriss model has been fixed, we show by a number of examples that the\nregularity of the dark-pool model hinges in a subtle way on the interplay of\nall model parameters and on the liquidation time constraint. The paper can also\nbe seen as a case study for the regularity of market impact models in general.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1205.4008v4", "link": "http://arxiv.org/abs/1205.4008v4"}, {"title": "The randomised Heston model", "summary": "We propose a randomised version of the Heston model-a widely used stochastic\nvolatility model in mathematical finance-assuming that the starting point of\nthe variance process is a random variable. In such a system, we study the\nsmall-and large-time behaviours of the implied volatility, and show that the\nproposed randomisation generates a short-maturity smile much steeper (`with\nexplosion') than in the standard Heston model, thereby palliating the\ndeficiency of classical stochastic volatility models in short time. We\nprecisely quantify the speed of explosion of the smile for short maturities in\nterms of the right tail of the initial distribution, and in particular show\nthat an explosion rate of~$t^\\gamma$ ($\\gamma\\in[0,1/2]$) for the squared\nimplied volatility--as observed on market data--can be obtained by a suitable\nchoice of randomisation. The proofs are based on large deviations techniques\nand the theory of regular variations.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1608.07158v3", "link": "http://arxiv.org/abs/1608.07158v3"}, {"title": "A two-stage model for dealing with temporal degradation of credit\n  scoring", "summary": "This work is attached to the BRICS 2013 competition. We propose a two-stage\nmodel for dealing with the temporal degradation of credit scoring models. This\nmethodology produced motivating results in a 1-year horizon. We anticipate that\nit can be extended to other applications of risk assessment with great success.\nFuture extensions should cover predictions in larger time frames and consider\nlagged periods. This methodology can be further improved if more information\nabout the economic cycles is integrated in the forecasting of default.", "category": ["q-fin.RM", "q-fin.GN"], "id": "http://arxiv.org/abs/1406.7775v1", "link": "http://arxiv.org/abs/1406.7775v1"}, {"title": "Cascading Failures in Bi-partite Graphs: Model for Systemic Risk\n  Propagation", "summary": "As economic entities become increasingly interconnected, a shock in a\nfinancial network can provoke significant cascading failures throughout the\nsystem. To study the systemic risk of financial systems, we create a bi-partite\nbanking network model composed of banks and bank assets and propose a cascading\nfailure model to describe the risk propagation process during crises. We\nempirically test the model with 2007 US commercial banks balance sheet data and\ncompare the model prediction of the failed banks with the real failed banks\nafter 2007. We find that our model efficiently identifies a significant portion\nof the actual failed banks reported by Federal Deposit Insurance Corporation.\nThe results suggest that this model could be useful for systemic risk stress\ntesting for financial systems. The model also identifies that commercial rather\nthan residential real estate assets are major culprits for the failure of over\n350 US commercial banks during 2008-2011.", "category": ["q-fin.GN", "q-fin.RM"], "id": "http://arxiv.org/abs/1210.4973v3", "link": "http://dx.doi.org/10.1038/srep01219"}, {"title": "The Industry Supply Function and the Long-Run Competitive Equilibrium\n  with Heterogeneous Firms", "summary": "In developing the theory of long-run competitive equilibrium (LRCE), Marshall\n(1890) used the notion of a representative firm. The identity of this firm,\nhowever, remained unclear. Subsequent theory either focused on the case where\nall firms are identical or else incorporated heterogeneity but disregarded the\nnotion of a representative firm. Using Hopenhayn's (1992) model of competitive\nindustry dynamics, we extend the theory of LRCE to account for heterogeneous\nfirms and show that the long-run supply function can indeed be characterized as\nthe solution to the minimization of a representative average cost function.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1612.09549v3", "link": "http://arxiv.org/abs/1612.09549v3"}, {"title": "The Minority Game: a statistical physics perspective", "summary": "A brief review is given of the minority game, an idealized model stimulated\nby a market of speculative agents, and its complex many-body behaviour.\nParticular consideration is given to analytic results for the model rather than\ndiscussions of its relevance in real-world situations.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0512308v1", "link": "http://dx.doi.org/10.1016/j.physa.2006.04.039"}, {"title": "Bayesian DEJD model and detection of asymmetric jumps", "summary": "News might trigger jump arrivals in financial time series. The \"bad\" and\n\"good\" news seems to have distinct impact. In the research, a double\nexponential jump distribution is applied to model downward and upward jumps.\nBayesian double exponential jump-diffusion model is proposed. Theorems stated\nin the paper enable estimation of the model's parameters, detection of jumps\nand analysis of jump frequency. The methodology, founded upon the idea of\nlatent variables, is illustrated with two empirical studies, employing both\nsimulated and real-world data (the KGHM index). News might trigger jump\narrivals in financial time series. The \"bad\" and \"good\" news seems to have\ndistinct impact. In the research, a double exponential jump distribution is\napplied to model downward and upward jumps. Bayesian double exponential\njump-diffusion model is proposed. Theorems stated in the paper enable\nestimation of the model's parameters, detection of jumps and analysis of jump\nfrequency. The methodology, founded upon the idea of latent variables, is\nillustrated with two empirical studies, employing both simulated and real-world\ndata (the KGHM index).", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1404.2050v1", "link": "http://arxiv.org/abs/1404.2050v1"}, {"title": "DGM: A deep learning algorithm for solving partial differential\n  equations", "summary": "High-dimensional PDEs have been a longstanding computational challenge. We\npropose to solve high-dimensional PDEs by approximating the solution with a\ndeep neural network which is trained to satisfy the differential operator,\ninitial condition, and boundary conditions. Our algorithm is meshfree, which is\nkey since meshes become infeasible in higher dimensions. Instead of forming a\nmesh, the neural network is trained on batches of randomly sampled time and\nspace points. The algorithm is tested on a class of high-dimensional free\nboundary PDEs, which we are able to accurately solve in up to $200$ dimensions.\nThe algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE\nand Burgers' equation. The deep learning algorithm approximates the general\nsolution to the Burgers' equation for a continuum of different boundary\nconditions and physical conditions (which can be viewed as a high-dimensional\nspace). We call the algorithm a \"Deep Galerkin Method (DGM)\" since it is\nsimilar in spirit to Galerkin methods, with the solution approximated by a\nneural network instead of a linear combination of basis functions. In addition,\nwe prove a theorem regarding the approximation power of neural networks for a\nclass of quasilinear parabolic PDEs.", "category": ["q-fin.MF", "q-fin.CP"], "id": "http://arxiv.org/abs/1708.07469v5", "link": "http://dx.doi.org/10.1016/j.jcp.2018.08.029"}, {"title": "Momentum and liquidity in cryptocurrencies", "summary": "The goal of this paper is to explore the relationship between momentum\neffects and liquidity in cryptocurrency markets. Portfolios based on\nmomentum-liquidity bivariate sorts are formed and rebalanced on a varying\nnumber of cryptocurrencies through time. We find a strong momentum effect in\nthe most liquid cryptocurrencies, which supports the theories of investor\nherding behavior. Moreover, we propose two profitable long-only strategies: the\nilliquid losers and liquid winners, which exhibit improved risk adjusted\nperformance over the market capitalization weighted portfolio.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1904.00890v1", "link": "http://arxiv.org/abs/1904.00890v1"}, {"title": "Strategic behaviour and indicative price diffusion in Paris Stock\n  Exchange auctions", "summary": "We report statistical regularities of the opening and closing auctions of\nFrench equities, focusing on the diffusive properties of the indicative auction\nprice. Two mechanisms are at play as the auction end time nears: the typical\nprice change magnitude decreases, favoring underdiffusion, while the rate of\nthese events increases, potentially leading to overdiffusion. A third\nmechanism, caused by the strategic behavior of traders, is needed to produce\nnearly diffusive prices: waiting to submit buy orders until sell orders have\ndecreased the indicative price and vice-versa.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1807.00573v1", "link": "http://arxiv.org/abs/1807.00573v1"}, {"title": "A network approach to cartel detection in public auction markets", "summary": "Competing firms can increase profits by setting prices collectively, imposing\nsignificant costs on consumers. Such groups of firms are known as cartels and\nbecause this behavior is illegal, their operations are secretive and difficult\nto detect. Cartels feel a significant internal obstacle: members feel short-run\nincentives to cheat. Here we present a network-based framework to detect\npotential cartels in bidding markets based on the idea that the chance a group\nof firms can overcome this obstacle and sustain cooperation depends on the\npatterns of its interactions. We create a network of firms based on their\nco-bidding behavior, detect interacting groups, and measure their cohesion and\nexclusivity, two group-level features of their collective behavior. Applied to\na market for school milk, our method detects a known cartel and calculates that\nit has high cohesion and exclusivity. In a comprehensive set of nearly 150,000\npublic contracts awarded by the Republic of Georgia from 2011 to 2016, detected\ngroups with high cohesion and exclusivity are significantly more likely to\ndisplay traditional markers of cartel behavior. We replicate this relationship\nbetween group topology and the emergence of cooperation in a simulation model.\nOur method presents a scalable, unsupervised method to find groups of firms in\nbidding markets ideally positioned to form lasting cartels.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1906.08667v1", "link": "http://dx.doi.org/10.1038/s41598-019-47198-1"}, {"title": "A Method of Reducing Dimension of Space Variables in Multi-dimensional\n  Black-Scholes Equations", "summary": "We study a method of reducing space dimension in multi-dimensional\nBlack-Scholes partial differential equations as well as in multi-dimensional\nparabolic equations. We prove that a multiplicative transformation of space\nvariables in the Black-Scholes partial differential equation reserves the form\nof Black-Scholes partial differential equation and reduces the space dimension.\nWe show that this transformation can reduce the number of sources of risks by\ntwo or more in some cases by giving remarks and several examples of financial\npricing problems. We also present that the invariance of the form of\nBlack-Scholes equations is based on the invariance of the form of parabolic\nequation under a change of variables with the linear combination of variables.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1406.2053v1", "link": "http://arxiv.org/abs/1406.2053v1"}, {"title": "P2P Loan acceptance and default prediction with Artificial Intelligence", "summary": "Logistic Regression and Support Vector Machine algorithms, together with\nLinear and Non-Linear Deep Neural Networks, are applied to lending data in\norder to replicate lender acceptance of loans and predict the likelihood of\ndefault of issued loans. A two phase model is proposed; the first phase\npredicts loan rejection, while the second one predicts default risk for\napproved loans. Logistic Regression was found to be the best performer for the\nfirst phase, with test set recall macro score of $77.4 \\%$. Deep Neural\nNetworks were applied to the second phase only, were they achieved best\nperformance, with validation set recall score of $72 \\%$, for defaults. This\nshows that AI can improve current credit risk models reducing the default risk\nof issued loans by as much as $70 \\%$. The models were also applied to loans\ntaken for small businesses alone. The first phase of the model performs\nsignificantly better when trained on the whole dataset. Instead, the second\nphase performs significantly better when trained on the small business subset.\nThis suggests a potential discrepancy between how these loans are screened and\nhow they should be analysed in terms of default prediction.", "category": ["q-fin.RM", "q-fin.GN"], "id": "http://arxiv.org/abs/1907.01800v1", "link": "http://arxiv.org/abs/1907.01800v1"}, {"title": "Stochastic Local Intensity Loss Models with Interacting Particle Systems", "summary": "It is well-known from the work of Sch\\\"onbucher (2005) that the marginal laws\nof a loss process can be matched by a unit increasing time inhomogeneous Markov\nprocess, whose deterministic jump intensity is called local intensity. The\nStochastic Local Intensity (SLI) models such as the one proposed by Arnsdorf\nand Halperin (2008) allow to get a stochastic jump intensity while keeping the\nsame marginal laws. These models involve a non-linear SDE with jumps. The first\ncontribution of this paper is to prove the existence and uniqueness of such\nprocesses. This is made by means of an interacting particle system, whose\nconvergence rate towards the non-linear SDE is analyzed. Second, this approach\nprovides a powerful way to compute pathwise expectations with the SLI model: we\nshow that the computational cost is roughly the same as a crude Monte-Carlo\nalgorithm for standard SDEs.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1302.2009v1", "link": "http://dx.doi.org/10.1111/mafi.12059"}, {"title": "Emergence of Price Divergence in a Model Short-Term Electric Power\n  Market", "summary": "A minimal model of a market of myopic non-cooperative agents who trade\nbilaterally with random bids reproduces qualitative features of short-term\nelectric power markets, such as those in California and New England. Each agent\nknows its own budget and preferences but not those of any other agent. The\nnear-equilibrium price established mid-way through the trading session diverges\nto both much higher and much lower prices towards the end of the trading\nsession. This price divergence emerges in the model without any possibility\nthat the agents could have conspired to \"game\" the market. The results were\nweakly sensitive to the endowments but strongly sensitive to the nature of the\nagent's preferences and budget constraints.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0905.2366v1", "link": "http://arxiv.org/abs/0905.2366v1"}, {"title": "A new concept of technology with systemic-purposeful perpsective:\n  theory, examples and empirical application", "summary": "Although definitions of technology exist to explain the patterns of\ntechnological innovations, there is no general definition that explain the role\nof technology for humans and other animal species in environment. The goal of\nthis study is to suggest a new concept of technology with a systemic-purposeful\nperspective for technology analysis. Technology here is a complex system of\nartifact, made and_or used by living systems, that is composed of more than one\nentity or sub-system and a relationship that holds between each entity and at\nleast one other entity in the system, selected considering practical, technical\nand_or economic characteristics to satisfy needs, achieve goals and_or solve\nproblems of users for purposes of adaptation and_or survival in environment.\nTechnology T changes current modes of cognition and action to enable makers\nand_or users to take advantage of important opportunities or to cope with\nconsequential environmental threats. Technology, as a complex system, is formed\nby different elements given by incremental and radical innovations.\nTechnological change generates the progress from a system T1 to T2, T3, etc.\ndriven by changes of technological trajectories and technological paradigms.\nSeveral examples illustrate here these concepts and a simple model with a\npreliminary empirical analysis shows how to operationalize the suggested\ndefinition of technology. Overall, then, the role of adaptation (i.e.\nreproductive advantage) can be explained as a main driver of technology use for\nadopters to take advantage of important opportunities or to cope with\nenvironmental threats. This study begins the process of clarifying and\ngeneralizing, as far as possible, the concept of technology with a new\nperspective that it can lay a foundation for the development of more\nsophisticated concepts and theories to explain technological and economic\nchange in environment.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.05689v1", "link": "http://arxiv.org/abs/1909.05689v1"}, {"title": "Efficient and accurate log-L\u00e9vy approximations to L\u00e9vy driven LIBOR\n  models", "summary": "The LIBOR market model is very popular for pricing interest rate derivatives,\nbut is known to have several pitfalls. In addition, if the model is driven by a\njump process, then the complexity of the drift term is growing exponentially\nfast (as a function of the tenor length). In this work, we consider a\nL\\'evy-driven LIBOR model and aim at developing accurate and efficient\nlog-L\\'evy approximations for the dynamics of the rates. The approximations are\nbased on truncation of the drift term and Picard approximation of suitable\nprocesses. Numerical experiments for FRAs, caps, swaptions and sticky ratchet\ncaps show that the approximations perform very well. In addition, we also\nconsider the log-L\\'evy approximation of annuities, which offers good\napproximations for high volatility regimes.", "category": ["q-fin.CP", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1106.0866v2", "link": "http://arxiv.org/abs/1106.0866v2"}, {"title": "A transactional theory of fluctuations in company size", "summary": "Detailed empirical studies of publicly traded business firms have established\nthat the standard deviation of annual sales growth rates decreases with\nincreasing firm sales as a power law, and that the sales growth distribution is\nnon-Gaussian with slowly decaying tails. To explain these empirical facts, a\ntheory is developed that incorporates both the fluctuations of a single firm's\nsales and the statistical differences among many firms. The theory reproduces\nboth the scaling in the standard deviation and the non-Gaussian distribution of\ngrowth rates. Earlier models reproduce the same empirical features by splitting\nfirms into somewhat ambiguous subunits; by decomposing total sales into\nindividual transactions, this ambiguity is removed. The theory yields\nverifiable predictions and accommodates any form of business organization\nwithin a firm. Furthermore, because transactions are fundamental to economic\nactivity at all scales, the theory can be extended to all levels of the\neconomy, from individual products to multinational corporations.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0703023v1", "link": "http://arxiv.org/abs/physics/0703023v1"}, {"title": "On track for retirement?", "summary": "Over sixty percent of employees at a large South African financial services\ncompany select the minimum rate of 7.5 percent for their monthly retirement\ncontributions, far below the recommended rate of 15 percent. I use a field\nexperiment to investigate whether providing employees with a retirement\ncalculator, which shows projections of retirement income, leads to increases in\ncontributions. The average treatment effect is positive but very small and not\nstatistically different from zero.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.01692v1", "link": "http://arxiv.org/abs/2005.01692v1"}, {"title": "Stochastic finite differences and multilevel Monte Carlo for a class of\n  SPDEs in finance", "summary": "In this article, we propose a Milstein finite difference scheme for a\nstochastic partial differential equation (SPDE) describing a large particle\nsystem. We show, by means of Fourier analysis, that the discretisation on an\nunbounded domain is convergent of first order in the timestep and second order\nin the spatial grid size, and that the discretisation is stable with respect to\nboundary data. Numerical experiments clearly indicate that the same convergence\norder also holds for boundary-value problems. Multilevel path simulation,\npreviously used for SDEs, is shown to give substantial complexity gains\ncompared to a standard discretisation of the SPDE or direct simulation of the\nparticle system. We derive complexity bounds and illustrate the results by an\napplication to basket credit derivatives.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1204.1442v1", "link": "http://arxiv.org/abs/1204.1442v1"}, {"title": "Multiple time scales and the exponential Ornstein-Uhlenbeck stochastic\n  volatility model", "summary": "We study the exponential Ornstein-Uhlenbeck stochastic volatility model and\nobserve that the model shows a multiscale behavior in the volatility\nautocorrelation. It also exhibits a leverage correlation and a probability\nprofile for the stationary volatility which are consistent with market\nobservations. All these features make the model quite appealing since it\nappears to be more complete than other stochastic volatility models also based\non a two-dimensional diffusion. We finally present an approximate solution for\nthe return probability density designed to capture the kurtosis and skewness\neffects.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0501639v1", "link": "http://dx.doi.org/10.1080/14697680600727547"}, {"title": "Effects of the Affordable Care Act Dependent Coverage Mandate on Health\n  Insurance Coverage for Individuals in Same-Sex Couples", "summary": "A large body of research documents that the 2010 dependent coverage mandate\nof the Affordable Care Act was responsible for significantly increasing health\ninsurance coverage among young adults. No prior research has examined whether\nsexual minority young adults also benefitted from the dependent coverage\nmandate, despite previous studies showing lower health insurance coverage among\nsexual minorities and the fact that their higher likelihood of strained\nrelationships with their parents might predict a lower ability to use parental\ncoverage. Our estimates from the American Community Surveys using\ndifference-in-differences and event study models show that men in same-sex\ncouples age 21-25 were significantly more likely to have any health insurance\nafter 2010 compared to the associated change for slightly older 27 to\n31-year-old men in same-sex couples. This increase is concentrated among\nemployer-sponsored insurance, and it is robust to permutations of time periods\nand age groups. Effects for women in same-sex couples and men in different-sex\ncouples are smaller than the associated effects for men in same-sex couples.\nThese findings confirm the broad effects of expanded dependent coverage and\nsuggest that eliminating the federal dependent mandate could reduce health\ninsurance coverage among young adult sexual minorities in same-sex couples.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.02296v1", "link": "http://arxiv.org/abs/2004.02296v1"}, {"title": "Multivariate stable distributions and their applications for modelling\n  cryptocurrency-returns", "summary": "In this paper we extend the known methodology for fitting stable\ndistributions to the multivariate case and apply the suggested method to the\nmodelling of daily cryptocurrency-return data. The investigated time period is\ncut into 10 non-overlapping sections, thus the changes can also be observed. We\napply bootstrap tests for checking the models and compare our approach to the\nmore traditional extreme-value and copula models.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1810.09521v1", "link": "http://arxiv.org/abs/1810.09521v1"}, {"title": "Tracking VIX with VIX Futures: Portfolio Construction and Performance", "summary": "We study a series of static and dynamic portfolios of VIX futures and their\neffectiveness to track the VIX index. We derive each portfolio using\noptimization methods, and evaluate its tracking performance from both empirical\nand theoretical perspectives. Among our results, we show that static portfolios\nof different VIX futures fail to track VIX closely. VIX futures simply do not\nreact quickly enough to movements in the spot VIX. In a discrete-time model, we\ndesign and implement a dynamic trading strategy that adjusts daily to optimally\ntrack VIX. The model is calibrated to historical data and a simulation study is\nperformed to understand the properties exhibited by the strategy. In addition,\ncomparing to the volatility ETN, VXX, we find that our dynamic strategy has a\nsuperior tracking performance.", "category": ["q-fin.RM", "q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1907.00293v1", "link": "http://arxiv.org/abs/1907.00293v1"}, {"title": "Common Markets, Strong Currencies & the Collective Welfare", "summary": "The so called \"globalization\" process (i.e. the inexorable integration of\nmarkets, currencies, nation-states, technologies and the intensification of\nconsciousness of the world as a whole) has a behavior exactly equivalent to a\nsystem that is tending to a maximum entropy state. This globalization process\nobeys a collective welfare principle in where the maximum payoff is given by\nthe equilibrium of the system and its stability by the maximization of the\nwelfare of the collective besides the individual welfare. This let us predict\nthe apparition of big common markets and strong common currencies. They will\nreach the \"equilibrium\" by decreasing its number until they reach a state\ncharacterized by only one common currency and only one big common community\naround the world.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0710.1307v3", "link": "http://arxiv.org/abs/0710.1307v3"}, {"title": "Regressions with Berkson errors in covariates - A nonparametric approach", "summary": "This paper establishes that so-called instrumental variables enable the\nidentification and the estimation of a fully nonparametric regression model\nwith Berkson-type measurement error in the regressors. An estimator is proposed\nand proven to be consistent. Its practical performance and feasibility are\ninvestigated via Monte Carlo simulations as well as through an epidemiological\napplication investigating the effect of particulate air pollution on\nrespiratory health. These examples illustrate that Berkson errors can clearly\nnot be neglected in nonlinear regression models and that the proposed method\nrepresents an effective remedy.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1308.2836v1", "link": "http://dx.doi.org/10.1214/13-AOS1122"}, {"title": "Denting the FRTB IMA computational challenge via Orthogonal Chebyshev\n  Sliding Technique", "summary": "In this paper we introduce a new technique based on high-dimensional\nChebyshev Tensors that we call \\emph{Orthogonal Chebyshev Sliding Technique}.\nWe implemented this technique inside the systems of a tier-one bank, and used\nit to approximate Front Office pricing functions in order to reduce the\nsubstantial computational burden associated with the capital calculation as\nspecified by FRTB IMA. In all cases, the computational burden reductions\nobtained were of more than $90\\%$, while keeping high degrees of accuracy, the\nlatter obtained as a result of the mathematical properties enjoyed by Chebyshev\nTensors.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1911.10948v1", "link": "http://arxiv.org/abs/1911.10948v1"}, {"title": "Optimal partial hedging in a discrete-time market as a knapsack problem", "summary": "We present a new approach for studying the problem of optimal hedging of a\nEuropean option in a finite and complete discrete-time market model. We\nconsider partial hedging strategies that maximize the success probability or\nminimize the expected shortfall under a cost constraint and show that these\nproblems can be treated as so called knapsack problems, which are a widely\nresearched subject in linear programming. This observation gives us better\nunderstanding of the problem of optimal hedging in discrete time.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0910.5101v1", "link": "http://arxiv.org/abs/0910.5101v1"}, {"title": "Positive skewness, anti-leverage, reverse volatility asymmetry, and\n  short sale constraints: Evidence from the Chinese markets", "summary": "There are some statistical anomalies in the Chinese stock market, i.e.,\npositive return skewness, anti-leverage effect (positive returns induce higher\nvolatility than negative returns); and reverse volatility asymmetry\n(contemporaneous return-volatility correlation is positive). In this paper, we\nfirst confirm the existence of these anomalies using daily firm-level stock\nreturn data on the raw returns, excess returns and normalized excess returns.\nWe empirically show that the asymmetry response of investors to news is one\ncause of the statistical anomalies if short sales are constrained. Then in the\ncontext of slow adoption of security lending policy, we conduct panel analysis\nand empirically verify that the lifting of short sale constraints leads to\nsignificantly less skewness, less anti-leverage effect and less reverse\nvolatility asymmetry. Positive skewness is a feature of lottery. Investors are\nencouraged to bet on the upside lottery like potentials in the Chinese markets\nwhere the stocks skew more to the upside when short sales are constrained.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1511.01824v1", "link": "http://arxiv.org/abs/1511.01824v1"}, {"title": "Speculative Futures Trading under Mean Reversion", "summary": "This paper studies the problem of trading futures with transaction costs when\nthe underlying spot price is mean-reverting. Specifically, we model the spot\ndynamics by the Ornstein-Uhlenbeck (OU), Cox-Ingersoll-Ross (CIR), or\nexponential Ornstein-Uhlenbeck (XOU) model. The futures term structure is\nderived and its connection to futures price dynamics is examined. For each\nfutures contract, we describe the evolution of the roll yield, and compute\nexplicitly the expected roll yield. For the futures trading problem, we\nincorporate the investor's timing option to enter or exit the market, as well\nas a chooser option to long or short a futures upon entry. This leads us to\nformulate and solve the corresponding optimal double stopping problems to\ndetermine the optimal trading strategies. Numerical results are presented to\nillustrate the optimal entry and exit boundaries under different models. We\nfind that the option to choose between a long or short position induces the\ninvestor to delay market entry, as compared to the case where the investor\npre-commits to go either long or short.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1601.04210v1", "link": "http://arxiv.org/abs/1601.04210v1"}, {"title": "Applications of statistical mechanics to economics: Entropic origin of\n  the probability distributions of money, income, and energy consumption", "summary": "This Chapter is written for the Festschrift celebrating the 70th birthday of\nthe distinguished economist Duncan Foley from the New School for Social\nResearch in New York. This Chapter reviews applications of statistical physics\nmethods, such as the principle of entropy maximization, to the probability\ndistributions of money, income, and global energy consumption per capita. The\nexponential probability distribution of wages, predicted by the statistical\nequilibrium theory of a labor market developed by Foley in 1996, is supported\nby empirical data on income distribution in the USA for the majority (about\n97%) of population. In addition, the upper tail of income distribution (about\n3% of population) follows a power law and expands dramatically during financial\nbubbles, which results in a significant increase of the overall income\ninequality. A mathematical analysis of the empirical data clearly demonstrates\nthe two-class structure of a society, as pointed out Karl Marx and recently\nhighlighted by the Occupy Movement. Empirical data for the energy consumption\nper capita around the world are close to an exponential distribution, which can\nbe also explained by the entropy maximization principle.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1204.6483v1", "link": "http://arxiv.org/abs/1204.6483v1"}, {"title": "An Optimized Microeconomic Modeling System for Analyzing Industrial\n  Externalities in Non-OECD Countries", "summary": "In this paper, we provide an integrated systems modeling approach to\nanalyzing global externalities from a microeconomic perspective. Various forms\nof policy (fiscal, monetary, etc.) have addressed flaws and market failures in\nmodels, but few have been able to successfully eliminate modern externalities\nthat remain an environmental and human threat. We assess three primary global\nindustries (pollution, agriculture, and energy) with respect to non-OECD\nentities through both qualitative and quantitative studies. By combining key\nmutual points of specific externalities present within each respective\nindustry, we are able to propose an alternative and optimized solution to\ninternalizing them via incentives and cooperative behavior rather than by\ntraditional Pigouvian taxes and subsidies.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1710.02755v1", "link": "http://arxiv.org/abs/1710.02755v1"}, {"title": "An Explicit Martingale Version of Brenier's Theorem", "summary": "By investigating model-independent bounds for exotic options in financial\nmathematics, a martingale version of the Monge-Kantorovich mass transport\nproblem was introduced in \\cite{BeiglbockHenry\nLaborderePenkner,GalichonHenry-LabordereTouzi}. In this paper, we extend the\none-dimensional Brenier's theorem to the present martingale version. We provide\nthe explicit martingale optimal transference plans for a remarkable class of\ncoupling functions corresponding to the lower and upper bounds. These explicit\nextremal probability measures coincide with the unique left and right monotone\nmartingale transference plans, which were introduced in \\cite{BeiglbockJuillet}\nby suitable adaptation of the notion of cyclic monotonicity. Instead, our\napproach relies heavily on the (weak) duality result stated in\n\\cite{BeiglbockHenry-LaborderePenkner}, and provides, as a by-product, an\nexplicit expression for the corresponding optimal semi-static hedging\nstrategies. We finally provide an extension to the multiple marginals case.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1302.4854v3", "link": "http://arxiv.org/abs/1302.4854v3"}, {"title": "The Mathematics of Market Timing", "summary": "Market timing is an investment technique that tries to continuously switch\ninvestment into assets forecast to have better returns. What is the likelihood\nof having a successful market timing strategy? With an emphasis on modeling\nsimplicity, I calculate the feasible set of market timing portfolios using\nindex mutual fund data for perfectly timed (by hindsight) all or nothing\nquarterly switching between two asset classes, US stocks and bonds over the\ntime period 1993--2017. The historical optimal timing path of switches is shown\nto be indistinguishable from a random sequence. The key result is that the\nprobability distribution function of market timing returns is asymetric, that\nthe highest probability outcome for market timing is a below median return. Put\nanother way, simple math says market timing is more likely to lose than to\nwin---even before accounting for costs. The median of the market timing return\nprobability distribution can be directly calculated as a weighted average of\nthe returns of the model assets with the weights given by the fraction of time\neach asset has a higher return than the other. For the time period of the data\nthe median return was close to, but not identical with, the return of a static\n60:40 stock:bond portfolio. These results are illustrated through Monte Carlo\nsampling of timing paths within the feasible set and by the observed return\npaths of several market timing mutual funds.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1712.05031v1", "link": "http://dx.doi.org/10.1371/journal.pone.0200561"}, {"title": "Asymmetric volatility connectedness on forex markets", "summary": "We show how bad and good volatility propagate through forex markets, i.e., we\nprovide evidence for asymmetric volatility connectedness on forex markets.\nUsing high-frequency, intra-day data of the most actively traded currencies\nover 2007 - 2015 we document the dominating asymmetries in spillovers that are\ndue to bad rather than good volatility. We also show that negative spillovers\nare chiefly tied to the dragging sovereign debt crisis in Europe while positive\nspillovers are correlated with the subprime crisis, different monetary policies\namong key world central banks, and developments on commodities markets. It\nseems that a combination of monetary and real-economy events is behind the net\npositive asymmetries in volatility spillovers, while fiscal factors are linked\nwith net negative spillovers.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1607.08214v1", "link": "http://arxiv.org/abs/1607.08214v1"}, {"title": "Kinetic theory and Brazilian income distribution", "summary": "We investigate the Brazilian personal income distribution using data from\nNational Household Sample Survey (PNAD), an annual research available by the\nBrazilian Institute of Geography and Statistics (IBGE). It provides general\ncharacteristics of the country's population. Using PNAD data background we also\nconfirm the effectiveness of a semi-empirical model that reconciles Pareto\npower-law for high-income people and Boltzmann- Gibbs distribution for the rest\nof population. We use three measures of income inequality: the Pareto index,\nthe average income and the crossover income. In order to cope with many\ndimensions of the income inequality, we calculate these three indices and also\nthe Gini coefficient for the general population as well as for two kinds of\npopulation dichotomies: black / indigenous / mixed race versus white / yellow;\nand men versus women. We also followed the time series of these indices for the\nperiod 2001-2014. The results suggest a decreasing of Brazilian income\ninequality over the selected period. Another important result is that\nhistorically-disadvantaged subgroups (Women and black / indigenous / mixed\nrace),that are the majority of the population, have a more equalitarian income\ndistribution. These groups have also a smaller monthly income than the others\nand this social structure remained virtually unchanged in the period of time.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1709.06480v1", "link": "http://arxiv.org/abs/1709.06480v1"}, {"title": "Time discounting under uncertainty", "summary": "We study intertemporal decision making under uncertainty. We fully\ncharacterize discounted expected utility in a framework \\`a la Savage. Despite\nthe popularity of this model, no characterization is available in this setting.\nThe concept of stationarity, introduced by Koopmans for deterministic\ndiscounted utility, plays a central role for both attitudes towards time and\ntowards uncertainty. We show that a strong stationarity axiom characterizes\ndiscounted expected utility. When hedging considerations are taken into\naccount, a weaker stationarity axiom generalizes discounted expected utility to\nChoquet discounted expected utility, allowing for non-neutral attitudes towards\nuncertainty.", "category": [], "id": "http://arxiv.org/abs/1911.00370v2", "link": "http://arxiv.org/abs/1911.00370v2"}, {"title": "Learning short-option valuation in the presence of rare events", "summary": "We present a neural-network valuation of financial derivatives in the case of\nfat-tailed underlying asset returns. A two-layer perceptron is trained on\nsimulated prices taking into account the well-known effect of volatility smile.\nThe prices of the underlier are generated using fractional calculus algorithms,\nand option prices are computed by means of the Bouchaud-Potters formula. This\nlearning scheme is tested on market data; the results show a very good\nagreement between perceptron option prices and real market ones.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/0001253v1", "link": "http://dx.doi.org/10.1142/S0219024900000590"}, {"title": "Boundary-degenerate elliptic operators and Holder continuity for\n  solutions to variational equations and inequalities", "summary": "The Heston stochastic volatility process, which is widely used as an asset\nprice model in mathematical finance, is a paradigm for a degenerate diffusion\nprocess where the degeneracy in the diffusion coefficient is proportional to\nthe square root of the distance to the boundary of the half-plane. The\ngenerator of this process with killing, called the elliptic Heston operator, is\na second-order, degenerate-elliptic partial differential operator whose\ncoefficients have linear growth in the spatial variables and where the\ndegeneracy in the operator symbol is proportional to the distance to the\nboundary of the half-plane. With the aid of weighted Sobolev spaces, we prove\nsupremum bounds, a Harnack inequality, and H\\\"older continuity near the\nboundary for solutions to variational equations defined by the elliptic Heston\noperator, as well as H\\\"older continuity up to the boundary for solutions to\nvariational inequalities defined by the elliptic Heston operator. In\nmathematical finance, solutions to obstacle problems for the elliptic Heston\noperator correspond to value functions for perpetual American-style options on\nthe underlying asset.", "category": ["math.PR", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1110.5594v4", "link": "http://arxiv.org/abs/1110.5594v4"}, {"title": "Cascades of Dynamical Transitions in an Adaptive Population", "summary": "In an adaptive population which models financial markets and distributed\ncontrol, we consider how the dynamics depends on the diversity of the agents'\ninitial preferences of strategies. When the diversity decreases, more agents\ntend to adapt their strategies together. This change in the environment results\nin dynamical transitions from vanishing to non-vanishing step sizes. When the\ndiversity decreases further, we find a cascade of dynamical transitions for the\ndifferent signal dimensions, supported by good agreement between simulations\nand theory. Besides, the signal of the largest step size at the steady state is\nlikely to be the initial signal.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0609230v1", "link": "http://arxiv.org/abs/physics/0609230v1"}, {"title": "Dynamic Pricing and Learning with Competition: Insights from the Dynamic\n  Pricing Challenge at the 2017 INFORMS RM & Pricing Conference", "summary": "This paper presents the results of the Dynamic Pricing Challenge, held on the\noccasion of the 17th INFORMS Revenue Management and Pricing Section Conference\non June 29-30, 2017 in Amsterdam, The Netherlands. For this challenge,\nparticipants submitted algorithms for pricing and demand learning of which the\nnumerical performance was analyzed in simulated market environments. This\nallows consideration of market dynamics that are not analytically tractable or\ncan not be empirically analyzed due to practical complications. Our findings\nimplicate that the relative performance of algorithms varies substantially\nacross different market dynamics, which confirms the intrinsic complexity of\npricing and learning in the presence of competition.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1804.03219v1", "link": "http://dx.doi.org/10.1057/s41272-018-00164-4"}, {"title": "On the Optimal Dividend Problem for Insurance Risk Models with\n  Surplus-Dependent Premiums", "summary": "This paper concerns an optimal dividend distribution problem for an insurance\ncompany with surplus-dependent premium. In the absence of dividend payments,\nsuch a risk process is a particular case of so-called piecewise deterministic\nMarkov processes. The control mechanism chooses the size of dividend payments.\nThe objective consists in maximazing the sum of the expected cumulative\ndiscounted dividend payments received until the time of ruin and a penalty\npayment at the time of ruin, which is an increasing function of the size of the\nshortfall at ruin. A complete solution is presented to the corresponding\nstochastic control problem. We identify the associated Hamilton-Jacobi-Bellman\nequation and find necessary and sufficient conditions for optimality of a\nsingle dividend-band strategy, in terms of particular Gerber-Shiu functions. A\nnumber of concrete examples are analyzed.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1604.06892v1", "link": "http://arxiv.org/abs/1604.06892v1"}, {"title": "Semiclosed Pricing Mechanism", "summary": "This paper aims at designing the different important components of a\nsemi-closed simulated stock market (pricing mechanism, stock allocation and\nnews generation). The purpose is to understand the interactions of the\ndifferent aspects within a 'semi-closed' system. The complexity and nature of\nthe system led to the process of modifying the pricing mechanism which is\nviewed from a different angle to the classical Brownian Motion and the Random\nWalk model. However, it incorporates the essence of these two fundamental\ntheories and then investigates the matrix of investors' behaviours in relation\nto news feedbacks. This paper also explores the realm of randomly generated\nnews to the responses of participants to determine rational and irrational\nbehaviours. This is carried out through uncompressing the time within the\nexperiment and looking at concordant and disconcordant behaviour. The focus is\non how the modified pricing equation adapts to the conditions and uniqueness\nsurrounding a semi-closed stock market. Thus, this paper looks at how a simple\nmarket system where the main determinants of share prices are news, demand and\nsupply along with some filtering of the external forces can affect the\nbehaviours of investors in terms of their portfolio composition. The return\ndistributions can then be stipulated as arising from rational or irrational\ntrajectories and subsequently be simulated and matched via the proposed\nmodified Brownian Motion model to empirical return distributions in specific\ntime periods and markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1112.0342v2", "link": "http://arxiv.org/abs/1112.0342v2"}, {"title": "The beneficial role of random strategies in social and financial systems", "summary": "In this paper we focus on the beneficial role of random strategies in social\nsciences by means of simple mathematical and computational models. We briefly\nreview recent results obtained by two of us in previous contributions for the\ncase of the Peter principle and the efficiency of a Parliament. Then, we\ndevelop a new application of random strategies to the case of financial trading\nand discuss in detail our findings about forecasts of markets dynamics.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1209.5881v2", "link": "http://dx.doi.org/10.1007/s10955-013-0691-2"}, {"title": "The pricing of contingent claims and optimal positions in asymptotically\n  complete markets", "summary": "We study utility indifference prices and optimal purchasing quantities for a\ncontingent claim, in an incomplete semi-martingale market, in the presence of\nvanishing hedging errors and/or risk aversion. Assuming that the average\nindifference price converges to a well defined limit, we prove that optimally\ntaken positions become large in absolute value at a specific rate. We draw\nmotivation from and make connections to Large Deviations theory, and in\nparticular, the celebrated G\\\"{a}rtner-Ellis theorem. We analyze a series of\nwell studied examples where this limiting behavior occurs, such as fixed\nmarkets with vanishing risk aversion, the basis risk model with high\ncorrelation, models of large markets with vanishing trading restrictions and\nthe Black-Scholes-Merton model with either vanishing default probabilities or\nvanishing transaction costs. Lastly, we show that the large claim regime could\nnaturally arise in partial equilibrium models.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1509.06210v2", "link": "http://arxiv.org/abs/1509.06210v2"}, {"title": "Testing the Goodwin growth-cycle macroeconomic dynamics in Brazil", "summary": "This paper discusses the empirical validity of Goodwin's (1967) macroeconomic\nmodel of growth with cycles by assuming that the individual income distribution\nof the Brazilian society is described by the Gompertz-Pareto distribution\n(GPD). This is formed by the combination of the Gompertz curve, representing\nthe overwhelming majority of the population (~99%), with the Pareto power law,\nrepresenting the tiny richest part (~1%). In line with Goodwin's original\nmodel, we identify the Gompertzian part with the workers and the Paretian\ncomponent with the class of capitalists. Since the GPD parameters are obtained\nfor each year and the Goodwin macroeconomics is a time evolving model, we use\npreviously determined, and further extended here, Brazilian GPD parameters, as\nwell as unemployment data, to study the time evolution of these quantities in\nBrazil from 1981 to 2009 by means of the Goodwin dynamics. This is done in the\noriginal Goodwin model and an extension advanced by Desai et al. (2006). As far\nas Brazilian data is concerned, our results show partial qualitative and\nquantitative agreement with both models in the studied time period, although\nthe original one provides better data fit. Nevertheless, both models fall short\nof a good empirical agreement as they predict single center cycles which were\nnot found in the data. We discuss the specific points where the Goodwin\ndynamics must be improved in order to provide a more realistic representation\nof the dynamics of economic systems.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1301.1090v3", "link": "http://dx.doi.org/10.1016/j.physa.2013.01.024"}, {"title": "Pricing Variable Annuity Guarantees in a Local Volatility framework", "summary": "In this paper, we study the price of Variable Annuity Guarantees, especially\nof Guaranteed Annuity Options (GAO) and Guaranteed Minimum Income Benefit\n(GMIB), and this in the settings of a derivative pricing model where the\nunderlying spot (the fund) is locally governed by a geometric Brownian motion\nwith local volatility, while interest rates follow a Hull-White one-factor\nGaussian model. Notwithstanding the fact that in this framework, the local\nvolatility depends on a particularly complicated expectation where no\nclosed-form expression exists and it is neither directly related to European\ncall prices or other liquid products, we present in this contribution different\nmethods to calibrate the local volatility model. We further compare Variable\nAnnuity Guarantee prices obtained in three different settings, namely the local\nvolatility, the stochastic volatility and the constant volatility models all\ncombined with stochastic interest rates and show that an appropriate volatility\nmodelling is important for these long-dated derivatives. More precisely, we\ncompare prices of GAO, GMIB Rider and barrier types GAO obtained by using local\nvolatility, stochastic volatility and constant volatility models.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1204.0453v2", "link": "http://arxiv.org/abs/1204.0453v2"}, {"title": "The Minimal Model of Financial Complexity", "summary": "A representative investor generates realistic and complex security price\npaths by following this trading strategy: if, a few ticks ago, the market asset\nhad two consecutive upticks or two consecutive downticks, then sell, and\notherwise buy. This simple, unique, and robust model is the smallest possible\ndeterministic model of financial complexity, and its generalization leads to\ncomplex variety. Compared to a random walk, the minimal model generates time\nseries with fatter tails and more frequent crashes, thus more closely matching\nthe real world. It does all this without any parameter fitting.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0901.3812v2", "link": "http://arxiv.org/abs/0901.3812v2"}, {"title": "On Capturing the Spreading Dynamics over Trading Prices in the Market", "summary": "While market is a social field where information flows over the interacting\nagents, there have been not so many methods to observe the spreading\ninformation in the prices comprising the market. By incorporating the entropy\ntransfer in information theory in its relation to the Granger causality, the\npaper proposes a tree of weighted directed graph of market to detect the\nchanges of price might affect other price changes. We compare the proposed\nanalysis with the similar tree representation built from the correlation\ncoefficients of stock prices in order to have insight of possibility in seeing\nthe collective behavior of the market in general.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1510.04690v1", "link": "http://arxiv.org/abs/1510.04690v1"}, {"title": "A Bayesian Non-Parametric Approach to Asymmetric Dynamic Conditional\n  Correlation Model With Application to Portfolio Selection", "summary": "We propose a Bayesian non-parametric approach for modeling the distribution\nof multiple returns. In particular, we use an asymmetric dynamic conditional\ncorrelation (ADCC) model to estimate the time-varying correlations of financial\nreturns where the individual volatilities are driven by GJR-GARCH models. The\nADCC-GJR-GARCH model takes into consideration the asymmetries in individual\nassets' volatilities, as well as in the correlations. The errors are modeled\nusing a Dirichlet location-scale mixture of multivariate Gaussian distributions\nallowing for a great flexibility in the return distribution in terms of\nskewness and kurtosis. Model estimation and prediction are developed using MCMC\nmethods based on slice sampling techniques. We carry out a simulation study to\nillustrate the flexibility of the proposed approach. We find that the proposed\nDPM model is able to adapt to several frequently used distribution models and\nalso accurately estimates the posterior distribution of the volatilities of the\nreturns, without assuming any underlying distribution. Finally, we present a\nfinancial application using Apple and NASDAQ Industrial index data to solve a\nportfolio allocation problem. We find that imposing a restrictive parametric\ndistribution can result into underestimation of the portfolio variance, whereas\nDPM model is able to overcome this problem.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1301.5129v2", "link": "http://dx.doi.org/10.1016/j.csda.2014.12.005"}, {"title": "Pricing Bermudan options under local L\u00e9vy models with default", "summary": "We consider a defaultable asset whose risk-neutral pricing dynamics are\ndescribed by an exponential L\\'evy-type martingale. This class of models allows\nfor a local volatility, local default intensity and a locally dependent L\\'evy\nmeasure. We present a pricing method for Bermudan options based on an\nanalytical approximation of the characteristic function combined with the COS\nmethod. Due to a special form of the obtained characteristic function the price\ncan be computed using a Fast Fourier Transform-based algorithm resulting in a\nfast and accurate calculation. The Greeks can be computed at almost no\nadditional computational cost. Error bounds for the approximation of the\ncharacteristic function as well as for the total option price are given.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1604.08735v1", "link": "http://arxiv.org/abs/1604.08735v1"}, {"title": "Modelling the health impact of food taxes and subsidies with price\n  elasticities: the case for additional scaling of food consumption using the\n  total food expenditure elasticity", "summary": "Background Food taxes and subsidies are one intervention to address poor\ndiets. Price elasticity (PE) matrices are commonly used to model the change in\nfood purchasing. Usually a PE matrix is generated in one setting then applied\nto another setting with differing starting consumption and prices of foods.\nThis violates econometric assumptions resulting in likely misestimation of\ntotal food consumption. We illustrate rescaling all consumption after applying\na PE matrix using a total food expenditure elasticity (TFEe, the expenditure\nelasticity for all food combined given the policy induced change in the total\nprice of food). We use case studies of NZ$2 per 100g saturated fat (SAFA) tax,\nNZ$0.4 per 100g sugar tax, and a 20% fruit and vegetable (F&V) subsidy. Methods\nWe estimated changes in food purchasing using a NZ PE matrix applied\nconventionally, then with TFEe adjustment. Impacts were quantified for total\nfood expenditure and health adjusted life years (HALYs) for the total NZ\npopulation alive in 2011 over the rest of their lifetime using a multistate\nlifetable model. Results Two NZ studies gave TFEes of 0.68 and 0.83, with\ninternational estimates ranging from 0.46 to 0.90. Without TFEe adjustment,\ntotal food expenditure decreased with the tax policies and increased with the\nF&V subsidy, implausible directions of shift given economic theory. After TFEe\nadjustment, HALY gains reduced by a third to a half for the two taxes and\nreversed from an apparent health loss to a health gain for the F&V subsidy.\nWith TFEe adjustment, HALY gains (in 1000s) were 1,805 (95% uncertainty\ninterval 1,337 to 2,340) for the SAFA tax, 1,671 (1,220 to 2,269) for the sugar\ntax, and 953 (453 to 1,308) for the F&V subsidy. Conclusions If PE matrices are\napplied in settings beyond where they were derived, additional scaling is\nlikely required. We suggest that the TFEe is a useful scalar.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.13179v1", "link": "http://arxiv.org/abs/1909.13179v1"}, {"title": "Weak Limits of Random Coefficient Autoregressive Processes and their\n  Application in Ruin Theory", "summary": "We prove that a large class of discrete-time insurance surplus processes\nconverge weakly to a generalized Ornstein-Uhlenbeck process, under a suitable\nre-normalization and when the time-step goes to 0. Motivated by ruin theory, we\nuse this result to obtain approximations for the moments, the ultimate ruin\nprobability and the discounted penalty function of the discrete-time process.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1907.01828v2", "link": "http://arxiv.org/abs/1907.01828v2"}, {"title": "What do adoption patterns of solar panels observed so far tell about\n  governments' incentive? insight from diffusion models", "summary": "The paper uses diffusion models to understand the main determinants of\ndiffusion of solar photovoltaic panels (SPP) worldwide, focusing on the role of\npublic incentives. We applied the generalized Bass model (GBM) to adoption data\nof 26 countries between 1992-2016. The SPP market appears as a frail and\ncomplicate one, lacking public media support. Even the major shocks in adoption\ncurves, following state incentive implemented after 2006, failed to go beyond\nshort-term effects and therefore were unable to provide sustained momentum to\nthe market. This suggests that further barriers to adoption should be removed.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.10017v1", "link": "http://arxiv.org/abs/1909.10017v1"}, {"title": "Correlations Between Reconstructed EUR Exchange Rates vs. CHF, DKK, GBP,\n  JPY and USD", "summary": "On Jan. 1, 1999 the European Union introduced a common currency Euro ($EUR$),\nto become the legal currency in all eleven countries which form the $EUR$. In\norder to test the $EUR$ behavior and understand various features, the $EUR$\nexchange rate is artificially extrapolated back to 1993 by a linear\nsuperposition of the exchange rates of the 11 currencies composing $EUR$ with\nrespect to several currencies not belonging to the $EUR$, i.e. Swiss Franc\n($CHF$), Danish Kroner ($DKK$), British Pound ($GBP$), Japanese Yen ($JPY$) and\nU.S. Dollar ($USD$) of interest for reasons given in the text. The distribution\nof fluctuations of the exchange rates is shown to be Gaussian for the central\npart of the distribution, and having fat tails for the large size fluctuations.\nWithin the {\\it Detrended Fluctuation Analysis} ($DFA$) statistical method we\nhave obtained the power law behavior describing the root-mean-square deviation\nof the exchange rate fluctuations as a function of time. For the period between\nJan. 1995 and Jan. 1999 we have compared the time-dependent exponent of these\nexchange rate fluctuations for $EUR$ and that of the 11 currencies which form\nthe $EUR$. The German Mark ($DEM$) and the French Franc ($FRF$) have been the\ncurrencies primarily leading the fluctuations of the exchange rates, while\nItalian Lira ($ITL$) and ($PTE$) Portuguese Escudo are the less relevant\ncurrencies from this point of view. Technical considerations for the $EUR$\nimplementation are given as conclusions. The cases of exchange rates with $DKK$\nappear quite different from the other four major currencies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0104260v1", "link": "http://dx.doi.org/10.1142/S0129183101001572"}, {"title": "Closed-form approximations with respect to the mixing solution for\n  option pricing under stochastic volatility", "summary": "We consider closed-form approximations for European put option prices within\nseveral stochastic volatility frameworks with time-dependent parameters. Our\nmethodology involves writing the put option price as an expectation of a\nBlack-Scholes formula and performing a second-order Taylor expansion around the\nmean of its argument. The difficulties then faced are simplifying a number of\nexpectations induced by the Taylor expansion. Under the assumption of\npiecewise-constant parameters, we derive explicit pricing formulas and devise a\nfast calibration scheme. Furthermore, we perform a numerical error and\nsensitivity analysis to investigate the quality of our approximation and show\nthat the errors are well within the acceptable range for application purposes.\nLastly, we derive bounds on the remainder term generated by the Taylor\nexpansion.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1812.07803v5", "link": "http://arxiv.org/abs/1812.07803v5"}, {"title": "Analysis of continuous strict local martingales via h-transforms", "summary": "We study strict local martingales via h-transforms, a method which first\nappeared in Delbaen-Schachermayer. We show that strict local martingales arise\nwhenever there is a consistent family of change of measures where the two\nmeasures are not equivalent to one another. Several old and new strict local\nmartingales are identified. We treat examples of diffusions with various\nboundary behavior, size-bias sampling of diffusion paths, and non-colliding\ndiffusions. A multidimensional generalization to conformal strict local\nmartingales is achieved through Kelvin transform. As curious examples of\nnon-standard behavior, we show by various examples that strict local\nmartingales do not behave uniformly when the function (x-K)^+ is applied to\nthem. Implications to the recent literature on financial bubbles are discussed.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0711.1136v4", "link": "http://dx.doi.org/10.1016/j.spa.2010.04.004"}, {"title": "Calibration of Local Volatility Model with Stochastic Interest Rates by\n  Efficient Numerical PDE Method", "summary": "Long maturity options or a wide class of hybrid products are evaluated using\na local volatility type modelling for the asset price S(t) with a stochastic\ninterest rate r(t). The calibration of the local volatility function is usually\ntime-consuming because of the multi-dimensional nature of the problem. In this\npaper, we develop a calibration technique based on a partial differential\nequation (PDE) approach which allows an efficient implementation. The essential\nidea is based on solving the derived forward equation satisfied by P(t; S;\nr)Z(t; S; r), where P(t; S; r) represents the risk neutral probability density\nof (S(t); r(t)) and Z(t; S; r) the projection of the stochastic discounting\nfactor in the state variables (S(t); r(t)). The solution provides effective and\nsufficient information for the calibration and pricing. The PDE solver is\nconstructed by using ADI (Alternative Direction Implicit) method based on an\nextension of the Peaceman-Rachford scheme. Furthermore, an efficient algorithm\nto compute all the corrective terms in the local volatility function due to the\nstochastic interest rates is proposed by using the PDE solutions and grid\npoints. Different numerical experiments are examined and compared to\ndemonstrate the results of our theoretical analysis.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1803.03941v1", "link": "http://arxiv.org/abs/1803.03941v1"}, {"title": "Quantum Structures in Human Decision-making: Towards Quantum Expected\n  Utility", "summary": "{\\it Ellsberg thought experiments} and empirical confirmation of Ellsberg\npreferences pose serious challenges to {\\it subjective expected utility theory}\n(SEUT). We have recently elaborated a quantum-theoretic framework for human\ndecisions under uncertainty which satisfactorily copes with the Ellsberg\nparadox and other puzzles of SEUT. We apply here the quantum-theoretic\nframework to the {\\it Ellsberg two-urn example}, showing that the paradox can\nbe explained by assuming a state change of the conceptual entity that is the\nobject of the decision ({\\it decision-making}, or {\\it DM}, {\\it entity}) and\nrepresenting subjective probabilities by quantum probabilities. We also model\nthe empirical data we collected in a DM test on human participants within the\ntheoretic framework above. The obtained results are relevant, as they provide a\nline to model real life, e.g., financial and medical, decisions that show the\nsame empirical patterns as the two-urn experiment.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.00875v1", "link": "http://dx.doi.org/10.1007/s10773-019-04022-w"}, {"title": "Liquidity Management with Decreasing-returns-to-scale and Secured Credit\n  Line", "summary": "This paper examines the dividend and investment policies of a cash\nconstrained firm that has access to costly external funding. We depart from the\nliterature by allowing the firm to issue collateralized debt to increase its\ninvestment in productive assets resulting in a performance sensitive interest\nrate on debt. We formulate this problem as a bi-dimensional singular control\nproblem and use both a viscosity solution approch and a verification technique\nto get qualitative properties of the value function. We further solve\nquasi-explicitly the control problem in two special cases.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1411.7670v2", "link": "http://arxiv.org/abs/1411.7670v2"}, {"title": "How many independent bets are there?", "summary": "The benefits of portfolio diversification is a central tenet implicit to\nmodern financial theory and practice. Linked to diversification is the notion\nof breadth. Breadth is correctly thought of as the number of in- dependent bets\navailable to an investor. Conventionally applications us- ing breadth\nfrequently assume only the number of separate bets. There may be a large\ndiscrepancy between these two interpretations. We uti- lize a simple\nsingular-value decomposition (SVD) and the Keiser-Gutman stopping criterion to\nselect the integer-valued effective dimensionality of the correlation matrix of\nreturns. In an emerging market such as South African we document an estimated\nbreadth that is considerably lower than anticipated. This lack of\ndiversification may be because of market concentration, exposure to the global\ncommodity cycle and local currency volatility. We discuss some practical\nextensions to a more statistically correct interpretation of market breadth,\nand its theoretical implications for both global and domestic investors.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/physics/0601166v3", "link": "http://dx.doi.org/10.1057/jam.2008.26"}, {"title": "How log-normal is your country? An analysis of the statistical\n  distribution of the exported volumes of products", "summary": "We have considered the statistical distributions of the volumes of the\ndifferent products exported by 148 countries. We have found that the form of\nthese distributions is not unique but heavily depends on the level of\ndevelopment of the nation, as expressed by macroeconomic indicators like GDP,\nGDP per capita, total export and a recently introduced measure for countries'\neconomic complexity called fitness. We have identified three major classes: a)\nan incomplete log-normal shape, truncated on the left side, for the less\ndeveloped countries, b) a complete log-normal, with a wider range of volumes,\nfor nations characterized by intermediate economy, and c) a strongly asymmetric\nshape for countries with a high degree of development. The ranking curves of\nthe exported volumes from each country seldom cross each other, showing a clear\nhierarchy of export volumes. Finally, the log-normality hypothesis has been\nchecked for the distributions of all the 148 countries through different tests,\nKolmogorov-Smirnov and Cramer-Von Mises, confirming that it cannot be rejected\nonly for the countries of intermediate economy.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1506.03597v1", "link": "http://dx.doi.org/10.1140/epjst/e2015-50320-7"}, {"title": "Continuous-time Mean-Variance Portfolio Selection with Stochastic\n  Parameters", "summary": "This paper studies a continuous-time market {under stochastic environment}\nwhere an agent, having specified an investment horizon and a target terminal\nmean return, seeks to minimize the variance of the return with multiple stocks\nand a bond. In the considered model firstly proposed by [3], the mean returns\nof individual assets are explicitly affected by underlying Gaussian economic\nfactors. Using past and present information of the asset prices, a\npartial-information stochastic optimal control problem with random coefficients\nis formulated. Here, the partial information is due to the fact that the\neconomic factors can not be directly observed. Via dynamic programming theory,\nthe optimal portfolio strategy can be constructed by solving a deterministic\nforward Riccati-type ordinary differential equation and two linear\ndeterministic backward ordinary differential equations.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1302.6669v1", "link": "http://arxiv.org/abs/1302.6669v1"}, {"title": "Customer Selection Model with Grouping and Hierarchical Ranking Analysis", "summary": "The purpose of this study was to build a customer selection model based on 20\ndimensions, including customer codes, total contribution, assets, deposit,\nprofit, profit rate, trading volume, trading amount, turnover rate, order\namount, withdraw amount, withdraw rate, process fee, process fee submitted,\nprocess fee retained, net process fee retained, interest revenue, interest\nreturn, exchange house return I and exchange house return II to group and rank\ncustomers. The traditional way to group customers in securities or futures\ncompanies is simply based on their assets. However, grouping customers with\nrespect to only one dimension cannot give us a full picture about customers'\nattributions. It is hard to group customers' with similar attributions or\nvalues into one group if we just consider assets as the only grouping\ncriterion. Nowadays, securities or futures companies usually group customers\nbased on managers' experience with lack of quantitative analysis, which is not\neffective. Therefore, we use kmeans unsupervised learning methods to group\ncustomers with respect to significant dimensions so as to cluster customers\nwith similar attributions together. Grouping is our first step. It is the\nhorizontal analysis in customer study. The next step is customer ranking. It is\nthe longitudinal analysis. It ranks customers by assigning each customer with a\ncertain score given by our weighted customer value calculation formula.\nTherefore, by grouping and ranking customers, we can differentiate our\ncustomers and rank them based on values instead of blindly reaching everyone.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1711.05598v1", "link": "http://arxiv.org/abs/1711.05598v1"}, {"title": "Competition of noise and collectivity in global cryptocurrency trading:\n  route to a self-contained market", "summary": "Cross-correlations in fluctuations of the daily exchange rates within the\nbasket of the 100 highest-capitalization cryptocurrencies over the period\nOctober 1, 2015, through March 31, 2019, are studied. The corresponding\ndynamics predominantly involve one leading eigenvalue of the correlation\nmatrix, while the others largely coincide with those of Wishart random\nmatrices. However, the magnitude of the principal eigenvalue, and thus the\ndegree of collectivity, strongly depends on which cryptocurrency is used as a\nbase. It is largest when the base is the most peripheral cryptocurrency; when\nmore significant ones are taken into consideration, its magnitude\nsystematically decreases, nevertheless preserving a sizable gap with respect to\nthe random bulk, which in turn indicates that the organization of correlations\nbecomes more heterogeneous. This finding provides a criterion for recognizing\nwhich currencies or cryptocurrencies play a dominant role in the global\ncrypto-market. The present study shows that over the period under\nconsideration, the Bitcoin (BTC) predominates, hallmarking exchange rate\ndynamics at least as influential as the US dollar. The BTC started dominating\naround the year 2017, while further cryptocurrencies, like the Ethereum (ETH)\nand even Ripple (XRP), assumed similar trends. At the same time, the USD, an\noriginal value determinant for the cryptocurrency market, became increasingly\ndisconnected, its related characteristics eventually approaching those of a\nfictitious currency. These results are strong indicators of incipient\nindependence of the global cryptocurrency market, delineating a self-contained\ntrade resembling the Forex.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/1911.08944v2", "link": "http://dx.doi.org/10.1063/1.5139634"}, {"title": "How to Cut a Cake Fairly: A Generalization to Groups", "summary": "A fundamental result in cake cutting states that for any number of players\nwith arbitrary preferences over a cake, there exists a division of the cake\nsuch that every player receives a single contiguous piece and no player is left\nenvious. We generalize this result by showing that it is possible to partition\nthe players into groups of any desired sizes and divide the cake among the\ngroups, so that each group receives a single contiguous piece and no player\nfinds the piece of another group better than that of the player's own group.", "category": [], "id": "http://arxiv.org/abs/2001.03327v3", "link": "http://arxiv.org/abs/2001.03327v3"}, {"title": "Cyclical properties of supply-side and demand-side shocks in oil-based\n  commodity markets", "summary": "Oil markets profoundly influence world economies through determination of\nprices of energy and transports. Using novel methodology devised in frequency\ndomain, we study the information transmission mechanisms in oil-based commodity\nmarkets. Taking crude oil as a supply-side benchmark and heating oil and\ngasoline as demand-side benchmarks, we document new stylized facts about\ncyclical properties of the transmission mechanism generated by volatility\nshocks with heterogeneous frequency responses. Our first key finding is that\nshocks to volatility with response shorter than one week are increasingly\nimportant to the transmission mechanism over the studied period. Second,\ndemand-side shocks to volatility are becoming increasingly important in\ncreating short-run connectedness. Third, the supply-side shocks to volatility\nresonating in both the long run and short run are important sources of\nconnectedness.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1603.07020v2", "link": "http://arxiv.org/abs/1603.07020v2"}, {"title": "Using generalized estimating equations to estimate nonlinear models with\n  spatial data", "summary": "In this paper, we study estimation of nonlinear models with cross sectional\ndata using two-step generalized estimating equations (GEE) in the quasi-maximum\nlikelihood estimation (QMLE) framework. In the interest of improving\nefficiency, we propose a grouping estimator to account for the potential\nspatial correlation in the underlying innovations. We use a Poisson model and a\nNegative Binomial II model for count data and a Probit model for binary\nresponse data to demonstrate the GEE procedure. Under mild weak dependency\nassumptions, results on estimation consistency and asymptotic normality are\nprovided. Monte Carlo simulations show efficiency gain of our approach in\ncomparison of different estimation methods for count data and binary response\ndata. Finally we apply the GEE approach to study the determinants of the inflow\nforeign direct investment (FDI) to China.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.05855v1", "link": "http://arxiv.org/abs/1810.05855v1"}, {"title": "Explicit Solutions for Optimal Resource Extraction Problems under Regime\n  Switching L\u00e9vy Models", "summary": "This paper studies the problem of optimally extracting nonrenewable natural\nresources. Taking into account the fact that the market values of the main\nnatural resources i.e. oil, natural gas, copper,..., etc, fluctuate randomly\nfollowing global and seasonal macroeconomic parameters, the prices of natural\nresources are modeled using Markov switching L\\'evy processes. We formulate\nthis optimal extraction problem as an infinite-time horizon optimal control\nproblem. We derive closed-form solutions for the value function as well as the\noptimal extraction policy. Numerical examples are presented to illustrate these\nresults.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1806.06105v1", "link": "http://arxiv.org/abs/1806.06105v1"}, {"title": "New Analytical Solutions of a Modified Black-Scholes Equation with the\n  European Put Option", "summary": "Using Maple, we compute some analytical solutions of a modified Black-Scholes\nequation, recently proposed, in the case of the European put option. We show\nthat the modified Black-Scholes equation with the European put option is\nexactly solvable in terms of associated Laguerre polynomials. We make some\nnumerical experiments with the analytical solutions and we compare our results\nwith the results derived from numerical experiments using the standard\nBlack-Scholes equation.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1508.03841v1", "link": "http://arxiv.org/abs/1508.03841v1"}, {"title": "Payoff Information and Learning in Signaling Games", "summary": "We add the assumption that players know their opponents' payoff functions and\nrationality to a model of non-equilibrium learning in signaling games. Agents\nare born into player roles and play against random opponents every period.\nInexperienced agents are uncertain about the prevailing distribution of\nopponents' play, but believe that opponents never choose conditionally\ndominated strategies. Agents engage in active learning and update beliefs based\non personal observations. Payoff information can refine or expand learning\npredictions, since patient young senders' experimentation incentives depend on\nwhich receiver responses they deem plausible. We show that with payoff\nknowledge, the limiting set of long-run learning outcomes is bounded above by\nrationality-compatible equilibria (RCE), and bounded below by uniform RCE. RCE\nrefine the Intuitive Criterion (Cho and Kreps, 1987) and include all divine\nequilibria (Banks and Sobel, 1987). Uniform RCE sometimes but not always\nexists, and implies universally divine equilibrium.", "category": [], "id": "http://arxiv.org/abs/1709.01024v5", "link": "http://dx.doi.org/10.1016/j.geb.2019.11.011"}, {"title": "On the bail-out dividend problem for spectrally negative Markov additive\n  models", "summary": "This paper studies the bail-out optimal dividend problem with regime\nswitching under the constraint that the cumulative dividend strategy is\nabsolutely continuous. We confirm the optimality of the regime-modulated\nrefraction-reflection strategy when the underlying risk model follows a general\nspectrally negative Markov additive process. To verify the conjecture of a\nbarrier type optimal control, we first introduce and study an auxiliary problem\nwith the final payoff at an exponential terminal time and characterize the\noptimal threshold explicitly using fluctuation identities of the\nrefracted-reflected Levy process. Second, we transform the problem with\nregime-switching into an equivalent local optimization problem with a final\npayoff up to the first regime switching time. The refraction-reflection\nstrategy with regime-modulated thresholds can be shown as optimal by using\nresults in the first step and some fixed point arguments for auxiliary\nrecursive iterations.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1901.03021v4", "link": "http://arxiv.org/abs/1901.03021v4"}, {"title": "Local Operators in Kinetic Wealth Distribution", "summary": "The statistical mechanics approach to wealth distribution is based on the\nconservative kinetic multi-agent model for money exchange, where the local\ninteraction rule between the agents is analogous to the elastic particle\nscattering process. Here, we discuss the role of a class of conservative local\noperators, and we show that, depending on the values of their parameters, they\ncan be used to generate all the relevant distributions. We also show\nnumerically that in order to generate the power-law tail an heterogeneous risk\naversion model is required. By changing the parameters of these operators one\ncan also fine tune the resulting distributions in order to provide support for\nthe emergence of a more egalitarian wealth distribution.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1606.04790v1", "link": "http://dx.doi.org/10.1142/S0129183116501321"}, {"title": "Population Growth and Economic Development in Bangladesh: Revisited\n  Malthus", "summary": "Bangladesh is the 2nd largest growing country in the world in 2016 with 7.1%\nGDP growth. This study undertakes an econometric analysis to examine the\nrelationship between population growth and economic development. This result\nindicates population growth adversely related to per capita GDP growth, which\nmeans rapid population growth is a real problem for the development of\nBangladesh.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1812.09393v2", "link": "http://arxiv.org/abs/1812.09393v2"}, {"title": "Uncovering Offshore Financial Centers: Conduits and Sinks in the Global\n  Corporate Ownership Network", "summary": "Multinational corporations use highly complex structures of parents and\nsubsidiaries to organize their operations and ownership. Offshore Financial\nCenters (OFCs) facilitate these structures through low taxation and lenient\nregulation, but are increasingly under scrutiny, for instance for enabling tax\navoidance. Therefore, the identification of OFC jurisdictions has become a\npoliticized and contested issue. We introduce a novel data-driven approach for\nidentifying OFCs based on the global corporate ownership network, in which over\n98 million firms (nodes) are connected through 71 million ownership relations.\nThis granular firm-level network data uniquely allows identifying both\nsink-OFCs and conduit-OFCs. Sink-OFCs attract and retain foreign capital while\nconduit-OFCs are attractive intermediate destinations in the routing of\ninternational investments and enable the transfer of capital without taxation.\nWe identify 24 sink-OFCs. In addition, a small set of five countries -- the\nNetherlands, the United Kingdom, Ireland, Singapore and Switzerland -- canalize\nthe majority of corporate offshore investment as conduit-OFCs. Each conduit\njurisdiction is specialized in a geographical area and there is significant\nspecialization based on industrial sectors. Against the idea of OFCs as exotic\nsmall islands that cannot be regulated, we show that many sink and conduit-OFCs\nare highly developed countries.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1703.03016v2", "link": "http://dx.doi.org/10.1038/s41598-017-06322-9"}, {"title": "Capacitary measures for completely monotone kernels via singular control", "summary": "We give a singular control approach to the problem of minimizing an energy\nfunctional for measures with given total mass on a compact real interval, when\nenergy is defined in terms of a completely monotone kernel. This problem occurs\nboth in potential theory and when looking for optimal financial order execution\nstrategies under transient price impact. In our setup, measures or order\nexecution strategies are interpreted as singular controls, and the capacitary\nmeasure is the unique optimal control. The minimal energy, or equivalently the\ncapacity of the underlying interval, is characterized by means of a nonstandard\ninfinite-dimensional Riccati differential equation, which is analyzed in some\ndetail. We then show that the capacitary measure has two Dirac components at\nthe endpoints of the interval and a continuous Lebesgue density in between.\nThis density can be obtained as the solution of a certain Volterra integral\nequation of the second kind.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1201.2756v4", "link": "http://arxiv.org/abs/1201.2756v4"}, {"title": "Prosumage of solar electricity: tariff design, capacity investments, and\n  power system effects", "summary": "We analyze how tariff design incentivizes households to invest in residential\nphotovoltaic and battery systems, and explore selected power sector effects. To\nthis end, we apply an open-source power system model featuring prosumage agents\nto German 2030 scenarios. Results show that lower feed-in tariffs substantially\nreduce investments in photovoltaics, yet optimal battery sizing and\nself-generation are relatively robust. With increasing fixed parts of retail\ntariffs, optimal battery capacities and self-generation are smaller, and\nhouseholds contribute more to non-energy power sector costs. When choosing\ntariff designs, policy makers should not aim to (dis-)incentivize prosumage as\nsuch, but balance effects on renewable capacity expansion and system cost\ncontribution.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.09855v1", "link": "http://arxiv.org/abs/1907.09855v1"}, {"title": "The Jarrow & Turnbull setting revisited", "summary": "We consider a financial market with zero-coupon bonds that are exposed to\ncredit and liquidity risk. We revisit the famous Jarrow & Turnbull setting in\norder to account for these two intricately intertwined risk types. We utilise\nthe foreign exchange analogy that interprets defaultable zero-coupon bonds as a\nconversion of non-defaultable foreign counterparts. The relevant exchange rate\nis only partially observable in the market filtration, which leads us naturally\nto an application of the concept of platonic financial markets. We provide an\nexample of tractable term structure models that are driven by a two-dimensional\naffine jump diffusion. Furthermore, we derive explicit valuation formulae for\nmarketable products, e.g., for credit default swaps.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2004.12392v1", "link": "http://arxiv.org/abs/2004.12392v1"}, {"title": "A Comparison of Nineteen Various Electricity Consumption Forecasting\n  Approaches and Practicing to Five Different Households in Turkey", "summary": "The accuracy of the household electricity consumption forecast is vital in\ntaking better cost effective and energy efficient decisions. In order to design\naccurate, proper and efficient forecasting model, characteristics of the series\nhave to been analyzed. The source of time series data comes from Online\nEnerjisa System, the system of electrical energy provider in capital of Turkey,\nwhich consumers can reach their latest two year period electricity\nconsumptions; in our study the period was May 2014 to May 2016. Various\ntechniques had been applied in order to analyze the data; classical\ndecomposition models; standard typed and also with the centering moving average\nmethod, regression equations, exponential smoothing models and ARIMA models. In\nour study, nine teen different approaches; all of these have at least\ndiversified aspects of methodology, had been compared and the best model for\nforecasting were decided by considering the smallest values of MAPE, MAD and\nMSD. As a first step we took the time period May 2014 to May 2016 and found\npredicted value for June 2016 with the best forecasting model. After finding\nthe best forecasting model and fitted value for June 2016, than validating\nprocess had been taken place; we made comparisons to see how well the real\nvalue of June 2016 and forecasted value for that specific period matched.\nAfterwards we made electrical consumption forecast for the following 3 months;\nJune-September 2016 for each of five households individually.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1607.05660v1", "link": "http://arxiv.org/abs/1607.05660v1"}, {"title": "Backtesting Expected Shortfall: a simple recipe?", "summary": "We propose a new backtesting framework for Expected Shortfall that could be\nused by the regulator. Instead of looking at the estimated capital reserve and\nthe realised cash-flow separately, one could bind them into the secured\nposition, for which risk measurement is much easier. Using this simple concept\ncombined with monotonicity of Expected Shortfall with respect to its target\nconfidence level we introduce a natural and efficient backtesting framework.\nOur test statistics is given by the biggest number of worst realisations for\nthe secured position that add up to a negative total. Surprisingly, this simple\nquantity could be used to construct an efficient backtesting framework for\nunconditional coverage of Expected Shortfall in a natural extension of the\nregulatory traffic-light approach for Value-at-Risk. While being easy to\ncalculate, the test statistic is based on the underlying duality between\ncoherent risk measures and scale-invariant performance measures.", "category": ["q-fin.RM", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1709.01337v3", "link": "http://arxiv.org/abs/1709.01337v3"}, {"title": "Mathematical modeling of physical capital using the spatial Solow model", "summary": "This research deals with the mathematical modeling of the physical capital\ndiffusion through the borders of the countries. The physical capital is\nconsidered an important variable for the economic growth of a country. Here we\nuse an extension of the economic Solow model to describe how the smuggling\naffects the economic growth of the countries. In this study we rely on a\nproduction function that is non-concave instead of the classical Cobb-Douglas\nproduction function. In order to model the physical capital diffusion through\nthe borders of the country, we developed a model based on a parabolic partial\ndifferential equation that describes the dynamics of physical capital and\nboundary conditions of Neumann type. Smuggling is present in many borders\nbetween countries and may include fuel, machinery and food. This smuggling\nthrough the borders is a problematic issue for the country's economies. The\nsmuggling problem usually is related mainly to a non-official exchange rate\nthat is different than the official rate or subsides. Numerical simulations are\nobtained using an explicit finite difference scheme that shows how the physical\ncapital diffusion through the border of the countries. The study of physical\ncapital is a paramount issue for the economic growth of many countries for the\nnext years. The results show that the dynamics of the physical capital when\nboundary conditions of Neumann type are different than zero differ from the\nclassical economic behavior observed in the classical spatial Solow model\nwithout physical capital flux through the borders of countries. Finally, it can\nbe concluded that avoiding the smuggling through the frontiers is an important\nfactor that affects the economic growth of the countries.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1504.04388v1", "link": "http://arxiv.org/abs/1504.04388v1"}, {"title": "Hedging of Defaultable Contingent Claims using BSDE with uncertain time\n  horizon", "summary": "This article focuses on the mathematical problem of existence and uniqueness\nof BSDE with a random terminal time which is a general random variable but not\na stopping time, as it has been usually the case in the previous literature of\nBSDE with random terminal time. The main motivation of this work is a financial\nor actuarial problem of hedging of defaultable contingent claims or life\ninsurance contracts, for which the terminal time is a default time or a death\ntime, which are not stopping times. We have to use progressive enlargement of\nthe Brownian filtration, and to solve the obtained BSDE under this enlarged\nfiltration. This work gives a solution to the mathematical problem and proves\nthe existence and uniqueness of solutions of such BSDE under certain general\nconditions. This approach is applied to the financial problem of hedging of\ndefaultable contingent claims, and an expression of the hedging strategy is\ngiven for a defaultable contingent claim or a life insurance contract.", "category": ["q-fin.CP", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/0811.4039v2", "link": "http://arxiv.org/abs/0811.4039v2"}, {"title": "Stochastic calculus for uncoupled continuous-time random walks", "summary": "The continuous-time random walk (CTRW) is a pure-jump stochastic process with\nseveral applications in physics, but also in insurance, finance and economics.\nA definition is given for a class of stochastic integrals driven by a CTRW,\nthat includes the Ito and Stratonovich cases. An uncoupled CTRW with zero-mean\njumps is a martingale. It is proved that, as a consequence of the martingale\ntransform theorem, if the CTRW is a martingale, the Ito integral is a\nmartingale too. It is shown how the definition of the stochastic integrals can\nbe used to easily compute them by Monte Carlo simulation. The relations between\na CTRW, its quadratic variation, its Stratonovich integral and its Ito integral\nare highlighted by numerical calculations when the jumps in space of the CTRW\nhave a symmetric Levy alpha-stable distribution and its waiting times have a\none-parameter Mittag-Leffler distribution. Remarkably these distributions have\nfat tails and an unbounded quadratic variation. In the diffusive limit of\nvanishing scale parameters, the probability density of this kind of CTRW\nsatisfies the space-time fractional diffusion equation (FDE) or more in general\nthe fractional Fokker-Planck equation, that generalize the standard diffusion\nequation solved by the probability density of the Wiener process, and thus\nprovides a phenomenologic model of anomalous diffusion. We also provide an\nanalytic expression for the quadratic variation of the stochastic process\ndescribed by the FDE, and check it by Monte Carlo.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/0802.3769v2", "link": "http://dx.doi.org/10.1103/PhysRevE.79.066102"}, {"title": "USLV: Unspanned Stochastic Local Volatility Model", "summary": "We propose a new framework for modeling stochastic local volatility, with\npotential applications to modeling derivatives on interest rates, commodities,\ncredit, equity, FX etc., as well as hybrid derivatives. Our model extends the\nlinearity-generating unspanned volatility term structure model by Carr et al.\n(2011) by adding a local volatility layer to it. We outline efficient numerical\nschemes for pricing derivatives in this framework for a particular four-factor\nspecification (two \"curve\" factors plus two \"volatility\" factors). We show that\nthe dynamics of such a system can be approximated by a Markov chain on a\ntwo-dimensional space (Z_t,Y_t), where coordinates Z_t and Y_t are given by\ndirect (Kroneker) products of values of pairs of curve and volatility factors,\nrespectively. The resulting Markov chain dynamics on such partly \"folded\" state\nspace enables fast pricing by the standard backward induction. Using a\nnonparametric specification of the Markov chain generator, one can accurately\nmatch arbitrary sets of vanilla option quotes with different strikes and\nmaturities. Furthermore, we consider an alternative formulation of the model in\nterms of an implied time change process. The latter is specified\nnonparametrically, again enabling accurate calibration to arbitrary sets of\nvanilla option quotes.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1301.4442v2", "link": "http://arxiv.org/abs/1301.4442v2"}, {"title": "Validation of internal rating systems and PD estimates", "summary": "This paper elaborates on the validation requirements for rating systems and\nprobabilities of default (PDs) which were introduced with the New Capital\nStandards (Basel II). We start in Section 2 with some introductory remarks on\nthe topics and approaches that will be discussed later on. Then we have a view\non the developments in banking regulation that have enforced the interest of\nthe public in validation techniques. When doing so, we put the main emphasis on\nthe issues with quantitative validation. The techniques discussed here could be\nused in order to meet the quantitative regulatory requirements. However, their\nappropriateness will depend on the specific conditions under which they are\napplied. In order to have a common ground for the description of the different\ntechniques, we introduce in Section 3 a theoretical framework that will be the\nbasis for the further considerations. Intuitively, a good rating system should\nshow higher probabilities of default for the less creditworthy rating grades.\nTherefore, in Section 4, we discuss how this monotonicity property is reflected\nin the theoretical framework from Section 3. In Section 5, we study the meaning\nof discriminatory power and some tools for measuring it in some detail. We will\nsee that there are tools that might be more appropriate than others for the\npurpose of regulatory validation of discriminatory power. The topic in Section\n6 is calibration of rating systems. We introduce some of the tests that can be\nused for checking correct calibration and discuss the properties of the\ndifferent tests. We then conclude in Section 7 with some comments on the\nquestion which tools might be most appropriate for quantitative validation of\nrating systems and probabilities of default.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/physics/0606071v1", "link": "http://arxiv.org/abs/physics/0606071v1"}, {"title": "Program Evaluation and Causal Inference with High-Dimensional Data", "summary": "In this paper, we provide efficient estimators and honest confidence bands\nfor a variety of treatment effects including local average (LATE) and local\nquantile treatment effects (LQTE) in data-rich environments. We can handle very\nmany control variables, endogenous receipt of treatment, heterogeneous\ntreatment effects, and function-valued outcomes. Our framework covers the\nspecial case of exogenous receipt of treatment, either conditional on controls\nor unconditionally as in randomized control trials. In the latter case, our\napproach produces efficient estimators and honest bands for (functional)\naverage treatment effects (ATE) and quantile treatment effects (QTE). To make\ninformative inference possible, we assume that key reduced form predictive\nrelationships are approximately sparse. This assumption allows the use of\nregularization and selection methods to estimate those relations, and we\nprovide methods for post-regularization and post-selection inference that are\nuniformly valid (honest) across a wide-range of models. We show that a key\ningredient enabling honest inference is the use of orthogonal or doubly robust\nmoment conditions in estimating certain reduced form functional parameters. We\nillustrate the use of the proposed methods with an application to estimating\nthe effect of 401(k) eligibility and participation on accumulated assets.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1311.2645v8", "link": "http://arxiv.org/abs/1311.2645v8"}, {"title": "The Politics of Attention", "summary": "We develop an equilibrium theory of attention and politics. In a spatial\nmodel of electoral competition where candidates have varying policy\npreferences, we examine what kinds of political behaviors capture voters'\nlimited attention and how this concern affects the overall political outcomes.\nFollowing the seminal works of Downs (1957) and Sims (1998), we assume that\nvoters are rationally inattentive and can process information about the\npolicies at a cost proportional to entropy reduction. The main finding is an\nequilibrium phenomenon called attention- and media-driven extremism, namely as\nwe increase the attention cost or garble the news technology, a truncated set\nof the equilibria captures voters' attention through enlarging the policy\ndifferentials between the varying types of the candidates. We supplement our\nanalysis with historical accounts, and discuss its relevance in the new era\nfeatured with greater media choices and distractions, as well as the rise of\npartisan media and fake news.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.11449v3", "link": "http://arxiv.org/abs/1810.11449v3"}, {"title": "A High Frequency Trade Execution Model for Supervised Learning", "summary": "This paper introduces a high frequency trade execution model to evaluate the\neconomic impact of supervised machine learners. Extending the concept of a\nconfusion matrix, we present a 'trade information matrix' to attribute the\nexpected profit and loss of the high frequency strategy under execution\nconstraints, such as fill probabilities and position dependent trade rules, to\ncorrect and incorrect predictions. We apply the trade execution model and trade\ninformation matrix to Level II E-mini S&P 500 futures history and demonstrate\nan estimation approach for measuring the sensitivity of the P&L to the error of\na Recurrent Neural Network. Our approach directly evaluates the performance\nsensitivity of a market making strategy to prediction error and augments\ntraditional market simulation based testing.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1710.03870v3", "link": "http://arxiv.org/abs/1710.03870v3"}, {"title": "From Local to Global: External Validity in a Fertility Natural\n  Experiment", "summary": "We study issues related to external validity for treatment effects using over\n100 replications of the Angrist and Evans (1998) natural experiment on the\neffects of sibling sex composition on fertility and labor supply. The\nreplications are based on census data from around the world going back to 1960.\nWe decompose sources of error in predicting treatment effects in external\ncontexts in terms of macro and micro sources of variation. In our empirical\nsetting, we find that macro covariates dominate over micro covariates for\nreducing errors in predicting treatments, an issue that past studies of\nexternal validity have been unable to evaluate. We develop methods for two\napplications to evidence-based decision-making, including determining where to\nlocate an experiment and whether policy-makers should commission new\nexperiments or rely on an existing evidence base for making a policy decision.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1906.08096v1", "link": "http://arxiv.org/abs/1906.08096v1"}, {"title": "A hybrid approach for risk assessment of loan guarantee network", "summary": "Groups of Small and Medium Enterprises (SME) back each other and form\nguarantee network to obtain loan from banks. The risk over the networked\nenterprises may cause significant contagious damage. To dissolve such risks, we\npropose a hybrid feature representation, which is feeded into a gradient\nboosting model for credit risk assessment of guarantee network. Empirical study\nis performed on a ten-year guarantee loan record from commercial banks. We find\nthat often hundreds or thousands of enterprises back each other and constitute\na sparse complex network. We study the risk of various structures of loan\nguarantee network, and observe the high correlation between defaults with\ncentrality, and with the communities of the network. In particular, our\nquantitative risk evaluation model shows promising prediction performance on\nreal-world data, which can be useful to both regulators and stakeholders.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1702.04642v2", "link": "http://arxiv.org/abs/1702.04642v2"}, {"title": "Marginal density expansions for diffusions and stochastic volatility,\n  part II: Applications [to the Stein--Stein model]", "summary": "In the compagnion paper [Marginal density expansions for diffusions and\nstochastic volatility, part I] we discussed density expansions for\nmultidimensional diffusions $(X^1,...,X^d)$, at fixed time $T$ and projected to\ntheir first $l$ coordinates, in the small noise regime. Global conditions were\nfound which replace the well-known \"not-in-cutlocus\" condition known from\nheat-kernel asymptotics. In the present paper we discuss financial\napplications; these include tail and implied volatility asymptotics in some\ncorrelated stochastic volatility models. In particular, we solve a problem left\nopen by A. Gulisashvili and E.M. Stein (2009).", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1305.6765v1", "link": "http://arxiv.org/abs/1305.6765v1"}, {"title": "Nonparametric Expected Shortfall Forecasting Incorporating Weighted\n  Quantiles", "summary": "A new semi-parametric Expected Shortfall (ES) estimation and forecasting\nframework is proposed. The proposed approach is based on a two step estimation\nprocedure. The first step involves the estimation of Value-at-Risk (VaR) at\ndifferent levels through a set of quantile time series regressions. Then, the\nES is computed as a weighted average of the estimated quantiles. The quantiles\nweighting structure is parsimoniously parameterized by means of a Beta function\nwhose coefficients are optimized by minimizing a joint VaR and ES loss function\nof the Fissler-Ziegel class. The properties of the proposed approach are first\nevaluated with an extensive simulation study using various data generating\nprocesses. Two forecasting studies with different out-of-sample sizes are\nconducted, one of which focuses on the 2008 Global Financial Crisis (GFC)\nperiod. The proposed models are applied to 7 stock market indices and their\nforecasting performances are compared to those of a range of parametric,\nnon-parametric and semi-parametric models, including GARCH, Conditional\nAutoRegressive Expectile (CARE, Taylor 2008), joint VaR and ES quantile\nregression models (Taylor, 2019) and simple average of quantiles. The results\nof the forecasting experiments provide clear evidence in support of the\nproposed models.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/2005.04868v1", "link": "http://arxiv.org/abs/2005.04868v1"}, {"title": "Testing for Causality in Continuous Time Bayesian Network Models of\n  High-Frequency Data", "summary": "Continuous time Bayesian networks are investigated with a special focus on\ntheir ability to express causality. A framework is presented for doing\ninference in these networks. The central contributions are a representation of\nthe intensity matrices for the networks and the introduction of a causality\nmeasure. A new model for high-frequency financial data is presented. It is\ncalibrated to market data and by the new causality measure it performs better\nthan older models.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1601.06651v1", "link": "http://arxiv.org/abs/1601.06651v1"}, {"title": "The Sound of Silence: equilibrium filtering and optimal censoring in\n  financial markets", "summary": "Following the approach of standard filtering theory, we analyse\ninvestor-valuation of firms, when these are modelled as geometric-Brownian\nstate processes that are privately and partially observed, at random (Poisson)\ntimes, by agents. Tasked with disclosing forecast values, agents are able\npurposefully to withhold their observations; explicit filtering formulas are\nderived for downgrading the valuations in the absence of disclosures. The\nanalysis is conducted for both a solitary firm and m co-dependent firms.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1606.04039v1", "link": "http://arxiv.org/abs/1606.04039v1"}, {"title": "Identification and Estimation of a Partially Linear Regression Model\n  using Network Data", "summary": "I study a regression model in which one covariate is an unknown function of a\nlatent driver of link formation in a network. Rather than specify or fit a\nparametric network formation model, I introduce a new method based on matching\npairs of agents with similar columns of the squared adjacency matrix, the ijth\nentry of which contains the number of other agents linked to both agents i and\nj. The intuition behind this approach is that for a large class of network\nformation models the columns of this matrix characterize all of the\nidentifiable information about individual linking behavior. In the paper, I\nfirst describe the model and formalize this intuition. I then introduce\nestimators for the parameters of the regression model and characterize their\nlarge sample properties.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.09679v1", "link": "http://arxiv.org/abs/1903.09679v1"}, {"title": "Does Non-Farm Income Improve The Poverty and Income Inequality Among\n  Agricultural Household In Rural Kedah?", "summary": "This paper used a primary data collected through a surveys among farmers in\nrural Kedah to examine the effect of non farm income on poverty and income\ninequality. This paper employed two method, for the first objective which is to\nexamine the impact of non farm income to poverty, we used poverty decomposition\ntechniques - Foster, greer and Thorbecke (FGT) as has been done by Adams\n(2004). For the second objective, which is to examine the impact of non farm\nincome to income inequality, we used Gini decomposition techniques.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.03487v1", "link": "http://arxiv.org/abs/2001.03487v1"}, {"title": "Can GDP measurement be further improved? Data revision and\n  reconciliation", "summary": "Recent years have seen many attempts to combine expenditure-side estimates of\nU.S. real output (GDE) growth with income-side estimates (GDI) to improve\nestimates of real GDP growth. We show how to incorporate information from\nmultiple releases of noisy data to provide more precise estimates while\navoiding some of the identifying assumptions required in earlier work. This\nrelies on a new insight: using multiple data releases allows us to distinguish\nnews and noise measurement errors in situations where a single vintage does\nnot.\n  Our new measure, GDP++, fits the data better than GDP+, the GDP growth\nmeasure of Aruoba et al. (2016) published by the Federal Reserve Bank of\nPhiladephia. Historical decompositions show that GDE releases are more\ninformative than GDI, while the use of multiple data releases is particularly\nimportant in the quarters leading up to the Great Recession.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.04970v1", "link": "http://arxiv.org/abs/1808.04970v1"}, {"title": "Conditional Davis Pricing", "summary": "We study the set of marginal utility-based prices of a financial derivative\nin the case where the investor has a non-replicable random endowment. We\nprovide an example showing that even in the simplest of settings - such as\nSamuelson's geometric Brownian motion model - the interval of marginal\nutility-based prices can be a non-trivial strict subinterval of the set of all\nno-arbitrage prices. This is in stark contrast to the case with a replicable\nendowment where non- uniqueness is exceptional. We provide formulas for the end\npoints for these prices and illustrate the theory with several examples.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1702.02087v2", "link": "http://arxiv.org/abs/1702.02087v2"}, {"title": "The Validity of Company Valuation Using Discounted Cash Flow Methods", "summary": "This paper closely examines theoretical and practical aspects of the widely\nused discounted cash flows (DCF) valuation method. It assesses its potentials\nas well as several weaknesses. A special emphasize is being put on the\nvaluation of companies using the DCF method. The paper finds that the\ndiscounted cash flow method is a powerful tool to analyze even complex\nsituations. However, the DCF method is subject to massive assumption bias and\neven slight changes in the underlying assumptions of an analysis can\ndrastically alter the valuation results. A practical example of these\nimplications is given using a scenario analysis.", "category": ["q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1003.4881v2", "link": "http://arxiv.org/abs/1003.4881v2"}, {"title": "Supply and demand shocks in the COVID-19 pandemic: An industry and\n  occupation perspective", "summary": "We provide quantitative predictions of first order supply and demand shocks\nfor the U.S. economy associated with the COVID-19 pandemic at the level of\nindividual occupations and industries. To analyze the supply shock, we classify\nindustries as essential or non-essential and construct a Remote Labor Index,\nwhich measures the ability of different occupations to work from home. Demand\nshocks are based on a study of the likely effect of a severe influenza epidemic\ndeveloped by the US Congressional Budget Office. Compared to the pre-COVID\nperiod, these shocks would threaten around 22% of the US economy's GDP,\njeopardise 24% of jobs and reduce total wage income by 17%. At the industry\nlevel, sectors such as transport are likely to have output constrained by\ndemand shocks, while sectors relating to manufacturing, mining and services are\nmore likely to be constrained by supply shocks. Entertainment, restaurants and\ntourism face large supply and demand shocks. At the occupation level, we show\nthat high-wage occupations are relatively immune from adverse supply and\ndemand-side shocks, while low-wage occupations are much more vulnerable. We\nshould emphasize that our results are only first-order shocks -- we expect them\nto be substantially amplified by feedback effects in the production network.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.06759v1", "link": "http://arxiv.org/abs/2004.06759v1"}, {"title": "A Feynman-Kac type formula for a fixed delay CIR model", "summary": "Stochastic delay differential equations (SDDE's) have been used for financial\nmodeling. In this article, we study a SDDE obtained by the equation of a CIR\nprocess, with an additional fixed delay term in drift; in particular, we prove\nthat there exists a unique strong solution (positive and integrable) which we\ncall fixed delay CIR process. Moreover, for the fixed delay CIR process, we\nderive a Feynman-Kac type formula, leading to a generalized exponential-affine\nformula, which is used to determine a bond pricing formula when the interest\nrate follows the delay's equation. It turns out that, for each maturity time T,\nthe instantaneous forward rate is an affine function (with time dependent\ncoefficients) of the rate process and of an auxiliary process (also depending\non T). The coefficients satisfy a system of deterministic delay differential\nequations.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1806.00997v1", "link": "http://arxiv.org/abs/1806.00997v1"}, {"title": "On approximations of Value at Risk and Expected Shortfall involving\n  kurtosis", "summary": "We derive new approximations for the Value at Risk and the Expected Shortfall\nat high levels of loss distributions with positive skewness and excess\nkurtosis, and we describe their precisions for notable ones such as for\nexponential, Pareto type I, lognormal and compound (Poisson) distributions. Our\napproximations are motivated by that kind of extensions of the so-called Normal\nPower Approximation, used for approximating the cumulative distribution\nfunction of a random variable, which incorporate not only the skewness but the\nkurtosis of the random variable in question as well. We show the performance of\nour approximations in numerical examples and we also give comparisons with some\nknown ones in the literature.", "category": ["q-fin.RM", "math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1811.06361v2", "link": "http://arxiv.org/abs/1811.06361v2"}, {"title": "Actuarial fairness and solidarity in pooled annuity funds", "summary": "Various types of structures that enable a group of individuals to pool their\nmortality risk have been proposed in the literature. Collectively, the\nstructures are called pooled annuity funds. Since the pooled annuity funds\npropose different methods of pooling mortality risk, we investigate the\nconnections between them and find that they are genuinely different for a\nfinite heterogeneous membership profile.\n  We discuss the importance of actuarial fairness, defined as the expected\nbenefits equalling the contributions for each member, in the context of pooling\nmortality risk and comment on whether actuarial unfairness can be seen as\nsolidarity between members. We show that, with a finite number of members in\nthe fund, the group self-annuitization scheme is not actuarially fair: some\nmembers subsidize the other members. The implication is that the members who\nare subsidizing the others may obtain a higher expected benefit by joining a\nfund with a more favourable membership profile. However, we find that the\nsubsidies are financially significant only for very small or highly\nheterogeneous membership profiles.", "category": ["q-fin.PM", "q-fin.GN"], "id": "http://arxiv.org/abs/1311.5120v4", "link": "http://arxiv.org/abs/1311.5120v4"}, {"title": "Ruin probability of a discrete-time risk process with proportional\n  reinsurance and investment for exponential and Pareto distributions", "summary": "In this paper a quantitative analysis of the ruin probability in finite time\nof discrete risk process with proportional reinsurance and investment of\nfinance surplus is focused on. It is assumed that the total loss on a unit\ninterval has a light-tailed distribution -- exponential distribution and a\nheavy-tailed distribution -- Pareto distribution. The ruin probability for\nfinite-horizon 5 and 10 was determined from recurrence equations. Moreover for\nexponential distribution the upper bound of ruin probability by Lundberg\nadjustment coefficient is given. For Pareto distribution the adjustment\ncoefficient does not exist, hence an asymptotic approximation of the ruin\nprobability if an initial capital tends to infinity is given. Obtained\nnumerical results are given as tables and they are illustrated as graphs.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1306.3479v2", "link": "http://arxiv.org/abs/1306.3479v2"}, {"title": "Frictional Unemployment on Labor Flow Networks", "summary": "We develop an alternative theory to the aggregate matching function in which\nworkers search for jobs through a network of firms: the labor flow network. The\nlack of an edge between two companies indicates the impossibility of labor\nflows between them due to high frictions. In equilibrium, firms' hiring\nbehavior correlates through the network, generating highly disaggregated local\nunemployment. Hence, aggregation depends on the topology of the network in\nnon-trivial ways. This theory provides new micro-foundations for the Beveridge\ncurve, wage dispersion, and the employer-size premium. We apply our model to\nemployer-employee matched records and find that network topologies with\nPareto-distributed connections cause disproportionately large changes on\naggregate unemployment under high labor supply elasticity.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.04954v1", "link": "http://arxiv.org/abs/1903.04954v1"}, {"title": "Testing the Capital Asset Pricing Model (CAPM) on the Uganda Stock\n  Exchange", "summary": "This paper examines the validity of the Capital Asset Pricing Model (CAPM) on\nthe Ugandan stock market using monthly stock returns from 10 of the 11\ncompanies listed on the Uganda Stock Exchange (USE), for the period 1st March\n2007 to 10th November 2009. Due to the absence of readily available Uganda\nStock Exchange(USE) data, and the placement of daily price lists in pdf only,\non the USE website: http://www.use.or.ug, the article also discusses the\nprocedures taken to mine the data needed. The securities were all put in one\nportfolio in order to diversify away the firm-specific part of returns thereby\nenhancing the precision of the beta estimates. This paper should be of interest\nto both Ugandan and non-Ugandan investors and market researchers. While many\ndeveloping countries have legal restrictions against foreign participation in\ncapital and money markets, this is not so in Uganda, where it has become part\nof government policy to encourage foreign capital in flow, inorder to stimulate\nthe development of the small and underdeveloped markets.\n  The Black, Jensen, and Scholes (1972) CAPM version is examined in this\narticle. This version predicts a non zero-beta rate, along with the relation of\nhigher returns to higher risk. The estimated zero-beta rate obtained is not\nstatistically different from zero, and the estimated portfolio beta coefficient\nis statistically significant, providing evidence that the traditional form of\nCAPM holds on the USE, albeit having a beta coefficient that is not good at\nexplaining the relationship between risk and return.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1101.0184v1", "link": "http://arxiv.org/abs/1101.0184v1"}, {"title": "Self-Consistent Asset Pricing Models", "summary": "We discuss the foundations of factor or regression models in the light of the\nself-consistency condition that the market portfolio (and more generally the\nrisk factors) is (are) constituted of the assets whose returns it is (they are)\nsupposed to explain. As already reported in several articles, self-consistency\nimplies correlations between the return disturbances. As a consequence, the\nalpha's and beta's of the factor model are unobservable. Self-consistency leads\nto renormalized beta's with zero effective alpha's, which are observable with\nstandard OLS regressions. Analytical derivations and numerical simulations show\nthat, for arbitrary choices of the proxy which are different from the true\nmarket portfolio, a modified linear regression holds with a non-zero value\n$\\alpha_i$ at the origin between an asset $i$'s return and the proxy's return.\nSelf-consistency also introduces ``orthogonality'' and ``normality'' conditions\nlinking the beta's, alpha's (as well as the residuals) and the weights of the\nproxy portfolio. Two diagnostics based on these orthogonality and normality\nconditions are implemented on a basket of 323 assets which have been components\nof the S&P500 in the period from Jan. 1990 to Feb. 2005. These two diagnostics\nshow interesting departures from dynamical self-consistency starting about 2\nyears before the end of the Internet bubble. Finally, the factor decomposition\nwith the self-consistency condition derives a risk-factor decomposition in the\nmulti-factor case which is identical to the principal components analysis\n(PCA), thus providing a direct link between model-driven and data-driven\nconstructions of risk factors.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/physics/0608284v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.076"}, {"title": "Modeling a foreign exchange rate using moving average of Yen-Dollar\n  market data", "summary": "We introduce an autoregressive-type model with self-modulation effects for a\nforeign exchange rate by separating the foreign exchange rate into a moving\naverage rate and an uncorrelated noise. From this model we indicate that\ntraders are mainly using strategies with weighted feedbacks of the past rates\nin the exchange market. These feedbacks are responsible for a power law\ndistribution and characteristic autocorrelations of rate changes.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0508162v1", "link": "http://arxiv.org/abs/physics/0508162v1"}, {"title": "Typical properties of optimal growth in the Von Neumann expanding model\n  for large random economies", "summary": "We calculate the optimal solutions of the fully heterogeneous Von Neumann\nexpansion problem with $N$ processes and $P$ goods in the limit $N\\to\\infty$.\nThis model provides an elementary description of the growth of a production\neconomy in the long run. The system turns from a contracting to an expanding\nphase as $N$ increases beyond $P$. The solution is characterized by a universal\nbehavior, independent of the parameters of the disorder statistics. Associating\ntechnological innovation to an increase of $N$, we find that while such an\nincrease has a large positive impact on long term growth when $N\\ll P$, its\neffect on technologically advanced economies ($N\\gg P$) is very weak.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0507032v1", "link": "http://dx.doi.org/10.1088/1742-5468/2005/09/L09003"}, {"title": "Strategic Payments in Financial Networks", "summary": "In their seminal work on systemic risk in financial markets, Eisenberg and\nNoe proposed and studied a model with $n$ firms embedded into a network of debt\nrelations. We analyze this model from a game-theoretic point of view. Every\nfirm is a rational agent in a directed graph that has an incentive to allocate\npayments in order to clear as much of its debt as possible. Each edge is\nweighted and describes a liability between the firms. We consider several\nvariants of the game that differ in the permissible payment strategies. We\nstudy the existence and computational complexity of pure Nash and strong\nequilibria, and we provide bounds on the (strong) prices of anarchy and\nstability for a natural notion of social welfare. Our results highlight the\npower of financial regulation -- if payments of insolvent firms can be\ncentrally assigned, a socially optimal strong equilibrium can be found in\npolynomial time. In contrast, worst-case strong equilibria can be a factor of\n$\\Omega(n)$ away from optimal, and, in general, computing a best response is an\nNP-hard problem. For less permissible sets of strategies, we show that pure\nequilibria might not exist, and deciding their existence as well as computing\nthem if they exist constitute NP-hard problems.", "category": ["q-fin.GN", "q-fin.RM"], "id": "http://arxiv.org/abs/1908.01714v2", "link": "http://arxiv.org/abs/1908.01714v2"}, {"title": "Modeling Market Inefficiencies within a Single Instrument", "summary": "In this paper, we propose a minimal model beyond geometric Brownian motion\nthat aims to describe price actions with market inefficiency. From simple\nfinancial theory considerations, we arrive at a simple two-variable hidden\nMarkovian time series model, with one of the variable entirely unobserved.\nThen, we analyze the simplest version of the model, using path integral and\nGreen's function techniques from physics. We show that in this model, the\ninefficient market price is trend-following when the standard deviation of the\nlog reasonable price ($\\sigma$) is larger than that of the log market price\n($\\sigma'$), and mean-reversing when it is smaller. The risk premium is\nproportional to the difference between the current market price and the\nexponential moving average (EMA) of the past prices. This model thus provides a\ntheoretical explanation how the EMA of the past price can directly affect\nfuture prices, i.e., the so-called ``Bollinger bands\" in technical analyses. We\nthen carry out a maximum likelihood estimate for the model parameters from the\nobserved market price, by integrating out the reasonable price in Fourier\nspace. Finally we analyze recent S\\&P500 index data and see to what extent the\nreal world data can be described by this simple model.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1511.02046v1", "link": "http://arxiv.org/abs/1511.02046v1"}, {"title": "Private Information, Credit Risk and Graph Structure in P2P Lending\n  Networks", "summary": "This research investigated the potential for improving Peer-to-Peer (P2P)\ncredit scoring by using \"private information\" about communications and travels\nof borrowers. We found that P2P borrowers' ego networks exhibit scale-free\nbehavior driven by underlying preferential attachment mechanisms that connect\nborrowers in a fashion that can be used to predict loan profitability. The\nprojection of these private networks onto networks of mobile phone\ncommunication and geographical locations from mobile phone GPS potentially give\nloan providers access to private information through graph and location metrics\nwhich we used to predict loan profitability. Graph topology was found to be an\nimportant predictor of loan profitability, explaining over 5.5% of variability.\nNetworks of borrower location information explain an additional 19% of the\nprofitability. Machine learning algorithms were applied to the data set\npreviously analyzed to develop the predictive model and resulted in a 4%\nreduction in mean squared error.", "category": ["q-fin.GN", "q-fin.RM"], "id": "http://arxiv.org/abs/1802.10000v1", "link": "http://arxiv.org/abs/1802.10000v1"}, {"title": "World Financial 2014-2016 Market Bubbles: Oil Negative - US Dollar\n  Positive", "summary": "Based on the Log-Periodic Power Law (LPPL) methodology, with the universal\npreferred scaling factor $\\lambda \\approx 2$, the negative bubble on the oil\nmarket in 2014-2016 has been detected. Over the same period a positive bubble\non the so called commodity currencies expressed in terms of the US dollar\nappears to take place with the oscillation pattern which largely is mirror\nreflected relative to oil price oscillation pattern. This documents recent\nstrong anti-correlation between the dynamics of the oil price and of the USD. A\nrelated forecast made at the time of FENS 2015 conference (beginning of\nNovember) turned out to be quite satisfactory. These findings provide also\nfurther indication that such a log-periodically accelerating down-trend signals\ntermination of the corresponding decreases.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1606.01218v1", "link": "http://dx.doi.org/10.12693/APhysPolA.129.932"}, {"title": "Statistical physics of adaptive correlation of agents in a market", "summary": "Recent results and interpretations are presented for the thermal minority\ngame, concentrating on deriving and justifying the fundamental stochastic\ndifferential equation for the microdynamics.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0010455v1", "link": "http://dx.doi.org/10.1063/1.1358169"}, {"title": "Optimal Capital Structure with Scale Effects under Spectrally Negative\n  Levy Models", "summary": "The optimal capital structure model with endogenous bankruptcy was first\nstudied by Leland (1994) and Leland and Toft (1996), and was later extended to\nthe spectrally negative Levy model by Hilberink and Rogers (2002) and Kyprianou\nand Surya (2007). This paper incorporates the scale effects by allowing the\nvalues of bankruptcy costs and tax benefits to be dependent on the firm's asset\nvalue. By using the fluctuation identities for the spectrally negative Levy\nprocess, we obtain a candidate bankruptcy level as well as a sufficient\ncondition for optimality. The optimality holds in particular when,\nmonotonically in the asset value, the value of tax benefits is increasing, the\nloss amount at bankruptcy is increasing, and its proportion relative to the\nasset value is decreasing. The solution admits a semi-explicit form in terms of\nthe scale function. A series of numerical studies are given to analyze the\nimpacts of scale effects on the default strategy and the optimal capital\nstructure.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1109.0897v4", "link": "http://dx.doi.org/10.1142/S0219024914500137"}, {"title": "Life Insurance Purchasing to Maximize Utility of Household Consumption", "summary": "We determine the optimal amount of life insurance for a household of two wage\nearners. We consider the simple case of exponential utility, thereby removing\nwealth as a factor in buying life insurance, while retaining the relationship\namong life insurance, income, and the probability of dying and thus losing that\nincome. For insurance purchased via a single premium or premium payable\ncontinuously, we explicitly determine the optimal death benefit. We show that\nif the premium is determined to target a specific probability of loss per\npolicy, then the rates of consumption are identical under single premium or\ncontinuously payable premium. Thus, not only is equivalence of consumption\nachieved for the households under the two premium schemes, it is also obtained\nfor the insurance company in the sense of equivalence of loss probabilities.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1205.5958v2", "link": "http://arxiv.org/abs/1205.5958v2"}, {"title": "Limit Theorems for Network Dependent Random Variables", "summary": "This paper is concerned with cross-sectional dependence arising because\nobservations are interconnected through an observed network. Following Doukhan\nand Louhichi (1999), we measure the strength of dependence by covariances of\nnonlinearly transformed variables. We provide a law of large numbers and\ncentral limit theorem for network dependent variables. We also provide a method\nof calculating standard errors robust to general forms of network dependence.\nFor that purpose, we rely on a network heteroskedasticity and autocorrelation\nconsistent (HAC) variance estimator, and show its consistency. The results rely\non conditions characterized by tradeoffs between the rate of decay of\ndependence across a network and network's denseness. Our approach can\naccommodate data generated by network formation models, random fields on\ngraphs, conditional dependency graphs, and large functional-causal systems of\nequations.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.01059v2", "link": "http://arxiv.org/abs/1903.01059v2"}, {"title": "The Macro Model of the Inequality Process and The Surging Relative\n  Frequency of Large Wage Incomes", "summary": "This paper presents a model of the dynamics of the wage income distribution.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0705.3430v1", "link": "http://dx.doi.org/10.1007/978-88-470-0665-2_14"}, {"title": "Statistical Industry Classification", "summary": "We give complete algorithms and source code for constructing (multilevel)\nstatistical industry classifications, including methods for fixing the number\nof clusters at each level (and the number of levels). Under the hood there are\nclustering algorithms (e.g., k-means). However, what should we cluster?\nCorrelations? Returns? The answer turns out to be neither and our backtests\nsuggest that these details make a sizable difference. We also give an algorithm\nand source code for building \"hybrid\" industry classifications by improving\noff-the-shelf \"fundamental\" industry classifications by applying our\nstatistical industry classification methods to them. The presentation is\nintended to be pedagogical and geared toward practical applications in\nquantitative trading.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1607.04883v4", "link": "http://arxiv.org/abs/1607.04883v4"}, {"title": "Granger causality on horizontal sum of Boolean algebras", "summary": "The intention of this paper is to discuss the mathematical model of causality\nintroduced by C.W.J. Granger in 1969. The Granger's model of causality has\nbecome well-known and often used in various econometric models describing\ncausal systems, e.g., between commodity prices and exchange rates.\n  Our paper presents a new mathematical model of causality between two measured\nobjects. We have slightly modified the well-known Kolmogorovian probability\nmodel. In particular, we use the horizontal sum of set $\\sigma$-algebras\ninstead of their direct product.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.01654v1", "link": "http://arxiv.org/abs/1810.01654v1"}, {"title": "Cross-border Portfolio Investment Networks and Indicators for Financial\n  Crises", "summary": "Cross-border equity and long-term debt securities portfolio investment\nnetworks are analysed from 2002 to 2012, covering the 2008 global financial\ncrisis. They serve as network-proxies for measuring the robustness of the\nglobal financial system and the interdependence of financial markets,\nrespectively. Two early-warning indicators for financial crises are identified:\nFirst, the algebraic connectivity of the equity securities network, as a\nmeasure for structural robustness, drops close to zero already in 2005, while\nthere is an over-representation of high-degree off-shore financial centres\namong the countries most-related to this observation, suggesting an\ninvestigation of such nodes with respect to the structural stability of the\nglobal financial system. Second, using a phenomenological model, the edge\ndensity of the debt securities network is found to describe, and even forecast,\nthe proliferation of several over-the-counter-traded financial derivatives,\nmost prominently credit default swaps, enabling one to detect potentially\ndangerous levels of market interdependence and systemic risk.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1306.0215v3", "link": "http://dx.doi.org/10.1038/srep03991"}, {"title": "Fast Calculation of Credit Exposures for Barrier and Bermudan options\n  using Chebyshev interpolation", "summary": "We introduce a new method to calculate the credit exposure of Bermudan,\ndiscretely monitored barrier and European options. Core of the approach is the\napplication of the dynamic Chebyshev method of Glau et al. (2019). The dynamic\nChebyshev method delivers a closed form approximation of the option prices\nalong the paths together with the options' delta and gamma. Key advantage is\nthe polynomial structure of the approximation, which allows us a highly\nefficient evaluation of the credit exposures, even for a large number of\nsimulated paths. The approach is highly flexible in the model choice, payoff\nprofiles and asset classes. We compute the exposure profiles for Bermudan and\nbarrier options in three different equity models and compare them to the\nprofiles of European options. The analysis reveals potential shortcomings of\ncommon simplifications in the exposure calculation. The proposed method is\nsufficiently simple and efficient to avoid such risk-bearing simplifications.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1905.00238v1", "link": "http://arxiv.org/abs/1905.00238v1"}, {"title": "On the probability density function of baskets", "summary": "The state price density of a basket, even under uncorrelated Black-Scholes\ndynamics, does not allow for a closed from density. (This may be rephrased as\nstatement on the sum of lognormals and is especially annoying for such are used\nmost frequently in Financial and Actuarial Mathematics.) In this note we\ndiscuss short time and small volatility expansions, respectively. The method\nworks for general multi-factor models with correlations and leads to the\nanalysis of a system of ordinary (Hamiltonian) differential equations.\nSurprisingly perhaps, even in two asset Black-Scholes situation (with its flat\ngeometry), the expansion can degenerate at a critical (basket) strike level; a\nphenomena which seems to have gone unnoticed in the literature to date.\nExplicit computations relate this to a phase transition from a unique to more\nthan one \"most-likely\" paths (along which the diffusion, if suitably\nconditioned, concentrates in the afore-mentioned regimes). This also provides a\n(quantifiable) understanding of how precisely a presently out-of-money basket\noption may still end up in-the-money.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1306.2793v2", "link": "http://dx.doi.org/10.1007/978-3-319-11605-1_16"}, {"title": "Fundamental Measurements in Economics and in the Theory of Consciousness\n  (Manifestation of quantum-mechanical properties of economic objects in slit\n  measurements)", "summary": "A new constructivist approach to modeling in economics and theory of\nconsciousness is proposed. The state of elementary object is defined as a set\nof its measurable consumer properties. A proprietor's refusal or consent for\nthe offered transaction is considered as a result of elementary economic\nmeasurement. We were also able to obtain the classical interpretation of the\nquantum-mechanical law of addition of probabilities by introducing a number of\nnew notions. The principle of \"local equity\" assumes the transaction completed\n(regardless of the result) of the states of transaction partners are not\nchanged in connection with the reception of new information on proposed offers\nor adopted decisions (consent or refusal of the transaction). However it has no\nrelation to the paradoxes of quantum theory connected with non-local\ninteraction of entangled states. In the economic systems the mechanism of\nentangling has a classical interpretation, while the quantum-mechanical\nformalism of the description of states appears as a result of idealization of\nthe selection mechanism in the proprietor's consciousness.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1110.5288v1", "link": "http://arxiv.org/abs/1110.5288v1"}, {"title": "Permanent market impact can be nonlinear", "summary": "There are two schools of thought regarding market impact modeling. On the one\nhand, seminal papers by Almgren and Chriss introduced a decomposition between a\npermanent market impact and a temporary (or instantaneous) market impact. This\ndecomposition is used by most practitioners in execution models. On the other\nhand, recent research advocates for the use of a new modeling framework that\ngoes down to the resilient dynamics of order books: transient market impact.\nOne of the main criticisms against permanent market impact is that it has to be\nlinear to avoid dynamic arbitrage. This important discovery made by Huberman\nand Stanzl and Gatheral favors the transient market impact framework, as linear\npermanent market impact is at odds with reality. In this paper, we reconsider\nthe point made by Gatheral using a simple model for market impact and show that\npermanent market impact can be nonlinear. Also, and this is the most important\npart from a practical point of view, we propose different statistics to\nestimate permanent market impact and execution costs that generalize the ones\nproposed in Almgren at al. (2005).", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1305.0413v4", "link": "http://arxiv.org/abs/1305.0413v4"}, {"title": "Systemic Risk and the Dependence Structures", "summary": "We propose a dynamic model of dependence structure between financial\ninstitutions within a financial system and we construct measures for dependence\nand financial instability. Employing Markov structures of joint credit\nmigrations, our model allows for contagious simultaneous jumps in credit\nratings and provides flexibility in modeling dependence structures. Another key\naspect is that the proposed measures consider the interdependence and reflect\nthe changing economic landscape as financial institutions evolve over time. In\nthe final part, we give several examples, where we study various dependence\nstructures and investigate their systemic instability measures. In particular,\nwe show that subject to the same pool of Markov chains, the simulated Markov\nstructures with distinct dependence structures generate different sequences of\nsystemic instability.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1809.03425v1", "link": "http://arxiv.org/abs/1809.03425v1"}, {"title": "An exact and explicit formula for pricing Asian options with regime\n  switching", "summary": "This paper studies the pricing of European-style Asian options when the price\ndynamics of the underlying risky asset are assumed to follow a Markov-\nmodulated geometric Brownian motion; that is, the appreciation rate and the\nvolatility of the underlying risky asset depend on unobservable states of the\neconomy described by a continuous-time hidden Markov process. We derive the\nexact, explicit and closed-form solutions for European-style Asian options in a\ntwo-state regime switching model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1407.5091v1", "link": "http://arxiv.org/abs/1407.5091v1"}, {"title": "Fast Computation of the Expected Loss of a Loan Portfolio Tranche in the\n  Gaussian Factor Model: Using Hermite Expansions for Higher Accuracy", "summary": "We propose a fast algorithm for computing the expected tranche loss in the\nGaussian factor model. We test it on portfolios ranging in size from 25 (the\nsize of DJ iTraxx Australia) to 100 (the size of DJCDX.NA.HY) with a single\nfactor Gaussian model and show that the algorithm gives accurate results. The\nalgorithm proposed here is an extension of the algorithm proposed in \\cite{PO}.\nThe advantage of the new algorithm is that it works well for portfolios of\nsmaller size for which the normal approximation proposed in \\cite{PO} in not\nsufficiently accurate. The algorithm is intended as an alternative to the much\nslower Fourier transform based methods \\cite{MD}.", "category": ["math.PR", "q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/math/0506378v1", "link": "http://arxiv.org/abs/math/0506378v1"}, {"title": "An Asymptotically F-Distributed Chow Test in the Presence of\n  Heteroscedasticity and Autocorrelation", "summary": "This study proposes a simple, trustworthy Chow test in the presence of\nheteroscedasticity and autocorrelation. The test is based on a series\nheteroscedasticity and autocorrelation robust variance estimator with\njudiciously crafted basis functions. Like the Chow test in a classical normal\nlinear regression, the proposed test employs the standard F distribution as the\nreference distribution, which is justified under fixed-smoothing asymptotics.\nMonte Carlo simulations show that the null rejection probability of the\nasymptotic F test is closer to the nominal level than that of the chi-square\ntest.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1911.03771v1", "link": "http://arxiv.org/abs/1911.03771v1"}, {"title": "Spectral Risk Measures: Properties and Limitations", "summary": "Spectral risk measures (SRMs) are risk measures that take account of user\nriskaversion, but to date there has been little guidance on the choice of\nutility function underlying them. This paper addresses this issue by examining\nalternative approaches based on exponential and power utility functions. A\nnumber of problems are identified with both types of spectral risk measure. The\ngeneral lesson is that users of spectral risk measures must be careful to\nselect utility functions that fit the features of the particular problems they\nare dealing with, and should be especially careful when using power SRMs.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1103.5674v1", "link": "http://arxiv.org/abs/1103.5674v1"}, {"title": "Double/Debiased Machine Learning for Dynamic Treatment Effects", "summary": "We consider the estimation of treatment effects in settings when multiple\ntreatments are assigned over time and treatments can have a causal effect on\nfuture outcomes. We formulate the problem as a linear state space Markov\nprocess with a high dimensional state and propose an extension of the\ndouble/debiased machine learning framework to estimate the dynamic effects of\ntreatments. Our method allows the use of arbitrary machine learning methods to\ncontrol for the high dimensional state, subject to a mean square error\nguarantee, while still allowing parametric estimation and construction of\nconfidence intervals for the dynamic treatment effect parameters of interest.\nOur method is based on a sequential regression peeling process, which we show\ncan be equivalently interpreted as a Neyman orthogonal moment estimator. This\nallows us to show root-n asymptotic normality of the estimated causal effects.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.07285v1", "link": "http://arxiv.org/abs/2002.07285v1"}, {"title": "Do COVID-19 and crude oil prices drive the US economic policy\n  uncertainty?", "summary": "This paper investigates the effect of the novel coronavirus and crude oil\nprices on the United States (US) economic policy uncertainty (EPU). Using daily\ndata for the period January 21-March 13, 2020, our Autoregressive Distributed\nLag (ARDL) model shows that the new infection cases reported at global level,\nand the death ratio, have no significant effect on the US EPU, whereas the oil\nprice negative dynamics leads to increased uncertainty. However, analyzing the\nsituation outside China, we discover that both new case announcements and the\nCOVID-19 associated death ratio have a positive influence on the US EPU.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/2003.07591v1", "link": "http://arxiv.org/abs/2003.07591v1"}, {"title": "Taxation and Social Justice", "summary": "The link between taxation and justice is a classic debate issue, while also\nbeing very relevant at a time of changing environmental factors and conditions\nof the social and economic system. Technologically speaking, there are three\ntypes of taxes: progressive, proportional and regressive. Although justice,\nlike freedom, is an element and manifestation of the imagined reality in\ncitizens minds, the state must comply with it. In particular, the tax system\nhas to adapt to the mass imagined reality in order for it to appear fairer and\nmore acceptable.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.04155v1", "link": "http://arxiv.org/abs/1910.04155v1"}, {"title": "Return or stock price differences", "summary": "The analysis which assumes that tick by tick data is linear may lead to wrong\nconclusions if the underlying process is multiplicative. We compare data\nanalysis done with the return and stock differences and we study the limits\nwithin the two approaches are equivalent. Some illustrative examples concerning\nthese two approaches are given. Actual data is taken from S&P 500 stock cash\nindex.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0111529v1", "link": "http://arxiv.org/abs/cond-mat/0111529v1"}, {"title": "Uncovering the network structure of the world currency market:\n  Cross-correlations in the fluctuations of daily exchange rates", "summary": "The cross-correlations between the exchange rate fluctuations of 74\ncurrencies over the period 1995-2012 are analyzed in this paper. The eigenvalue\ndistribution of the cross-correlation matrix exhibits a bulk which\napproximately matches the bounds predicted from random matrices constructed\nusing mutually uncorrelated time-series. However, a few large eigenvalues\ndeviating from the bulk contain important information about the global market\nmode as well as important clusters of strongly interacting currencies.We\nreconstruct the network structure of the world currency market by using two\ndifferent graph representation techniques, after filtering out the effects of\nglobal or market-wide signals on the one hand and random effects on the other.\nThe two networks reveal complementary insights about the major motive forces of\nthe global economy, including the identification of a group of potentially fast\ngrowing economies whose development trajectory may affect the global economy in\nthe future as profoundly as the rise of India and China has affected it in the\npast decades.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1305.0239v1", "link": "http://arxiv.org/abs/1305.0239v1"}, {"title": "Theory of market fluctuations", "summary": "We propose coalescent mechanism of economic grow because of redistribution of\nexternal resources. It leads to Zipf distribution of firms over their sizes,\nturning to stretched exponent because of size-dependent effects, and predicts\nexponential distribution of income between individuals. We also present new\napproach to describe fluctuations on the market, based on separation of hot\n(short-time) and cold (long-time) degrees of freedoms, which predicts tent-like\ndistribution of fluctuations with stable tail exponent mu=3 (mu=2 for news).\nThe theory predicts observable asymmetry of the distribution, and its size\ndependence. In the case of financial markets the theory explains first time\nmarket mill patterns, conditional distribution, D-smile, z-shaped response,\nconditional double dynamics, the skewness and so on. We derive the set of\nLangeven equations, which predicts logarithmic dependence of price shift on\ntrading volume and volatility patterns after jumps. We calculate parameters of\nprice distributions, correlation functions and Hurst exponents at different\ntime scales. At large times the price experiences fractional Brownian motion\nwith chaotically switching of long-time persistent and anti-persistent\nbehavior, and we calculate corresponding probabilities, response functions, and\nrisks.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0804.4191v3", "link": "http://arxiv.org/abs/0804.4191v3"}, {"title": "Networks, Dynamic Factors, and the Volatility Analysis of\n  High-Dimensional Financial Series", "summary": "We consider weighted directed networks for analysing, over the period\n2000-2013, the interdependencies between volatilities of a large panel of\nstocks belonging to the S\\&P100 index. In particular, we focus on the so-called\n{\\it Long-Run Variance Decomposition Network} (LVDN), where the nodes are\nstocks, and the weight associated with edge $(i,j)$ represents the proportion\nof $h$-step-ahead forecast error variance of variable $i$ accounted for by\nvariable $j$'s innovations. To overcome the curse of dimensionality, we\ndecompose the panel into a component driven by few global, market-wide,\nfactors, and an idiosyncratic one modelled by means of a sparse vector\nautoregression (VAR) model. Inversion of the VAR together with suitable\nidentification restrictions, produces the estimated network, by means of which\nwe can assess how {\\it systemic} each firm is.~Our analysis demonstrates the\nprominent role of financial firms as sources of contagion, especially during\nthe~2007-2008 crisis.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1510.05118v2", "link": "http://dx.doi.org/10.1111/rssc.12177"}, {"title": "Instability and network effects in innovative markets", "summary": "We consider a network of interacting agents and we model the process of\nchoice on the adoption of a given innovative product by means of\nstatistical-mechanics tools. The modelization allows us to focus on the effects\nof direct interactions among agents in establishing the success or failure of\nthe product itself. Mimicking real systems, the whole population is divided\ninto two sub-communities called, respectively, Innovators and Followers, where\nthe former are assumed to display more influence power. We study in detail and\nvia numerical simulations on a random graph two different scenarios:\nno-feedback interaction, where innovators are cohesive and not sensitively\naffected by the remaining population, and feedback interaction, where the\ninfluence of followers on innovators is non negligible. The outcomes are\nmarkedly different: in the former case, which corresponds to the creation of a\nniche in the market, Innovators are able to drive and polarize the whole\nmarket. In the latter case the behavior of the market cannot be definitely\npredicted and become unstable. In both cases we highlight the emergence of\ncollective phenomena and we show how the final outcome, in terms of the number\nof buyers, is affected by the concentration of innovators and by the\ninteraction strengths among agents.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1409.3837v1", "link": "http://dx.doi.org/10.1016/j.matcom.2014.05.013"}, {"title": "Dynamical model of financial markets: fluctuating `temperature' causes\n  intermittent behavior of price changes", "summary": "We present a model of financial markets originally proposed for a turbulent\nflow, as a dynamic basis of its intermittent behavior. Time evolution of the\nprice change is assumed to be described by Brownian motion in a power-law\npotential, where the `temperature' fluctuates slowly. The model generally\nyields a fat-tailed distribution of the price change. Specifically a Tsallis\ndistribution is obtained if the inverse temperature is $\\chi^{2}$-distributed,\nwhich qualitatively agrees with intraday data of foreign exchange market. The\nso-called `volatility', a quantity indicating the risk or activity in financial\nmarkets, corresponds to the temperature of markets and its fluctuation leads to\nintermittency.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0210090v1", "link": "http://dx.doi.org/10.1016/S0378-4371(03)00592-2"}, {"title": "Price mediated contagion through capital ratio requirements", "summary": "We develop a framework for price-mediated contagion in financial systems\nwhere banks are forced to liquidate assets to satisfy a risk-weight based\ncapital adequacy requirement. In constructing this modeling framework, we\nintroduce a two-tier pricing structure: the volume weighted average price that\nis obtained by any bank liquidating assets and the terminal mark-to-market\nprice used to account for all assets held at the end of the clearing process.\nWe consider the case of multiple illiquid assets and develop conditions for the\nexistence and uniqueness of clearing prices. We provide a closed-form\nrepresentation for the sensitivity of these clearing prices to the system\nparameters, and use this result to quantify: (1) the cost of regulation, in\nstress scenarios, faced by the system as a whole and the individual banks, and\n(2) the value of providing bailouts and bail-ins to consider when such notions\nare financially advisable. Numerical case studies are provided to study the\napplication of this model to data.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1910.12130v2", "link": "http://arxiv.org/abs/1910.12130v2"}, {"title": "Inferring agent objectives at different scales of a complex adaptive\n  system", "summary": "We introduce a framework to study the effective objectives at different time\nscales of financial market microstructure. The financial market can be regarded\nas a complex adaptive system, where purposeful agents collectively and\nsimultaneously create and perceive their environment as they interact with it.\nIt has been suggested that multiple agent classes operate in this system, with\na non-trivial hierarchy of top-down and bottom-up causation classes with\ndifferent effective models governing each level. We conjecture that agent\nclasses may in fact operate at different time scales and thus act differently\nin response to the same perceived market state. Given scale-specific temporal\nstate trajectories and action sequences estimated from aggregate market\nbehaviour, we use Inverse Reinforcement Learning to compute the effective\nreward function for the aggregate agent class at each scale, allowing us to\nassess the relative attractiveness of feature vectors across different scales.\nDifferences in reward functions for feature vectors may indicate different\nobjectives of market participants, which could assist in finding the scale\nboundary for agent classes. This has implications for learning algorithms\noperating in this domain.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1712.01137v1", "link": "http://arxiv.org/abs/1712.01137v1"}, {"title": "Consumer finance data generator - a new approach to Credit Scoring\n  technique comparison", "summary": "This paper aims to present a general idea of method comparison of Credit\nScoring techniques. Any scorecard can be made in various methods based on\nvariable transformations in the logistic regression model. To make a comparison\nand come up with the proof that one technique is better than another is a big\nchallenge due to the limited availability of data. The same conclusion cannot\nbe guaranteed when using other data from another source. The following research\nchallenge can therefore be formulated: how should the comparison be managed in\norder to get general results that are not biased by particular data? The\nsolution may be in the use of various random data generators. The data\ngenerator uses two approaches: transition matrix and scorings. Here are\npresented both: results of comparison methods and the methodology of these\ncomparison techniques creating. Before building a new model the modeler can\nundertake a comparison exercise that aims at identifying the best method in the\ncase of the particular data. Here are presented various measures of predictive\nmodel like: Gini, Delta Gini, VIF and Max p-value, emphasizing the\nmulti-criteria problem of a \"Good model\". The idea that is being suggested is\nof particular use in the model building process where there are defined complex\ncriteria trying to cover the important problems of model stability over a\nperiod of time, in order to avoid a crisis. Some arguments for choosing Logit\nor WOE approach as the best scorecard technique are presented.", "category": ["q-fin.ST", "q-fin.RM"], "id": "http://arxiv.org/abs/1210.0057v1", "link": "http://arxiv.org/abs/1210.0057v1"}, {"title": "Optimal portfolios in commodity futures markets", "summary": "We consider portfolio optimization in futures markets. We model the entire\nfutures price curve at once as a solution of a stochastic partial differential\nequation. The agents objective is to maximize her utility from the final wealth\nwhen investing in futures contracts. We study a class of futures price curve\nmodels which admit a finite-dimensional realization. Using this, we recast the\nportfolio optimization problem as a finite-dimensional control problem and\nstudy its solvability.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1204.2667v1", "link": "http://arxiv.org/abs/1204.2667v1"}, {"title": "Hedging in an equilibrium-based model for a large investor", "summary": "We study a financial model with a non-trivial price impact effect. In this\nmodel we consider the interaction of a large investor trading in an illiquid\nsecurity, and a market maker who is quoting prices for this security. We assume\nthat the market maker quotes the prices such that by taking the other side of\nthe investor's demand, the market maker will arrive at maturity with maximal\nexpected wealth. Within this model we concentrate on the issue of contingent\nclaims' hedging.", "category": ["q-fin.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/0910.3258v1", "link": "http://arxiv.org/abs/0910.3258v1"}, {"title": "Optimal investment with random endowments in incomplete markets", "summary": "In this paper, we study the problem of expected utility maximization of an\nagent who, in addition to an initial capital, receives random endowments at\nmaturity. Contrary to previous studies, we treat as the variables of the\noptimization problem not only the initial capital but also the number of units\nof the random endowments. We show that this approach leads to a dual problem,\nwhose solution is always attained in the space of random variables. In\nparticular, this technique does not require the use of finitely additive\nmeasures and the related assumption that the endowments are bounded.", "category": ["math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/math/0405293v1", "link": "http://dx.doi.org/10.1214/105051604000000134"}, {"title": "Queueing theoretical analysis of foreign currency exchange rates", "summary": "We propose a useful approach for investigating the statistical properties of\nforeign currency exchange rates. Our approach is based on queueing theory,\nparticularly, the so-called renewal-reward theorem. For the first passage\nprocesses of the Sony Bank US dollar/Japanese yen (USD/JPY) exchange rate, we\nevaluate the average waiting time which is defined as the average time that\ncustomers have to wait between any instant when they want to observe the rate\n(e.g. when they log in to their computer systems) and the next rate change. We\nfind that the assumption of exponential distribution for the first-passage\nprocess should be rejected and that a Weibull distribution seems more suitable\nfor explaining the stochastic process of the Sony Bank rate. Our approach also\nenables us to evaluate the expected reward for customers, i.e. one can predict\nhow long customers must wait and how much reward they will obtain by the next\nprice change after they log in to their computer systems. We check the validity\nof our prediction by comparing it with empirical data analysis.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0606040v3", "link": "http://arxiv.org/abs/physics/0606040v3"}, {"title": "Effects of diversification among assets in an agent-based market model", "summary": "We extend to the multi-asset case the framework of a discrete time model of a\nsingle asset financial market developed in Ghoulmie et al (2005). In\nparticular, we focus on adaptive agents with threshold behavior allocating\ntheir resources among two assets. We explore numerically the effect of this\ndiversification as an additional source of complexity in the financial market\nand we discuss its destabilizing role. We also point out the relevance of these\nstudies for financial decision making.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0712.3611v1", "link": "http://dx.doi.org/10.1117/12.758912"}, {"title": "Taxation and Valuation", "summary": "The greatest harm from highway robbers lies not in seized wallets but in\ninhibited travel. Similarly, incentives for tax-reducing strategies put much\nsand in the wheels of the economy. Demands to replace our monumental tax code\nwith a simple, graceful one that does not distort economic incentives heat up\nperiodically in political debate, but such dreams never materialize. A\nFUNDAMENTAL obstacle, not yet well understood in the economic literature, is\nthe impossibility of objectively evaluating the tax base -- assets, income,\netc. One can see this even in toy examples, say, trying to assess the value of\na position in chess: great masters' assessments will all differ. Here computer\ntheory can add an insight not provided by classical economics tools.\n  A way around is to avoid evaluations by expressing the tax in natural units,\nnot in cash. For publicly traded corporations, these could be corporate shares.\nI discuss a simple (postcard-sized in ALL details) corporate tax system that\navoids ANY distortion of incentives. (Tax tools MEANT to influence corporate\npolicies should be set as explicit separate taxes or credits, open to public\nscrutiny, not hidden between lines of an incomprehensible tax code.) Roughly,\nthe~system is to periodically take a t*i fraction of shares to auction, where t\nis the tax rate, i is the interest rate. It replaces all income taxes on\npublicly traded corporations, their subsidiaries, and shareholders.\n  The interest rate is defined via specially designed bonds, so that the whole\nsystem can be shown PRECISELY equivalent to a flat tax on INVESTMENT RETURN.\nNote that taxing the return DIRECTLY is impossible: it would invite\nmanipulation of stock market~prices. The main feature is that nothing\ncorporations and investors do can change their tax (t*i fraction of shares), so\nthey would do business exactly the SAME WAY they would WITHOUT TAXES.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cs/0012013v21", "link": "http://arxiv.org/abs/cs/0012013v21"}, {"title": "Black swans or dragon kings? A simple test for deviations from the power\n  law", "summary": "We develop a simple test for deviations from power law tails, which is based\non the asymptotic properties of the empirical distribution function. We use\nthis test to answer the question whether great natural disasters, financial\ncrashes or electricity price spikes should be classified as dragon kings or\n'only' as black swans.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1102.3712v1", "link": "http://dx.doi.org/10.1140/epjst/e2012-01563-9"}, {"title": "Set-Valued Risk Measures as Backward Stochastic Difference Inclusions\n  and Equations", "summary": "Scalar dynamic risk measures in continuous time are commonly represented as\nbackward stochastic differential equations. There are two possible extensions\nfor scalar backward stochastic differential equations for the set-valued\nframework: (1) backward stochastic differential inclusions; or (2) set-valued\nbackward stochastic differential equations. In this work, the discrete-time\nsetting is investigated with difference inclusions and difference equations in\norder to provide insights for such differential representations for set-valued\ndynamic risk measures.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1912.06916v1", "link": "http://arxiv.org/abs/1912.06916v1"}, {"title": "The Principle of the Malevolent Hiding Hand; or, the Planning Fallacy\n  Writ Large", "summary": "We identify and document a new principle of economic behavior: the principle\nof the Malevolent Hiding Hand. In a famous discussion, Albert Hirschman\ncelebrated the Hiding Hand, which he saw as a benevolent mechanism by which\nunrealistically optimistic planners embark on unexpectedly challenging plans,\nonly to be rescued by human ingenuity, which they could not anticipate, but\nwhich ultimately led to success, principally in the form of unexpectedly high\nnet benefits. Studying eleven projects, Hirschman suggested that the Hiding\nHand is a general phenomenon. But the Benevolent Hiding Hand has an evil twin,\nthe Malevolent Hiding Hand, which blinds excessively optimistic planners not\nonly to unexpectedly high costs but also to unexpectedly low net benefits.\nStudying a much larger sample than Hirschman did, we find that the Malevolent\nHiding Hand is common and that the phenomenon that Hirschman identified is\nrare. This sobering finding suggests that Hirschman's phenomenon is a special\ncase; it attests to the pervasiveness of the planning fallacy, writ very large.\nOne implication involves the continuing need for unbiased cost-benefit analyses\nand other economic decision support tools; another is that such tools might\nsometimes prove unreliable.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1509.01526v1", "link": "http://arxiv.org/abs/1509.01526v1"}, {"title": "The alchemy of probability distributions: beyond Gram-Charlier\n  expansions, and a skew-kurtotic-normal distribution from a rank transmutation\n  map", "summary": "Motivated by the need for parametric families of rich and yet tractable\ndistributions in financial mathematics, both in pricing and risk management\nsettings, but also considering wider statistical applications, we investigate a\nnovel technique for introducing skewness or kurtosis into a symmetric or other\ndistribution. We use a \"transmutation\" map, which is the functional composition\nof the cumulative distribution function of one distribution with the inverse\ncumulative distribution (quantile) function of another. In contrast to the\nGram-Charlier approach, this is done without resorting to an asymptotic\nexpansion, and so avoids the pathologies that are often associated with it.\nExamples of parametric distributions that we can generate in this way include\nthe skew-uniform, skew-exponential, skew-normal, and skew-kurtotic-normal.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/0901.0434v1", "link": "http://arxiv.org/abs/0901.0434v1"}, {"title": "Outsider Trading", "summary": "In this paper we examine inefficiencies and information disparity in the\nJapanese stock market. By carefully analysing information publicly available on\nthe internet, an `outsider' to conventional statistical arbitrage\nstrategies--which are based on market microstructure, company releases, or\nanalyst reports--can nevertheless pursue a profitable trading strategy. A large\nvolume of blog data is used to demonstrate the existence of an inefficiency in\nthe market. An information-based model that replicates the trading strategy is\ndeveloped to estimate the degree of information disparity.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1003.0764v2", "link": "http://arxiv.org/abs/1003.0764v2"}, {"title": "Finance from the viewpoint of physics", "summary": "In this note we review the basic mathematical ideas used in finance in the\nlanguage of modern physics. We focus on discrete time formalism, derive path\nintegral and Green's function formulas for pricing. We also discuss various\nrisk mitigation methods.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2001.09446v2", "link": "http://arxiv.org/abs/2001.09446v2"}, {"title": "A cutting surface algorithm for semi-infinite convex programming with an\n  application to moment robust optimization", "summary": "We present and analyze a central cutting surface algorithm for general\nsemi-infinite convex optimization problems, and use it to develop a novel\nalgorithm for distributionally robust optimization problems in which the\nuncertainty set consists of probability distributions with given bounds on\ntheir moments. Moments of arbitrary order, as well as non-polynomial moments\ncan be included in the formulation. We show that this gives rise to a hierarchy\nof optimization problems with decreasing levels of risk-aversion, with classic\nrobust optimization at one end of the spectrum, and stochastic programming at\nthe other. Although our primary motivation is to solve distributionally robust\noptimization problems with moment uncertainty, the cutting surface method for\ngeneral semi-infinite convex programs is also of independent interest. The\nproposed method is applicable to problems with non-differentiable semi-infinite\nconstraints indexed by an infinite-dimensional index set. Examples comparing\nthe cutting surface algorithm to the central cutting plane algorithm of\nKortanek and No demonstrate the potential of our algorithm even in the solution\nof traditional semi-infinite convex programming problems whose constraints are\ndifferentiable and are indexed by an index set of low dimension. After the rate\nof convergence analysis of the cutting surface algorithm, we extend the\nauthors' moment matching scenario generation algorithm to a probabilistic\nalgorithm that finds optimal probability distributions subject to moment\nconstraints. The combination of this distribution optimization method and the\ncentral cutting surface algorithm yields a solution to a family of\ndistributionally robust optimization problems that are considerably more\ngeneral than the ones proposed to date.", "category": ["q-fin.CP", "q-fin.PM"], "id": "http://arxiv.org/abs/1306.3437v3", "link": "http://arxiv.org/abs/1306.3437v3"}, {"title": "Multidimensional dynamic risk measure via conditional g-expectation", "summary": "This paper deals with multidimensional dynamic risk measures induced by\nconditional $g$-expectations. A notion of multidimensional $g$-expectation is\nproposed to provide a multidimensional version of nonlinear expectations. By a\ntechnical result on explicit expressions for the comparison theorem, uniqueness\ntheorem and viability on a rectangle of solutions to multidimensional backward\nstochastic differential equations, some necessary and sufficient conditions are\ngiven for the constancy, monotonicity, positivity, homogeneity and\ntranslatability properties of multidimensional conditional $g$-expectations and\nmultidimensional dynamic risk measures; we prove that a multidimensional\ndynamic $g$-risk measure is nonincreasingly convex if and only if the generator\n$g$ satisfies a quasi-monotone increasingly convex condition. A general dual\nrepresentation is given for the multidimensional dynamic convex $g$-risk\nmeasure in which the penalty term is expressed more precisely. It is shown that\nmodel uncertainty leads to the convexity of risk measures. As to applications,\nwe show how this multidimensional approach can be applied to measure the\ninsolvency risk of a firm with interacted subsidiaries; optimal risk sharing\nfor $\\protect\\gamma $-tolerant $g$-risk measures is investigated. Insurance\n$g$-risk measure and other ways to induce $g$-risk measures are also studied at\nthe end of the paper.", "category": ["q-fin.RM", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1011.3685v4", "link": "http://arxiv.org/abs/1011.3685v4"}, {"title": "Do the rich get richer? An empirical analysis of the BitCoin transaction\n  network", "summary": "The possibility to analyze everyday monetary transactions is limited by the\nscarcity of available data, as this kind of information is usually considered\nhighly sensitive. Present econophysics models are usually employed on presumed\nrandom networks of interacting agents, and only macroscopic properties (e.g.\nthe resulting wealth distribution) are compared to real-world data. In this\npaper, we analyze BitCoin, which is a novel digital currency system, where the\ncomplete list of transactions is publicly available. Using this dataset, we\nreconstruct the network of transactions, and extract the time and amount of\neach payment. We analyze the structure of the transaction network by measuring\nnetwork characteristics over time, such as the degree distribution, degree\ncorrelations and clustering. We find that linear preferential attachment drives\nthe growth of the network. We also study the dynamics taking place on the\ntransaction network, i.e. the flow of money. We measure temporal patterns and\nthe wealth accumulation. Investigating the microscopic statistics of money\nmovement, we find that sublinear preferential attachment governs the evolution\nof the wealth distribution. We report a scaling relation between the degree and\nwealth associated to individual nodes.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1308.3892v3", "link": "http://dx.doi.org/10.1371/journal.pone.0086197"}, {"title": "Calibration of Machine Learning Classifiers for Probability of Default\n  Modelling", "summary": "Binary classification is highly used in credit scoring in the estimation of\nprobability of default. The validation of such predictive models is based both\non rank ability, and also on calibration (i.e. how accurately the probabilities\noutput by the model map to the observed probabilities). In this study we cover\nthe current best practices regarding calibration for binary classification, and\nexplore how different approaches yield different results on real world credit\nscoring data. The limitations of evaluating credit scoring models using only\nrank ability metrics are explored. A benchmark is run on 18 real world\ndatasets, and results compared. The calibration techniques used are Platt\nScaling and Isotonic Regression. Also, different machine learning models are\nused: Logistic Regression, Random Forest Classifiers, and Gradient Boosting\nClassifiers. Results show that when the dataset is treated as a time series,\nthe use of re-calibration with Isotonic Regression is able to improve the long\nterm calibration better than the alternative methods. Using re-calibration, the\nnon-parametric models are able to outperform the Logistic Regression on Brier\nScore Loss.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1710.08901v1", "link": "http://arxiv.org/abs/1710.08901v1"}, {"title": "Regret Theory And Asset Pricing Anomalies In Incomplete Markets With\n  Dynamic Un-Aggregated Preferences", "summary": "Although the CML (Capital Market Line), the Intertemporal-CAPM, the CAPM/SML\n(Security Market Line) and the Intertemporal Arbitrage Pricing Theory (IAPT)\nare widely used in portfolio management, valuation and capital markets\nfinancing; these theories are inaccurate and can adversely affect risk\nmanagement and portfolio management processes. This article introduces several\nempirically testable financial theories that provide insights, and can be\ncalibrated to real data and used to solve problems, and contributes to the\nliterature by: i) explaining the conditions under which ICAPM/CAPM, IAPT and\nCML may be accurate, and why such conditions are not feasible; and explaining\nwhy the existence of incomplete markets and dynamic un-aggregated markets\nrender CML, IAPT and ICAPM inaccurate; ii) explaining why the\nConsumption-Savings-InvestmentProduction framework is insufficient for asset\npricing and analysis of changes in risk and asset values; and introducing a\nunified approach to asset pricing that simultaneously considers six factors,\nand the conditions under which this approach will work; iii) explaining why\nleisure, taxes and housing are equally as important as consumption and\ninvestment in asset pricing; iv) introducing the Marginal Rate of Intertemporal\nJoint Substitution (MRIJS) among Consumption, Taxes, Investment, Leisure,\nIntangibles and Housing - this model incorporates Regret Theory and captures\nfeatures of reality that dont fit well into standard asset pricing models, and\nthis framework can support specific or very general finance theories and or\nvery complicated models; v) showing why the Elasticity of Intertemporal\nSubstitution (EIS) is inaccurate and is insufficient for asset pricing and\nanalysis of investor preferences.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2005.01709v1", "link": "http://dx.doi.org/10.1057/978-1-137-44698-5"}, {"title": "The Hunt Hypothesis and the Dividend Policy of the Firm. The Chaotic\n  Motion of the Profits", "summary": "We have carried out simulations of a financial model of the firm to analyse\nthe validity of the concept of Trade on Equity in dynamics. The results exhibit\nthe ability of the borrowing policy connected to a cautious dividend\ndistribution to inject chaos into the profit motion. The 3D system built with\nthe van der Pol's oscillator produces a new class of strange attractors.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/nlin/0206032v1", "link": "http://arxiv.org/abs/nlin/0206032v1"}, {"title": "Fact Sheet Research on Bayesian Decision Theory", "summary": "In this fact sheet we give some preliminary research results on the Bayesian\nDecision Theory. This theory has been under construction for the past two\nyears. But what started as an intuitive enough idea, now seems to have the\nmakings of something more fundamental.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1409.8269v4", "link": "http://arxiv.org/abs/1409.8269v4"}, {"title": "Quantum Field Theory of Treasury Bonds", "summary": "The Heath-Jarrow-Morton (HJM) formulation of treasury bonds in terms of\nforward rates is recast as a problem in path integration. The HJM-model is\ngeneralized to the case where all the forward rates are allowed to fluctuate\nindependently. The resulting theory is shown to be a two-dimensional Gaussian\nquantum field theory. The no arbitrage condition is obtained and a functional\nintegral derivation is given for the price of a futures and an options\ncontract.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/9809199v1", "link": "http://arxiv.org/abs/cond-mat/9809199v1"}, {"title": "Gradually Truncated Log-normal distribution - Size distribution of firms", "summary": "Gradually Truncated Log-normal distribution - Size distribution of firms\n  Abstract\n  Many natural and economical phenomena are described through power law or log-\nnormal distributions. In these cases, probability decreases very slowly with\nstep size compared to normal distribution. Thus it is essential to cut-off\nthese distributions for larger step size. Recently we introduce the gradually\ntruncated power law distribution to successfully describe variation of\nfinancial, educational, physical and citation index. In the present work, we\nintroduce gradually truncated log-normal distribution in which we gradually\ncut- off larger steps due to physical limitation of the system. We applied this\ndistribution successfully to size distribution of USA.\\'{}s manufactoring firms\nwhich is measured through their annual sell. The physical limitation are due to\nlimited market size or shortage of highly competent executives.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0111579v1", "link": "http://arxiv.org/abs/cond-mat/0111579v1"}, {"title": "Liquidity induced asset bubbles via flows of ELMMs", "summary": "We consider a constructive model for asset price bubbles, where the market\nprice $W$ is endogenously determined by the trading activity on the market and\nthe fundamental price $W^F$ is exogenously given, as in the work of Jarrow,\nProtter and Roch (2012). To justify $W^F$ from a fundamental point of view, we\nembed this constructive approach in the martingale theory of bubbles, see\nJarrow, Protter and Shimbo (2010) and Biagini, F\\\"ollmer and Nedelcu (2014), by\nshowing the existence of a flow of equivalent martingale measures for $W$,\nunder which $W^F$ equals the expectation of the discounted future cash flow. As\nan application, we study bubble formation and evolution in a financial network.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1611.01440v2", "link": "http://arxiv.org/abs/1611.01440v2"}, {"title": "Application of spectral methods for high-frequency financial data to\n  quantifying states of market participants", "summary": "Empirical analysis of the foreign exchange market is conducted based on\nmethods to quantify similarities among multi-dimensional time series with\nspectral distances introduced in [A.-H. Sato, Physica A, 382 (2007) 258--270].\nAs a result it is found that the similarities among currency pairs fluctuate\nwith the rotation of the earth, and that the similarities among best quotation\nrates are associated with those among quotation frequencies. Furthermore it is\nshown that the Jensen-Shannon spectral divergence is proportional to a mean of\nthe Kullback-Leibler spectral distance both empirically and numerically. It is\nconfirmed that these spectral distances are connected with distributions for\nbehavioral parameters of the market participants from numerical simulation.\nThis concludes that spectral distances of representative quantities of\nfinancial markets are related into diversification of behavioral parameters of\nthe market participants.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0709.1530v1", "link": "http://dx.doi.org/10.1016/j.physa.2008.01.044"}, {"title": "Multi-View Graph Convolutional Networks for Relationship-Driven Stock\n  Prediction", "summary": "Stock price movement prediction is commonly accepted as a very challenging\ntask due to the extremely volatile nature of financial markets. Previous works\ntypically focus on understanding the temporal dependency of stock price\nmovement based on the history of individual stock movement, but they do not\ntake the complex relationships among involved stocks into consideration.\nHowever it is well known that an individual stock price is correlated with\nprices of other stocks. To address that, we propose a deep learning-based\nframework, which utilizes recurrent neural network (RNN) and graph\nconvolutional network (GCN) to predict stock movement. Specifically, we first\nuse RNN to model the temporal dependency of each related stock' price movement\nbased on their own information of the past time slices, then we employ GCN to\nmodel the influence from involved stock based on three novel graphs which\nrepresent the shareholder relationship, industry relationship and concept\nrelationship among stocks based on investment decisions. Experiments on two\nstock indexes in China market show that our model outperforms other baselines.\nTo our best knowledge, it is the first time to incorporate multi-relationships\namong involved stocks into a GCN based deep learning framework for predicting\nstock price movement.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2005.04955v1", "link": "http://arxiv.org/abs/2005.04955v1"}, {"title": "Long term memories of developed and emerging markets: using the scaling\n  analysis to characterize their stage of development", "summary": "The scaling properties encompass in a simple analysis many of the volatility\ncharacteristics of financial markets. That is why we use them to probe the\ndifferent degree of markets development. We empirically study the scaling\nproperties of daily Foreign Exchange rates, Stock Market indices and fixed\nincome instruments by using the generalized Hurst approach. We show that the\nscaling exponents are associated with characteristics of the specific markets\nand can be used to differentiate markets in their stage of development. The\nrobustness of the results is tested by both Monte-Carlo studies and a\ncomputation of the scaling in the frequency-domain.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0403681v1", "link": "http://arxiv.org/abs/cond-mat/0403681v1"}, {"title": "Process-Based Risk Measures and Risk-Averse Control of Discrete-Time\n  Systems", "summary": "For controlled discrete-time stochastic processes we introduce a new class of\ndynamic risk measures, which we call process-based. Their main features are\nthat they measure risk of processes that are functions of the history of a base\nprocess. We introduce a new concept of conditional stochastic time consistency\nand we derive the structure of process-based risk measures enjoying this\nproperty. We show that they can be equivalently represented by a collection of\nstatic law-invariant risk measures on the space of functions of the state of\nthe base process. We apply this result to controlled Markov processes and we\nderive dynamic programming equations.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1411.2675v5", "link": "http://arxiv.org/abs/1411.2675v5"}, {"title": "Statistical Laws in the Income of Japanese Companies", "summary": "Following the work of Okuyama, Takayasu and Takayasu [Okuyama, Takayasu and\nTakayasu 1999] we analyze huge databases of Japanese companies' financial\nfigures and confirm that the Zipf's law, a power law distribution with the\nexponent -1, has been maintained over 30 years in the income distribution of\nJapanese companies with very high precision. Similar power laws are found not\nonly in income distribution of company's income, but also in the distributions\nof capital, sales and number of employees. From the data we find an important\ntime evolutionary property that the growth rate of income is approximately\nindependent of the value of income, namely, small companies and large ones have\nsimilar statistical chances of growth. This observational fact suggests the\napplicability of the theory of multiplicative stochastic processes developed in\nstatistical physics. We introduce a discrete version of Langevin equation with\nadditive and multiplicative noises as a simple time evolution model of\ncompany's income. We test the validity of the Takayasu-Sato-Takayasu condition\n[Takayasu, Sato and Takayasu 1997] for having an asymptotic power law\ndistribution as a unique statistically steady solution. Directly estimated\npower law exponents and theoretically evaluated ones are compared resulting a\nreasonable fit by introducing a normalization to reduce the effect of gross\neconomic change.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0308365v1", "link": "http://arxiv.org/abs/cond-mat/0308365v1"}, {"title": "Perpetual American vanilla option pricing under single regime change\n  risk. An exhaustive study", "summary": "Perpetual American options are financial instruments that can be readily\nexercised and do not mature. In this paper we study in detail the problem of\npricing this kind of derivatives, for the most popular flavour, within a\nframework in which some of the properties |volatility and dividend policy| of\nthe underlying stock can change at a random instant of time, but in such a way\nthat we can forecast their final values. Under this assumption we can model\nactual market conditions because most relevant facts usually entail sharp\npredictable consequences. The effect of this potential risk on perpetual\nAmerican vanilla options is remarkable: the very equation that will determine\nthe fair price depends on the solution to be found. Sound results are found\nunder the optics both of finance and physics. In particular, a parallelism\namong the overall outcome of this problem and a phase transition is\nestablished.", "category": ["q-fin.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/0812.0556v2", "link": "http://dx.doi.org/10.1088/1742-5468/2009/07/P07016"}, {"title": "The Illusion of the Perpetual Money Machine", "summary": "We argue that the present crisis and stalling economy continuing since 2007\nare rooted in the delusionary belief in policies based on a \"perpetual money\nmachine\" type of thinking. We document strong evidence that, since the early\n1980s, consumption has been increasingly funded by smaller savings, booming\nfinancial profits, wealth extracted from house price appreciation and explosive\ndebt. This is in stark contrast with the productivity-fueled growth that was\nseen in the 1950s and 1960s. This transition, starting in the early 1980s, was\nfurther supported by a climate of deregulation and a massive growth in\nfinancial derivatives designed to spread and diversify the risks globally. The\nresult has been a succession of bubbles and crashes, including the worldwide\nstock market bubble and great crash of October 1987, the savings and loans\ncrisis of the 1980s, the burst in 1991 of the enormous Japanese real estate and\nstock market bubbles, the emerging markets bubbles and crashes in 1994 and\n1997, the LTCM crisis of 1998, the dotcom bubble bursting in 2000, the recent\nhouse price bubbles, the financialization bubble via special investment\nvehicles, the stock market bubble, the commodity and oil bubbles and the debt\nbubbles, all developing jointly and feeding on each other. Rather than still\nhoping that real wealth will come out of money creation, we need fundamentally\nnew ways of thinking. In uncertain times, it is essential, more than ever, to\nthink in scenarios: what can happen in the future, and, what would be the\neffect on your wealth and capital? How can you protect against adverse\nscenarios? We thus end by examining the question \"what can we do?\" from the\nmacro level, discussing the fundamental issue of incentives and of constructing\nand predicting scenarios as well as developing investment insights.", "category": ["q-fin.GN", "q-fin.RM"], "id": "http://arxiv.org/abs/1212.2833v1", "link": "http://arxiv.org/abs/1212.2833v1"}, {"title": "Who voted for a No Deal Brexit? A Composition Model of Great Britains\n  2019 European Parliamentary Elections", "summary": "The purpose of this paper is to use the votes cast at the 2019 European\nelections held in United Kingdom to re-visit the analysis conducted subsequent\nto its 2016 European Union referendum vote. This exercise provides a staging\npost on public opinion as the United Kingdom moves to leave the European Union\nduring 2020. A composition data analysis in a seemingly unrelated regression\nframework is adopted that respects the compositional nature of the vote\noutcome; each outcome is a share that adds up to 100% and each outcome is\nrelated to the alternatives. Contemporary explanatory data for each counting\narea is sourced from the themes of socio-demographics, employment, life\nsatisfaction and place. The study find that there are still strong and stark\ndivisions in the United Kingdom, defined by age, qualifications, employment and\nplace. The use of a compositional analysis approach produces challenges in\nregards to the interpretation of these models, but marginal plots are seen to\naid the interpretation somewhat.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.06548v1", "link": "http://arxiv.org/abs/2001.06548v1"}, {"title": "Exponential utility maximization and indifference valuation with\n  unbounded payoffs", "summary": "We solve an exponential utility maximization problem with unbounded payoffs\nand portfolio constraints, via the theory of quadratic backward stochastic\ndifferential equations with unbounded terminal data. This generalizes the\nprevious work of Hu et al. (2005) [Ann. Appl. Probab., 15, 1691--1712] from the\nbounded to an unbounded framework. Furthermore, we study utility indifference\nvaluation of financial derivatives with unbounded payoffs, and derive a novel\nconvex dual representation of the prices. In particular, we obtain new\nasymptotic behavior as the risk aversion parameter tends to either zero or\ninfinity.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1707.00199v3", "link": "http://arxiv.org/abs/1707.00199v3"}, {"title": "Continuous growth models in terms of generalized logarithm and\n  exponential functions", "summary": "Consider the one-parameter generalizations of the logarithmic and exponential\nfunctions which are obtained from the integration of non-symmetrical\nhyperboles. These generalizations coincide to the one obtained in the context\nof non-extensive thermostatistics. We show that these functions are suitable to\ndescribe and unify the great majority of continuous growth models, which we\nbriefly review. Physical interpretation to the generalization function\nparameter is given for the Richards' model, which has an underlying microscopic\nmodel to justify it.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0803.2635v2", "link": "http://dx.doi.org/10.1016/j.physa.2008.06.015"}, {"title": "A Novel Approach to Forecasting Financial Volatility with Gaussian\n  Process Envelopes", "summary": "In this paper we use Gaussian Process (GP) regression to propose a novel\napproach for predicting volatility of financial returns by forecasting the\nenvelopes of the time series. We provide a direct comparison of their\nperformance to traditional approaches such as GARCH. We compare the forecasting\npower of three approaches: GP regression on the absolute and squared returns;\nregression on the envelope of the returns and the absolute returns; and\nregression on the envelope of the negative and positive returns separately. We\nuse a maximum a posteriori estimate with a Gaussian prior to determine our\nhyperparameters. We also test the effect of hyperparameter updating at each\nforecasting step. We use our approaches to forecast out-of-sample volatility of\nfour currency pairs over a 2 year period, at half-hourly intervals. From three\nkernels, we select the kernel giving the best performance for our data. We use\ntwo published accuracy measures and four statistical loss functions to evaluate\nthe forecasting ability of GARCH vs GPs. In mean squared error the GP's perform\n20% better than a random walk model, and 50% better than GARCH for the same\ndata.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1705.00891v1", "link": "http://arxiv.org/abs/1705.00891v1"}, {"title": "Optimal portfolio selection in an It\u00f4-Markov additive market", "summary": "We study a portfolio selection problem in a continuous-time It\\^o-Markov\nadditive market with prices of financial assets described by Markov additive\nprocesses which combine L\\'evy processes and regime switching models. Thus the\nmodel takes into account two sources of risk: the jump diffusion risk and the\nregime switching risk. For this reason the market is incomplete. We complete\nthe market by enlarging it with the use of a set of Markovian jump securities,\nMarkovian power-jump securities and impulse regime switching securities.\nMoreover, we give conditions under which the market is\nasymptotic-arbitrage-free. We solve the portfolio selection problem in the\nIt\\^o-Markov additive market for the power utility and the logarithmic utility.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1806.03496v1", "link": "http://arxiv.org/abs/1806.03496v1"}, {"title": "Detecting subtle effects of persistence in the stock market dynamics", "summary": "The conventional formal tool to detect effects of the financial persistence\nis in terms of the Hurst exponent. A typical corresponding result is that its\nvalue comes out close to 0.5, as characteristic for geometric Brownian motion,\nwith at most small departures from this value in either direction depending on\nthe market and on the time scales involved. We study the high frequency price\nchanges on the American and on the German stock markets. For both corresponding\nindices, the Dow Jones and the DAX respectively, the Hurst exponent analysis\nresults in values close to 0.5. However, by decomposing the market dynamics\ninto pairs of steps such that an elementary move up (down) is followed by\nanother move up (down) and explicitly counting the resulting conditional\nprobabilities we find values typically close to 60%. This effect of persistence\nis particularly visible on the short time scales ranging from 1 up to 3\nminutes, decreasing gradually to 50% and even significantly below this value on\nthe larger time scales. We also detect some asymmetry in persistence related to\nthe moves up and down, respectively. This indicates a subtle nature of the\nfinancial persistence whose characteristics escape detection within the\nconventional Hurst exponent formalism.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0504158v1", "link": "http://arxiv.org/abs/physics/0504158v1"}, {"title": "Dynamics of the return distribution in the Korean financial market", "summary": "In this paper, we studied the dynamics of the log-return distribution of the\nKorean Composition Stock Price Index (KOSPI) from 1992 to 2004. Based on the\nmicroscopic spin model, we found that while the index during the late 1990s\nshowed a power-law distribution, the distribution in the early 2000s was\nexponential. This change in distribution shape was caused by the duration and\nvelocity, among other parameters, of the information that flowed into the\nmarket.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0511119v3", "link": "http://dx.doi.org/10.1016/j.physa.2005.12.039"}, {"title": "Dynamics of symmetric SSVI smiles and implied volatility bubbles", "summary": "We develop a dynamic version of the SSVI parameterisation for the total\nimplied variance, ensuring that European vanilla option prices are martingales,\nhence preventing the occurrence of arbitrage, both static and dynamic.\nInsisting on the constraint that the total implied variance needs to be null at\nthe maturity of the option, we show that no model--in our setting--allows for\nsuch behaviour. This naturally gives rise to the concept of implied volatility\nbubbles, whereby trading in an arbitrage-free way is only possible during part\nof the life of the contract, but not all the way until expiry.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1909.10272v1", "link": "http://arxiv.org/abs/1909.10272v1"}, {"title": "Big is Fragile: An Attempt at Theorizing Scale", "summary": "In this paper we characterise the propensity of big capital investments to\nsystematically deliver poor outcomes as \"fragility,\" a notion suggested by\nNassim Taleb. A thing or system that is easily harmed by randomness is fragile.\nWe argue that, contrary to their appearance, big capital investments break\neasily - i.e. deliver negative net present value - due to various sources of\nuncertainty that impact them during their long gestation, implementation, and\noperation periods. We do not refute the existence of economies of scale and\nscope. Instead we argue that big capital investments have a disproportionate\n(non-linear) exposure to uncertainties that deliver poor or negative returns\nabove and beyond their economies of scale and scope. We further argue that to\nsucceed, leaders of capital projects need to carefully consider where scaling\npays off and where it does not. To automatically assume that \"bigger is\nbetter,\" which is common in megaproject management, is a recipe for failure.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1603.01416v2", "link": "http://arxiv.org/abs/1603.01416v2"}, {"title": "Applications of Gaussian Process Latent Variable Models in Finance", "summary": "Estimating covariances between financial assets plays an important role in\nrisk management. In practice, when the sample size is small compared to the\nnumber of variables, the empirical estimate is known to be very unstable. Here,\nwe propose a novel covariance estimator based on the Gaussian Process Latent\nVariable Model (GP-LVM). Our estimator can be considered as a non-linear\nextension of standard factor models with readily interpretable parameters\nreminiscent of market betas. Furthermore, our Bayesian treatment naturally\nshrinks the sample covariance matrix towards a more structured matrix given by\nthe prior and thereby systematically reduces estimation errors. Finally, we\ndiscuss some financial applications of the GP-LVM.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1806.03294v2", "link": "http://arxiv.org/abs/1806.03294v2"}, {"title": "A predictive pan-European economic and production dispatch model for the\n  energy transition in the electricity sector", "summary": "The energy transition is well underway in most European countries. It has a\ngrowing impact on electric power systems as it dramatically modifies the way\nelectricity is produced. To ensure a safe and smooth transition towards a\npan-European electricity production dominated by renewable sources, it is of\nparamount importance to anticipate how production dispatches will evolve, to\nunderstand how increased fluctuations in power generations can be absorbed at\nthe pan-European level and to evaluate where the resulting changes in power\nflows will require significant grid upgrades. To address these issues, we\nconstruct an aggregated model of the pan-European transmission network which we\ncouple to an optimized, few-parameter dispatch algorithm to obtain time- and\ngeographically-resolved production profiles. We demonstrate the validity of our\ndispatch algorithm by reproducing historical production time series for all\npower productions in fifteen different European countries. Having calibrated\nour model in this way, we investigate future production profiles at later\nstages of the energy transition - determined by planned future production\ncapacities - and the resulting interregional power flows. We find that large\npower fluctuations from increasing penetrations of renewable sources can be\nabsorbed at the pan-European level via significantly increased electricity\nexchanges between different countries. We identify where these increased\nexchanges will require additional power transfer capacities. We finally\nintroduce a physically-based economic indicator which allows to predict future\nfinancial conditions in the electricity market. We anticipate new economic\nopportunities for dam hydroelectricity and pumped-storage plants.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1706.01666v1", "link": "http://dx.doi.org/10.1109/PTC.2017.7980982"}, {"title": "Desperate times call for desperate measures: government spending\n  multipliers in hard times", "summary": "We investigate state-dependent effects of fiscal multipliers and allow for\nendogenous sample splitting to determine whether the US economy is in a slack\nstate. When the endogenized slack state is estimated as the period of the\nunemployment rate higher than about 12 percent, the estimated cumulative\nmultipliers are significantly larger during slack periods than non-slack\nperiods and are above unity. We also examine the possibility of time-varying\nregimes of slackness and find that our empirical results are robust under a\nmore flexible framework. Our estimation results points out the importance of\nthe heterogenous effects of fiscal policy and shed light on the prospect of\nfiscal policy in response to economic shocks of the current coronavirus\npandemic.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.09824v2", "link": "http://arxiv.org/abs/1909.09824v2"}, {"title": "Risk in a large claims insurance market with bipartite graph structure", "summary": "We model the influence of sharing large exogeneous losses to the reinsurance\nmarket by a bipartite graph. Using Pareto-tailed claims and multivariate\nregular variation we obtain asymptotic results for the Value-at-Risk and the\nConditional Tail Expectation. We show that the dependence on the network\nstructure plays a fundamental role in their asymptotic behaviour. As is\nwell-known in a non-network setting, if the Pareto exponent is larger than 1,\nthen for the individual agent (reinsurance company) diversification is\nbeneficial, whereas when it is less than 1, concentration on a few objects is\nthe better strategy. An additional aspect of this paper is the amount of\nuninsured losses which have to be convered by society. In the situation of\nnetworks of agents, in our setting diversification is never detrimental\nconcerning the amount of uninsured losses. If the Pareto-tailed claims have\nfinite mean, diversification turns out to be never detrimental, both for\nsociety and for individual agents. In contrast, if the Pareto-tailed claims\nhave infinite mean, a conflicting situation may arise between the incentives of\nindividual agents and the interest of some regulator to keep risk for society\nsmall. We explain the influence of the network structure on diversification\neffects in different network scenarios.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1410.8671v3", "link": "http://arxiv.org/abs/1410.8671v3"}, {"title": "Bayesian estimation of probabilities of default for low default\n  portfolios", "summary": "The estimation of probabilities of default (PDs) for low default portfolios\nby means of upper confidence bounds is a well established procedure in many\nfinancial institutions. However, there are often discussions within the\ninstitutions or between institutions and supervisors about which confidence\nlevel to use for the estimation. The Bayesian estimator for the PD based on the\nuninformed, uniform prior distribution is an obvious alternative that avoids\nthe choice of a confidence level. In this paper, we demonstrate that in the\ncase of independent default events the upper confidence bounds can be\nrepresented as quantiles of a Bayesian posterior distribution based on a prior\nthat is slightly more conservative than the uninformed prior. We then describe\nhow to implement the uninformed and conservative Bayesian estimators in the\ndependent one- and multi-period default data cases and compare their estimates\nto the upper confidence bound estimates. The comparison leads us to suggest a\nconstrained version of the uninformed (neutral) Bayesian estimator as an\nalternative to the upper confidence bound estimators.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1112.5550v5", "link": "http://arxiv.org/abs/1112.5550v5"}, {"title": "Conditional Value-at-Risk: Theory and Applications", "summary": "This thesis presents the Conditional Value-at-Risk concept and combines an\nanalysis that covers its application as a risk measure and as a vector norm.\nFor both areas of application the theory is revised in detail and examples are\ngiven to show how to apply the concept in practice.\n  In the first part, CVaR as a risk measure is introduced and the analysis\ncovers the mathematical definition of CVaR and different methods to calculate\nit. Then, CVaR optimization is analysed in the context of portfolio selection\nand how to apply CVaR optimization for hedging a portfolio consisting of\noptions. The original contributions in this part are an alternative proof of\nAcerbi's Integral Formula in the continuous case and an explicit programme\nformulation for portfolio hedging.\n  The second part first analyses the Scaled and Non-Scaled CVaR norm as new\nfamily of norms in $\\mathbb{R}^n$ and compares this new norm family to the more\nwidely known $L_p$ norms. Then, model (or signal) recovery problems are\ndiscussed and it is described how appropriate norms can be used to recover a\nsignal with less observations than the dimension of the signal. The last\nchapter of this dissertation then shows how the Non-Scaled CVaR norm can be\nused in this model recovery context. The original contributions in this part\nare an alternative proof of the equivalence of two different characterizations\nof the Scaled CVaR norm, a new proposition that the Scaled CVaR norm is\npiecewise convex, and the entire \\autoref{chapter:Recovery_using_CVaR}. Since\nthe CVaR norm is a rather novel concept, its applications in a model recovery\ncontext have not been researched yet. Therefore, the final chapter of this\nthesis might lay the basis for further research in this area.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1511.00140v1", "link": "http://arxiv.org/abs/1511.00140v1"}, {"title": "An Optimal Pairs-Trading Rule", "summary": "This paper is concerned with a pairs trading rule. The idea is to monitor two\nhistorically correlated securities. When divergence is underway, i.e., one\nstock moves up while the other moves down, a pairs trade is entered which\nconsists of a pair to short the outperforming stock and to long the\nunderperforming one. Such a strategy bets the \"spread\" between the two would\neventually converge. In this paper, a difference of the pair is governed by a\nmean-reverting model. The objective is to trade the pair so as to maximize an\noverall return. A fixed commission cost is charged with each transaction. In\naddition, a stop-loss limit is imposed as a state constraint. The associated\nHJB equations (quasi-variational inequalities) are used to characterize the\nvalue functions. It is shown that the solution to the optimal stopping problem\ncan be obtained by solving a number of quasi-algebraic equations. We provide a\nset of sufficient conditions in terms of a verification theorem. Numerical\nexamples are reported to demonstrate the results.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1302.6120v1", "link": "http://arxiv.org/abs/1302.6120v1"}, {"title": "From anti-conformism to extremism", "summary": "We here present a model of the dynamics of extremism based on opinion\ndynamics in order to understand the circumstances which favour its emergence\nand development in large fractions of the general public. Our model is based on\nthe bounded confidence hypothesis and on the evolution of initially\nanti-conformist agents to extreme positions.\n  Numerical analyses demonstrate that a few anti-conformists are able to drag a\nlarge fraction of conformists agents to their position provided that they\nexpress their views more often than the conformists. The most influential\nparameter controlling the outcome of the dynamics is the uncertainty of the\nconformist agents; the higher their uncertainty, the higher is the influence of\nanti-conformists. Systematic scans of the parameter space show the existence of\ntwo regime transitions, one following the conformists uncertainty parameter and\nthe other one following the anti-conformism strength.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1503.04799v1", "link": "http://arxiv.org/abs/1503.04799v1"}, {"title": "Structural Hamiltonian of the international trade network", "summary": "It is common wisdom that no nation is an isolated economic island. All\nnations participate in the global economy and are linked together through trade\nand finance. Here we analyze international trade network (ITN), being the\nnetwork of import-export relationships between countries. We show that in each\nyear over the analyzed period of 50 years (since 1950) the network is a typical\nrepresentative of the ensemble of maximally random networks. Structural\nHamiltonians characterizing binary and weighted versions of ITN are formulated\nand discussed. In particular, given binary representation of ITN (i.e. binary\nnetwork of trade channels) we show that the network of partnership in trade is\nwell described by the configuration model. We also show that in the weighted\nversion of ITN, bilateral trade volumes (i.e. directed connections which\nrepresent trade/money flows between countries) are only characterized by the\nproduct of the trading countries' GDPs, like in the famous gravity model of\ntrade.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1205.4589v1", "link": "http://arxiv.org/abs/1205.4589v1"}, {"title": "Prediction in locally stationary time series", "summary": "We develop an estimator for the high-dimensional covariance matrix of a\nlocally stationary process with a smoothly varying trend and use this statistic\nto derive consistent predictors in non-stationary time series. In contrast to\nthe currently available methods for this problem the predictor developed here\ndoes not rely on fitting an autoregressive model and does not require a\nvanishing trend. The finite sample properties of the new methodology are\nillustrated by means of a simulation study and a financial indices study.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2001.00419v2", "link": "http://arxiv.org/abs/2001.00419v2"}, {"title": "On martingale measures and pricing for continuous bond-stock market with\n  stochastic bond", "summary": "This papers addresses the stock option pricing problem in a continuous time\nmarket model where there are two stochastic tradable assets, and one of them is\nselected as a num\\'eraire. It is shown that the presence of arbitrarily small\nstochastic deviations in the evolution of the num\\'eraire process causes\nsignificant changes in the market properties. In particular, an equivalent\nmartingale measure is not unique for this market, and there are non-replicable\nclaims. The martingale prices and the hedging error can vary significantly and\ntake extreme values, for some extreme choices of the equivalent martingale\nmeasures. Some rational choices of the equivalent martingale measures are\nsuggested and discussed, including implied measures calculated from observed\nbond prices. This allows to calculate the implied market price of risk process.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1108.0719v4", "link": "http://arxiv.org/abs/1108.0719v4"}, {"title": "Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator", "summary": "We consider discrete default intensity based and logit type reduced form\nmodels for conditional default probabilities for corporate loans where we\ndevelop simple closed form approximations to the maximum likelihood estimator\n(MLE) when the underlying covariates follow a stationary Gaussian process. In a\npractically reasonable asymptotic regime where the default probabilities are\nsmall, say 1-3% annually, the number of firms and the time period of data\navailable is reasonably large, we rigorously show that the proposed estimator\nbehaves similarly or slightly worse than the MLE when the underlying model is\ncorrectly specified. For more realistic case of model misspecification, both\nestimators are seen to be equally good, or equally bad. Further, beyond a\npoint, both are more-or-less insensitive to increase in data. These conclusions\nare validated on empirical and simulated data. The proposed approximations\nshould also have applications outside finance, where logit-type models are used\nand probabilities of interest are small.", "category": ["econ.EM", "q-fin.PM"], "id": "http://arxiv.org/abs/1912.12611v1", "link": "http://arxiv.org/abs/1912.12611v1"}, {"title": "Optimistic versus Pessimistic--Optimal Judgemental Bias with Reference\n  Point", "summary": "This paper develops a model of reference-dependent assessment of subjective\nbeliefs in which loss-averse people optimally choose the expectation as the\nreference point to balance the current felicity from the optimistic\nanticipation and the future disappointment from the realisation. The choice of\nover-optimism or over-pessimism depends on the real chance of success and\noptimistic decision makers prefer receiving early information. In the portfolio\nchoice problem, pessimistic investors tend to trade conservatively, however,\nthey might trade aggressively if they are sophisticated enough to recognise the\nbiases since low expectation can reduce their fear of loss.", "category": ["q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/1310.2964v1", "link": "http://arxiv.org/abs/1310.2964v1"}, {"title": "Asymptotics for the Discrete-Time Average of the Geometric Brownian\n  Motion and Asian Options", "summary": "The time average of geometric Brownian motion plays a crucial role in the\npricing of Asian options in mathematical finance. In this paper we consider the\nasymptotics of the discrete-time average of a geometric Brownian motion sampled\non uniformly spaced times in the limit of a very large number of averaging time\nsteps. We derive almost sure limit, fluctuations, large deviations, and also\nthe asymptotics of the moment generating function of the average. Based on\nthese results, we derive the asymptotics for the price of Asian options with\ndiscrete-time averaging in the Black-Scholes model, with both fixed and\nfloating strike.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1706.09659v1", "link": "http://arxiv.org/abs/1706.09659v1"}, {"title": "Duality for pathwise superhedging in continuous time", "summary": "We provide a model-free pricing-hedging duality in continuous time. For a\nfrictionless market consisting of $d$ risky assets with continuous price\ntrajectories, we show that the purely analytic problem of finding the minimal\nsuperhedging price of a path dependent European option has the same value as\nthe purely probabilistic problem of finding the supremum of the expectations of\nthe option over all martingale measures. The superhedging problem is formulated\nwith simple trading strategies, the claim is the limit inferior of continuous\nfunctions, which allows for upper and lower semi-continuous claims, and\nsuperhedging is required in the pathwise sense on a $\\sigma$-compact sample\nspace of price trajectories. If the sample space is stable under stopping, the\nprobabilistic problem reduces to finding the supremum over all martingale\nmeasures with compact support. As an application of the general results we\ndeduce dualities for Vovk's outer measure and semi-static superhedging with\nfinitely many securities.", "category": ["q-fin.MF", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1705.02933v3", "link": "http://arxiv.org/abs/1705.02933v3"}, {"title": "Arbitrage theory without a num\u00e9raire", "summary": "This note develops an arbitrage theory for a discrete-time market model\nwithout the assumption of the existence of a num\\'eraire asset. Fundamental\ntheorems of asset pricing are stated and proven in this context. The\ndistinction between the notions of investment-consumption arbitrage and\npure-investment arbitrage provide a discrete-time analogue of the distinction\nbetween the notions of absolute arbitrage and relative arbitrage in the\ncontinuous-time theory. Applications to the modelling of bubbles is discussed.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1410.2976v2", "link": "http://arxiv.org/abs/1410.2976v2"}, {"title": "Inference on a Distribution from Noisy Draws", "summary": "We consider a situation where the distribution of a random variable is being\nestimated by the empirical distribution of noisy measurements of that variable.\nThis is common practice in, for example, teacher value-added models and other\nfixed-effect models for panel data. We use an asymptotic embedding where the\nnoise shrinks with the sample size to calculate the leading bias in the\nempirical distribution arising from the presence of noise. The leading bias in\nthe empirical quantile function is equally obtained. These calculations are new\nin the literature, where only results on smooth functionals such as the mean\nand variance have been derived. Given a closed-form expression for the bias,\nbias-corrected estimator of the distribution function and quantile function can\nbe constructed. We provide both analytical and jackknife corrections that\nrecenter the limit distribution and yield confidence intervals with correct\ncoverage in large samples. These corrections are non-parametric and easy to\nimplement. Our approach can be connected to corrections for selection bias and\nshrinkage estimation and is to be contrasted with deconvolution. Simulation\nresults confirm the much-improved sampling behavior of the corrected\nestimators.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1803.04991v4", "link": "http://arxiv.org/abs/1803.04991v4"}, {"title": "Dynamics and Stability in Retail Competition", "summary": "Retail competition today can be described by three main features: i)\noligopolistic competition, ii) multi-store settings, and iii) the presence of\nlarge economies of scale. In these markets, firms usually apply a centralized\ndecisions making process in order to take full advantage of economies of\nscales, e.g. retail distribution centers. In this paper, we model and analyze\nthe stability and chaos of retail competition considering all these issues. In\nparticular, a dynamic multi-market Cournot-Nash equilibrium with global\neconomies and diseconomies of scale model is developed. We confirm the\nnon-intuitive hypothesis that retail multi-store competition is more unstable\nthat traditional small business that cover the same demand. The main sources of\nstability are the scale parameter and the number of markets", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1510.04550v2", "link": "http://dx.doi.org/10.1016/j.matcom.2016.09.011"}, {"title": "Fundamental defect of the macroeconomic thinking as one of the main\n  causes of the crisis endured", "summary": "The main points of the first section of the article written by S.I.\nChernyshov, A.V. Voronin and S.A. Razumovsky arXiv:1003.4382), which deals with\nthe fundamental bases of the macroeconomic theory, have been analyzed. An\nincorrectness of the Harrod's model of the economical growth in its generally\naccepted interpretation was specifically considered. The inevitability of the\neconomic crisis has been shown to follow directly from the premises of this\nmodel. At the same time there is an opportunity to realize the damping\nstrategies.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1004.3067v1", "link": "http://arxiv.org/abs/1004.3067v1"}, {"title": "Bounds on Multi-asset Derivatives via Neural Networks", "summary": "Using neural networks, we compute bounds on the prices of multi-asset\nderivatives given information on prices of related payoffs. As a main example,\nwe focus on European basket options and include information on the prices of\nother similar options, such as spread options and/or basket options on\nsubindices. We show that, in most cases, adding further constraints gives rise\nto bounds that are considerably tighter and discuss the maximizing/minimizing\ncopulas achieving such bounds. Our approach follows the literature on\nconstrained optimal transport and, in particular, builds on a recent paper by\nEckstein and Kupper (2019, Appl. Math. Optim.).", "category": ["q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1911.05523v5", "link": "http://arxiv.org/abs/1911.05523v5"}, {"title": "Sustainability in the Stochastic Ramsey Model", "summary": "In this paper we provide a self-contained exposition of the problem of\nsustaining a constant consumption level in a Ramsey model. Our focus is on the\ncase in which the output capital-ratio is random. After a brief review of the\nknown results on the probabilities of sustaining a target consumption from an\ninitial stock, we present some new results on estimating the probabilities by\nusing Chebyshev inequalities. Some numerical calculations for these estimates\nare also provided.", "category": ["q-fin.EC", "math.PR"], "id": "http://arxiv.org/abs/1511.07419v1", "link": "http://arxiv.org/abs/1511.07419v1"}, {"title": "Mean Escape Time in a System with Stochastic Volatility", "summary": "We study the mean escape time in a market model with stochastic volatility.\nThe process followed by the volatility is the Cox Ingersoll and Ross process\nwhich is widely used to model stock price fluctuations. The market model can be\nconsidered as a generalization of the Heston model, where the geometric\nBrownian motion is replaced by a random walk in the presence of a cubic\nnonlinearity. We investigate the statistical properties of the escape time of\nthe returns, from a given interval, as a function of the three parameters of\nthe model. We find that the noise can have a stabilizing effect on the system,\nas long as the global noise is not too high with respect to the effective\npotential barrier experienced by a fictitious Brownian particle. We compare the\nprobability density function of the return escape times of the model with those\nobtained from real market data. We find that they fit very well.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0612077v1", "link": "http://dx.doi.org/10.1103/PhysRevE.75.016106"}, {"title": "Quantile Dependence between Stock Markets and its Application in\n  Volatility Forecasting", "summary": "This paper examines quantile dependence between international stock markets\nand evaluates its use for improving volatility forecasting. First, we analyze\nquantile dependence and directional predictability between the US stock market\nand stock markets in the UK, Germany, France and Japan. We use the\ncross-quantilogram, which is a correlation statistic of quantile hit processes.\nThe detailed dependence between stock markets depends on specific quantile\nranges and this dependence is generally asymmetric; the negative spillover\neffect is stronger than the positive spillover effect and there exists strong\ndirectional predictability from the US market to the UK, Germany, France and\nJapan markets. Second, we consider a simple quantile-augmented volatility model\nthat accommodates the quantile dependence and directional predictability\nbetween the US market and these other markets. The quantile-augmented\nvolatility model provides superior in-sample and out-of-sample volatility\nforecasts.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1608.07193v1", "link": "http://arxiv.org/abs/1608.07193v1"}, {"title": "Hedging of game options in discrete markets with transaction costs", "summary": "We construct algorithms for computation of prices and superhedging strategies\nfor game options in general discrete markets both from the seller and the buyer\npoints of view.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1206.4506v1", "link": "http://arxiv.org/abs/1206.4506v1"}, {"title": "The cyclicality of loan loss provisions under three different accounting\n  models: the United Kingdom, Spain, and Brazil", "summary": "A controversy involving loan loss provisions in banks concerns their\nrelationship with the business cycle. While international accounting standards\nfor recognizing provisions (incurred loss model) would presumably be\npro-cyclical, accentuating the effects of the current economic cycle, an\nalternative model, the expected loss model, has countercyclical\ncharacteristics, acting as a buffer against economic imbalances caused by\nexpansionary or contractionary phases in the economy. In Brazil, a mixed\naccounting model exists, whose behavior is not known to be pro-cyclical or\ncountercyclical. The aim of this research is to analyze the behavior of these\naccounting models in relation to the business cycle, using an econometric model\nconsisting of financial and macroeconomic variables. The study allowed us to\nidentify the impact of credit risk behavior, earnings management, capital\nmanagement, Gross Domestic Product (GDP) behavior, and the behavior of the\nunemployment rate on provisions in countries that use different accounting\nmodels. Data from commercial banks in the United Kingdom (incurred loss), in\nSpain (expected loss), and in Brazil (mixed model) were used, covering the\nperiod from 2001 to 2012. Despite the accounting models of the three countries\nbeing formed by very different rules regarding possible effects on the business\ncycles, the results revealed a pro-cyclical behavior of provisions in each\ncountry, indicating that when GDP grows, provisions tend to fall and vice\nversa. The results also revealed other factors influencing the behavior of loan\nloss provisions, such as earning management.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.07491v1", "link": "http://dx.doi.org/10.1590/1808-057x201804490"}, {"title": "A housing-demographic multi-layered nonlinear model to test regulation\n  strategies", "summary": "We propose a novel multi-layered nonlinear model that is able to capture and\npredict the housing-demographic dynamics of the real-state market by simulating\nthe transitions of owners among price-based house layers. This model allows us\nto determine which parameters are most effective to smoothen the severity of a\npotential market crisis. The International Monetary Fund (IMF) has issued\nsevere warnings about the current real-state bubble in the United States, the\nUnited Kingdom, Ireland, the Netherlands, Australia and Spain in the last\nyears. Madrid (Spain), in particular, is an extreme case of this bubble. It is,\ntherefore, an excellent test case to analyze housing dynamics in the context of\nthe empirical data provided by the Spanish National Institute of Statistics and\nother sources of data. The model is able to predict the mean house occupancy,\nand shows that i) the house market conditions in Madrid are unstable but not\ncritical; and ii) the regulation of the construction rate is more effective\nthan interest rate changes. Our results indicate that to accommodate the\nconstruction rate to the total population of first-time buyers is the most\neffective way to maintain the system near equilibrium conditions. In addition,\nwe show that to raise interest rates will heavily affect the poorest housing\nbands of the population while the middle class layers remain nearly unaffected.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0809.0979v1", "link": "http://arxiv.org/abs/0809.0979v1"}, {"title": "Causal Links Between US Economic Sectors", "summary": "In this paper, we perform a comparative segmentation and clustering analysis\nof the time series for the ten Dow Jones US economic sector indices between 14\nFebruary 2000 and 31 August 2008. From the temporal distributions of clustered\nsegments, we find that the US economy took one and a half years to recover from\nthe mid-1998-to-mid-2003 financial crisis, but only two months to completely\nenter the present financial crisis. We also find the oil & gas and basic\nmaterials sectors leading the recovery from the previous financial crisis,\nwhile the consumer goods and utilities sectors led the descent into the present\nfinancial crisis. On a macroscopic level, we find sectors going earlier into a\ncrisis emerge later from it, whereas sectors going later into the crisis emerge\nearlier. On the mesoscopic level, we find leading sectors experiencing stronger\nand longer volatility shocks, while trailing sectors experience weaker and\nshorter volatility shocks. In our shock-by-shock causal-link analysis, we also\nfind shorter delays between corresponding shocks in more closely related\neconomic sectors. In addition, our analysis reveals evidences for complex\nsectorial structures, as well as nonlinear amplification in the propagating\nvolatility shocks. From a perspective relevant to public policy, our study\nsuggests an endogeneous sectorial dynamics during the mid-2003 economic\nrecovery, in contrast to strong exogeneous driving by Federal Reserve interest\nrate cuts during the mid-2007 onset. Most interestingly, we find for the\nsequence of closely spaced interest rate cuts instituted in 2007/2008, the\nfirst few cuts effectively lowered market volatilities, while the next few cuts\ncounter-effectively increased market volatilities. Subsequent cuts evoked\nlittle response from the market.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/0911.4763v3", "link": "http://arxiv.org/abs/0911.4763v3"}, {"title": "On the support of extremal martingale measures with given marginals: the\n  countable case", "summary": "We investigate the supports of extremal martingale measures with\npre-specified marginals in a two-period setting. First, we establish in full\ngenerality the equivalence between the extremality of a given measure $Q$ and\nthe denseness in $L^1(Q)$ of a suitable linear subspace, which can be seen in a\nfinancial context as the set of all semi-static trading strategies. Moreover,\nwhen the supports of both marginals are countable, we focus on the slightly\nstronger notion of weak exact predictable representation property (henceforth,\nWEP) and provide two combinatorial sufficient conditions, called \"2-link\nproperty\" and \"full erasability\", on how the points in the supports are linked\nto each other for granting extremality. When the support of the first marginal\nis a finite set, we give a necessary and sufficient condition for the WEP to\nhold in terms of the new concepts of $2$-net and deadlock. Finally, we study\nthe relation between cycles and extremality.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1607.07197v4", "link": "http://arxiv.org/abs/1607.07197v4"}, {"title": "RPS(1) Preferences", "summary": "We consider a model for decision making based on an adaptive, k-period,\nlearning process where the priors are selected according to Von\nNeumann-Morgenstern expected utility principle. A preference relation between\ntwo prospects is introduced, defined by the condition which prospect is\nselected more often. We show that the new preferences have similarities with\nthe preferences obtained by Kahneman and Tversky (1979) in the context of the\nprospect theory. Additionally, we establish that in the limit of large learning\nperiod, the new preferences coincide with the expected utility principle.", "category": [], "id": "http://arxiv.org/abs/1901.04995v3", "link": "http://arxiv.org/abs/1901.04995v3"}, {"title": "The Declining Price Anomaly is not Universal in Multi-Buyer Sequential\n  Auctions (but almost is)", "summary": "The declining price anomaly states that the price weakly decreases when\nmultiple copies of an item are sold sequentially over time. The anomaly has\nbeen observed in a plethora of practical applications. On the theoretical side,\nGale and Stegeman proved that the anomaly is guaranteed to hold in full\ninformation sequential auctions with exactly two buyers. We prove that the\ndeclining price anomaly is not guaranteed in full information sequential\nauctions with three or more buyers. This result applies to both first-price and\nsecond-price sequential auctions. Moreover, it applies regardless of the\ntie-breaking rule used to generate equilibria in these sequential auctions. To\nprove this result we provide a refined treatment of subgame perfect equilibria\nthat survive the iterative deletion of weakly dominated strategies and use this\nframework to experimentally generate a very large number of random sequential\nauction instances. In particular, our experiments produce an instance with\nthree bidders and eight items that, for a specific tie-breaking rule, induces a\nnon-monotonic price trajectory. Theoretic analyses are then applied to show\nthat this instance can be used to prove that for every possible tie-breaking\nrule there is a sequential auction on which it induces a non-monotonic price\ntrajectory. On the other hand, our experiments show that non-monotonic price\ntrajectories are extremely rare. In over six million experiments only a\n0.000183 proportion of the instances violated the declining price anomaly.", "category": [], "id": "http://arxiv.org/abs/1905.00853v1", "link": "http://arxiv.org/abs/1905.00853v1"}, {"title": "Alternative Asymptotics and the Partially Linear Model with Many\n  Regressors", "summary": "Non-standard distributional approximations have received considerable\nattention in recent years. They often provide more accurate approximations in\nsmall samples, and theoretical improvements in some cases. This paper shows\nthat the seemingly unrelated \"many instruments asymptotics\" and \"small\nbandwidth asymptotics\" share a common structure, where the object determining\nthe limiting distribution is a V-statistic with a remainder that is an\nasymptotically normal degenerate U-statistic. We illustrate how this general\nstructure can be used to derive new results by obtaining a new asymptotic\ndistribution of a series estimator of the partially linear model when the\nnumber of terms in the series approximation possibly grows as fast as the\nsample size, which we call \"many terms asymptotics\".", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1505.08120v1", "link": "http://arxiv.org/abs/1505.08120v1"}, {"title": "Multiple Time Series Ising Model for Financial Market Simulations", "summary": "In this paper we propose an Ising model which simulates multiple financial\ntime series. Our model introduces the interaction which couples to spins of\nother systems. Simulations from our model show that time series exhibit the\nvolatility clustering that is often observed in the real financial markets.\nFurthermore we also find non-zero cross correlations between the volatilities\nfrom our model. Thus our model can simulate stock markets where volatilities of\nstocks are mutually correlated.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1611.08088v1", "link": "http://dx.doi.org/10.1088/1742-6596/574/1/012149"}, {"title": "Polynomial processes for power prices", "summary": "Polynomial processes have the property that expectations of polynomial\nfunctions (of degree $n$, say) of the future state of the process conditional\non the current state are given by polynomials (of degree $\\leq n$) of the\ncurrent state. Here we explore the application of polynomial processes in the\ncontext of structural models for energy prices. We focus on the example of\nAlberta power prices, derive one- and two-factor models for spot prices. We\nexamine their performance in numerical experiments, and demonstrate that the\nrichness of the dynamics they are able to generate makes them well suited for\nmodelling even extreme examples of energy price behaviour.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1710.10293v2", "link": "http://arxiv.org/abs/1710.10293v2"}, {"title": "Saddlepoint methods in portfolio theory", "summary": "We discuss the use of saddlepoint methods in the analysis of portfolios, with\nparticular reference to credit portfolios. The objective is to proceed from a\nmodel of the loss distribution, given through probabilities, correlations and\nthe like, to an analytical approximation of the distribution. Once this is done\nwe show how to derive the so-called risk contributions which are the\nderivatives of risk measures, such as a given quantile (VaR) or expected\nshortfall, to the allocations in the underlying assets. These show, informally,\nwhere the risk is coming from, and also indicate how to go about optimising the\nportfolio.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1201.0106v1", "link": "http://arxiv.org/abs/1201.0106v1"}, {"title": "Horizon-unbiased Investment with Ambiguity", "summary": "In the presence of ambiguity on the driving force of market randomness, we\nconsider the dynamic portfolio choice without any predetermined investment\nhorizon. The investment criteria is formulated as a robust forward performance\nprocess, reflecting an investor's dynamic preference. We show that the market\nrisk premium and the utility risk premium jointly determine the investors'\ntrading direction and the worst-case scenarios of the risky asset's mean return\nand volatility. The closed-form formulas for the optimal investment strategies\nare given in the special settings of the CRRA preference.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1904.09379v1", "link": "http://arxiv.org/abs/1904.09379v1"}, {"title": "Generalized Log-Normal Chain-Ladder", "summary": "We propose an asymptotic theory for distribution forecasting from the log\nnormal chain-ladder model. The theory overcomes the difficulty of convoluting\nlog normal variables and takes estimation error into account. The results\ndiffer from that of the over-dispersed Poisson model and from the chain-ladder\nbased bootstrap. We embed the log normal chain-ladder model in a class of\ninfinitely divisible distributions called the generalized log normal\nchain-ladder model. The asymptotic theory uses small $\\sigma$ asymptotics where\nthe dimension of the reserving triangle is kept fixed while the standard\ndeviation is assumed to decrease. The resulting asymptotic forecast\ndistributions follow t distributions. The theory is supported by simulations\nand an empirical application.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1806.05939v1", "link": "http://arxiv.org/abs/1806.05939v1"}, {"title": "Bayesian inference with an adaptive proposal density for GARCH models", "summary": "We perform the Bayesian inference of a GARCH model by the Metropolis-Hastings\nalgorithm with an adaptive proposal density. The adaptive proposal density is\nassumed to be the Student's t-distribution and the distribution parameters are\nevaluated by using the data sampled during the simulation. We apply the method\nfor the QGARCH model which is one of asymmetric GARCH models and make empirical\nstudies for for Nikkei 225, DAX and Hang indexes. We find that autocorrelation\ntimes from our method are very small, thus the method is very efficient for\ngenerating uncorrelated Monte Carlo data. The results from the QGARCH model\nshow that all the three indexes show the leverage effect, i.e. the volatility\nis high after negative observations.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/0908.2982v1", "link": "http://dx.doi.org/10.1088/1742-6596/221/1/012011"}, {"title": "Incentive-Compatible Critical Values", "summary": "Statistical hypothesis tests are a cornerstone of scientific research. The\ntests are informative when their size is properly controlled, so the frequency\nof rejecting true null hypotheses (type I error) stays below a prespecified\nnominal level. Publication bias exaggerates test sizes, however. Since\nscientists can typically only publish results that reject the null hypothesis,\nthey have the incentive to continue conducting studies until attaining\nrejection. Such $p$-hacking takes many forms: from collecting additional data\nto examining multiple regression specifications, all in the search of\nstatistical significance. The process inflates test sizes above their nominal\nlevels because the critical values used to determine rejection assume that test\nstatistics are constructed from a single study---abstracting from $p$-hacking.\nThis paper addresses the problem by constructing critical values that are\ncompatible with scientists' behavior given their incentives. We assume that\nresearchers conduct studies until finding a test statistic that exceeds the\ncritical value, or until the benefit from conducting an extra study falls below\nthe cost. We then solve for the incentive-compatible critical value (ICCV).\nWhen the ICCV is used to determine rejection, readers can be confident that\nsize is controlled at the desired significance level, and that the researcher's\nresponse to the incentives delineated by the critical value is accounted for.\nSince they allow researchers to search for significance among multiple studies,\nICCVs are larger than classical critical values. Yet, for a broad range of\nresearcher behaviors and beliefs, ICCVs lie in a fairly narrow range.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2005.04141v1", "link": "http://arxiv.org/abs/2005.04141v1"}, {"title": "Can Analysts Predict Rallies Better Than Crashes?", "summary": "We use the copula approach to study the structure of dependence between\nsell-side analysts' consensus recommendations and subsequent security returns,\nwith a focus on asymmetric tail dependence. We match monthly vintages of\nI/B/E/S recommendations for the period January to December 2011 with excess\nsecurity returns during six months following recommendation issue. Using a\nsymmetrized Joe-Clayton Copula (SJC) model we find evidence to suggest that\nanalysts can identify stocks that will substantially outperform, but not\nunderperform relative to the market, and that their predictive ability is\nconditional on recommendation changes.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1405.3225v1", "link": "http://arxiv.org/abs/1405.3225v1"}, {"title": "Optimal execution of Portfolio transactions with geometric price process", "summary": "In this paper we derive the optimal execution trajectory for a trader who\nwishes to buy or sell a large position of shares which evolve as a geometric\nBrownian process in contrast to the arithmetic model which prevails in the\nexisting literature, and with a general temporary impact $h$. We provide a\ncouple of examples which illustrate the results. We would like to stress the\nfact that in this paper we use understandable user-friendly techniques.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/0908.1211v2", "link": "http://arxiv.org/abs/0908.1211v2"}, {"title": "Quantum Nash Equilibria and Quantum Computing", "summary": "In this paper we review our earlier work on quantum computing and the Nash\nEquilibrium, in particular, tracing the history of the discovery of new Nash\nEquilibria and then reviewing the ways in which quantum computing may be\nexpected to generate new classes of Nash equilibria. We then extend this work\nthrough a substantive analysis of examples provided by Meyer, Flitney, Iqbal\nand Weigert and Cheon and Tsutsui with respect to quantized games, quantum game\nstrategies and the extension of Nash Equilibrium to solvable games in Hilbert\nspace. Finally, we review earlier work by Sato, Taiji and Ikegami on non-linear\ncomputation and computational classes by way of reference to coherence,\ndecoherence and quantum computating systems.", "category": ["q-fin.GN", "q-fin.CP"], "id": "http://arxiv.org/abs/0707.0324v1", "link": "http://dx.doi.org/10.1007/978-3-540-85081-6_56"}, {"title": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data", "summary": "Faced with the growing research towards crude oil price fluctuations\ninfluential factors following the accelerated development of Internet\ntechnology, accessible data such as Google search volume index are increasingly\nquantified and incorporated into forecasting approaches. In this paper, we\napply multi-scale data that including both GSVI data and traditional economic\ndata related to crude oil price as independent variables and propose a new\nhybrid approach for monthly crude oil price forecasting. This hybrid approach,\nbased on divide and conquer strategy, consists of K-means method, kernel\nprincipal component analysis and kernel extreme learning machine , where\nK-means method is adopted to divide input data into certain clusters, KPCA is\napplied to reduce dimension, and KELM is employed for final crude oil price\nforecasting. The empirical result can be analyzed from data and method levels.\nAt the data level, GSVI data perform better than economic data in level\nforecasting accuracy but with opposite performance in directional forecasting\naccuracy because of Herd Behavior, while hybrid data combined their advantages\nand obtain best forecasting performance in both level and directional accuracy.\nAt the method level, the approaches with K-means perform better than those\nwithout K-means, which demonstrates that divide and conquer strategy can\neffectively improve the forecasting performance.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2002.09656v1", "link": "http://arxiv.org/abs/2002.09656v1"}, {"title": "Approximating the zero-coupon bond price in a general one-factor model\n  with constant coefficients", "summary": "We consider a general one-factor short rate model, in which the instantaneous\ninterest rate is driven by a univariate diffusion with time independent drift\nand volatility. We construct recursive formula for the coefficients of the\nTaylor expansion of the bond price and its logarithm around $\\tau=0$, where\n$\\tau$ is time to maturity. We provide numerical examples of convergence of the\npartial sums of the series and compare them with the known exact values in the\ncase of Cox-Ingersoll-Ross and Dothan model.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1408.5673v1", "link": "http://arxiv.org/abs/1408.5673v1"}, {"title": "Why Markets are Inefficient: A Gambling \"Theory\" of Financial Markets\n  For Practitioners and Theorists", "summary": "The purpose of this article is to propose a new \"theory,\" the Strategic\nAnalysis of Financial Markets (SAFM) theory, that explains the operation of\nfinancial markets using the analytical perspective of an enlightened gambler.\nThe gambler understands that all opportunities for superior performance arise\nfrom suboptimal decisions by humans, but understands also that knowledge of\nhuman decision making alone is not enough to understand market behavior --- one\nmust still model how those decisions lead to market prices. Thus are there\nthree parts to the model: gambling theory, human decision making, and strategic\nproblem solving. A new theory is necessary because at this writing in 2017,\nthere is no theory of financial markets acceptable to both practitioners and\ntheorists. Theorists' efficient market theory, for example, cannot explain\nbubbles and crashes nor the exceptional returns of famous investors and\nspeculators such as Warren Buffett and George Soros. At the same time, a new\ntheory must be sufficiently quantitative, explain market \"anomalies\" and\nprovide predictions in order to satisfy theorists. It is hoped that the SAFM\nframework will meet these requirements.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1801.01948v1", "link": "http://arxiv.org/abs/1801.01948v1"}, {"title": "Existence, uniqueness and stability of optimal portfolios of eligible\n  assets", "summary": "In a capital adequacy framework, risk measures are used to determine the\nminimal amount of capital that a financial institution has to raise and invest\nin a portfolio of pre-specified eligible assets in order to pass a given\ncapital adequacy test. From a capital efficiency perspective, it is important\nto identify the set of portfolios of eligible assets that allow to pass the\ntest by raising the least amount of capital. We study the existence and\nuniqueness of such optimal portfolios as well as their sensitivity to changes\nin the underlying capital position. This naturally leads to investigating the\ncontinuity properties of the set-valued map associating to each capital\nposition the corresponding set of optimal portfolios. We pay special attention\nto lower semicontinuity, which is the key continuity property from a financial\nperspective. This \"stability\" property is always satisfied if the test is based\non a polyhedral risk measure but it generally fails once we depart from\npolyhedrality even when the reference risk measure is convex. However, lower\nsemicontinuity can be often achieved if one if one is willing to focuses on\nportfolios that are close to being optimal. Besides capital adequacy, our\nresults have a variety of natural applications to pricing, hedging, and capital\nallocation problems.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1702.01936v2", "link": "http://arxiv.org/abs/1702.01936v2"}, {"title": "On the Relation Between Linearity-Generating Processes and\n  Linear-Rational Models", "summary": "We review the notion of a linearity-generating (LG) process introduced by\nGabaix (2007) and relate LG processes to linear-rational (LR) models studied by\nFilipovic, Larsson, and Trolle (2017). We show that every LR model can be\nrepresented as an LG process and vice versa. We find that LR models have two\nbasic properties which make them an important representation of LG processes.\nFirst, LR models can be easily specified and made consistent with nonnegative\ninterest rates. Second, LR models go naturally with the long-term risk\nfactorization due to Alvarez and Jermann (2005), Hansen and Scheinkman (2009),\nand Qin and Linetsky (2017). Every LG process under the long forward measure\ncan be represented as a lower dimensional LR model.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1806.03153v1", "link": "http://arxiv.org/abs/1806.03153v1"}, {"title": "The subjective discount factor and the coefficient of relative risk\n  aversion under time-additive isoelastic expected utility model", "summary": "By analysing the restrictions that ensure the existence of capital market\nequilibrium, we show that the coefficient of relative risk aversion and the\nsubjective discount factor cannot be high simultaneously as they are supposed\nto be to make the standard asset pricing consistent with financial stylised\nfacts.", "category": ["q-fin.CP", "q-fin.GN"], "id": "http://arxiv.org/abs/1604.03337v2", "link": "http://arxiv.org/abs/1604.03337v2"}, {"title": "Statistical Properties of Statistical Ensembles of Stock Returns", "summary": "We select n stocks traded in the New York Stock Exchange and we form a\nstatistical ensemble of daily stock returns for each of the k trading days of\nour database from the stock price time series. We analyze each ensemble of\nstock returns by extracting its first four central moments. We observe that\nthese moments are fluctuating in time and are stochastic processes themselves.\nWe characterize the statistical properties of central moments by investigating\ntheir probability density function and temporal correlation properties.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9909302v1", "link": "http://arxiv.org/abs/cond-mat/9909302v1"}, {"title": "Capital allocation and risk appetite under Solvency II framework", "summary": "The aim of this paper is to introduce a method for computing the allocated\nSolvency II Capital Requirement (SCR) of each Risk which the company is exposed\nto, taking in account for the diversification effect among different risks. The\nmethod suggested is based on the Euler principle. We show that it has very\nsuitable properties like coherence in the sense of Denault (2001) and RORAC\ncompatibility, and practical implications for the companies that use the\nstandard formula. Further, we show how this approach can be used to evaluate\nthe underwriting and reinsurance policies and to define a measure of the\nCompany's risk appetite, based on the capital at risk return.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1511.02934v1", "link": "http://dx.doi.org/10.13140/RG.2.1.1136.8404"}, {"title": "The Impact of Taxation on GMWB Contract in a Stochastic Interest Rate\n  Framework", "summary": "Modeling taxation in GMWB Variable Annuities has been frequently neglected\nbut accounting for it can significantly improve the explanation of the\nwithdrawal dynamics and lead to a better modeling of the financial cost of\nthese insurance products. The importance of including a model of taxation has\nfirst been observed by Moenig and Bauer while considering the Black-Scholes\nmodel to describe the underlying security. Anyway, GMWB are long term products\nand thus accounting for stochastic interest rate has a relevant impact on both\nthe financial evaluation and the policy holder behavior, as observed by\nGouden\\`ege et al.. In this paper we investigate the impact of these two\nelements together on GMWB evaluation. To this aim we develop a numerical\nframework which allows one to efficiently compute the fair fee value of a\ncontract. Results show that accounting for both taxation and stochastic\ninterest rate has determinant effects on the behavior of the policy holder and\non the cost of GMWB contracts.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1901.11296v1", "link": "http://arxiv.org/abs/1901.11296v1"}, {"title": "Generalized framework for applying the Kelly criterion to stock markets", "summary": "We develop a general framework for applying the Kelly criterion to stock\nmarkets. By supplying an arbitrary probability distribution modeling the future\nprice movement of a set of stocks, the Kelly fraction for investing each stock\ncan be calculated by inverting a matrix involving only first and second\nmoments. The framework works for one or a portfolio of stocks and the Kelly\nfractions can be efficiently calculated. For a simple model of geometric\nBrownian motion of a single stock we show that our calculated Kelly fraction\nagrees with existing results. We demonstrate that the Kelly fractions can be\ncalculated easily for other types of probabilities such as the Gaussian\ndistribution and correlated multivariate assets.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1806.05293v1", "link": "http://dx.doi.org/10.1142/S0219024918500334"}, {"title": "Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\n  Futures Market Realized Volatility", "summary": "Financial markets for Liquified Natural Gas (LNG) are an important and\nrapidly-growing segment of commodities markets. Like other commodities markets,\nthere is an inherent spatial structure to LNG markets, with different price\ndynamics for different points of delivery hubs. Certain hubs support highly\nliquid markets, allowing efficient and robust price discovery, while others are\nhighly illiquid, limiting the effectiveness of standard risk management\ntechniques. We propose a joint modeling strategy, which uses high-frequency\ninformation from thickly-traded hubs to improve volatility estimation and risk\nmanagement at thinly traded hubs. The resulting model has superior in- and\nout-of-sample predictive performance, particularly for several commonly used\nrisk management metrics, demonstrating that joint modeling is indeed possible\nand useful. To improve estimation, a Bayesian estimation strategy is employed\nand data-driven weakly informative priors are suggested. Our model is robust to\nsparse data and can be effectively used in any market with similar irregular\npatterns of data availability.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1907.10152v1", "link": "http://arxiv.org/abs/1907.10152v1"}, {"title": "Breakdown of the mean-field approximation in a wealth distribution model", "summary": "One of the key socioeconomic phenomena to explain is the distribution of\nwealth. Bouchaud and M\\'ezard have proposed an interesting model of economy\n[Bouchaud and M\\'ezard (2000)] based on trade and investments of agents. In the\nmean-field approximation, the model produces a stationary wealth distribution\nwith a power-law tail. In this paper we examine characteristic time scales of\nthe model and show that for any finite number of agents, the validity of the\nmean-field result is time-limited and the model in fact has no stationary\nwealth distribution. Further analysis suggests that for heterogeneous agents,\nthe limitations are even stronger. We conclude with general implications of the\npresented results.", "category": ["q-fin.ST", "q-fin.GN"], "id": "http://arxiv.org/abs/0809.4139v2", "link": "http://dx.doi.org/10.1088/1742-5468/2009/02/P02014"}, {"title": "Credit acceptance process strategy case studies - the power of Credit\n  Scoring", "summary": "The paper is aware of the importance of certain figures that are essential to\nan understanding of Credit Scoring models in credit acceptance process\noptimization, namely if the power of discrimination measured by Gini value is\nincreased by 5% then the profit of the process can be increased monthly by\nabout 1 500 kPLN (300 kGBP, 500 kUSD, 350 kEUR). Simple business models of\ncredit loans are also presented: acquisition - installment loan (low price) and\ncross-sell - cash loans (high price). Scoring models are used to optimize\nprocess, to become profitable. Various acceptance strategies with different\ncutoffs are presented, some are profitable and some are not. Moreover, in a\ntime of prosperity some are preferable whilst the inverse is true during a\nperiod of high risk or crisis. To optimize the process four models are\nemployed: three risk models, to predict the probability of default and one\ntypical propensity model to predict the probability of response. It is a simple\nbut very important example of the Customer Lifetime Value (CLTV or CLV) model\nbusiness, where risk and response models are working together to become a\nprofitable process.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1403.6531v1", "link": "http://arxiv.org/abs/1403.6531v1"}, {"title": "Optimal insurance contract with benefits in kind under adverse selection", "summary": "An income loss can have a negative impact on households, forcing them to\nreduce their consumption of some staple goods. This can lead to health issues\nand, consequently, generate significant costs for society. We suggest that\nconsumers can, to prevent these negative consequences, buy insurance to secure\nsufficient consumption of a staple good if they lose part of their income. We\ndevelop a two-period/two-good principal-agent problem with adverse selection\nand endogenous reservation utility to model insurance with in-kind benefits.\nThis model allows us to obtain semi-explicit solutions for the insurance\ncontract and is applied to the context of fuel poverty. For this application,\nour model allows to conclude that, even in the least efficient scenario from\nthe households point of view, i.e., when the insurance is provided by a\nmonopoly, this mechanism improves significantly the living conditions of the\nriskiest households by ensuring them a sufficient consumption of energy.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.02099v2", "link": "http://arxiv.org/abs/2001.02099v2"}, {"title": "Martingales, the Efficient Market Hypothesis, and Spurious Stylized\n  Facts", "summary": "The condition for stationary increments, not scaling, detemines long time\npair autocorrelations. An incorrect assumption of stationary increments\ngenerates spurious stylized facts, fat tails and a Hurst exponent H_s=1/2, when\nthe increments are nonstationary, as they are in FX markets. The\nnonstationarity arises from systematic uneveness in noise traders' behavior.\nSpurious results arise mathematically from using a log increment with a\n'sliding window'. We explain why a hard to beat market demands martingale\ndynamics , and martingales with nonlinear variance generate nonstationary\nincrements. The nonstationarity is exhibited directly for Euro/Dollar FX data.\nWe observe that the Hurst exponent H_s generated by the using the sliding\nwindow technique on a time series plays the same role as does Mandelbrot's\nJoseph exponent. Finally, Mandelbrot originally assumed that the 'badly\nbehaved' second moment of cotton returns is due to fat tails, but that\nnonconvergent behavior is instead direct evidence for nonstationary increments.\nSummarizing, the evidence for scaling and fat tails as the basis for\neconophysics and financial economics is provided neither by FX markets nor by\ncotton price data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0710.2583v1", "link": "http://arxiv.org/abs/0710.2583v1"}, {"title": "Market Price of Trading Liquidity Risk and Market Depth", "summary": "Price impact of a trade is an important element in pre-trade and post-trade\nanalyses. We introduce a framework to analyze the market price of liquidity\nrisk, which allows us to derive an inhomogeneous Bernoulli ordinary\ndifferential equation. We obtain two closed form solutions, one of which\nreproduces the linear function of the order flow in Kyle (1985) for informed\ntraders. However, when traders are not as asymmetrically informed, an S-shape\nfunction of the order flow is obtained. We perform an empirical intra-day\nanalysis on Nikkei futures to quantify the price impact of order flow and\ncompare our results with industry's heuristic price impact functions. Our model\nof order flow yields a rich framework for not only to estimate the liquidity\nrisk parameters, but also to provide a plausible cause of why volatility and\ncorrelation are stochastic in nature. Finally, we find that the market depth\nencapsulates the market price of liquidity risk.", "category": ["q-fin.TR", "econ.EM", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1912.04565v1", "link": "http://dx.doi.org/10.1142/S0219024919500456"}, {"title": "Identification of cross and autocorrelations in time series within an\n  approach based on Wigner eigenspectrum of random matrices", "summary": "We present an original and novel method based on random matrix approach that\nenables to distinguish the respective role of temporal autocorrelations inside\ngiven time series and cross correlations between various time series. The\nproposed algorithm is based on properties of Wigner eigenspectrum of random\nmatrices instead of commonly used Wishart eigenspectrum methodology. The\nproposed approach is then qualitatively and quantitatively applied to financial\ndata in stocks building WIG (Warsaw Stock Exchange Index).", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1407.4702v1", "link": "http://arxiv.org/abs/1407.4702v1"}, {"title": "Irreversible Investment under L\u00e9vy Uncertainty: an Equation for the\n  Optimal Boundary", "summary": "We derive a new equation for the optimal investment boundary of a general\nirreversible investment problem under exponential L\\'evy uncertainty. The\nproblem is set as an infinite time-horizon, two-dimensional degenerate singular\nstochastic control problem. In line with the results recently obtained in a\ndiffusive setting, we show that the optimal boundary is intimately linked to\nthe unique optional solution of an appropriate Bank-El Karoui representation\nproblem. Such a relation and the Wiener Hopf factorization allow us to derive\nan integral equation for the optimal investment boundary. In case the\nunderlying L\\'evy process hits any real point with positive probability we show\nthat the integral equation for the investment boundary is uniquely satisfied by\nthe unique solution of another equation which is easier to handle. As a\nremarkable by-product we prove the continuity of the optimal investment\nboundary. The paper is concluded with explicit results for profit functions of\n(i) Cobb-Douglas type and (ii) CES type. In the first case the function is\nseparable and in the second case non-separable.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1411.2395v1", "link": "http://arxiv.org/abs/1411.2395v1"}, {"title": "Robustness of Delta hedging in a jump-diffusion model", "summary": "Suppose an investor aims at Delta hedging a European contingent claim\n$h(S(T))$ in a jump-diffusion model, but incorrectly specifies the stock\nprice's volatility and jump sensitivity, so that any hedging strategy is\ncalculated under a misspecified model. When does the erroneously computed\nstrategy super-replicate the true claim in an appropriate sense? If the\nmisspecified volatility and jump sensitivity dominate the true ones, we show\nthat following the misspecified Delta strategy does super-replicate $h(S(T))$\nin expectation among a wide collection of models. We also show that if a robust\npricing operator with a whole class of models is used, the corresponding hedge\nis dominating the contingent claim under each model in expectation. Our results\nrely on proving stochastic flow properties of the jump-diffusion and the\nconvexity of the value function. In the pure Poisson case, we establish that an\noverestimation of the jump sensitivity results in an almost sure one-sided\nhedge. Moreover, in general the misspecified price of the option dominates the\ntrue one if the volatility and the jump sensitivity are overestimated.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1910.08946v1", "link": "http://arxiv.org/abs/1910.08946v1"}, {"title": "Improving Regression-based Event Study Analysis Using a Topological\n  Machine-learning Method", "summary": "This paper introduces a new correction scheme to a conventional\nregression-based event study method: a topological machine-learning approach\nwith a self-organizing map (SOM).We use this new scheme to analyze a major\nmarket event in Japan and find that the factors of abnormal stock returns can\nbe easily can be easily identified and the event-cluster can be depicted.We\nalso find that a conventional event study method involves an empirical analysis\nmechanism that tends to derive bias due to its mechanism, typically in an\nevent-clustered market situation. We explain our new correction scheme and\napply it to an event in the Japanese market --- the holding disclosure of the\nGovernment Pension Investment Fund (GPIF) on July 31, 2015.", "category": ["econ.GN", "q-fin.EC", "q-fin.ST"], "id": "http://arxiv.org/abs/1905.06536v1", "link": "http://arxiv.org/abs/1905.06536v1"}, {"title": "Numerical methods for the quadratic hedging problem in Markov models\n  with jumps", "summary": "We develop algorithms for the numerical computation of the quadratic hedging\nstrategy in incomplete markets modeled by pure jump Markov process. Using the\nHamilton-Jacobi-Bellman approach, the value function of the quadratic hedging\nproblem can be related to a triangular system of parabolic partial\nintegro-differential equations (PIDE), which can be shown to possess unique\nsmooth solutions in our setting. The first equation is non-linear, but does not\ndepend on the pay-off of the option to hedge (the pure investment problem),\nwhile the other two equations are linear. We propose convergent finite\ndifference schemes for the numerical solution of these PIDEs and illustrate our\nresults with an application to electricity markets, where time-inhomogeneous\npure jump Markov processes appear in a natural manner.", "category": ["q-fin.RM", "q-fin.PR"], "id": "http://arxiv.org/abs/1206.5393v3", "link": "http://arxiv.org/abs/1206.5393v3"}, {"title": "A New Nonparametric Estimate of the Risk-Neutral Density with\n  Applications to Variance Swaps", "summary": "We develop a new nonparametric approach for estimating the risk-neutral\ndensity of asset prices and reformulate its estimation into a\ndouble-constrained optimization problem. We evaluate our approach using the\nS\\&P 500 market option prices from 1996 to 2015. A comprehensive\ncross-validation study shows that our approach outperforms the existing\nnonparametric quartic B-spline and cubic spline methods, as well as the\nparametric method based on the Normal Inverse Gaussian distribution. As an\napplication, we use the proposed density estimator to price long-term variance\nswaps, and the model-implied prices match reasonably well with those of the\nvariance future downloaded from the CBOE website.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1808.05289v2", "link": "http://arxiv.org/abs/1808.05289v2"}, {"title": "Warehousing Credit (CVA) Risk, Capital (KVA) and Tax (TVA) Consequences", "summary": "Credit risk may be warehoused by choice, or because of limited hedging\npossibilities. Credit risk warehousing increases capital requirements and\nleaves open risk. Open risk must be priced in the physical measure, rather than\nthe risk neutral measure, and implies profits and losses. Furthermore the rate\nof return on capital that shareholders require must be paid from profits.\nProfits are taxable and losses provide tax credits. Here we extend the\nsemi-replication approach of Burgard and Kjaer (2013) and the capital formalism\n(KVA) of Green, Kenyon, and Dennis (2014) to cover credit risk warehousing and\ntax, formalized as double-semi-replication and TVA (Tax Valuation Adjustment)\nto enable quantification.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1407.3201v3", "link": "http://arxiv.org/abs/1407.3201v3"}, {"title": "Elusive Longer-Run Impacts of Head Start: Replications Within and Across\n  Cohorts", "summary": "Using an additional decade of CNLSY data, this study replicated and extended\nDeming's (2009) evaluation of Head Start's life-cycle skill formation impacts\nin three ways. Extending the measurement interval for Deming's adulthood\noutcomes, we found no statistically significant impacts on earnings and mixed\nevidence of impacts on other adult outcomes. Applying Deming's sibling\ncomparison framework to more recent birth cohorts born to CNLSY mothers\nrevealed mostly negative Head Start impacts. Combining all cohorts shows\ngenerally null impacts on school-age and early adulthood outcomes.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1903.01954v4", "link": "http://arxiv.org/abs/1903.01954v4"}, {"title": "Revisiting a Theorem of L.A. Shepp on Optimal Stopping", "summary": "Using a bondholder who seeks to determine when to sell his bond as our\nmotivating example, we revisit one of Larry Shepp's classical theorems on\noptimal stopping. We offer a novel proof of Theorem 1 from from \\cite{Shepp}.\nOur approach is that of guessing the optimal control function and proving its\noptimality with martingales. Without martingale theory one could hardly prove\nour guess to be correct.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1605.00762v1", "link": "http://arxiv.org/abs/1605.00762v1"}, {"title": "Optimal Trading with Linear and (small) Non-Linear Costs", "summary": "We reconsider the problem of optimal trading in the presence of linear and\nquadratic costs, for arbitrary linear costs but in the limit where quadratic\ncosts are small. Using matched asymptotic expansion techniques, we find that\nthe trading speed vanishes inside a band that is narrower than in the absence\nof quadratic costs, by an amount that scales as the one-third power of\nquadratic costs. Outside of the band, we find three regimes: a small boundary\nlayer where the velocity vanishes linearly with the distance to the band, an\nintermediate region where the velocity behaves as a square-root of that\ndistance, and a far region where it becomes linear. Our solution is consistent\nwith available numerical results. We determine the conditions in which our\nexpansion is useful in practical applications, and generalize our solution to\nother forms of non-linear costs.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1511.07359v3", "link": "http://arxiv.org/abs/1511.07359v3"}, {"title": "Consistent Re-Calibration of the Discrete-Time Multifactor Vasi\u010dek\n  Model", "summary": "The discrete-time multifactor Vasi\\v{c}ek model is a tractable Gaussian spot\nrate model. Typically, two- or three-factor versions allow one to capture the\ndependence structure between yields with different times to maturity in an\nappropriate way. In practice, re-calibration of the model to the prevailing\nmarket conditions leads to model parameters that change over time. Therefore,\nthe model parameters should be understood as being time-dependent or even\nstochastic. Following the consistent re-calibration (CRC) approach, we\nconstruct models as concatenations of yield curve increments of Hull-White\nextended multifactor Vasi\\v{c}ek models with different parameters. The CRC\napproach provides attractive tractable models that preserve the no-arbitrage\npremise. As a numerical example, we fit Swiss interest rates using CRC\nmultifactor Vasi\\v{c}ek models.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1512.06454v2", "link": "http://dx.doi.org/10.3390/risks4030018"}, {"title": "Single Curve Collapse of the Price Impact Function for the New York\n  Stock Exchange", "summary": "We study the average price impact of a single trade executed in the NYSE.\nAfter appropriate averaging and rescaling, the data for the 1000 most highly\ncapitalized stocks collapse onto a single function, giving average price shift\nas a function of trade size. This function increases as a power that is the\norder of 1/2 for small volumes, but then increases more slowly for large\nvolumes. We obtain similar results in each year from the period 1995 - 1998. We\nalso find that small volume liquidity scales as a power of the stock\ncapitalization.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0207428v1", "link": "http://arxiv.org/abs/cond-mat/0207428v1"}, {"title": "Estimation of the Parameters of Symmetric Stable ARMA and ARMA-GARCH\n  Models", "summary": "In this article, we first propose the modified Hannan-Rissanen Method for\nestimating the parameters of the autoregressive moving average (ARMA) process\nwith symmetric stable noise and symmetric stable generalized autoregressive\nconditional heteroskedastic (GARCH) noise. Next, we propose the modified\nempirical characteristic function method for the estimation of GARCH parameters\nwith symmetric stable noise. Further, we show the efficiency, accuracy, and\nsimplicity of our methods through Monte-Carlo simulation. Finally, we apply our\nproposed methods to model financial data.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1911.09985v1", "link": "http://arxiv.org/abs/1911.09985v1"}, {"title": "Statistical Methods for Estimating the non-random Content of Financial\n  Markets", "summary": "For the pedestrian observer, financial markets look completely random with\nerratic and uncontrollable behavior. To a large extend, this is correct. At\nfirst approximation the difference between real price changes and the random\nwalk model is too small to be detected using traditional time series analysis.\nHowever, we show in the following that this difference between real financial\ntime series and random walks, as small as it is, is detectable using modern\nstatistical multivariate analysis, with several triggers encoded in trading\nsystems. This kind of analysis are based on methods widely used in nuclear\nphysics, with large samples of data and advanced statistical inference.\nConsidering the movements of the Euro future contract at high frequency, we\nshow that a part of the non-random content of this series can be inferred,\nnamely the trend-following content depending on volatility ranges.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1108.2937v2", "link": "http://arxiv.org/abs/1108.2937v2"}, {"title": "Bi-Demographic Changes and Current Account using SVAR Modeling", "summary": "The paper aims to explore the impacts of bi-demographic structure on the\ncurrent account and growth. Using a SVAR modeling, we track the dynamic impacts\nbetween these underlying variables. New insights have been developed about the\ndynamic interrelation between population growth, current account and economic\ngrowth. The long-run net impact on economic growth of the domestic working\npopulation growth and demand labor for emigrants is positive, due to the\npredominant contribution of skilled emigrant workers. Besides, the positive\nlong-run contribution of emigrant workers to the current account growth largely\ncompensates the negative contribution from the native population, because of\nthe predominance of skilled compared to unskilled workforce. We find that a\npositive shock in demand labor for emigrant workers leads to an increasing\neffect on native active age ratio. Thus, the emigrants appear to be more\ncomplements than substitutes for native workers.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.11161v4", "link": "http://arxiv.org/abs/1803.11161v4"}, {"title": "Designing an Optimal Portfolio for Iran's Stock Market with Genetic\n  Algorithm using Neural Network Prediction of Risk and Return Stocks", "summary": "Optimal capital allocation between different assets is an important financial\nproblem, which is generally framed as the portfolio optimization problem.\nGeneral models include the single-period and multi-period cases. The\ntraditional Mean-Variance model introduced by Harry Markowitz has been the\nbasis of many models used to solve the portfolio optimization problem. The\noverall goal is to achieve the highest return and lowest risk in portfolio\noptimization problems. In this paper, we will present an optimal portfolio\nbased the Markowitz Mean-Variance-Skewness with weight constraints model for\nshort-term investment opportunities in Iran's stock market. We will use a\nneural network based predictor to predict the stock returns and measure the\nrisk of stocks based on the prediction errors in the neural network. We will\nperform a series of experiments on our portfolio optimization model with the\nreal data from Iran's stock market indices including Bank, Insurance,\nInvestment, Petroleum Products and Chemicals indices. Finally, 8 different\nportfolios with low, medium and high risks for different type of investors\n(risk-averse or risk taker) using genetic algorithm will be designed and\nanalyzed.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1903.06632v1", "link": "http://arxiv.org/abs/1903.06632v1"}, {"title": "Optimal consumption and investment in incomplete markets with general\n  constraints", "summary": "We study an optimal consumption and investment problem in a possibly\nincomplete market with general, not necessarily convex, stochastic constraints.\nWe give explicit solutions for investors with exponential, logarithmic and\npower utility. Our approach is based on martingale methods which rely on recent\nresults on the existence and uniqueness of solutions to BSDEs with drivers of\nquadratic growth.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1010.0080v2", "link": "http://arxiv.org/abs/1010.0080v2"}, {"title": "Agent-based modeling of a price information trading business", "summary": "We describe an agent-based simulation of a fictional (but feasible)\ninformation trading business. The Gas Price Information Trader (GPIT) buys\ninformation about real-time gas prices in a metropolitan area from drivers and\nresells the information to drivers who need to refuel their vehicles.\n  Our simulation uses real world geographic data, lifestyle-dependent driving\npatterns and vehicle models to create an agent-based model of the drivers. We\nuse real world statistics of gas price fluctuation to create scenarios of\ntemporal and spatial distribution of gas prices. The price of the information\nis determined on a case-by-case basis through a simple negotiation model. The\ntrader and the customers are adapting their negotiation strategies based on\ntheir historical profits.\n  We are interested in the general properties of the emerging information\nmarket: the amount of realizable profit and its distribution between the trader\nand customers, the business strategies necessary to keep the market operational\n(such as promotional deals), the price elasticity of demand and the impact of\npricing strategies on the profit.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1303.7445v1", "link": "http://arxiv.org/abs/1303.7445v1"}, {"title": "A Simple Measure of Economic Complexity", "summary": "Researchers developed the Economic Complexity Index (ECI) as a measure of the\noverall sophistication of a country's products. They argued that this measure\nexplains economic growth better than the conventional variables such as human\ncapital. This paper suggests a simpler measure of production complexity, the\nlogarithm of product diversification, which has a natural foundation in\ninformation theory: it measures the information needed to encode the knowledge\nrequired to make a country's products. This measure explains well the income\ndifferences between countries. It has a basic link with ECI that is strongly\nsupported by the data.", "category": ["q-fin.ST", "q-fin.EC"], "id": "http://arxiv.org/abs/1601.05012v3", "link": "http://arxiv.org/abs/1601.05012v3"}, {"title": "Religion and Terrorism: Evidence from Ramadan Fasting", "summary": "We study the effect of religion and intense religious experiences on\nterrorism by focusing on one of the five pillars of Islam: Ramadan fasting. For\nidentification, we exploit two facts: First, daily fasting from dawn to sunset\nduring Ramadan is considered mandatory for most Muslims. Second, the Islamic\ncalendar is not synchronized with the solar cycle. We find a robust negative\neffect of more intense Ramadan fasting on terrorist events within districts and\ncountry-years in predominantly Muslim countries. This effect seems to operate\npartly through decreases in public support for terrorism and the operational\ncapabilities of terrorist groups.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.09869v3", "link": "http://arxiv.org/abs/1810.09869v3"}, {"title": "Neural networks for stock price prediction", "summary": "Due to the extremely volatile nature of financial markets, it is commonly\naccepted that stock price prediction is a task full of challenge. However in\norder to make profits or understand the essence of equity market, numerous\nmarket participants or researchers try to forecast stock price using various\nstatistical, econometric or even neural network models. In this work, we survey\nand compare the predictive power of five neural network models, namely, back\npropagation (BP) neural network, radial basis function (RBF) neural network,\ngeneral regression neural network (GRNN), support vector machine regression\n(SVMR), least squares support vector machine regresssion (LS-SVMR). We apply\nthe five models to make price prediction of three individual stocks, namely,\nBank of China, Vanke A and Kweichou Moutai. Adopting mean square error and\naverage absolute percentage error as criteria, we find BP neural network\nconsistently and robustly outperforms the other four models.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1805.11317v1", "link": "http://arxiv.org/abs/1805.11317v1"}, {"title": "Signal and Noise in Financial Correlation Matrices", "summary": "Using Random Matrix Theory one can derive exact relations between the\neigenvalue spectrum of the covariance matrix and the eigenvalue spectrum of its\nestimator (experimentally measured correlation matrix). These relations will be\nused to analyze a particular case of the correlations in financial series and\nto show that contrary to earlier claims, correlations can be measured also in\nthe ``random'' part of the spectrum. Implications for the portfolio\noptimization are briefly discussed.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0312496v2", "link": "http://dx.doi.org/10.1016/j.physa.2004.06.089"}, {"title": "Power Utility Maximization in Constrained Exponential L\u00e9vy Models", "summary": "We study power utility maximization for exponential L\\'evy models with\nportfolio constraints, where utility is obtained from consumption and/or\nterminal wealth. For convex constraints, an explicit solution in terms of the\nL\\'evy triplet is constructed under minimal assumptions by solving the Bellman\nequation. We use a novel transformation of the model to avoid technical\nconditions. The consequences for q-optimal martingale measures are discussed as\nwell as extensions to non-convex constraints.", "category": ["q-fin.PM", "q-fin.PR"], "id": "http://arxiv.org/abs/0912.1885v2", "link": "http://dx.doi.org/10.1111/j.1467-9965.2011.00480.x"}, {"title": "An Efficient, Distributable, Risk Neutral Framework for CVA Calculation", "summary": "The importance of counterparty credit risk to the derivative contracts was\ndemonstrated consistently throughout the financial crisis of 2008. Accurate\nvaluation of Credit value adjustment (CVA) is essential to reflect the economic\nvalues of these risks. In the present article, we reviewed several different\napproaches for calculating CVA, and compared the advantage and disadvantage for\neach method. We also introduced an more efficient and scalable computational\nframework for this calculation.", "category": ["q-fin.CP", "q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1010.1689v1", "link": "http://arxiv.org/abs/1010.1689v1"}, {"title": "World Input-Output Network", "summary": "Economic systems, traditionally analyzed as almost independent national\nsystems, are increasingly connected on a global scale. Only recently becoming\navailable, the World Input-Output Database (WIOD) is one of the first efforts\nto construct the multi-regional input-output (MRIO) tables at the global level.\nBy viewing the world input-output system as an interdependent network where the\nnodes are the individual industries in different economies and the edges are\nthe monetary goods flows between industries, we study the network properties of\nthe so-called world input-output network (WION) and document its evolution over\ntime. We are able to quantify not only some global network properties such as\nassortativity, clustering coefficient, and degree and strength distributions,\nbut also its subgraph structure and dynamics by using community detection\ntechniques. Over time, we detect a marked increase in cross-country\nconnectivity of the production system, only temporarily interrupted by the\n2008-2009 crisis. Moreover, we find a growing input-output regional community\nin Europe led by Germany and the rise of China in the global production system.\nFinally, we use the network-based PageRank centrality and community coreness\nmeasure to identify the key industries and economies in the WION and the\nresults are different from the one obtained by the traditional\nfinal-demand-weighted backward linkage measure.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1407.0225v1", "link": "http://dx.doi.org/10.1371/journal.pone.0134025"}, {"title": "Reconstruction of Interbank Network using Ridge Entropy Maximization\n  Model", "summary": "We develop a network reconstruction model based on the entropy maximization\nconsidering the sparsity of network. Here the reconstruction is to estimate\nnetwork's adjacency matrix from node's local information. We reconstruct the\ninterbank network in Japan from financial data in balance sheets of individual\nbanks using the developed reconstruction model in the period from 2000 to 2016.\nThe sparsity of the interbank network is successfully reproduced in the\nreconstructed network. We examine the accuracy of the reconstructed interbank\nnetwork by comparing the actual data and analyze the characteristics of the\ninterbank network. The comparison confirms that the accuracy of the\nreconstruction model is acceptably good. For the reconstructed interbank\nnetwork, we obtain the following characteristics which are consistent with the\npreviously known stylized facts: the short path length, the small clustering\ncoefficient, the disassortative property, and the core and peripheral\nstructure. Community analysis shows that the number of communities is 2-3 in\nthe normal period, 1 in the economic crisis (2003, 2008-2013). The major nodes\nin each community have been the major commercial banks. Since 2013, the major\ncommercial banks have lost the average PageRank and the leading regional banks\nhave obtained both the average degree and the average PageRank. The observed\nchanging role of banks is considered as a result of the quantitative and\nqualitative easing monetary policy started by Bank of Japan in April of 2013.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.04097v1", "link": "http://arxiv.org/abs/2001.04097v1"}, {"title": "Transforming public pensions: A mixed scheme with a credit granted by\n  the state", "summary": "Birth rates have dramatically decreased and, with continuous improvements in\nlife expectancy, pension expenditure is on an irreversibly increasing path.\nThis will raise serious concerns for the sustainability of the public pension\nsystems usually financed on a pay-as-you-go (PAYG) basis where current\ncontributions cover current pension expenditure. With this in mind, the aim of\nthis paper is to propose a mixed pension system that consists of a combination\nof a classical PAYG scheme and an increase of the contribution rate invested in\na funding scheme. The investment of the funding part is designed so that the\nPAYG pension system is financially sustainable at a particular level of\nprobability and at the same time provide some gains to individuals. In this\nsense, we make the individuals be an active part to face the demographic risks\ninherent in the PAYG and re-establish its financial sustainability.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1912.12329v1", "link": "http://arxiv.org/abs/1912.12329v1"}, {"title": "Review of the Plan for Integrating Big Data Analytics Program for the\n  Electronic Marketing System and Customer Relationship Management: A Case\n  Study XYZ Institution", "summary": "This research aims to explore business processes and what the factors have\nmajor influence on electronic marketing and CRM systems? Which data needs to be\nanalyzed and integrated in the system, and how to do that? How effective of\nintegration the electronic marketing and CRM with big data enabled to support\nMarketing and Customer Relation operations. Research based on case studies at\nXYZ Organization: International Language Education Service in Surabaya.\nResearch is studying secondary data which is supported by qualitative research\nmethods. Using purposive sampling technique with observation and interviewing\nseveral respondents who need the system integration. The documentation of\ninterview is coded to keep confidentiality of the informant. Method of\nextending participation, triangulation of data sources, discussions and the\nadequacy of the theory are uses to validate data. Miles and Huberman models is\nuses to do analysis the data interview. Results of the research are expected to\nbecome a holistic approach to fully integrate the Big Data Analytics program\nwith electronic marketing and CRM systems.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.02430v1", "link": "http://dx.doi.org/10.5281/zenodo.3361854"}, {"title": "The digital traces of bubbles: feedback cycles between socio-economic\n  signals in the Bitcoin economy", "summary": "What is the role of social interactions in the creation of price bubbles?\nAnswering this question requires obtaining collective behavioural traces\ngenerated by the activity of a large number of actors. Digital currencies offer\na unique possibility to measure socio-economic signals from such digital\ntraces. Here, we focus on Bitcoin, the most popular cryptocurrency. Bitcoin has\nexperienced periods of rapid increase in exchange rates (price) followed by\nsharp decline; we hypothesise that these fluctuations are largely driven by the\ninterplay between different social phenomena. We thus quantify four\nsocio-economic signals about Bitcoin from large data sets: price on on-line\nexchanges, volume of word-of-mouth communication in on-line social media,\nvolume of information search, and user base growth. By using vector\nautoregression, we identify two positive feedback loops that lead to price\nbubbles in the absence of exogenous stimuli: one driven by word of mouth, and\nthe other by new Bitcoin adopters. We also observe that spikes in information\nsearch, presumably linked to external events, precede drastic price declines.\nUnderstanding the interplay between the socio-economic signals we measured can\nlead to applications beyond cryptocurrencies to other phenomena which leave\ndigital footprints, such as on-line social network usage.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1408.1494v1", "link": "http://dx.doi.org/10.1098/?rsif.2014.0623"}, {"title": "Fractal approach towards power-law coherency to measure\n  cross-correlations between time series", "summary": "We focus on power-law coherency as an alternative approach towards studying\npower-law cross-correlations between simultaneously recorded time series. To be\nable to study empirical data, we introduce three estimators of the power-law\ncoherency parameter $H_{\\rho}$ based on popular techniques usually utilized for\nstudying power-law cross-correlations -- detrended cross-correlation analysis\n(DCCA), detrending moving-average cross-correlation analysis (DMCA) and height\ncross-correlation analysis (HXA). In the finite sample properties study, we\nfocus on the bias, variance and mean squared error of the estimators. We find\nthat the DMCA-based method is the safest choice among the three. The HXA method\nis reasonable for long time series with at least $10^4$ observations, which can\nbe easily attainable in some disciplines but problematic in others. The\nDCCA-based method does not provide favorable properties which even deteriorate\nwith an increasing time series length. The paper opens a new venue towards\nstudying cross-correlations between time series.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1608.06781v2", "link": "http://dx.doi.org/10.1016/j.cnsns.2017.02.018"}, {"title": "Analysis of intra-day fluctuations in the Mexican financial market index", "summary": "In this paper, a statistical analysis of high frequency fluctuations of the\nIPC, the Mexican Stock Market Index, is presented. A sample of tick-to-tick\ndata covering the period from January 1999 to December 2002 was analyzed, as\nwell as several other sets obtained using temporal aggregation. Our results\nindicates that the highest frequency is not useful to understand the Mexican\nmarket because almost two thirds of the information corresponds to inactivity.\nFor the frequency where fluctuations start to be relevant, the IPC data does\nnot follows any alpha-stable distribution, including the Gaussian, perhaps\nbecause of the presence of autocorrelations. For a long range of\nlower-frequencies, but still in the intra-day regime, fluctuations can be\ndescribed as a truncated L\\'evy flight, while for frequencies above two-days, a\nGaussian distribution yields the best fit. Thought these results are consistent\nwith other previously reported for several markets, there are significant\ndifferences in the details of the corresponding descriptions.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2002.05697v1", "link": "http://arxiv.org/abs/2002.05697v1"}, {"title": "Tail protection for long investors: Trend convexity at work", "summary": "The performance of trend following strategies can be ascribed to the\ndifference between long-term and short-term realized variance. We revisit this\ngeneral result and show that it holds for various definitions of trend\nstrategies. This explains the positive convexity of the aggregate performance\nof Commodity Trading Advisors (CTAs) which -- when adequately measured -- turns\nout to be much stronger than anticipated. We also highlight interesting\nconnections with so-called Risk Parity portfolios. Finally, we propose a new\nportfolio of strangle options that provides a pure exposure to the long-term\nvariance of the underlying, offering yet another viewpoint on the link between\ntrend and volatility.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1607.02410v1", "link": "http://arxiv.org/abs/1607.02410v1"}, {"title": "Remarks on stochastic automatic adjoint differentiation and financial\n  models calibration", "summary": "In this work, we discuss the Automatic Adjoint Differentiation (AAD) for\nfunctions of the form $G=\\frac{1}{2}\\sum_1^m (Ey_i-C_i)^2$, which often appear\nin the calibration of stochastic models. { We demonstrate that it allows a\nperfect SIMD\\footnote{Single Input Multiple Data} parallelization and provide\nits relative computational cost. In addition we demonstrate that this\ntheoretical result is in concordance with numeric experiments.}", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1901.04200v2", "link": "http://arxiv.org/abs/1901.04200v2"}, {"title": "Hedging and machine learning driven crude oil data analysis using a\n  refined Barndorff-Nielsen and Shephard model", "summary": "In this paper, a refined Barndorff-Nielsen and Shephard (BN-S) model is\nimplemented to find an optimal hedging strategy for commodity markets. The\nrefinement of the BN-S model is obtained with various machine and deep learning\nalgorithms. The refinement leads to the extraction of a deterministic parameter\nfrom the empirical data set. The problem is transformed to an appropriate\nclassification problem with a couple of different approaches: the volatility\napproach and the duration approach. The analysis is implemented to the Bakken\ncrude oil data and the aforementioned deterministic parameter is obtained for a\nwide range of data sets. With the implementation of this parameter in the\nrefined model, the resulting model performs much better than the classical BN-S\nmodel.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/2004.14862v1", "link": "http://arxiv.org/abs/2004.14862v1"}, {"title": "Roles of discount rate, risk premium, and device performance in\n  estimating the cost of energy for photovoltaics", "summary": "We show that different rates should be used for borrowing and discount rates,\nand that the risk-free rate should be used for discounting when assessing and\ncomparing the cost of energy accross diffferent producers and technologies, on\nthe example of photovoltaics. Recent quantitative models using the same rate\nfor borrowing and discounting lead to an underestimation of the cost for risky\nborrowers and to distorted sensitivities of the cost to financial and\nnon-financial factors. Specifically, it is shown that they may lead to gross\nunderestimation of the importance of solar-to-electricity conversion\nefficiency. The importance of device efficiency is re-established under the\ntreatment of the discount rate proposed here. The effects on the cost of energy\nof the installation efficiency and degradation rate, on the discount rate and\nrisk premium as well as on the project lifetime are estimated.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1209.1903v1", "link": "http://dx.doi.org/10.3390/ijfs1030054"}, {"title": "Interest Rate Manipulation Detection using Time Series Clustering\n  Approach", "summary": "The Interbank Offered Rate is a vital benchmark interest rate in the\nfinancial markets of every country to which financial contracts are tied. In\nthe light of the recent LIBOR manipulation incident, this paper seeks to\naddress the fear that Interbank Offered Rate are entirely controlled by the\nbank. The paper will focus on the comparison between LIBOR and SIBOR especially\nwith regards to the behavior of the interest rate with time. Because of the\nnature of IBORs, banks will naturally be submitting similar rates which should\nnot differ excessively from the market as well as the other banks. We will\ncompare the LIBOR and SIBOR from 2005 to 2011 with respect to the 1 month rates\non an annual basis. We will present the result that the SIBOR is not\nmanipulated like LIBOR.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1208.2878v1", "link": "http://arxiv.org/abs/1208.2878v1"}, {"title": "Comparing the performance of FA, DFA and DMA using different synthetic\n  long-range correlated time series", "summary": "Notwithstanding the significant efforts to develop estimators of long-range\ncorrelations (LRC) and to compare their performance, no clear consensus exists\non what is the best method and under which conditions. In addition, synthetic\ntests suggest that the performance of LRC estimators varies when using\ndifferent generators of LRC time series. Here, we compare the performances of\nfour estimators [Fluctuation Analysis (FA), Detrended Fluctuation Analysis\n(DFA), Backward Detrending Moving Average (BDMA), and centred Detrending Moving\nAverage (CDMA)]. We use three different generators [Fractional Gaussian Noises,\nand two ways of generating Fractional Brownian Motions]. We find that CDMA has\nthe best performance and DFA is only slightly worse in some situations, while\nFA performs the worst. In addition, CDMA and DFA are less sensitive to the\nscaling range than FA. Hence, CDMA and DFA remain \"The Methods of Choice\" in\ndetermining the Hurst index of time series.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1208.4158v1", "link": "http://dx.doi.org/10.1038/srep00835"}, {"title": "Stock markets are not what we think they are: the key roles of\n  cross-ownership and corporate treasury stock", "summary": "We describe and document three mechanisms by which corporations can influence\nor even control stock prices. (i) Parent and holding companies wield control\nover other publicly traded companies. (ii) Through clever management of\ntreasury stock based on buyback programs and stock issuance, stock price\nfluctuations can be amplified or curbed. (iii) Finally, history shows a close\ninterdependance between the level of stock prices on the one hand and merger\nand acquisition activity on the other hand. This perspective in which Boards of\nDirectors of major companies shepherd the market offers a natural\ninterpretation of the so-called \"herd behavior\" observed in stock markets. The\ntraditional view holds that by driving profit expectations, corporations have\nan indirect role in shaping the market. In this paper, we suggest that over the\nlast decades they became more and more the direct moving force of stock\nmarkets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0406704v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.09.020"}, {"title": "Optimal consumption and sale strategies for a risk averse agent", "summary": "In this article we consider a special case of an optimal consumption/optimal\nportfolio problem first studied by Constantinides and Magill and by Davis and\nNorman, in which an agent with constant relative risk aversion seeks to\nmaximise expected discounted utility of consumption over the infinite horizon,\nin a model comprising a risk-free asset and a risky asset with proportional\ntransaction costs. The special case that we consider is that the cost of\npurchases of the risky asset is infinite, or equivalently the risky asset can\nonly be sold and not bought.\n  In this special setting new solution techniques are available, and we can\nmake considerable progress towards an analytical solution. This means we are\nable to consider the comparative statics of the problem. There are some\nsurprising conclusions, such as consumption rates are not monotone increasing\nin the return of the asset, nor are the certainty equivalent values of the\nrisky positions monotone in the risk aversion.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1409.3394v1", "link": "http://arxiv.org/abs/1409.3394v1"}, {"title": "Sequential Detection of Market shocks using Risk-averse Agent Based\n  Models", "summary": "This paper considers a statistical signal processing problem involving agent\nbased models of financial markets which at a micro-level are driven by socially\naware and risk- averse trading agents. These agents trade (buy or sell) stocks\nby exploiting information about the decisions of previous agents (social\nlearning) via an order book in addition to a private (noisy) signal they\nreceive on the value of the stock. We are interested in the following: (1)\nModelling the dynamics of these risk averse agents, (2) Sequential detection of\na market shock based on the behaviour of these agents. Structural results which\ncharacterize social learning under a risk measure, CVaR (Conditional\nValue-at-risk), are presented and formulation of the Bayesian change point\ndetection problem is provided. The structural results exhibit two interesting\nprop- erties: (i) Risk averse agents herd more often than risk neutral agents\n(ii) The stopping set in the sequential detection problem is non-convex. The\nframework is validated on data from the Yahoo! Tech Buzz game dataset.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1511.01965v1", "link": "http://arxiv.org/abs/1511.01965v1"}, {"title": "On the multifractal effects generated by monofractal signals", "summary": "We study quantitatively the level of false multifractal signal one may\nencounter while analyzing multifractal phenomena in time series within\nmultifractal detrended fluctuation analysis (MF-DFA). The investigated effect\nappears as a result of finite length of used data series and is additionally\namplified by the long-term memory the data eventually may contain. We provide\nthe detailed quantitative description of such apparent multifractal background\nsignal as a threshold in spread of generalized Hurst exponent values $\\Delta h$\nor a threshold in the width of multifractal spectrum $\\Delta \\alpha$ below\nwhich multifractal properties of the system are only apparent, i.e. do not\nexist, despite $\\Delta\\alpha\\neq0$ or $\\Delta h\\neq 0$. We find this effect\nquite important for shorter or persistent series and we argue it is linear with\nrespect to autocorrelation exponent $\\gamma$. Its strength decays according to\npower law with respect to the length of time series. The influence of basic\nlinear and nonlinear transformations applied to initial data in finite time\nseries with various level of long memory is also investigated. This provides\nadditional set of semi-analytical results. The obtained formulas are\nsignificant in any interdisciplinary application of multifractality, including\nphysics, financial data analysis or physiology, because they allow to separate\nthe 'true' multifractal phenomena from the apparent (artificial) multifractal\neffects. They should be a helpful tool of the first choice to decide whether we\ndo in particular case with the signal with real multiscaling properties or not.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1307.2014v2", "link": "http://dx.doi.org/10.1016/j.physa.2013.07.045"}, {"title": "Super-Replication of the Best Pairs Trade in Hindsight", "summary": "This paper derives a robust on-line equity trading algorithm that achieves\nthe greatest possible percentage of the final wealth of the best pairs\nrebalancing rule in hindsight. A pairs rebalancing rule chooses some pair of\nstocks in the market and then perpetually executes rebalancing trades so as to\nmaintain a target fraction of wealth in each of the two. After each discrete\nmarket fluctuation, a pairs rebalancing rule will sell a precise amount of the\noutperforming stock and put the proceeds into the underperforming stock. Under\ntypical conditions, in hindsight one can find pairs rebalancing rules that\nwould have spectacularly beaten the market. Our trading strategy, which extends\nOrdentlich and Cover's (1998) \"max-min universal portfolio,\" guarantees to\nachieve an acceptable percentage of the hindsight-optimized wealth, a\npercentage which tends to zero at a slow (polynomial) rate. This means that on\na long enough investment horizon, the trader can enforce a compound-annual\ngrowth rate that is arbitrarily close to that of the best pairs rebalancing\nrule in hindsight. The strategy will \"beat the market asymptotically\" if there\nturns out to exist a pairs rebalancing rule that grows capital at a higher\nasymptotic rate than the market index. The advantages of our algorithm over the\nOrdentlich and Cover (1998) strategy are twofold. First, their strategy is\nimpossible to compute in practice. Second, in considering the more modest\nbenchmark (instead of the best all-stock rebalancing rule in hindsight), we\nreduce the \"cost of universality\" and achieve a higher learning rate.", "category": ["q-fin.PM", "econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1810.02444v3", "link": "http://arxiv.org/abs/1810.02444v3"}, {"title": "Time Series Analysis and Forecasting of the US Housing Starts using\n  Econometric and Machine Learning Model", "summary": "In this research paper, I have performed time series analysis and forecasted\nthe monthly value of housing starts for the year 2019 using several econometric\nmethods - ARIMA(X), VARX, (G)ARCH and machine learning algorithms - artificial\nneural networks, ridge regression, K-Nearest Neighbors, and support vector\nregression, and created an ensemble model. The ensemble model stacks the\npredictions from various individual models, and gives a weighted average of all\npredictions. The analyses suggest that the ensemble model has performed the\nbest among all the models as the prediction errors are the lowest, while the\neconometric models have higher error rates.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.07848v1", "link": "http://arxiv.org/abs/1905.07848v1"}, {"title": "A Second Look at Post Crisis Pricing of Derivatives - Part I: A Note on\n  Money Accounts and Collateral", "summary": "The paper reviews origins of the approach to pricing derivatives post-crisis\nby following three papers that have received wide acceptance from practitioners\nas the theoretical foundations for it - [Piterbarg 2010], [Burgard and Kjaer\n2010] and [Burgard and Kjaer 2013].\n  The review reveals several conceptual and technical inconsistencies with the\napproaches taken in these papers. In particular, a key component of the\napproach - prescription of cost components to a risk-free money account,\ngenerates derivative prices that are not cleared by the markets that trade the\nderivative and its underlying securities. It also introduces several risk-free\npositions (accounts) that accrue at persistently non-zero spreads with respect\nto each other and the risk free rate. In the case of derivatives with\ncounterparty default risk [Burgard and Kjaer 2013] introduces an approach\nreferred to as semi-replication, which through the choice of cost components in\nthe money account results in derivative prices that carry arbitrage\nopportunities in the form of holding portfolio of counterparty's bonds versus a\nderivative position with it.\n  This paper derives no-arbitrage expressions for default-risky derivative\ncontracts with and without collateral, avoiding these inconsistencies.", "category": ["q-fin.PR", "q-fin.MF", "q-fin.RM", "q-fin.TR"], "id": "http://arxiv.org/abs/1806.09198v4", "link": "http://arxiv.org/abs/1806.09198v4"}, {"title": "Inflation securities valuation with macroeconomic-based no-arbitrage\n  dynamics", "summary": "We develop a model to price inflation and interest rates derivatives using\ncontinuous-time dynamics that have some links with macroeconomic monetary DSGE\nmodels equipped with a Taylor rule: in particular, the reaction function of the\ncentral bank, the bond market liquidity, inflation and growth expectations play\nan important role. The model can explain the effects of non-standard monetary\npolicies (like quantitative easing or its tapering) and shed light on how\ncentral bank policy can affect the value of inflation and interest rates\nderivatives.\n  The model is built under standard no-arbitrage assumptions. Interestingly,\nthe model yields short rate dynamics that are consistent with a time-varying\nHull-White model, therefore making the calibration to the nominal interest\ncurve and options straightforward. Further, we obtain closed forms for both\nzero-coupon and year-on-year inflation swap and options. The calibration\nstrategy we propose is fully separable, which means that the calibration can be\ncarried out in subsequent simple steps that do not require heavy computation. A\nmarket calibration example is provided.\n  The advantages of such structural inflation modelling become apparent when\none starts doing risk analysis on an inflation derivatives book: because the\nmodel explicitly takes into account economic variables, a trader can easily\nassess the impact of a change in central bank policy on a complex book of fixed\nincome instruments, which is normally not straightforward if one is using\nstandard inflation pricing models.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1403.7799v3", "link": "http://arxiv.org/abs/1403.7799v3"}, {"title": "Numerical stability of a hybrid method for pricing options", "summary": "We develop and study stability properties of a hybrid approximation of\nfunctionals of the Bates jump model with stochastic interest rate that uses a\ntree method in the direction of the volatility and the interest rate and a\nfinite-difference approach in order to handle the underlying asset price\nprocess. We also propose hybrid simulations for the model, following a binomial\ntree in the direction of both the volatility and the interest rate, and a\nspace-continuous approximation for the underlying asset price process coming\nfrom a Euler-Maruyama type scheme. We show that our methods allow to obtain\nefficient and accurate European and American option prices. Numerical\nexperiments are provided, and show the reliability and the efficiency of the\nalgorithms.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1603.07225v4", "link": "http://dx.doi.org/10.1142/S0219024919500365"}, {"title": "TPLVM: Portfolio Construction by Student's $t$-process Latent Variable\n  Model", "summary": "Optimal asset allocation is a key topic in modern finance theory. To realize\nthe optimal asset allocation on investor's risk aversion, various portfolio\nconstruction methods have been proposed. Recently, the applications of machine\nlearning are rapidly growing in the area of finance. In this article, we\npropose the Student's $t$-process latent variable model (TPLVM) to describe\nnon-Gaussian fluctuations of financial timeseries by lower dimensional latent\nvariables. Subsequently, we apply the TPLVM to minimum-variance portfolio as an\nalternative of existing nonlinear factor models. To test the performance of the\nproposed portfolio, we construct minimum-variance portfolios of global stock\nmarket indices based on the TPLVM or Gaussian process latent variable model. By\ncomparing these portfolios, we confirm the proposed portfolio outperforms that\nof the existing Gaussian process latent variable model.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/2002.06243v1", "link": "http://dx.doi.org/10.3390/math8030449"}, {"title": "Conditional Expectation as Quantile Derivative", "summary": "For a linear combination of random variables, fix some confidence level and\nconsider the quantile of the combination at this level. We are interested in\nthe partial derivatives of the quantile with respect to the weights of the\nrandom variables in the combination. It turns out that under suitable\nconditions on the joint distribution of the random variables the derivatives\nexist and coincide with the conditional expectations of the variables given\nthat their combination just equals the quantile. Moreover, using this result,\nwe deduce formulas for the derivatives with respect to the weights of the\nvariables for the so-called expected shortfall (first or higher moments) of the\ncombination. Finally, we study in some more detail the coherence properties of\nthe expected shortfall in case it is defined as a first conditional moment. Key\nwords: quantile; value-at-risk; quantile derivative; conditional expectation;\nexpected shortfall; conditional value-at-risk; coherent risk measure.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0104190v1", "link": "http://arxiv.org/abs/math/0104190v1"}, {"title": "Comparing Alternatives to Measure the Impact of DDoS Attack\n  Announcements on Target Stock Prices", "summary": "The attack intensity of distributed denial of service (DDoS) attacks is\nincreasing every year. Botnets based on internet of things (IOT) devices are\nnow being used to conduct DDoS attacks. The estimation of direct and indirect\neconomic damages caused by these attacks is a complex problem. One of the\nindirect damage of a DDoS attack can be on the market value of the victim firm.\nIn this article we analyze the impact of 45 different DDoS attack announcements\non victim's stock prices. We find that previous studies have a mixed conclusion\non the impact of DDoS attack announcements on the victim's stock price. Hence,\nin this article we evaluate this impact using three different approaches and\ncompare the results. In the first approach, we use the assume the cumulative\nabnormal returns to be normally distributed and test the hypothesis that a DDoS\nattack announcement has no impact on the victim's stock price. In the latter\ntwo methods, we do not assume a distribution and use the empirical distribution\nof cumulative abnormal returns to test the hypothesis. We find that the\nassumption of cumulative abnormal returns being normally distributed leads to\noverestimation/underestimation of the impact. Finally, we analyze the impact of\nDDoS attack announcement on victim's stock price in each of the 45 cases and\npresent our results.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1806.01781v1", "link": "http://dx.doi.org/10.22667/JOWUA.2017.12.31.001"}, {"title": "Hedging crop yields against weather uncertainties -- a weather\n  derivative perspective", "summary": "The effects of weather on agriculture in recent years have become a major\nglobal concern. Hence, the need for an effective weather risk management tool\n(i.e., weather derivatives) that can hedge crop yields against weather\nuncertainties. However, most smallholder farmers and agricultural stakeholders\nare unwilling to pay for the price of weather derivatives (WD) because of the\npresence of basis risks (product-design and geographical) in the pricing\nmodels. To eliminate product-design basis risks, a machine learning ensemble\ntechnique was used to determine the relationship between maize yield and\nweather variables. The results revealed that the most significant weather\nvariable that affected the yield of maize was average temperature. A\nmean-reverting model with a time-varying speed of mean reversion, seasonal\nmean, and local volatility that depended on the local average temperature was\nthen proposed. The model was extended to a multi-dimensional model for\ndifferent but correlated locations. Based on these average temperature models,\npricing models for futures, options on futures, and basket futures for\ncumulative average temperature and growing degree-days are presented. Pricing\nfutures on baskets reduces geographical basis risk, as buyers have the\nopportunity to select the most appropriate weather stations with their desired\nweight preference. With these pricing models, farmers and agricultural\nstakeholders can hedge their crops against the perils of extreme weather.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1905.07546v2", "link": "http://dx.doi.org/10.3390/mca24030071"}, {"title": "Co-movement of energy commodities revisited: Evidence from wavelet\n  coherence analysis", "summary": "In this paper, we contribute to the literature on energy market co-movement\nby studying its dynamics in the time-frequency domain. The novelty of our\napproach lies in the application of wavelet tools to commodity market data. A\nmajor part of economic time series analysis is done in the time or frequency\ndomain separately. Wavelet analysis combines these two fundamental approaches\nallowing study of the time series in the time- frequency domain. Using this\nframework, we propose a new, model-free way of estimating time-varying cor-\nrelations. In the empirical analysis, we connect our approach to the dynamic\nconditional correlation approach of Engle (2002) on the main components of the\nenergy sector. Namely, we use crude oil, gasoline, heating oil, and natural gas\non a nearest-future basis over a period of approximately 16 and 1/2 years\nbeginning on November 1, 1993 and ending on July 21, 2010. Using wavelet\ncoherence, we uncover interesting dynamics of correlations between energy\ncommodities in the time-frequency space.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1201.4776v1", "link": "http://dx.doi.org/10.1016/j.eneco.2011.10.007"}, {"title": "Q-Gaussian diffusion in stock markets", "summary": "We analyze the Standard & Poor's 500 stock market index from the last 22\nyears. The probability density function of price returns exhibits two\nwell-distinguished regimes with self-similar structure: the first one displays\nstrong super-diffusion together with short-time correlations, and the second\none corresponds to weak super-diffusion with weak time correlations. Both\nregimes are well-described by q-Gaussian distributions. The porous media\nequation is used to derive the governing equation for these regimes, and the\nBlack-Scholes diffusion coefficient is explicitly obtained from the governing\nequation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1902.10500v1", "link": "http://arxiv.org/abs/1902.10500v1"}, {"title": "Exotic derivatives under stochastic volatility models with jumps", "summary": "In equity and foreign exchange markets the risk-neutral dynamics of the\nunderlying asset are commonly represented by stochastic volatility models with\njumps. In this paper we consider a dense subclass of such models and develop\nanalytically tractable formulae for the prices of a range of first-generation\nexotic derivatives. We provide closed form formulae for the Fourier transforms\nof vanilla and forward starting option prices as well as a formula for the\nslope of the implied volatility smile for large strikes. A simple explicit\napproximation formula for the variance swap price is given. The prices of\nvolatility swaps and other volatility derivatives are given as a\none-dimensional integral of an explicit function. Analytically tractable\nformulae for the Laplace transform (in maturity) of the double-no-touch options\nand the Fourier-Laplace transform (in strike and maturity) of the double\nknock-out call and put options are obtained. The proof of the latter formulae\nis based on extended matrix Wiener-Hopf factorisation results. We also provide\nconvergence results.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0912.2595v2", "link": "http://arxiv.org/abs/0912.2595v2"}, {"title": "Black-Litterman model with intuitionistic fuzzy posterior return", "summary": "The main objective is to present a some variant of the Black - Litterman\nmodel. We consider the canonical case when priori return is determined by means\nsuch excess return from the CAPM market portfolio which is derived using\nreverse optimization method. Then the a priori return is at risk quantified\nuncertainty. On the side, intensive discussion shows that the experts' views\nare under knightian uncertainty. For this reason, we propose such variant of\nthe Black - Litterman model in which the experts' views are described as\nintuitionistic fuzzy number. The existence of posterior return is proved for\nthis case.We show that then posterior return is an intuitionistic fuzzy\nprobabilistic set.", "category": ["q-fin.MF", "q-fin.GN"], "id": "http://arxiv.org/abs/1601.00354v1", "link": "http://dx.doi.org/10.2139/ssrn.2010280"}, {"title": "Power Series Representations for European Option Prices under Stochastic\n  Volatility Models", "summary": "In the context of stochastic volatility models, we study representation\nformulas in terms of expectations for the power series' coefficients associated\nto the call price-function. As in a recent paper by Antonelli and Scarlatti the\nexpansion is done w.r.t. the correlation between the noises driving the\nunderlying asset price process and the volatility process. We first obtain\nexpressions for the power series' coefficients from the generalized Hull and\nWhite formula obtained by Elisa Al\\`os. Afterwards, we provide representations\nturning out from the approach for the sensitivity problem tackled by Malliavin\ncalculus techniques, and these allow to handle not only vanilla options.\nFinally, we show the numerical performance of the associated Monte Carlo\nestimators for several stochastic volatility models.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1105.0068v2", "link": "http://arxiv.org/abs/1105.0068v2"}, {"title": "Multiple risk factor dependence structures: Distributional properties", "summary": "We introduce a class of dependence structures, that we call the Multiple Risk\nFactor (MRF) dependence structures. On the one hand, the new constructions\nextend the popular CreditRisk+ approach, and as such they formally describe\ndefault risk portfolios exposed to an arbitrary number of fatal risk factors\nwith conditionally exponential and dependent hitting (or occurrence) times. On\nthe other hand, the MRF structures can be seen as an encompassing family of\nmultivariate probability distributions with univariate margins distributed\nPareto of the 2nd kind, and in this role they can be used to model insurance\nrisk portfolios of dependent and heavy tailed risk components.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1607.04739v1", "link": "http://arxiv.org/abs/1607.04739v1"}, {"title": "New Financial Research Program: General Option-Price Wave Modeling", "summary": "Recently, a novel adaptive wave model for financial option pricing has been\nproposed in the form of adaptive nonlinear Schr\\\"{o}dinger (NLS) equation\n[Ivancevic a], as a high-complexity alternative to the linear\nBlack-Scholes-Merton model [Black-Scholes-Merton]. Its quantum-mechanical basis\nhas been elaborated in [Ivancevic b]. Both the solitary and shock-wave\nsolutions of the nonlinear model, as well as its linear (periodic) quantum\nsimplification are shown to successfully fit the Black-Scholes data, and define\nthe financial Greeks. This initial wave model (called the Ivancevic option\npricing model) has been further extended in [Yan], by providing the new NLS\nsolutions in the form of rogue waves (one-rogon and two-rogon solutions). In\nthis letter, I propose a new financial research program, with a goal to develop\na general wave-type model for realistic option-pricing prediction and control.\n  Keywords: General option-price wave modeling, new financial research program", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1001.4151v1", "link": "http://arxiv.org/abs/1001.4151v1"}, {"title": "Long-term correlations and multifractal analysis of trading volumes for\n  Chinese stocks", "summary": "We investigate the temporal correlations and multifractal nature of trading\nvolume of 22 liquid stocks traded on the Shenzhen Stock Exchange in 2003. We\nfind that the trading volume exhibit size-dependent non-universal long memory\nand multifractal nature. No crossover in the power-law dependence of the\ndetrended fluctuation functions is observed. Our results show that the intraday\npattern in the trading volume has negligible impact on the long memory and\nmultifractality.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0904.1042v1", "link": "http://dx.doi.org/10.1016/j.phpro.2010.07.003"}, {"title": "The Impact of Birth Order on Behavior in Contact Team Sports: the\n  Evidence of Rugby Teams in Argentina", "summary": "Several studies have shown that birth order and the sex of siblings may have\nan influence on individual behavioral traits. In particular, it has been found\nthat second brothers (of older male siblings) tend to have more disciplinary\nproblems. If this is the case, this should also be shown in contact sports. To\nassess this hypothesis we use a data set from the South Rugby Union (URS) from\nBah\\'ia Blanca, Argentina, and information obtained by surveying more than four\nhundred players of that league. We find a statistically significant positive\nrelation between being a second-born male rugby player with an older male\nbrother and the number of yellow cards received.\n  \\textbf{Keywords:} Birth Order; Behavior; Contact Sports; Rugby.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.09421v1", "link": "http://arxiv.org/abs/2004.09421v1"}, {"title": "Time-changed CIR default intensities with two-sided mean-reverting jumps", "summary": "The present paper introduces a jump-diffusion extension of the classical\ndiffusion default intensity model by means of subordination in the sense of\nBochner. We start from the bi-variate process $(X,D)$ of a diffusion state\nvariable $X$ driving default intensity and a default indicator process $D$ and\ntime change it with a L\\'{e}vy subordinator ${\\mathcal{T}}$. We characterize\nthe time-changed process\n$(X^{\\phi}_t,D^{\\phi}_t)=(X({\\mathcal{T}}_t),D({\\mathcal{T}}_t))$ as a\nMarkovian--It\\^{o} semimartingale and show from the Doob--Meyer decomposition\nof $D^{\\phi}$ that the default time in the time-changed model has a\njump-diffusion or a pure jump intensity. When $X$ is a CIR diffusion with\nmean-reverting drift, the default intensity of the subordinate model (SubCIR)\nis a jump-diffusion or a pure jump process with mean-reverting jumps in both\ndirections that stays nonnegative. The SubCIR default intensity model is\nanalytically tractable by means of explicitly computed eigenfunction expansions\nof relevant semigroups, yielding closed-form pricing of credit-sensitive\nsecurities.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1403.5402v1", "link": "http://dx.doi.org/10.1214/13-AAP936"}, {"title": "Pricing European Options in Realistic Markets", "summary": "We investigate the relation between the fair price for European-style vanilla\noptions and the distribution of short-term returns on the underlying asset\nignoring transaction and other costs. We compute the risk-neutral probability\ndensity conditional on the total variance of the asset's returns when the\noption expires. If the asset's future price has finite expectation, the\noption's fair value satisfies a parabolic partial differential equation of the\nBlack-Scholes type in which the variance of the asset's returns rather than a\ntrading time is the evolution parameter. By immunizing the portfolio against\nlarge-scale price fluctuations of the asset, the valuation of options is\nextended to the realistic case\\cite{St99} of assets whose short-term returns\nhave finite variance but very large, or even infinite, higher moments. A\ndynamic Delta-hedged portfolio that is statically insured against exceptionally\nlarge fluctuations includes at least two different options on the asset. The\nfair value of an option in this case is determined by a universal drift\nfunction that is common to all options on the asset. This drift is interpreted\nas the premium for an investment exposed to risk due to exceptionally large\nvariations of the asset's price. It affects the option valuation like an\neffective cost-of-carry for the underlying in the Black-Scholes world would.\nThe derived pricing formula for options in realistic markets is arbitrage free\nby construction. A simple model with constant drift qualitatively reproduces\nthe often observed volatility -skew and -term structure.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/physics/0210025v1", "link": "http://arxiv.org/abs/physics/0210025v1"}, {"title": "Dynamic Clearing and Contagion in Financial Networks", "summary": "In this paper we will consider a generalized extension of the Eisenberg-Noe\nmodel of financial contagion to allow for time dynamics in both discrete and\ncontinuous time. Derivation and interpretation of the financial implications\nwill be provided. Emphasis will be placed on the continuous-time framework and\nits formulation as a differential equation driven by the operating cash flows.\nMathematical results on existence and uniqueness of firm wealths under the\ndiscrete and continuous-time models will be provided. Finally, the financial\nimplications of time dynamics will be considered. The focus will be on how the\ndynamic clearing solutions differ from those of the static Eisenberg-Noe model.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1801.02091v2", "link": "http://arxiv.org/abs/1801.02091v2"}, {"title": "An Adaptive Tabu Search Algorithm for Market Clearing Problem in Turkish\n  Day-Ahead Market", "summary": "In this study, we focus on the market clearing problem of Turkish day-ahead\nelectricity market. We propose a mathematical model by extending the variety of\nbid types for different price regions. The commercial solvers may not find any\nfeasible solution for the proposed problem in some instances within the given\ntime limits. Hence, we design an adaptive tabu search (ATS) algorithm to solve\nthe problem. ATS discretizes continuous search space arising from the flow\nvariables. Our method has adaptive radius and it achieves backtracking by a\ncommercial solver. Then, we compare the performance of ATS with a heuristic\ndecomposition method from the literature by using synthetic data sets. We\nevaluate the performances of the algorithms with respect to their solution\ntimes and surplus differences. ATS performs better in most of the sets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1809.10554v1", "link": "http://dx.doi.org/10.1109/EEM.2018.8469926"}, {"title": "On the Equilibrium Uniqueness in Cournot Competition with Demand\n  Uncertainty", "summary": "We revisit the linear Cournot model with uncertain demand that is studied in\nLagerl\\\"of (2006)* and provide sufficient conditions for equilibrium uniqueness\nthat complement the existing results. We show that if the distribution of the\ndemand intercept has the decreasing mean residual demand (DMRD) or the\nincreasing generalized failure rate (IGFR) property, then uniqueness of\nequilibrium is guaranteed. The DMRD condition implies log-concavity of the\nexpected profits per unit of output without additional assumptions on the\nexistence or the shape of the density of the demand intercept and, hence,\nanswers in the affirmative the conjecture of Lagerl\\\"of (2006) that such\nconditions may not be necessary. *Johan Lagerl\\\"of, Equilibrium uniqueness in a\nCournot model with demand uncertainty. The B.E. Journal in Theoretical\nEconomics, Vol. 6: Iss 1. (Topics), Article 19:1--6, 2006.", "category": [], "id": "http://arxiv.org/abs/1906.03558v2", "link": "http://dx.doi.org/10.1515/bejte-2019-0131"}, {"title": "An Empirical Study of the Behaviour of the Sample Kurtosis in Samples\n  from Symmetric Stable Distributions", "summary": "Kurtosis is seen as a measure of the discrepancy between the observed data\nand a Gaussian distribution and is defined when the 4th moment is finite. In\nthis work an empirical study is conducted to investigate the behaviour of the\nsample estimate of kurtosis with respect to sample size and the tail index when\napplied to heavy-tailed data where the 4th moment does not exist. The study\nwill focus on samples from the symmetric stable distributions. It was found\nthat the expected value of excess kurtosis divided by the sample size is finite\nfor any value of the tail index and the sample estimate of kurtosis increases\nas a linear function of sample size and tail index. It is very sensitive to\nchanges in the tail-index.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1811.00476v2", "link": "http://arxiv.org/abs/1811.00476v2"}, {"title": "Optical Proof of Work", "summary": "Most cryptocurrencies rely on Proof-of-Work (PoW) \"mining\" for resistance to\nSybil and double-spending attacks, as well as a mechanism for currency\nissuance. Hashcash PoW has successfully secured the Bitcoin network since its\ninception, however, as the network has expanded to take on additional value\nstorage and transaction volume, Bitcoin PoW's heavy reliance on electricity has\ncreated scalability issues, environmental concerns, and systemic risks. Mining\nefforts have concentrated in areas with low electricity costs, creating single\npoints of failure. Although PoW security properties rely on imposing a\ntrivially verifiable economic cost on miners, there is no fundamental reason\nfor it to consist primarily of electricity cost. The authors propose a novel\nPoW algorithm, Optical Proof of Work (oPoW), to eliminate energy as the primary\ncost of mining. Proposed algorithm imposes economic difficulty on the miners,\nhowever, the cost is concentrated in hardware (capital expense-CAPEX) rather\nthan electricity (operating expenses-OPEX). The oPoW scheme involves minimal\nmodifications to Hashcash-like PoW schemes, inheriting safety/security\nproperties from such schemes.\n  Rapid growth and improvement in silicon photonics over the last two decades\nhas led to the commercialization of silicon photonic co-processors (integrated\ncircuits that use photons instead of electrons to perform specialized computing\ntasks) for low-energy deep learning. oPoW is optimized for this technology such\nthat miners are incentivized to use specialized, energy-efficient photonics for\ncomputation. Beyond providing energy savings, oPoW has the potential to improve\nnetwork scalability, enable decentralized mining outside of low electricity\ncost areas, and democratize issuance. Due to the CAPEX dominance of mining\ncosts, oPoW hashrate will be significantly less sensitive to underlying coin\nprice declines.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1911.05193v2", "link": "http://arxiv.org/abs/1911.05193v2"}, {"title": "Optimal investment for participating insurance contracts under\n  VaR-Regulation", "summary": "This paper studies a Value-at-Risk (VaR)-regulated optimal portfolio problem\nof the equity holders of a participating life insurance contract. In a setting\nwith unhedgeable mortality risk and complete financial market, the optimal\nsolution is given explicitly for contracts with mortality risk using a\nmartingale approach for constrained non-concave optimization problems. We show\nthat regulatory VaR constraints for participating insurance contracts lead to\nmore prudent investment than in the case of no regulation. This result is\ncontrary to the situation where the insurer maximizes the utility of the total\nwealth of the company (without distinguishing between contributions of equity\nholders and policyholders), in which case a VaR constraint may induce the\ninsurer to take excessive risks leading to higher losses than in the case of no\nregulation. Compared to the unregulated problem, the VaR-constrained strategy\nleads to a higher expected utility for the policyholders, highlighting the\npotential usefulness of a VaR-regulation in the context of insurance. The\nprudent investment behavior is more significant if a VaR-type regulation is\nreplaced by a portfolio insurance (PI)-type regulation. Furthermore, a stricter\nregulation (a smaller allowed default probability in the VaR problem or a\nhigher minimum guarantee level in the PI problem) enhances the benefit of the\npolicyholder but deteriorates that of the insurer. For both types of\nregulation, the gains in terms of expected utility are greater for higher\nparticipation rates, while being smaller for higher bonus rates. We also extend\nour analysis to frameworks where dividend and premature death benefit payments\nare made at an intermediate time date.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1805.09068v4", "link": "http://dx.doi.org/10.1137/18M1217322"}, {"title": "Model-Independent Price Bounds for Catastrophic Mortality Bonds", "summary": "In this paper, we are concerned with the valuation of Catastrophic Mortality\nBonds and, in particular, we examine the case of the Swiss Re Mortality Bond\n2003 as a primary example of this class of assets. This bond was the first\nCatastrophic Mortality Bond to be launched in the market and encapsulates the\nbehaviour of a well-defined mortality index to generate payoffs for\nbondholders. Pricing this type of bonds is a challenging task and no closed\nform solution exists in the literature. In our approach, we adapt the payoff of\nsuch a bond in terms of the payoff of an Asian put option and present a new\napproach to derive model-independent bounds exploiting comonotonic theory as\nillustrated in \\cite{prime1} for the pricing of Asian options. We carry out\nMonte Carlo simulations to estimate the bond price and illustrate the strength\nof the bounds.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1607.07108v1", "link": "http://arxiv.org/abs/1607.07108v1"}, {"title": "Cross-Correlation Dynamics in Financial Time Series", "summary": "The dynamics of the equal-time cross-correlation matrix of multivariate\nfinancial time series is explored by examination of the eigenvalue spectrum\nover sliding time windows. Empirical results for the S&P 500 and the Dow Jones\nEuro Stoxx 50 indices reveal that the dynamics of the small eigenvalues of the\ncross-correlation matrix, over these time windows, oppose those of the largest\neigenvalue. This behaviour is shown to be independent of the size of the time\nwindow and the number of stocks examined.\n  A basic one-factor model is then proposed, which captures the main dynamical\nfeatures of the eigenvalue spectrum of the empirical data. Through the addition\nof perturbations to the one-factor model, (leading to a 'market plus sectors'\nmodel), additional sectoral features are added, resulting in an Inverse\nParticipation Ratio comparable to that found for empirical data. By\npartitioning the eigenvalue time series, we then show that negative index\nreturns, (drawdowns), are associated with periods where the largest eigenvalue\nis greatest, while positive index returns, (drawups), are associated with\nperiods where the largest eigenvalue is smallest. The study of correlation\ndynamics provides some insight on the collective behaviour of traders with\nvarying strategies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1002.0321v1", "link": "http://dx.doi.org/10.1016/j.physa.2008.10.047"}, {"title": "International Comparison of Labor Productivity Distribution for\n  Manufacturing and Non-Manufacturing Firms", "summary": "Labor productivity was studied at the microscopic level in terms of\ndistributions based on individual firm financial data from Japan and the US. A\npower-law distribution in terms of firms and sector productivity was found in\nboth countries' data. The labor productivities were not equal for nation and\nsectors, in contrast to the prevailing view in the field of economics. It was\nfound that the low productivity of the Japanese non-manufacturing sector\nreported in macro-economic studies was due to the low productivity of small\nfirms.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0812.0208v4", "link": "http://dx.doi.org/10.1143/PTPS.179.93"}, {"title": "Evaluating the Performance of Machine Learning Algorithms in Financial\n  Market Forecasting: A Comprehensive Survey", "summary": "With increasing competition and pace in the financial markets, robust\nforecasting methods are becoming more and more valuable to investors. While\nmachine learning algorithms offer a proven way of modeling non-linearities in\ntime series, their advantages against common stochastic models in the domain of\nfinancial market prediction are largely based on limited empirical results. The\nsame holds true for determining advantages of certain machine learning\narchitectures against others. This study surveys more than 150 related articles\non applying machine learning to financial market forecasting. Based on a\ncomprehensive literature review, we build a table across seven main parameters\ndescribing the experiments conducted in these studies. Through listing and\nclassifying different algorithms, we also introduce a simple, standardized\nsyntax for textually representing machine learning algorithms. Based on\nperformance metrics gathered from papers included in the survey, we further\nconduct rank analyses to assess the comparative performance of different\nalgorithm classes. Our analysis shows that machine learning algorithms tend to\noutperform most traditional stochastic methods in financial market forecasting.\nWe further find evidence that, on average, recurrent neural networks outperform\nfeed forward neural networks as well as support vector machines which implies\nthe existence of exploitable temporal dependencies in financial time series\nacross multiple asset classes and geographies.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1906.07786v2", "link": "http://arxiv.org/abs/1906.07786v2"}, {"title": "Econophysics of precious stones", "summary": "The importance of the power law has been well realized in econophysics over\nthe last decade. For instance, the distribution of the rate of stock price\nvariation and of personal assets show the power law. While these results reveal\nthe striking scale invariance of financial markets, the behaviour of price in\nreal economy is less known in spite of its extreme importance. As an example of\nmarkets in real economy, here we take up the price of precious stones which\nincreases with size while the amount of their production rapidly decreases with\nsize. We show for the first time that the price of natural precious stones\n(quartz crystal ball, gemstones such as diamond, emerald, and sapphire) as a\nfunction of weight obeys the power law. This indicates that the price is\ndetermined by the same evaluation measure for different sizes. Our results\ndemonstrate that not only the distribution of an economical observable but also\nthe price itself obeys the power law. We anticipate our findings to be a\nstarting point for the quantitative study of scale invariance in real economy.\nWhile the Black--Sholes model provided the framework for optimal pricing in\nfinancial markets, our method of analysis prvides a new framework that\ncharacterizes the market in real economy.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0611130v1", "link": "http://arxiv.org/abs/physics/0611130v1"}, {"title": "Financial Contagion in a Generalized Stochastic Block Model", "summary": "One of the most defining features of the global financial network is its\ninherent complex and intertwined structure. From the perspective of systemic\nrisk it is important to understand the influence of this network structure on\ndefault contagion. Using sparse random graphs to model the financial network,\nasymptotic methods turned out powerful to analytically describe the contagion\nprocess and to make statements about resilience. So far, however, they have\nbeen limited to so-called {\\em rank one} models in which informally the only\nnetwork parameter is the degree sequence (see (Amini et. al. 2016) and\n(Detering et. al. 2019) for example) and the contagion process can be described\nby a one dimensional fix-point equation. These networks fail to account for a\npronounced block structure such as core/periphery or a network composed of\ndifferent connected blocks for different countries. We present a much more\ngeneral model here, where we distinguish vertices (institutions) of different\ntypes and let edge probabilities and exposures depend on the types of both, the\nreceiving and the sending vertex plus additional parameters. Our main result\nallows to compute explicitly the systemic damage caused by some initial local\nshock event, and we derive a complete characterisation of resilient\nrespectively non-resilient financial systems. This is the first instance that\ndefault contagion is rigorously studied in a model outside the class of rank\none models and several technical challenges arise. Moreover, in contrast to\nprevious work, in which networks could be classified as resilient or non\nresilient, independent of the distribution of the shock, information about the\nshock becomes important in our model and a more refined resilience condition\narises. Among other applications of our theory we derive resilience conditions\nfor the global network based on subnetwork conditions only.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1803.08169v4", "link": "http://arxiv.org/abs/1803.08169v4"}, {"title": "Modeling Economic Networks with Firm-to-Firm Wire Transfers", "summary": "We study a novel economic network comprised of wire transfers (electronic\npayment transactions) among the universe of firms in Brazil (6.2 million\nfirms). We construct a directed and weighted network in which vertices\nrepresent cities and edges connote pairwise economic dependence between cities.\nEach city (vertex) represents the collection of all firms within that city.\nEdge weights are modeled by the total amount of wire transfers that arise due\nto business transactions between firms localized at different cities. The\nrationale is that the more they transact with each other, the more dependent\nthey become in the economic sense. We find a high degree of economic\nintegration among cities in the trade network, which is consistent with the\nhigh degree of specialization found across Brazilian cities. We are able to\nidentify which cities have a dominant role in the entire supply chain process\nusing centrality network measures. We find that the trade network has a\ndisassortative mixing pattern, which is consistent with the power-law shape of\nthe firm size distribution in Brazil. After the Brazilian recession in 2014, we\nfind that the disassortativity becomes even stronger as a result of the death\nof many small firms and the consequent concentration of economic flows on large\nfirms. Our results suggest that recessions have a large impact on the trade\nnetwork with meaningful and heterogeneous economic consequences across\nmunicipalities.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2001.06889v1", "link": "http://arxiv.org/abs/2001.06889v1"}, {"title": "Exeum: A Decentralized Financial Platform for Price-Stable\n  Cryptocurrencies", "summary": "Price stability has often been cited as a key reason that cryptocurrencies\nhave not gained widespread adoption as a medium of exchange and continue to\nprove incapable of powering the economy of decentralized applications (DApps)\nefficiently. Exeum proposes a novel method to provide price stable digital\ntokens whose values are pegged to real world assets, serving as a bridge\nbetween the real world and the decentralized economy.\n  Pegged tokens issued by Exeum - for example, USDE refers to a stable token\nissued by the system whose value is pegged to USD - are backed by virtual\nassets in a virtual asset exchange where users can deposit the base token of\nthe system and take long or short positions. Guaranteeing the stability of the\npegged tokens boils down to the problem of maintaining the peg of the virtual\nassets to real world assets, and the main mechanism used by Exeum is\ncontrolling the swap rate of assets. If the swap rate is fully controlled by\nthe system, arbitrageurs can be incentivized enough to restore a broken peg;\nExeum distributes statistical arbitrage trading software to decentralize this\ntype of market making activity. The last major component of the system is a\ncentral bank equivalent that determines the long term interest rate of the base\ntoken, pays interest on the deposit by inflating the supply if necessary, and\nremoves the need for stability fees on pegged tokens, improving their\nusability.\n  To the best of our knowledge, Exeum is the first to propose a truly\ndecentralized method for developing a stablecoin that enables 1:1 value\nconversion between the base token and pegged assets, completely removing the\nmismatch between supply and demand. In this paper, we will also discuss its\napplications, such as improving staking based DApp token models, price stable\ngas fees, pegging to an index of DApp tokens, and performing cross-chain asset\ntransfer of legacy crypto assets.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1808.03482v1", "link": "http://arxiv.org/abs/1808.03482v1"}, {"title": "Asymptotic approximation of the probability of ruin for large values of\n  the Poisson rate", "summary": "We analyze the probability of ruin for the {\\it scaled} classical\nCramer-Lundberg (CL) risk process and the corresponding diffusion\napproximation. The scaling, introduced by Iglehart (1969) to the actuarial\nliterature, amounts to multiplying the Poisson rate $\\lambda$ by $n$, dividing\nthe claim severity by $\\sqrt{n}$, and adjusting the premium rate so that net\npremium income remains constant. Therefore, we think of the associated\ndiffusion approximation as being ``asymptotic for large values of $\\lambda$.''\n  We are the first to use a comparison method to prove convergence of the\nprobability of ruin for the scaled CL process and to derive the rate of\nconvergence. Specifically, we prove a comparison lemma for the corresponding\nintegro-differential equation and use this comparison lemma to prove that the\nprobability of ruin for the scaled CL process converges to the probability of\nruin for the limiting diffusion process. Moreover, we show that the rate of\nconvergence for the ruin probability is of order $O(n^{-1/2})$, and we show\nthat the convergence is {\\it uniform} with respect to the surplus. To the best\nof our knowledge, this is the first rate of convergence achieved for these ruin\nprobabilities, and we show that it is the tightest one. For the case of\nexponentially-distributed claims, we are able to improve the approximation\narising from the diffusion, attaining a uniform $O(n^{-1})$ rate of\nconvergence. We also include two examples that illustrate our results.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1902.00706v1", "link": "http://arxiv.org/abs/1902.00706v1"}, {"title": "Estimating population average treatment effects from experiments with\n  noncompliance", "summary": "This paper extends a method of estimating population average treatment\neffects to settings with noncompliance. Simulations show the proposed\ncompliance-adjusted estimator performs better than its unadjusted counterpart\nwhen compliance is relatively low and can be predicted by observed covariates.\nWe apply the proposed estimator to measure the effect of Medicaid coverage on\nhealth care use for a target population of adults who may benefit from\nexpansions to the Medicaid program. We draw randomized control trial data from\na large-scale health insurance experiment in which a small subset of those\nrandomly selected to receive Medicaid benefits actually enrolled.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1901.02991v2", "link": "http://arxiv.org/abs/1901.02991v2"}, {"title": "A simple mathematical model for unemployment: a case study in Portugal\n  with optimal control", "summary": "We propose a simple mathematical model for unemployment. Despite its\nsimpleness, we claim that the model is more realistic and useful than recent\nmodels available in the literature. A case study with real data from Portugal\nsupports our claim. An optimal control problem is formulated and solved, which\nprovides some non-trivial and interesting conclusions.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1801.00058v1", "link": "http://dx.doi.org/10.19139/soic.v6i1.470"}, {"title": "Filling the gaps smoothly", "summary": "The calibration of a local volatility models to a given set of option prices\nis a classical problem of mathematical finance. It was considered in multiple\npapers where various solutions were proposed. In this paper an extension of the\napproach proposed in LiptonSepp2011 is developed by i) replacing a piecewise\nconstant local variance construction with a piecewise linear one, and ii)\nallowing non-zero interest rates and dividend yields. Our approach remains\nanalytically tractable; it combines the Laplace transform in time with an\nanalytical solution of the resulting spatial equations in terms of Kummer's\ndegenerate hypergeometric functions.", "category": ["q-fin.CP", "q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1608.05145v1", "link": "http://arxiv.org/abs/1608.05145v1"}, {"title": "Networks of companies and branches in Poland", "summary": "In this study we consider relations between companies in Poland taking into\naccount common branches they belong to. It is clear that companies belonging to\nthe same branch compete for similar customers, so the market induces\ncorrelations between them. On the other hand two branches can be related by\ncompanies acting in both of them. To remove weak, accidental links we shall use\na concept of threshold filtering for weighted networks where a link weight\ncorresponds to a number of existing connections (common companies or branches)\nbetween a pair of nodes.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0611147v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.04.095"}, {"title": "Simulation-based Value-at-Risk for Nonlinear Portfolios", "summary": "Value-at-risk (VaR) has been playing the role of a standard risk measure\nsince its introduction. In practice, the delta-normal approach is usually\nadopted to approximate the VaR of portfolios with option positions. Its\neffectiveness, however, substantially diminishes when the portfolios concerned\ninvolve a high dimension of derivative positions with nonlinear payoffs; lack\nof closed form pricing solution for these potentially highly correlated,\nAmerican-style derivatives further complicates the problem. This paper proposes\na generic simulation-based algorithm for VaR estimation that can be easily\napplied to any existing procedures. Our proposal leverages cross-sectional\ninformation and applies variable selection techniques to simplify the existing\nsimulation framework. Asymptotic properties of the new approach demonstrate\nfaster convergence due to the additional model selection component introduced.\nWe have also performed sets of numerical results that verify the effectiveness\nof our approach in comparison with some existing strategies.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1904.09088v1", "link": "http://arxiv.org/abs/1904.09088v1"}, {"title": "Bayesian estimation of GARCH model with an adaptive proposal density", "summary": "A Bayesian estimation of a GARCH model is performed for US Dollar/Japanese\nYen exchange rate by the Metropolis-Hastings algorithm with a proposal density\ngiven by the adaptive construction scheme. In the adaptive construction scheme\nthe proposal density is assumed to take a form of a multivariate Student's\nt-distribution and its parameters are evaluated by using the sampled data and\nupdated adaptively during Markov Chain Monte Carlo simulations. We find that\nthe autocorrelation times between the data sampled by the adaptive construction\nscheme are considerably reduced. We conclude that the adaptive construction\nscheme works efficiently for the Bayesian inference of the GARCH model.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1012.5986v2", "link": "http://dx.doi.org/10.1007/978-3-642-00909-9_61"}, {"title": "Default Swap Games Driven by Spectrally Negative Levy Processes", "summary": "This paper studies game-type credit default swaps that allow the protection\nbuyer and seller to raise or reduce their respective positions once prior to\ndefault. This leads to the study of an optimal stopping game subject to early\ndefault termination. Under a structural credit risk model based on spectrally\nnegative Levy processes, we apply the principles of smooth and continuous fit\nto identify the equilibrium exercise strategies for the buyer and the seller.\nWe then rigorously prove the existence of the Nash equilibrium and compute the\ncontract value at equilibrium. Numerical examples are provided to illustrate\nthe impacts of default risk and other contractual features on the players'\nexercise timing at equilibrium.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1105.0238v4", "link": "http://dx.doi.org/10.1016/j.spa.2012.09.008"}, {"title": "New Policy Design for Food Accessibility to the People in Need", "summary": "Food insecurity is a term used to measure hunger and food deprivation of a\nlarge population. As per the 2015 statistics provided by Feeding America - one\nof the largest domestic hunger-relief organizations in the United States, 42.2\nmillion Americans live in food insecure households, including 29.1 million\nadults and 13.1 million children. This constitutes about 13.1% of households\nthat are food insecure. Food Banks have been developed to improve food security\nfor the needy. We have developed a novel food distribution policy using\nsuitable welfare and poverty indices and functions. In this work, we propose an\nequitable and fair distribution of donated foods as per the demands and\nrequirements of the people, thus ensuring minimum wastage of food (perishable\nand non-perishable) with focus towards nutrition. We present results and\nanalysis based on the application of the proposed policy using the information\nof a local food bank as a case study. The results show that the new policy\nperforms better than the current methods in terms of population being covered\nand reduction of food wastage obtaining suitable levels of nutrition.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.08648v1", "link": "http://arxiv.org/abs/1909.08648v1"}, {"title": "Shrinkage in the Time-Varying Parameter Model Framework Using the R\n  Package shrinkTVP", "summary": "Time-varying parameter (TVP) models are widely used in time series analysis\nto flexibly deal with processes which gradually change over time. However, the\nrisk of overfitting in TVP models is well known. This issue can be dealt with\nusing appropriate global-local shrinkage priors, which pull time-varying\nparameters towards static ones. In this paper, we introduce the R package\nshrinkTVP (Knaus, Bitto-Nemling, Cadonna, and Fr\\\"uhwirth-Schnatter 2019),\nwhich provides a fully Bayesian implementation of shrinkage priors for TVP\nmodels, taking advantage of recent developments in the literature, in\nparticular that of Bitto and Fr\\\"uhwirth-Schnatter (2019). The package\nshrinkTVP allows for posterior simulation of the parameters through an\nefficient Markov Chain Monte Carlo (MCMC) scheme. Moreover, summary and\nvisualization methods, as well as the possibility of assessing predictive\nperformance through log predictive density scores (LPDSs), are provided. The\ncomputationally intensive tasks have been implemented in C++ and interfaced\nwith R. The paper includes a brief overview of the models and shrinkage priors\nimplemented in the package. Furthermore, core functionalities are illustrated,\nboth with simulated and real data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.07065v2", "link": "http://arxiv.org/abs/1907.07065v2"}, {"title": "Geometric Asian Option Pricing in General Affine Stochastic Volatility\n  Models with Jumps", "summary": "In this paper we present some results on Geometric Asian option valuation for\naffine stochastic volatility models with jumps. We shall provide a general\nframework into which several different valuation problems based on some average\nprocess can be cast, and we shall obtain close-form solutions for some relevant\naffine model classes.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1407.2514v1", "link": "http://arxiv.org/abs/1407.2514v1"}, {"title": "Non-linear dependences in finance", "summary": "The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1309.5073v1", "link": "http://arxiv.org/abs/1309.5073v1"}, {"title": "Fast estimation of multivariate stochastic volatility", "summary": "In this paper we develop a Bayesian procedure for estimating multivariate\nstochastic volatility (MSV) using state space models. A multiplicative model\nbased on inverted Wishart and multivariate singular beta distributions is\nproposed for the evolution of the volatility, and a flexible sequential\nvolatility updating is employed. Being computationally fast, the resulting\nestimation procedure is particularly suitable for on-line forecasting. Three\nperformance measures are discussed in the context of model selection: the\nlog-likelihood criterion, the mean of standardized one-step forecast errors,\nand sequential Bayes factors. Finally, the proposed methods are applied to a\ndata set comprising eight exchange rates vis-a-vis the US dollar.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0708.4376v2", "link": "http://arxiv.org/abs/0708.4376v2"}, {"title": "Predicting the Stock Price of Frontier Markets Using Modified\n  Black-Scholes Option Pricing Model and Machine Learning", "summary": "The Black-Scholes Option pricing model (BSOPM) has long been in use for\nvaluation of equity options to find the prices of stocks. In this work, using\nBSOPM, we have come up with a comparative analytical approach and numerical\ntechnique to find the price of call option and put option and considered these\ntwo prices as buying price and selling price of stocks of frontier markets so\nthat we can predict the stock price (close price). Changes have been made to\nthe model to find the parameters strike price and the time of expiration for\ncalculating stock price of frontier markets. To verify the result obtained\nusing modified BSOPM we have used machine learning approach using the software\nRapidminer, where we have adopted different algorithms like the decision tree,\nensemble learning method and neural network. It has been observed that, the\nprediction of close price using machine learning is very similar to the one\nobtained using BSOPM. Machine learning approach stands out to be a better\npredictor over BSOPM, because Black-Scholes-Merton equation includes risk and\ndividend parameter, which changes continuously. We have also numerically\ncalculated volatility. As the prices of the stocks goes high due to\noverpricing, volatility increases at a tremendous rate and when volatility\nbecomes very high market tends to fall, which can be observed and determined\nusing our modified BSOPM. The proposed modified BSOPM has also been explained\nbased on the analogy of Schrodinger equation (and heat equation) of quantum\nphysics.", "category": ["q-fin.ST", "q-fin.MF"], "id": "http://arxiv.org/abs/1812.10619v1", "link": "http://arxiv.org/abs/1812.10619v1"}, {"title": "The microscopic relationships between triangular arbitrage and\n  cross-currency correlations in a simple agent based model of foreign exchange\n  markets", "summary": "Foreign exchange rates movements exhibit significant cross-correlations even\non very short time-scales. The effect of these statistical relationships become\nevident during extreme market events, such as flash crashes.In this scenario,\nan abrupt price swing occurring on a given market is immediately followed by\nanomalous movements in several related foreign exchange rates. Although a deep\nunderstanding of cross-currency correlations would be clearly beneficial for\nconceiving more stable and safer foreign exchange markets, the microscopic\norigins of these interdependencies have not been extensively investigated. We\nintroduce an agent-based model which describes the emergence of cross-currency\ncorrelations from the interactions between market makers and an arbitrager. Our\nmodel qualitatively replicates the time-scale vs. cross-correlation diagrams\nobserved in real trading data, suggesting that triangular arbitrage plays a\nprimary role in the entanglement of the dynamics of different foreign exchange\nrates. Furthermore, the model shows how the features of the cross-correlation\nfunction between two foreign exchange rates, such as its sign and value, emerge\nfrom the interplay between triangular arbitrage and trend-following strategies.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/2002.02583v1", "link": "http://arxiv.org/abs/2002.02583v1"}, {"title": "A Correlated Random Coefficient Panel Model with Time-Varying\n  Endogeneity", "summary": "This paper studies a class of linear panel models with random coefficients.\nWe do not restrict the joint distribution of the time-invariant unobserved\nheterogeneity and the covariates. We investigate identification of the average\npartial effect (APE) when fixed-effect techniques cannot be used to control for\nthe correlation between the regressors and the time-varying disturbances.\nRelying on control variables, we develop a constructive two-step identification\nargument. The first step identifies nonparametrically the conditional\nexpectation of the disturbances given the regressors and the control variables,\nand the second step uses \"between-group\" variations, correcting for\nendogeneity, to identify the APE. We propose a natural semiparametric estimator\nof the APE, show its $\\sqrt{n}$ asymptotic normality and compute its asymptotic\nvariance. The estimator is computationally easy to implement, and Monte Carlo\nsimulations show favorable finite sample properties. Control variables arise in\nvarious economic and econometric models, and we provide variations of our\nargument to obtain identification in some applications. As an empirical\nillustration, we estimate the average elasticity of intertemporal substitution\nin a labor supply model with random coefficients.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.09367v1", "link": "http://arxiv.org/abs/2003.09367v1"}, {"title": "Measuring long-range dependence in electricity prices", "summary": "The price of electricity is far more volatile than that of other commodities\nnormally noted for extreme volatility. The possibility of extreme price\nmovements increases the risk of trading in electricity markets. However,\nunderlying the process of price returns is a strong mean-reverting mechanism.\nWe study this feature of electricity returns by means of Hurst R/S analysis,\nDetrended Fluctuation Analysis and periodogram regression.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0103621v1", "link": "http://arxiv.org/abs/cond-mat/0103621v1"}, {"title": "A Random Matrix Approach to VARMA Processes", "summary": "We apply random matrix theory to derive spectral density of large sample\ncovariance matrices generated by multivariate VMA(q), VAR(q) and VARMA(q1,q2)\nprocesses. In particular, we consider a limit where the number of random\nvariables N and the number of consecutive time measurements T are large but the\nratio N/T is fixed. In this regime the underlying random matrices are\nasymptotically equivalent to Free Random Variables (FRV). We apply the FRV\ncalculus to calculate the eigenvalue density of the sample covariance for\nseveral VARMA-type processes. We explicitly solve the VARMA(1,1) case and\ndemonstrate a perfect agreement between the analytical result and the spectra\nobtained by Monte Carlo simulations. The proposed method is purely algebraic\nand can be easily generalized to q1>1 and q2>1.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1002.0934v1", "link": "http://dx.doi.org/10.1088/1367-2630/12/7/075036"}, {"title": "The Mathematics of the Relationship between the Default Risk and\n  Yield-to-Maturity of Coupon Bonds", "summary": "The paper analyzes the mathematics of the relationship between the default\nrisk and yield-to-maturity of a coupon bond. It is shown that the\nyield-to-maturity is driven not only by the default probability and recovery\nrate of the bond but also by other contractual characteristics of the bond that\nare not commonly associated with default risk, such as the maturity and coupon\nrate of the bond. In particular, for given default probability and recovery\nrate, both the level and slope of the yield-to-maturity term structure depend\non the coupon rate, as the higher the coupon rate the higher the\nyield-to-maturity term structure. In addition, the yield-to-maturity term\nstructure is upward or downward sloping depending on whether the coupon rate is\nhigh or low enough. Similar qualitative results also holds for CDS spreads.\nConsequently, the yield-to-maturity is an indicator that must be used\ncautiously as a proxy for default risk.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1203.6723v1", "link": "http://arxiv.org/abs/1203.6723v1"}, {"title": "Model-Independent Pricing of Asian Options via Optimal Martingale\n  Transport", "summary": "In this article we discuss the problem of calculating optimal\nmodel-independent (robust) bounds for the price of Asian options with discrete\nand continuous averaging. We will give geometric characterisations of the\nmaximising and the minimising pricing model for certain types of Asian options\nin discrete and continuous time. In discrete time the problem is reduced to\nfinding the optimal martingale transport for the cost function $|x+y|$. In the\ncontinuous time case we consider the cases with one and two given marginals. We\ndescribe the maximising models in both of these cases as well as the minimising\nmodel in the one-marginal case and relate the two-marginals case to the\ndiscrete time problem with two marginals.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1412.1429v1", "link": "http://arxiv.org/abs/1412.1429v1"}, {"title": "Insider information and its relation with the arbitrage condition and\n  the utility maximization problem", "summary": "Within the well-known framework of financial portfolio optimization, we\nanalyze the existing relationships between the condition of arbitrage and the\nutility maximization in presence of \\emph{insider information}. We assume that,\nsince the initial time, the information flow is altered by adding the knowledge\nof an additional random variable including future information. In this context\nwe study the utility maximization problem under the logarithmic and the\nConstant Relative Risk Aversion (CRRA) utilities, with and without the\nrestriction of no temporary-bankruptcy. In particular, we show that the value\nof the insider information may be bounded while the arbitrage condition holds\nand we prove that the insider information does not always imply arbitrage for\nthe insider by providing an explicit example.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1909.03430v2", "link": "http://dx.doi.org/10.3934/mbe.2020053"}, {"title": "Pricing for Large Positions in Contingent Claims", "summary": "Approximations to utility indifference prices are provided for a contingent\nclaim in the large position size limit. Results are valid for general utility\nfunctions on the real line and semi-martingale models. It is shown that as the\nposition size approaches infinity, the utility function's decay rate for large\nnegative wealths is the primary driver of prices. For utilities with\nexponential decay, one may price like an exponential investor. For utilities\nwith a power decay, one may price like a power investor after a suitable\nadjustment to the rate at which the position size becomes large. In a sizable\nclass of diffusion models, limiting indifference prices are explicitly computed\nfor an exponential investor. Furthermore, the large claim limit is seen to\nendogenously arise as the hedging error for the claim vanishes.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1202.4007v3", "link": "http://arxiv.org/abs/1202.4007v3"}, {"title": "Fair bilateral prices in Bergman's model", "summary": "Bielecki and Rutkowski (2014) introduced and studied a generic nonlinear\nmarket model, which includes several risky assets, multiple funding accounts\nand margin accounts. In this paper, we examine the pricing and hedging of\ncontract both from the perspective of the hedger and the counterparty with\narbitrary initial endowments. We derive inequalities for unilateral prices and\nwe give the range for either fair bilateral prices or bilaterally profitable\nprices. We also study the monotonicity of a unilateral price with respect to\nthe initial endowment. Our study hinges on results for BSDE driven by\ncontinuous martingales obtained in Nie and Rutkowski (2014), but we also derive\nthe pricing PDEs for path-independent contingent claims of European style in a\nMarkovian framework.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1410.0673v2", "link": "http://arxiv.org/abs/1410.0673v2"}, {"title": "Optimal sequential treatment allocation", "summary": "In treatment allocation problems the individuals to be treated often arrive\nsequentially. We study a problem in which the policy maker is not only\ninterested in the expected cumulative welfare but is also concerned about the\nuncertainty/risk of the treatment outcomes. At the outset, the total number of\ntreatment assignments to be made may even be unknown. A sequential treatment\npolicy which attains the minimax optimal regret is proposed. We also\ndemonstrate that the expected number of suboptimal treatments only grows slowly\nin the number of treatments. Finally, we study a setting where outcomes are\nonly observed with delay.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1705.09952v4", "link": "http://arxiv.org/abs/1705.09952v4"}, {"title": "The structural role of weak and strong links in a financial market\n  network", "summary": "We investigate the properties of correlation based networks originating from\neconomic complex systems, such as the network of stocks traded at the New York\nStock Exchange (NYSE). The weaker links (low correlation) of the system are\nfound to contribute to the overall connectivity of the network significantly\nmore than the strong links (high correlation). We find that nodes connected\nthrough strong links form well defined communities. These communities are\nclustered together in more complex ways compared to the widely used\nclassification according to the economic activity. We find that some companies,\nsuch as General Electric (GE), Coca Cola (KO), and others, can be involved in\ndifferent communities. The communities are found to be quite stable over time.\nSimilar results were obtained by investigating markets completely different in\nsize and properties, such as the Athens Stock Exchange (ASE). The present\nmethod may be also useful for other networks generated through correlations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0805.2477v1", "link": "http://dx.doi.org/10.1140/epjb/e2008-00237-3"}, {"title": "Causal Inference in Case-Control Studies", "summary": "We investigate identification of causal parameters in case-control and\nrelated studies. The odds ratio in the sample is our main estimand of interest\nand we articulate its relationship with causal parameters under various\nscenarios. It turns out that the odds ratio is generally a sharp upper bound\nfor counterfactual relative risk under some monotonicity assumptions, without\nresorting to strong ignorability, nor to the rare-disease assumption. Further,\nwe propose semparametrically efficient, easy-to-implement,\nmachine-learning-friendly estimators of the aggregated (log) odds ratio by\nexploiting an explicit form of the efficient influence function. Using our new\nestimators, we develop methods for causal inference and illustrate the\nusefulness of our methods by a real-data example.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.08318v1", "link": "http://arxiv.org/abs/2004.08318v1"}, {"title": "FIEMS: Fast Italian Energy Market Simulator", "summary": "The article describes the algorithm used to define the electricity price in\nday-ahead and itraday energy markets in Italy. Details of Matlab implementation\nof one of its simplified versions, capable of producing good results in a\nextremely short time, are then provided and numerical results are discussed.", "category": ["q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1703.09782v1", "link": "http://arxiv.org/abs/1703.09782v1"}, {"title": "An extension of Paulsen-Gjessing's risk model with stochastic return on\n  investments", "summary": "We consider in this paper a general two-sided jump-diffusion risk model that\nallows for risky investments as well as for correlation between the two\nBrownian motions driving insurance risk and investment return. We first\nintroduce the model and then find the integro-differential equations satisfied\nby the Gerber-Shiu functions as well as the expected discounted penalty\nfunctions at ruin caused by a claim or by oscillation; We also study the\ndividend problem for the threshold and barrier strategies, the moments and\nmoment-generating function of the total discounted dividends until ruin are\ndiscussed. Some examples are given for special cases.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1302.6757v1", "link": "http://arxiv.org/abs/1302.6757v1"}, {"title": "Risk Measures in Quantitative Finance", "summary": "This paper was presented and written for two seminars: a national UK\nUniversity Risk Conference and a Risk Management industry workshop. The target\naudience is therefore a cross section of Academics and industry professionals.\n  The current ongoing global credit crunch has highlighted the importance of\nrisk measurement in Finance to companies and regulators alike. Despite risk\nmeasurement's central importance to risk management, few papers exist reviewing\nthem or following their evolution from its foremost beginnings up to the\npresent day risk measures.\n  This paper reviews the most important portfolio risk measures in Financial\nMathematics, from Bernoulli (1738) to Markowitz's Portfolio Theory, to the\npresently preferred risk measures such as CVaR (conditional Value at Risk). We\nprovide a chronological review of the risk measures and survey less commonly\nknown risk measures e.g. Treynor ratio.", "category": ["q-fin.RM", "q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/0904.0870v1", "link": "http://arxiv.org/abs/0904.0870v1"}, {"title": "Fitness-dependent topological properties of the World Trade Web", "summary": "Among the proposed network models, the hidden variable (or good get richer)\none is particularly interesting, even if an explicit empirical test of its\nhypotheses has not yet been performed on a real network. Here we provide the\nfirst empirical test of this mechanism on the world trade web, the network\ndefined by the trade relationships between world countries. We find that the\npower-law distributed gross domestic product can be successfully identified\nwith the hidden variable (or fitness) determining the topology of the world\ntrade web: all previously studied properties up to third-order correlation\nstructure (degree distribution, degree correlations and hierarchy) are found to\nbe in excellent agreement with the predictions of the model. The choice of the\nconnection probability is such that all realizations of the network with the\nsame degree sequence are equiprobable.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0403051v2", "link": "http://dx.doi.org/10.1103/PhysRevLett.93.188701"}, {"title": "Horizon dependence of utility optimizers in incomplete models", "summary": "This paper studies the utility maximization problem with changing time\nhorizons in the incomplete Brownian setting. We first show that the primal\nvalue function and the optimal terminal wealth are continuous with respect to\nthe time horizon $T$. Secondly, we exemplify that the expected utility stemming\nfrom applying the $T$-horizon optimizer on a shorter time horizon $S$, $S < T$,\nmay not converge as $S\\uparrow T$ to the $T$-horizon value. Finally, we provide\nnecessary and sufficient conditions preventing the existence of this\nphenomenon.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1006.5057v3", "link": "http://arxiv.org/abs/1006.5057v3"}, {"title": "Any Regulation of Risk Increases Risk", "summary": "We show that any objective risk measurement algorithm mandated by central\nbanks for regulated financial entities will result in more risk being taken on\nby those financial entities than would otherwise be the case. Furthermore, the\nrisks taken on by the regulated financial entities are far more systemically\nconcentrated than they would have been otherwise, making the entire financial\nsystem more fragile. This result leaves three directions for the future of\nfinancial regulation: continue regulating by enforcing risk measurement\nalgorithms at the cost of occasional severe crises, regulate more severely and\nsubjectively by fully nationalizing all financial entities, or abolish all\ncentral banking regulations including deposit insurance to let risk be\ndetermined by the entities themselves and, ultimately, by their depositors\nthrough voluntary market transactions rather than by the taxpayers through\nenforced government participation.", "category": ["q-fin.RM", "q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/1004.1670v4", "link": "http://arxiv.org/abs/1004.1670v4"}, {"title": "EB-dynaRE: Real-Time Adjustor for Brownian Movement with Examples of\n  Predicting Stock Trends Based on a Novel Event-Based Supervised Learning\n  Algorithm", "summary": "Stock prices are influenced over time by underlying macroeconomic factors.\nJumping out of the box of conventional assumptions about the unpredictability\nof the market noise, we modeled the changes of stock prices over time through\nthe Markov Decision Process, a discrete stochastic control process that aids\ndecision making in a situation that is partly random. We then did a \"Region of\nInterest\" (RoI) Pooling of the stock time-series graphs in order to predict\nfuture prices with existing ones. Generative Adversarial Network (GAN) is then\nused based on a competing pair of supervised learning algorithms, to regenerate\nfuture stock price projections on a real-time basis. The supervised learning\nalgorithm used in this research, moreover, is original to this study and will\nhave wider uses. With the ensemble of these algorithms, we are able to\nidentify, to what extent, each specific macroeconomic factor influences the\nchange of the Brownian/random market movement. In addition, our model will have\na wider influence on the predictions of other Brownian movements.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2003.11473v1", "link": "http://arxiv.org/abs/2003.11473v1"}, {"title": "Asymptotic Behavior of Bayesian Learners with Misspecified Models", "summary": "We consider an agent who represents uncertainty about the environment via a\npossibly misspecified model. Each period, the agent takes an action, observes a\nconsequence, and uses Bayes' rule to update her belief about the environment.\nThis framework has become increasingly popular in economics to study behavior\ndriven by incorrect or biased beliefs. Current literature has characterized\nasymptotic behavior under fairly specific assumptions. By first showing that\nthe key element to predict the agent's behavior is the frequency of her past\nactions, we are able to characterize asymptotic behavior in general settings in\nterms of the solutions of a generalization of a differential equation that\ndescribes the evolution of the frequency of actions. We then present a series\nof implications that can be readily applied to economic applications, thus\nproviding off-the-shelf tools that can be used to characterize behavior under\nmisspecified learning.", "category": [], "id": "http://arxiv.org/abs/1904.08551v2", "link": "http://arxiv.org/abs/1904.08551v2"}, {"title": "Copulas and time series with long-ranged dependences", "summary": "We review ideas on temporal dependences and recurrences in discrete time\nseries from several areas of natural and social sciences. We revisit existing\nstudies and redefine the relevant observables in the language of copulas (joint\nlaws of the ranks). We propose that copulas provide an appropriate mathematical\nframework to study non-linear time dependences and related concepts - like\naftershocks, Omori law, recurrences, waiting times. We also critically argue\nusing this global approach that previous phenomenological attempts involving\nonly a long-ranged autocorrelation function lacked complexity in that they were\nessentially mono-scale.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1311.5101v1", "link": "http://dx.doi.org/10.1103/PhysRevE.89.042117"}, {"title": "Description of the Operational Mechanics of a Basel Regulated Banking\n  System", "summary": "This paper presents a description of the mechanical operations of banking as\nused in modern banking systems regulated under the Basel Accords, in order to\nprovide support for a verifiable and complete description of the banking system\nsuitable for computer simulation. Feedback is requested on the contents of this\ndocument, both with respect to the operations described here, and any known\nnational, regional or local variations in their structure and practice.", "category": ["q-fin.GN", "q-fin.CP"], "id": "http://arxiv.org/abs/1204.1583v1", "link": "http://arxiv.org/abs/1204.1583v1"}, {"title": "Time series analysis for minority game simulations of financial markets", "summary": "The minority game (MG) model introduced recently provides promising insights\ninto the understanding of the evolution of prices, indices and rates in the\nfinancial markets. In this paper we perform a time series analysis of the model\nemploying tools from statistics, dynamical systems theory and stochastic\nprocesses. Using benchmark systems and a financial index for comparison,\nseveral conclusions are obtained about the generating mechanism for this kind\nof evolut ion. The motion is deterministic, driven by occasional random\nexternal perturbation. When the interval between two successive perturbations\nis sufficiently large, one can find low dimensional chaos in this regime.\nHowever, the full motion of the MG model is found to be similar to that of the\nfirst differences of the SP500 index: stochastic, nonlinear and (unit root)\nstationary.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0203038v3", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01733-8"}, {"title": "Power Law Distributions in Korean Household Incomes", "summary": "We investigate the distribution function and the cumulative probability for\nKorean household incomes, i.e., the current, labor, and property incomes. For\nour case, the distribution functions are consistent with a power law. It is\nalso showed that the probability density of income growth rates almost has the\nform of a exponential function. Our obtained results are compared with those of\nother numerical calculations.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0403161v1", "link": "http://arxiv.org/abs/cond-mat/0403161v1"}, {"title": "Central Clearing Valuation Adjustment", "summary": "This paper develops an XVA (costs) analysis of centrally cleared trading,\nparallel to the one that has been developed in the last years for bilateral\ntransactions. We introduce a dynamic framework that incorporates the sequence\nof cash-flows involved in the waterfall of resources of a clearing house. The\ntotal cost of the clearance framework for a clearing member, called CCVA for\ncentral clearing valuation adjustment, is decomposed into a CVA corresponding\nto the cost of its losses on the default fund in case of defaults of other\nmember, an MVA corresponding to the cost of funding its margins and a KVA\ncorresponding to the cost of the regulatory capital and also of the capital at\nrisk that the member implicitly provides to the CCP through its default fund\ncontribution. In the end the structure of the XVA equations for bilateral and\ncleared portfolios is similar, but the input data to these equations are not\nthe same, reflecting different financial network structures. The resulting XVA\nnumbers differ, but, interestingly enough, they become comparable after scaling\nby a suitable netting ratio.", "category": ["q-fin.RM", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1506.08595v3", "link": "http://arxiv.org/abs/1506.08595v3"}, {"title": "A boundary point lemma for Black-Scholes type operators", "summary": "We prove a sharp version of the Hopf boundary point lemma for Black-Scholes\ntype equations. We also investigate the existence and the regularity of the\nspatial derivative of the solutions at the spatial boundary.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/math/0509231v1", "link": "http://arxiv.org/abs/math/0509231v1"}, {"title": "Introduction into \"Local Correlation Modelling\"", "summary": "In this paper we provide evidence that financial option markets for equity\nindices give rise to non-trivial dependency structures between its\nconstituents. Thus, if the individual constituent distributions of an equity\nindex are inferred from the single-stock option markets and combined via a\nGaussian copula, for example, one fails to explain the steepness of the\nobserved volatility skew of the index. Intuitively, index option prices are\nencoding higher correlations in cases where the option is particularly\nsensitive to stress scenarios of the market. As a result, more complex\ndependency structures emerge than the ones described by Gaussian copulas or\n(state-independent) linear correlation structures.\n  In this paper we \"decode\" the index option market and extract this\ncorrelation information in order to extend the multi-asset version of Dupire's\n\"local volatility\" model by making correlations a dynamic variable of the\nmarket. A \"local correlation\" model (LCM) is introduced for the pricing of\nmulti-asset derivatives. We show how consistency with the index volatility data\ncan be achieved by construction.\n  LCM achieves consistency with both the constituent- and index option markets\nby construction while preserving the efficiency and easy implementation of\nDupire's model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0909.3441v3", "link": "http://arxiv.org/abs/0909.3441v3"}, {"title": "An Agent-Based Model to Explain the Emergence of Stylised Facts in Log\n  Returns", "summary": "This paper outlines an agent-based model of a simple financial market in\nwhich a single asset is available for trade by three different types of\ntraders. The model was first introduced in the PhD thesis of one of the\nauthors, see reference [1]. The simulated log returns are examined for the\npresence of the stylised facts of financial data. The features of\nleptokurtosis, volatility clustering and aggregational Gaussianity are\nespecially highlighted and studied in detail. The following ingredients are\nfound to be essential for the production of these stylised facts: the memory of\nnoise traders who make random trade decisions; the inclusion of technical\ntraders that trade in line with trends in the price and the inclusion of\nfundamental traders who know the \"fundamental value\" of the stock and trade\naccordingly. When these three basic types of traders are included log returns\nare produced with a leptokurtic distribution and volatility clustering as well\nas some further statistical features of empirical data. This enhances and\nbroadens our understanding of the fundamental processes involved in the\nproduction of empirical data by the market.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1901.05053v1", "link": "http://arxiv.org/abs/1901.05053v1"}, {"title": "Gender Differences in Wage Expectations", "summary": "Using a survey on wage expectations among students at two Swiss institutions\nof higher education, we examine the wage expectations of our respondents along\ntwo main lines. First, we investigate the rationality of wage expectations by\ncomparing average expected wages from our sample with those of similar\ngraduates; we further examine how our respondents revise their expectations\nwhen provided information about actual wages. Second, using causal mediation\nanalysis, we test whether the consideration of a rich set of personal and\nprofessional controls, namely concerning family formation and children in\naddition to professional preferences, accounts for the difference in wage\nexpectations across genders. We find that males and females overestimate their\nwages compared to actual ones, and that males respond in an overconfident\nmanner to information about outside wages. Despite the attenuation of the\ngender difference in wage expectations brought about by the comprehensive set\nof controls, gender generally retains a significant direct, unexplained effect\non wage expectations.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.11496v1", "link": "http://arxiv.org/abs/2003.11496v1"}, {"title": "Strategic Learning and the Topology of Social Networks", "summary": "We consider a group of strategic agents who must each repeatedly take one of\ntwo possible actions. They learn which of the two actions is preferable from\ninitial private signals, and by observing the actions of their neighbors in a\nsocial network.\n  We show that the question of whether or not the agents learn efficiently\ndepends on the topology of the social network. In particular, we identify a\ngeometric \"egalitarianism\" condition on the social network that guarantees\nlearning in infinite networks, or learning with high probability in large\nfinite networks, in any equilibrium. We also give examples of non-egalitarian\nnetworks with equilibria in which learning fails.", "category": ["math.PR"], "id": "http://arxiv.org/abs/1209.5527v2", "link": "http://dx.doi.org/10.3982/ECTA12058"}, {"title": "Optimal Investment Horizons for Stocks and Markets", "summary": "The inverse statistics is the distribution of waiting times needed to achieve\na predefined level of return obtained from (detrended) historic asset prices\n\\cite{optihori,gainloss}. Such a distribution typically goes through a maximum\nat a time coined the {\\em optimal investment horizon}, $\\tau^*_\\rho$, which\ndefines the most likely waiting time for obtaining a given return $\\rho$. By\nconsidering equal positive and negative levels of return, we reported in\n\\cite{gainloss} on a quantitative gain/loss asymmetry most pronounced for short\nhorizons. In the present paper, the inverse statistics for 2/3 of the\nindividual stocks presently in the DJIA is investigated. We show that this\ngain/loss asymmetry established for the DJIA surprisingly is {\\em not} present\nin the time series of the individual stocks nor their average. This observation\npoints towards some kind of collective movement of the stocks of the index\n(synchronization).", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0601002v1", "link": "http://dx.doi.org/10.1016/j.physa.2006.04.030"}, {"title": "Swap Portfolios and Reverse-Weighted Portfolios, with an Application to\n  Commodity Futures", "summary": "A market portfolio is a portfolio in which each asset is held at a weight\nproportional to its market value. A swap portfolio is a portfolio in which each\none of a pair of assets is held at a weight proportional to the market value of\nthe other. A reverse-weighted index portfolio is a portfolio in which the\nweights of the market portfolio are swapped pairwise by rank. Swap portfolios\nare functionally generated, and in a coherent market they have higher\nasymptotic growth rates than the market portfolio. Although reverse-weighted\nportfolios with two or more pairs of assets are not functionally generated, in\na market represented by a first-order model with symmetric variances, they will\ngrow faster than the market portfolio. This result is applied to a market of\ncommodity futures.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2001.06914v1", "link": "http://arxiv.org/abs/2001.06914v1"}, {"title": "Resource Abundance and Life Expectancy", "summary": "This paper investigates the impacts of major natural resource discoveries\nsince 1960 on life expectancy in the nations that they were resource poor prior\nto the discoveries. Previous literature explains the relation between nations\nwealth and life expectancy, but it has been silent about the impacts of\nresource discoveries on life expectancy. We attempt to fill this gap in this\nstudy. An important advantage of this study is that as the previous researchers\nargued resource discovery could be an exogenous variable. We use longitudinal\ndata from 1960 to 2014 and we apply three modern empirical methods including\nDifference-in-Differences, Event studies, and Synthetic Control approach, to\ninvestigate the main question of the research which is 'how resource\ndiscoveries affect life expectancy?'. The findings show that resource\ndiscoveries in Ecuador, Yemen, Oman, and Equatorial Guinea have positive and\nsignificant impacts on life expectancy, but the effects for the European\ncountries are mostly negative.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.00369v1", "link": "http://arxiv.org/abs/1801.00369v1"}, {"title": "MFGs for partially reversible investment", "summary": "This paper analyzes a class of MFGs with singular controls motivated from the\npartially reversible problem. It establishes the existence of the solution when\ncontrols are of bounded velocity, solves explicitly the game when controls are\nof finite variation, and presents sensitivity analysis to compare the\nsingle-player game with the MFG. Our analysis shows that MFGs, when\nappropriately formulated, can demonstrate genuine game effects even without\nheterogeneity among players and additional common noise.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1908.10916v1", "link": "http://arxiv.org/abs/1908.10916v1"}, {"title": "Inflation, unemployment, and labor force. Phillips curves and long-term\n  projections for Japan", "summary": "The evolution of the rate of price inflation and unemployment in Japan has\nbeen modeled within the Phillips curve framework. As an extension to the\nPhillips curve, we represent both variables as linear functions of the change\nrate of labor force. All models were first estimated in 2005 for the period\nbetween 1980 and 2003. Here we update these original models with data through\n2012. The revisited models accurately describe disinflation during the 1980s\nand 1990s as well as the whole deflationary period started in the late 1990s.\nThe Phillips curve for Japan confirms the original concept that growing\nunemployment results in decreasing inflation. A linear and lagged generalized\nPhillips curve expressed as a link between inflation, unemployment, and labor\nforce has been also re-estimated and validated by new data. Labor force\nprojections allow a long-term inflation and unemployment forecast: the GDP\ndeflator will be negative (between -0.5% and -2% per year) during the next 40\nyears. The rate of unemployment will increase from 4.3% in 2012 to 5.5% in\n2050.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1309.1757v1", "link": "http://arxiv.org/abs/1309.1757v1"}, {"title": "Perturbative Expansion Technique for Non-linear FBSDEs with Interacting\n  Particle Method", "summary": "In this paper, we propose an efficient Monte Carlo implementation of\nnon-linear FBSDEs as a system of interacting particles inspired by the ideas of\nbranching diffusion method. It will be particularly useful to investigate large\nand complex systems, and hence it is a good complement of our previous work\npresenting an analytical perturbation procedure for generic non-linear FBSDEs.\nThere appear multiple species of particles, where the first one follows the\ndiffusion of the original underlying state, and the others the Malliavin\nderivatives with a grading structure. The number of branching points are capped\nby the order of perturbation, which is expected to make the scheme less\nnumerically intensive. The proposed method can be applied to semi-linear\nproblems, such as American and Bermudan options, Credit Value Adjustment (CVA),\nand even fully non-linear issues, such as the optimal portfolio problems in\nincomplete and/or constrained markets, feedbacks from large investors, and also\nthe analysis of various risk measures.", "category": ["q-fin.CP", "q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1204.2638v2", "link": "http://arxiv.org/abs/1204.2638v2"}, {"title": "Future exchange rates and Siegel's paradox", "summary": "Siegel's paradox is a fundamental question in international finance about\nexchange rates for futures contracts and has puzzled many scholars for over\nforty years. The unorthodox approach presented in this article leads to an\narbitrage-free solution which is invariant under currency re-denominations and\nis symmetric, as explained. We will also give a complete classification of all\nsuch aggregators in the general case. The formula obtained in this setting\ntherefore describes all the negotiated no-arbitrage forward exchange rates in\nterms of a reciprocity function.\n  Keywords: Siegel's paradox, forward exchange rates, discount bias.", "category": ["q-fin.MF", "q-fin.EC"], "id": "http://arxiv.org/abs/1805.03347v1", "link": "http://dx.doi.org/10.1016/j.gfj.2018.04.007"}, {"title": "Estimation and simulation of the transaction arrival process in intraday\n  electricity markets", "summary": "We examine the novel problem of the estimation of transaction arrival\nprocesses in the intraday electricity markets. We model the inter-arrivals\nusing multiple time-varying parametric densities based on the generalized F\ndistribution estimated by maximum likelihood. We analyse both the in-sample\ncharacteristics and the probabilistic forecasting performance. In a rolling\nwindow forecasting study, we simulate many trajectories to evaluate the\nforecasts and gain significant insights into the model fit. The prediction\naccuracy is evaluated by a functional version of the MAE (mean absolute error),\nRMSE (root mean squared error) and CRPS (continuous ranked probability score)\nfor the simulated count processes. This paper fills the gap in the literature\nregarding the intensity estimation of transaction arrivals and is a major\ncontribution to the topic, yet leaves much of the field for further\ndevelopment. The study presented in this paper is conducted based on the German\nIntraday Continuous electricity market data, but this method can be easily\napplied to any other continuous intraday electricity market. For the German\nmarket, a specific generalized gamma distribution setup explains the overall\nbehaviour significantly best, especially as the tail behaviour of the process\nis well covered.", "category": ["econ.GN", "q-fin.EC", "q-fin.ST"], "id": "http://arxiv.org/abs/1901.09729v4", "link": "http://dx.doi.org/10.3390/en12234518"}, {"title": "Prediction Law of Mixed Gaussian Volterra Processes", "summary": "We study the regular conditional law of mixed Gaussian Volterra processes\nunder the influence of model disturbances. More precisely, we study prediction\nof Gaussian Volterra processes driven by a Brownian motion in a case where the\nBrownian motion is not observable, but only a noisy version is observed. As an\napplication, we discuss how our result can be applied to variance reduction in\nthe presence of measurement errors.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1904.09799v1", "link": "http://arxiv.org/abs/1904.09799v1"}, {"title": "Queue Imbalance as a One-Tick-Ahead Price Predictor in a Limit Order\n  Book", "summary": "We investigate whether the bid/ask queue imbalance in a limit order book\n(LOB) provides significant predictive power for the direction of the next\nmid-price movement. We consider this question both in the context of a simple\nbinary classifier, which seeks to predict the direction of the next mid-price\nmovement, and a probabilistic classifier, which seeks to predict the\nprobability that the next mid-price movement will be upwards. To implement\nthese classifiers, we fit logistic regressions between the queue imbalance and\nthe direction of the subsequent mid-price movement for each of 10 liquid stocks\non Nasdaq. In each case, we find a strongly statistically significant\nrelationship between these variables. Compared to a simple null model, which\nassumes that the direction of mid-price changes is uncorrelated with the queue\nimbalance, we find that our logistic regression fits provide a considerable\nimprovement in binary and probabilistic classification for large-tick stocks,\nand provide a moderate improvement in binary and probabilistic classification\nfor small-tick stocks. We also perform local logistic regression fits on the\nsame data, and find that this semi-parametric approach slightly outperform our\nlogistic regression fits, at the expense of being more computationally\nintensive to implement.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1512.03492v1", "link": "http://arxiv.org/abs/1512.03492v1"}, {"title": "An Analysis Framework for Metric Voting based on LP Duality", "summary": "Distortion-based analysis has established itself as a fruitful framework for\ncomparing voting mechanisms. m voters and n candidates are jointly embedded in\nan (unknown) metric space, and the voters submit rankings of candidates by\nnon-decreasing distance from themselves. Based on the submitted rankings, the\nsocial choice rule chooses a winning candidate; the quality of the winner is\nthe sum of the (unknown) distances to the voters. The rule's choice will in\ngeneral be suboptimal, and the worst-case ratio between the cost of its chosen\ncandidate and the optimal candidate is called the rule's distortion. It was\nshown in prior work that every deterministic rule has distortion at least 3,\nwhile the Copeland rule and related rules guarantee worst-case distortion at\nmost 5; a very recent result gave a rule with distortion $2+\\sqrt{5} \\approx\n4.236$.\n  We provide a framework based on LP-duality and flow interpretations of the\ndual which provides a simpler and more unified way for proving upper bounds on\nthe distortion of social choice rules. We illustrate the utility of this\napproach with three examples. First, we give a fairly simple proof of a strong\ngeneralization of the upper bound of 5 on the distortion of Copeland, to social\nchoice rules with short paths from the winning candidate to the optimal\ncandidate in generalized weak preference graphs. A special case of this result\nrecovers the recent $2+\\sqrt{5}$ guarantee. Second, using this generalized\nbound, we show that the Ranked Pairs and Schulze rules have distortion\n$\\Theta(\\sqrt(n))$. Finally, our framework naturally suggests a combinatorial\nrule that is a strong candidate for achieving distortion 3, which had also been\nproposed in recent work. We prove that the distortion bound of 3 would follow\nfrom any of three combinatorial conjectures we formulate.", "category": [], "id": "http://arxiv.org/abs/1911.07162v3", "link": "http://arxiv.org/abs/1911.07162v3"}, {"title": "University rankings from the revealed preferences of the applicants", "summary": "A methodology is presented to rank universities on the basis of the lists of\nprogrammes the students applied for. We exploit a crucial feature of the\ncentralised assignment system to higher education in Hungary: a student is\nadmitted to the first programme where the score limit is achieved. This makes\nit possible to derive a partial preference order of each applicant. Our\napproach integrates the information from all students participating in the\nsystem, is free of multicollinearity among the indicators, and contains few ad\nhoc parameters. The procedure is implemented to rank faculties in the Hungarian\nhigher education between 2001 and 2016. We demonstrate that the ranking given\nby the least squares method has favourable theoretical properties, is robust\nwith respect to the aggregation of preferences, and performs well in practice.\nThe suggested ranking is worth considering as a reasonable alternative to the\nstandard composite indices.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.04087v6", "link": "http://dx.doi.org/10.1016/j.ejor.2020.03.008"}, {"title": "A Stochastic Investment Model for South African Use", "summary": "The need for stochastic modelling is on the rise globally in the pension,\nlife insurance and investment industries due to both an increase in regulation\nand a natural requirement for stochastic analysis in modelling exercises.\nResearch in the area of stochastic models or recently called economic scenario\ngenerators for actuarial use in South Africa has largely been limited. The\nseminal papers in this regard have a number of practical limitations. In this\npaper, we propose a stochastic investment model for South Africa by modelling\nprice inflation rates, share dividends, long term and short-term interest rates\nfor the period 1960-2018 and inflation-linked bonds for the period 2000-2018.\nPossible by-directional relations between the economic series have been\nconsidered and the model is designed to provide long-term forecasts that should\nfind application in long-term modelling for both pension funds and life\ninsurance companies.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1912.12113v1", "link": "http://arxiv.org/abs/1912.12113v1"}, {"title": "The string prediction models as an invariants of time series in forex\n  market", "summary": "In this paper we apply a new approach of the string theory to the real\nfinancial market. It is direct extension and application of the work [1] into\nprediction of prices. The models are constructed with an idea of prediction\nmodels based on the string invariants (PMBSI). The performance of PMBSI is\ncompared to support vector machines (SVM) and artificial neural networks (ANN)\non an artificial and a financial time series. Brief overview of the results and\nanalysis is given. The first model is based on the correlation function as\ninvariant and the second one is an application based on the deviations from the\nclosed string/pattern form (PMBCS). We found the difference between these two\napproaches. The first model cannot predict the behavior of the forex market\nwith good efficiency in comparison with the second one which is, in addition,\nable to make relevant profit per year.", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1109.0435v2", "link": "http://dx.doi.org/10.1016/j.physa.2013.07.048"}, {"title": "Multi-agent based analysis of financial data", "summary": "In this work the system of agents is applied to establish a model of the\nnonlinear distributed signal processing. The evolution of the system of the\nagents - by the prediction time scale diversified trend followers, has been\nstudied for the stochastic time-varying environments represented by the real\ncurrency-exchange time series. The time varying population and its statistical\ncharacteristics have been analyzed in the non-interacting and interacting\ncases. The outputs of our analysis are presented in the form of the mean\nlife-times, mean utilities and corresponding distributions. They show that\npopulations are susceptible to the strength and form of inter-agent\ninteraction. We believe that our results will be useful for the development of\nthe robust adaptive prediction systems.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1110.2603v1", "link": "http://arxiv.org/abs/1110.2603v1"}, {"title": "Mortality and Healthcare: a Stochastic Control Analysis under\n  Epstein-Zin Preferences", "summary": "This paper studies optimal consumption, investment, and healthcare spending\nunder Epstein-Zin preferences. Given consumption and healthcare spending plans,\nEpstein-Zin utilities are defined over an agent's random lifetime, partially\ncontrollable by the agent as healthcare reduces mortality growth. To the best\nof our knowledge, this is the first time Epstein-Zin utilities are formulated\non a controllable random horizon, via an infinite-horizon backward stochastic\ndifferential equation with superlinear growth. A new comparison result is\nestablished for the uniqueness of associated utility value processes. In a\nBlack-Scholes market, the stochastic control problem is solved through the\nrelated Hamilton-Jacobi-Bellman (HJB) equation. The verification argument\nfeatures a delicate containment of the growth of the controlled morality\nprocess, which is unique to our framework, relying on a combination of\nprobabilistic arguments and analysis of the HJB equation. In contrast to prior\nwork under time-separable utilities, Epstein-Zin preferences largely facilitate\ncalibration. In four countries we examined, the model-generated mortality\nclosely approximates actual mortality data; moreover, the calibrated efficacy\nof healthcare is in broad agreement with empirical studies on healthcare across\ncountries.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2003.01783v4", "link": "http://arxiv.org/abs/2003.01783v4"}, {"title": "The Wishart short rate model", "summary": "We consider a short rate model, driven by a stochastic process on the cone of\npositive semidefinite matrices. We derive sufficient conditions ensuring that\nthe model replicates normal, inverse or humped yield curves.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1203.5513v2", "link": "http://dx.doi.org/10.1142/S0219024912500562"}, {"title": "American option of stochastic volatility model with negative Fichera\n  function on degenerate boundary", "summary": "In this paper we study a general framework of American put option with\nstochastic volatility whose value function is associated with a 2-dimensional\nparabolic variational inequality with degenerate boundaries. We apply PDE\nmethods to analyze the existences of the strong solution and the properties of\nthe 2-dimensional manifold for the free boundary. Thanks to the regularity\nresult on the solution of the underlying PDE, we can also provide the\nuniqueness of the solution by the argument of the verification theorem together\nwith the generalized Ito's formula even though the solution may not be second\norder differentiable in the space variable across the free boundary.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1306.0345v1", "link": "http://arxiv.org/abs/1306.0345v1"}, {"title": "Using String Invariants for Prediction Searching for Optimal Parameters", "summary": "We have developed a novel prediction method based on string invariants. The\nmethod does not require learning but a small set of parameters must be set to\nachieve optimal performance. We have implemented an evolutionary algorithm for\nthe parametric optimization. We have tested the performance of the method on\nartificial and real world data and compared the performance to statistical\nmethods and to a number of artificial intelligence methods. We have used data\nand the results of a prediction competition as a benchmark. The results show\nthat the method performs well in single step prediction but the methods\nperformance for multiple step prediction needs to be improved. The method works\nwell for a wide range of parameters.", "category": ["q-fin.ST", "q-fin.CP", "q-fin.MF"], "id": "http://arxiv.org/abs/1606.06003v1", "link": "http://dx.doi.org/10.1016/j.physa.2015.10.050"}, {"title": "A Theory of Market Efficiency", "summary": "We introduce a mathematical theory called market connectivity that gives\nconcrete ways to both measure the efficiency of markets and find inefficiencies\nin large markets. The theory leads to new methods for testing the famous\nefficient markets hypothesis that do not suffer from the joint-hypothesis\nproblem that has plagued past work. Our theory suggests metrics that can be\nused to compare the efficiency of one market with another, to find\ninefficiencies that may be profitable to exploit, and to evaluate the impact of\npolicy and regulations on market efficiency.\n  A market's efficiency is tied to its ability to communicate information\nrelevant to market participants. Market connectivity calculates the speed and\nreliability with which this communication is carried out via trade in the\nmarket. We model the market by a network called the trade network, which can be\ncomputed by recording transactions in the market over a fixed interval of time.\nThe nodes of the network correspond to participants in the market. Every pair\nof nodes that trades in the market is connected by an edge that is weighted by\nthe rate of trade, and associated with a vector that represents the type of\nitem that is bought or sold.\n  We evaluate the ability of the market to communicate by considering how it\ndeals with shocks. A shock is a change in the beliefs of market participants\nabout the value of the products that they trade. We compute the effect of every\npotential significant shock on trade in the market. We give mathematical\ndefinitions for a few concepts that measure the ability of the market to\neffectively dissipate shocks.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1702.03290v1", "link": "http://arxiv.org/abs/1702.03290v1"}, {"title": "On clustering financial time series: a need for distances between\n  dependent random variables", "summary": "The following working document summarizes our work on the clustering of\nfinancial time series. It was written for a workshop on information geometry\nand its application for image and signal processing. This workshop brought\nseveral experts in pure and applied mathematics together with applied\nresearchers from medical imaging, radar signal processing and finance. The\nauthors belong to the latter group. This document was written as a long\nintroduction to further development of geometric tools in financial\napplications such as risk or portfolio analysis. Indeed, risk and portfolio\nanalysis essentially rely on covariance matrices. Besides that the Gaussian\nassumption is known to be inaccurate, covariance matrices are difficult to\nestimate from empirical data. To filter noise from the empirical estimate,\nMantegna proposed using hierarchical clustering. In this work, we first show\nthat this procedure is statistically consistent. Then, we propose to use\nclustering with a much broader application than the filtering of empirical\ncovariance matrices from the estimate correlation coefficients. To be able to\ndo that, we need to obtain distances between the financial time series that\nincorporate all the available information in these cross-dependent random\nprocesses.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1603.07822v1", "link": "http://arxiv.org/abs/1603.07822v1"}, {"title": "Ownership Cost Calculations for Distributed Energy Resources Using\n  Uncertainty and Risk Analyses", "summary": "Ownership cost calculation plays an important role in optimal operation of\ndistributed energy resources (DERs) and microgrids (MGs) in the future power\nsystem, known as smart grid. In this paper, a general framework for ownership\ncost calculation is proposed using uncertainty and risk analyses. Four\nownership cost calculation approaches are introduced and compared based on\ntheir associated risk values. Finally, the best method is chosen based on a\nseries of simulation results, performed for a typical diesel generator (DiG).\nAlthough simulation results are given for a DiG (as commonly used in MGs), the\nproposed approaches can be applied to other MG components, such as batteries,\nwith slight modifications, as presented in this paper. The analyses and\nproposed approaches can be useful in MG optimal design, optimal power flow, and\nmarket-based operation of the smart grid for accurate operational cost\ncalculations.", "category": ["q-fin.EC", "q-fin.ST"], "id": "http://arxiv.org/abs/1709.08023v1", "link": "http://arxiv.org/abs/1709.08023v1"}, {"title": "Behavioural effects on XVA", "summary": "Bank behaviour is important for pricing XVA because it links different\ncounterparties and thus breaks the usual XVA pricing assumption of counterparty\nindependence. Consider a typical case of a bank hedging a client trade via a\nCCP. On client default the hedge (effects) will be removed (rebalanced). On the\nother hand, if the hedge counterparty defaults the hedge will be replaced. Thus\nif the hedge required initial margin then the default probability driving MVA\nis from the client not from the hedge counterparty. This is the opposite of\nusual assumptions where counterparty XVAs are computed independent of each\nother. Replacement of the hedge counterparty means multiple CVA costs on the\nhedge side need inclusion. Since hedge trades are generally at riskless mid (or\nworse) these costs are paid on the client side, and must be calculated before\nthe replacement hedge counterparties are known. We call these counterparties\nanonymous counterparties. The effects on CVA and MVA will generally be\nexclusive because MVA largely removes CVA, and CVA is hardly relevant for CCPs.\nEffects on KVA and FVA will resemble those on MVA. We provide a theoretical\nframework, including anonymous counterparties, and numerical examples. Pricing\nXVA by considering counterparties in isolation is inadequate and behaviour must\nbe taken into account.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1803.03477v1", "link": "http://arxiv.org/abs/1803.03477v1"}, {"title": "Dimension Reduction in Statistical Estimation of Partially Observed\n  Multiscale Processes", "summary": "We consider partially observed multiscale diffusion models that are specified\nup to an unknown vector parameter. We establish for a very general class of\ntest functions that the filter of the original model converges to a filter of\nreduced dimension. Then, this result is used to justify statistical estimation\nfor the unknown parameters of interest based on the model of reduced dimension\nbut using the original available data. This allows to learn the unknown\nparameters of interest while working in lower dimensions, as opposed to working\nwith the original high dimensional system. Simulation studies support and\nillustrate the theoretical results.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1607.06158v3", "link": "http://arxiv.org/abs/1607.06158v3"}, {"title": "Stochastic Comparative Statics in Markov Decision Processes", "summary": "In multi-period stochastic optimization problems, the future optimal decision\nis a random variable whose distribution depends on the parameters of the\noptimization problem. We analyze how the expected value of this random variable\nchanges as a function of the dynamic optimization parameters in the context of\nMarkov decision processes. We call this analysis \\emph{stochastic comparative\nstatics}. We derive both \\emph{comparative statics} results and\n\\emph{stochastic comparative statics} results showing how the current and\nfuture optimal decisions change in response to changes in the single-period\npayoff function, the discount factor, the initial state of the system, and the\ntransition probability function. We apply our results to various models from\nthe economics and operations research literature, including investment theory,\ndynamic pricing models, controlled random walks, and comparisons of stationary\ndistributions.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1904.05481v2", "link": "http://arxiv.org/abs/1904.05481v2"}, {"title": "Portfolio Optimization managing Value at Risk under heavy tail\n  distribution", "summary": "We consider an investor, whose portfolio consists of a single risky asset and\na risk free asset, who wants to maximize his expected utility of the portfolio\nsubject to managing the Value at Risk (VaR) assuming a heavy tailed\ndistribution of the stock prices return. We use a stochastic maximum principle\nto formulate the dynamic optimisation problem. The equations which we obtain\ndoes not have any explicit analytical solution, so we look for accurate\napproximations to estimate the value function and optimal strategy. As our\ncalibration strategy is non-parametric in nature, no prior knowledge on the\nform of the distribution function is needed. We also provide detailed empirical\nillustration using real life data. Our results show close concordance with\nfinancial intuition.We expect that our results will add to the arsenal of the\nhigh frequency traders.", "category": ["q-fin.PM", "q-fin.MF"], "id": "http://arxiv.org/abs/1908.03905v1", "link": "http://arxiv.org/abs/1908.03905v1"}, {"title": "Universality in DAX index returns fluctuations", "summary": "In terms of the stock exchange returns, we compute the analytic expression of\nthe probability distributions F{DAX,+} and F{DAX,-} of the normalized positive\nand negative DAX (Germany) index daily returns r(t). Furthermore, we define the\nalpha re-scaled DAX daily index positive returns r(t)^alpha and negative\nreturns (-r(t))^alpha that we call, after normalization, the alpha positive\nfluctuations and alpha negative fluctuations. We use the Kolmogorov-Smirnov\nstatistical test, as a method, to find the values of alpha that optimize the\ndata collapse of the histogram of the alpha fluctuations with the\nBramwell-Holdsworth-Pinton (BHP) probability density function. The optimal\nparameters that we found are alpha+=0.50 and alpha-=0.48. Since the BHP\nprobability density function appears in several other dissimilar phenomena, our\nresults reveal universality in the stock exchange markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1004.1136v2", "link": "http://arxiv.org/abs/1004.1136v2"}, {"title": "An adjoint method for the exact calibration of Stochastic Local\n  Volatility models", "summary": "This paper deals with the exact calibration of semidiscretized stochastic\nlocal volatility (SLV) models to their underlying semidiscretized local\nvolatility (LV) models. Under an SLV model, it is common to approximate the\nfair value of European-style options by semidiscretizing the backward\nKolmogorov equation using finite differences. In the present paper we introduce\nan adjoint semidiscretization of the corresponding forward Kolmogorov equation.\nThis adjoint semidiscretization is used to obtain an expression for the\nleverage function in the pertinent SLV model such that the approximated fair\nvalues defined by the LV and SLV models are identical for non-path-dependent\nEuropean-style options. In order to employ this expression, a large non-linear\nsystem of ODEs needs to be solved. The actual numerical calibration is\nperformed by combining ADI time stepping with an inner iteration to handle the\nnon-linearity. Ample numerical experiments are presented that illustrate the\neffectiveness of the calibration procedure.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1609.00232v1", "link": "http://arxiv.org/abs/1609.00232v1"}, {"title": "A Flexible Galerkin Scheme for Option Pricing in L\u00e9vy Models", "summary": "One popular approach to option pricing in L\\'evy models is through solving\nthe related partial integro differential equation (PIDE). For the numerical\nsolution of such equations powerful Galerkin methods have been put forward e.g.\nby Hilber et al. (2013). As in practice large classes of models are maintained\nsimultaneously, flexibility in the driving L\\'evy model is crucial for the\nimplementation of these powerful tools. In this article we provide such a\nflexible finite element Galerkin method. To this end we exploit the Fourier\nrepresentation of the infinitesimal generator, i.e. the related symbol, which\nis explicitly available for the most relevant L\\'evy models. Empirical studies\nfor the Merton, NIG and CGMY model confirm the numerical feasibility of the\nmethod.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1603.08216v1", "link": "http://arxiv.org/abs/1603.08216v1"}, {"title": "Stochastic impulse control on optimal execution with price impact and\n  transaction cost", "summary": "We study a single risky financial asset model subject to price impact and\ntransaction cost over an finite time horizon. An investor needs to execute a\nlong position in the asset affecting the price of the asset and possibly\nincurring in fixed transaction cost. The objective is to maximize the\ndiscounted revenue obtained by this transaction. This problem is formulated as\nan impulse control problem and we characterize the value function using the\nviscosity solutions framework. We establish an associated optimal stopping\nproblem that provides bounds and in some cases the solution of the value\nfunction.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1103.3482v4", "link": "http://arxiv.org/abs/1103.3482v4"}, {"title": "How local in time is the no-arbitrage property under capital gains taxes\n  ?", "summary": "In frictionless financial markets, no-arbitrage is a local property in time.\nThis means that a discrete time model is arbitrage-free if and only if there\ndoes not exist a one-period-arbitrage. With capital gains taxes, this\nequivalence fails. For a model with a linear tax and one non-shortable risky\nstock, we introduce the concept of robust local no-arbitrage (RLNA) as the\nweakest local condition which guarantees dynamic no-arbitrage. Under a sharp\ndichotomy condition, we prove (RLNA). Since no-one-period-arbitrage is\nnecessary for no-arbitrage, the latter is sandwiched between two local\nconditions, which allows us to estimate its non-locality.\n  Furthermore, we construct a stock price process such that two long positions\nin the same stock hedge each other. This puzzling phenomenon that cannot occur\nin arbitrage-free frictionless markets (or markets with proportional\ntransaction costs) is used to show that no-arbitrage alone does not imply the\nexistence of an equivalent separating measure if the probability space is\ninfinite.\n  Finally, we show that the model with a linear tax on capital gains can be\nwritten as a model with proportional transaction costs by introducing several\nfictitious securities.", "category": ["q-fin.PM", "math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1802.06386v3", "link": "http://arxiv.org/abs/1802.06386v3"}, {"title": "Volatility Spillovers and Heavy Tails: A Large t-Vector AutoRegressive\n  Approach", "summary": "Volatility is a key measure of risk in financial analysis. The high\nvolatility of one financial asset today could affect the volatility of another\nasset tomorrow. These lagged effects among volatilities - which we call\nvolatility spillovers - are studied using the Vector AutoRegressive (VAR)\nmodel. We account for the possible fat-tailed distribution of the VAR model\nerrors using a VAR model with errors following a multivariate Student\nt-distribution with unknown degrees of freedom. Moreover, we study volatility\nspillovers among a large number of assets. To this end, we use penalized\nestimation of the VAR model with t-distributed errors. We study volatility\nspillovers among energy, biofuel and agricultural commodities and reveal\nbidirectional volatility spillovers between energy and biofuel, and between\nenergy and agricultural commodities.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1708.02073v1", "link": "http://arxiv.org/abs/1708.02073v1"}, {"title": "Strict strategy-proofness", "summary": "A strictly strategy-proof mechanism is one that asks agents to use strictly\ndominant strategies. In the canonical one-dimensional mechanism design setting\nwith private values, we show that strict strategy-proofness is equivalent to\nstrict monotonicity plus the envelope formula, echoing a well-known\ncharacterisation of (weak) strategy-proofness. A consequence is that\nstrategy-proofness can be made strict by an arbitrarily small modification, so\nthat strictness is 'essentially for free'.", "category": [], "id": "http://arxiv.org/abs/1807.11864v1", "link": "http://arxiv.org/abs/1807.11864v1"}, {"title": "Reserve Requirement Analysis using a Dynamical System of a Bank based on\n  Monti-Klein model of Bank's Profit Function", "summary": "Commercial banks and other depository institutions in some countries are\nrequired to hold in reserve against deposits made by their customers at their\nCentral Bank or Federal Reserve. Although some countries have been eliminated\nit, this requirement is useful as one of many Central Bank's regulation made to\ncontrol rate of inflation and conditions of excess liquidity in banks which\ncould affect the monetary stability. The amount of this reserve is affected by\nthe volumes of the commercial bank's loan and deposit, and also by the bank's\nLoan to Deposit Ratio (LDR) value. In this research, a dynamical system of the\nvolume of deposits (dD/dt) and loans (dL/dt) of a bank is constructed from the\nbank profit equation by Monti-Klein. The model is implemented using the\nregulation of Bank of Indonesia, and analysed in terms of the behaviour of the\nsolution. Based on some simplifying assumptions in this model, the results show\nthat eventhough the LDR values at the initial points of two solutions are the\nsame, the behavior of solutions will be significantly different due to\ndifferent magnitude of L and D volumes.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1306.0468v1", "link": "http://arxiv.org/abs/1306.0468v1"}, {"title": "Linear Credit Risk Models", "summary": "We introduce a novel class of credit risk models in which the drift of the\nsurvival process of a firm is a linear function of the factors. The prices of\ndefaultable bonds and credit default swaps (CDS) are linear-rational in the\nfactors. The price of a CDS option can be uniformly approximated by polynomials\nin the factors. Multi-name models can produce simultaneous defaults, generate\npositively as well as negatively correlated default intensities, and\naccommodate stochastic interest rates. A calibration study illustrates the\nversatility of these models by fitting CDS spread time series. A numerical\nanalysis validates the efficiency of the option price approximation method.", "category": ["q-fin.MF", "q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1605.07419v4", "link": "http://arxiv.org/abs/1605.07419v4"}, {"title": "A Practical Approach to Social Learning", "summary": "Models of social learning feature either binary signals or abstract signal\nstructures often deprived of micro-foundations. Both models are limited when\nanalyzing interim results or performing empirical analysis. We present a method\nof generating signal structures which are richer than the binary model, yet are\ntractable enough to perform simulations and empirical analysis. We demonstrate\nthe method's usability by revisiting two classical papers: (1) we discuss the\neconomic significance of unbounded signals Smith and Sorensen (2000); (2) we\nuse experimental data from Anderson and Holt (1997) to perform econometric\nanalysis. Additionally, we provide a necessary and sufficient condition for the\noccurrence of action cascades.", "category": ["econ.EM", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.11017v1", "link": "http://arxiv.org/abs/2002.11017v1"}, {"title": "Modulated Information Flows in Financial Markets", "summary": "We model continuous-time information flows generated by a number of\ninformation sources that switch on and off at random times. By modulating a\nmulti-dimensional L\\'evy random bridge over a random point field, our framework\nrelates the discovery of relevant new information sources to jumps in\nconditional expectation martingales. In the canonical Brownian random bridge\ncase, we show that the underlying measure-valued process follows jump-diffusion\ndynamics, where the jumps are governed by information switches. The dynamic\nrepresentation gives rise to a set of stochastically-linked Brownian motions on\nrandom time intervals that capture evolving information states, as well as to a\nstate-dependent stochastic volatility evolution with jumps. The nature of\ninformation flows usually exhibits complex behaviour, however, we maintain\nanalytic tractability by introducing what we term the effective and\ncomplementary information processes, which dynamically incorporate active and\ninactive information, respectively. As an application, we price a financial\nvanilla option, which we prove is expressed by a weighted sum of option values\nbased on the possible state configurations at expiry. This result may be viewed\nas an information-based analogue of Merton's option price, but where\njump-diffusion arises endogenously. The proposed information flows also lend\nthemselves to the quantification of asymmetric informational advantage among\ncompetitive agents, a feature we analyse by notions of information geometry.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1708.06948v3", "link": "http://arxiv.org/abs/1708.06948v3"}, {"title": "On small-noise equations with degenerate limiting system arising from\n  volatility models", "summary": "The one-dimensional SDE with non Lipschitz diffusion coefficient $dX_{t} =\nb(X_{t})dt + \\sigma X_{t}^{\\gamma} dB_{t}, \\ X_{0}=x, \\ \\gamma<1$ is widely\nstudied in mathematical finance. Several works have proposed asymptotic\nanalysis of densities and implied volatilities in models involving instances of\nthis equation, based on a careful implementation of saddle-point methods and\n(essentially) the explicit knowledge of Fourier transforms. Recent research on\ntail asymptotics for heat kernels [J-D. Deuschel, P.~Friz, A.~Jacquier, and\nS.~Violante. Marginal density expansions for diffusions and stochastic\nvolatility, part II: Applications. 2013, arxiv:1305.6765] suggests to work with\nthe rescaled variable $X^{\\varepsilon}:=\\varepsilon^{1/(1-\\gamma)} X$: while\nallowing to turn a space asymptotic problem into a small-$\\varepsilon$ problem\nwith fixed terminal point, the process $X^{\\varepsilon}$ satisfies a SDE in\nWentzell--Freidlin form (i.e. with driving noise $\\varepsilon dB$). We prove a\npathwise large deviation principle for the process $X^{\\varepsilon}$ as\n$\\varepsilon \\to 0$. As it will become clear, the limiting ODE governing the\nlarge deviations admits infinitely many solutions, a non-standard situation in\nthe Wentzell--Freidlin theory. As for applications, the $\\varepsilon$-scaling\nallows to derive exact log-asymptotics for path functionals of the process:\nwhile on the one hand the resulting formulae are confirmed by the CIR-CEV\nbenchmarks, on the other hand the large deviation approach (i) applies to\nequations with a more general drift term and (ii) potentially opens the way to\nheat kernel analysis for higher-dimensional diffusions involving such an SDE as\na component.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1404.4464v2", "link": "http://arxiv.org/abs/1404.4464v2"}, {"title": "Information Token Driven Machine Learning for Electronic Markets:\n  Performance Effects in Behavioral Financial Big Data Analytics", "summary": "Conjunct with the universal acceleration in information growth, financial\nservices have been immersed in an evolution of information dynamics. It is not\njust the dramatic increase in volumes of data, but the speed, the complexity\nand the unpredictability of big-data phenomena that have compounded the\nchallenges faced by researchers and practitioners in financial services. Math,\nstatistics and technology have been leveraged creatively to create analytical\nsolutions. Given the many unique characteristics of financial bid data (FBD) it\nis necessary to gain insights into strategies and models that can be used to\ncreate FBD specific solutions. Behavioral finance data, a subset of FBD, is\nseeing exponential growth and this presents an unprecedented opportunity to\nstudy behavioral finance employing big data analytics methodologies. The\npresent study maps machine learning (ML) techniques and behavioral finance\ncategories to explore the potential for using ML techniques to address\nbehavioral aspects in FBD. The ontological feasibility of such an approach is\npresented and the primary purpose of this study is propositioned- ML based\nbehavioral models can effectively estimate performance in FBD. A simple machine\nlearning algorithm is successfully employed to study behavioral performance in\nan artificial stock market to validate the propositions.\n  Keywords: Information; Big Data; Electronic Markets; Analytics; Behavior", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2004.06642v1", "link": "http://dx.doi.org/10.4301/s1807-17752017000300005"}, {"title": "Market selection with learning and catching up with the Joneses", "summary": "We study the market selection hypothesis in complete financial markets,\npopulated by heterogeneous agents. We allow for a rich structure of\nheterogeneity: individuals may differ in their beliefs concerning the economy,\ninformation and learning mechanism, risk aversion, impatience and 'catching up\nwith Joneses' preferences. We develop new techniques for studying the long-run\nbehavior of such economies, based on the Strassen's functional law of iterated\nlogarithm. In particular, we explicitly determine an agent's survival index and\nshow how the latter depends on the agent's characteristics. We use these\nresults to study the long-run behavior of the equilibrium interest rate and the\nmarket price of risk.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1106.3025v3", "link": "http://arxiv.org/abs/1106.3025v3"}, {"title": "Cross-sectional Learning of Extremal Dependence among Financial Assets", "summary": "We propose a novel probabilistic model to facilitate the learning of\nmultivariate tail dependence of multiple financial assets. Our method allows\none to construct from known random vectors, e.g., standard normal,\nsophisticated joint heavy-tailed random vectors featuring not only distinct\nmarginal tail heaviness, but also flexible tail dependence structure. The\nnovelty lies in that pairwise tail dependence between any two dimensions is\nmodeled separately from their correlation, and can vary respectively according\nto its own parameter rather than the correlation parameter, which is an\nessential advantage over many commonly used methods such as multivariate $t$ or\nelliptical distribution. It is also intuitive to interpret, easy to track, and\nsimple to sample comparing to the copula approach. We show its flexible tail\ndependence structure through simulation. Coupled with a GARCH model to\neliminate serial dependence of each individual asset return series, we use this\nnovel method to model and forecast multivariate conditional distribution of\nstock returns, and obtain notable performance improvements in multi-dimensional\ncoverage tests. Besides, our empirical finding about the asymmetry of tails of\nthe idiosyncratic component as well as the market component is interesting and\nworth to be well studied in the future.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1905.13425v3", "link": "http://arxiv.org/abs/1905.13425v3"}, {"title": "The dynamics of traded value revisited", "summary": "We conclude from an analysis of high resolution NYSE data that the\ndistribution of the traded value $f_i$ (or volume) has a finite variance\n$\\sigma_i$ for the very large majority of stocks $i$, and the distribution\nitself is non-universal across stocks. The Hurst exponent of the same time\nseries displays a crossover from weakly to strongly correlated behavior around\nthe time scale of 1 day. The persistence in the strongly correlated regime\nincreases with the average trading activity $\\ev{f_i}$ as\n$H_i=H_0+\\gamma\\log\\ev{f_i}$, which is another sign of non-universal behavior.\nThe existence of such liquidity dependent correlations is consistent with the\nempirical observation that $\\sigma_i\\propto\\ev{f_i}^\\alpha$, where $\\alpha$ is\na non-trivial, time scale dependent exponent.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0608018v3", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.009"}, {"title": "Unfolding the innovation system for the development of countries:\n  co-evolution of Science, Technology and Production", "summary": "We show that the space in which scientific, technological and economic\ndevelopments interplay with each other can be mathematically shaped using\npioneering multilayer network and complexity techniques. We build the\ntri-layered network of human activities (scientific production, patenting, and\nindustrial production) and study the interactions among them, also taking into\naccount the possible time delays. Within this construction we can identify\nwhich capabilities and prerequisites are needed to be competitive in a given\nactivity, and even measure how much time is needed to transform, for instance,\nthe technological know-how into economic wealth and scientific innovation,\nbeing able to make predictions with a very long time horizon. Quite\nunexpectedly, we find empirical evidence that the naive knowledge flow from\nscience, to patents, to products is not supported by data, being instead\ntechnology the best predictor for industrial and scientific production for the\nnext decades.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1707.05146v3", "link": "http://dx.doi.org/10.1038/s41598-019-52767-5"}, {"title": "A tale of two sentiment scales: Disentangling short-run and long-run\n  components in multivariate sentiment dynamics", "summary": "We propose a novel approach to sentiment data filtering for a portfolio of\nassets. In our framework, a dynamic factor model drives the evolution of the\nobserved sentiment and allows to identify two distinct components: a long-term\ncomponent, modeled as a random walk, and a short-term component driven by a\nstationary VAR(1) process. Our model encompasses alternative approaches\navailable in literature and can be readily estimated by means of Kalman\nfiltering and expectation maximization. This feature makes it convenient when\nthe cross-sectional dimension of the sentiment increases. By applying the model\nto a portfolio of Dow Jones stocks, we find that the long term component\nco-integrates with the market principal factor, while the short term one\ncaptures transient swings of the market associated with the idiosyncratic\ncomponents and captures the correlation structure of returns. Finally, using\nquantile regressions, we assess the significance of the contemporaneous and\nlagged explanatory power of sentiment on returns finding strong statistical\nevidence when extreme returns, especially negative ones, are considered.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1910.01407v2", "link": "http://arxiv.org/abs/1910.01407v2"}, {"title": "The demise of constant price impact functions and single-time step\n  models of speculation", "summary": "Constant and symmetric price impact functions, most commonly used in\nagent-based market modelling, are shown to give rise to paradoxical and\ninconsistent outcomes in the simplest case of arbitrage exploitation when\nopen-hold-close actions are considered. The solution of the paradox lies in the\nnon-constant nature of real-life price impact functions. A simple model that\nincludes explicit position opening, holding, and closing is briefly introduced\nand its information ecology discussed, shedding new light on the relevance of\nthe Minority Game to the study of financial markets.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/physics/0608013v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.03.049"}, {"title": "Survivability and centrality measures for networks of financial market\n  indices", "summary": "Using data from 92 indices of stock exchanges worldwide, I analize the\ncluster formation and evolution from 2007 to 2010, which includes the Subprime\nMortgage Crisis of 2008, using asset graphs based on distance thresholds. I\nalso study the survivability of connections and of clusters through time and\nthe influence of noise in centrality measures applied to the networks of\nfinancial indices.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1201.4490v1", "link": "http://arxiv.org/abs/1201.4490v1"}, {"title": "The unfair consequences of equal opportunities: comparing exchange\n  models of wealth distribution", "summary": "Simple agent based exchange models are a commonplace in the study of wealth\ndistribution of artificial societies. Generally, each agent is characterized by\nits wealth and by a risk-aversion factor, and random exchanges between agents\nallow for a redistribution of the wealth. However, the detailed influence of\nthe amount of capital exchanged has not been fully analyzed yet. Here we\npresent a comparison of two exchange rules and also a systematic study of the\ntime evolution of the wealth distribution, its functional dependence, the Gini\ncoefficient and time correlation functions. In many cases a stable state is\nattained, but, interesting, some particular cases are found in which a very\nslow dynamics develops. Finally, we observe that the time evolution and the\nfinal wealth distribution are strongly dependent on the exchange rules in a\nnontrivial way.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/nlin/0611001v1", "link": "http://dx.doi.org/10.1140/epjst/e2007-00072-4"}, {"title": "Second Order Risk", "summary": "Managing a portfolio to a risk model can tilt the portfolio toward weaknesses\nof the model. As a result, the optimized portfolio acquires downside exposure\nto uncertainty in the model itself, what we call \"second order risk.\" We\npropose a risk measure that accounts for this bias. Studies of real portfolios,\nin asset-by-asset and factor model contexts, demonstrate that second order risk\ncontributes significantly to realized volatility, and that the proposed measure\naccurately forecasts the out-of-sample behavior of optimized portfolios.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/0908.2455v1", "link": "http://arxiv.org/abs/0908.2455v1"}, {"title": "Spontaneous centralization of control in a network of company ownerships", "summary": "We introduce a model for the adaptive evolution of a network of company\nownerships. In a recent work it has been shown that the empirical global\nnetwork of corporate control is marked by a central, tightly connected \"core\"\nmade of a small number of large companies which control a significant part of\nthe global economy. Here we show how a simple, adaptive \"rich get richer\"\ndynamics can account for this characteristic, which incorporates the increased\nbuying power of more influential companies, and in turn results in even higher\ncontrol. We conclude that this kind of centralized structure can emerge without\nit being an explicit goal of these companies, or as a result of a\nwell-organized strategy.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1306.3422v1", "link": "http://dx.doi.org/10.1371/journal.pone.0080303"}, {"title": "Control-stopping Games for Market Microstructure and Beyond", "summary": "In this paper, we present a family of a control-stopping games which arise\nnaturally in equilibrium-based models of market microstructure, as well as in\nother models with strategic buyers and sellers. A distinctive feature of this\nfamily of games is the fact that the agents do not have any exogenously given\nfundamental value for the asset, and they deduce the value of their position\nfrom the bid and ask prices posted by other agents (i.e. they are pure\nspeculators). As a result, in such a game, the reward function of each agent,\nat the time of stopping, depends directly on the controls of other players. The\nequilibrium problem leads naturally to a system of coupled control-stopping\nproblems (or, equivalently, Reflected Backward Stochastic Differential\nEquations (RBSDEs)), in which the individual reward functions (or, reflecting\nbarriers) depend on the value functions (or, solution components) of other\nagents. The resulting system, in general, presents multiple mathematical\nchallenges due to the non-standard form of coupling (or, reflection). In the\npresent case, this system is also complicated by the fact that the continuous\ncontrols of the agents, describing their posted bid and ask prices, are\nconstrained to take values in a discrete grid. The latter feature reflects the\npresence of a positive tick size in the market, and it creates additional\ndiscontinuities in the agents reward functions (or, reflecting barriers).\nHerein, we prove the existence of a solution to the associated system in a\nspecial Markovian framework, provide numerical examples, and discuss the\npotential applications.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1708.00506v2", "link": "http://arxiv.org/abs/1708.00506v2"}, {"title": "A game-theoretic derivation of the $\\sqrt{dt}$ effect", "summary": "We study the origins of the $\\sqrt{dt}$ effect in finance and SDE. In\nparticular, we show, in the game-theoretic framework, that market volatility is\na consequence of the absence of riskless opportunities for making money and\nthat too high volatility is also incompatible with such opportunities. More\nprecisely, riskless opportunities for making money arise whenever a traded\nsecurity has fractal dimension below or above that of the Brownian motion and\nits price is not almost constant and does not become extremely large. This is a\nsimple observation known in the measure-theoretic mathematical finance. At the\nend of the article we also consider the case of non-zero interest rate.\n  This version of the article was essentially written in March 2005 but remains\na working paper.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1802.01219v1", "link": "http://arxiv.org/abs/1802.01219v1"}, {"title": "Information geometries and Microeconomic Theories", "summary": "More than thirty years ago, Charnes, Cooper and Schinnar (1976) established\nan enlightening contact between economic production functions (EPFs) -- a\ncornerstone of neoclassical economics -- and information theory, showing how a\ngeneralization of the Cobb-Douglas production function encodes homogeneous\nfunctions.\n  As expected by Charnes \\textit{et al.}, the contact turns out to be much\nbroader: we show how information geometry as pioneered by Amari and others\nunderpins static and dynamic descriptions of microeconomic cornerstones.\n  We show that the most popular EPFs are fundamentally grounded in a very weak\naxiomatization of economic transition costs between inputs. The strength of\nthis characterization is surprising, as it geometrically bonds altogether a\nwealth of collateral economic notions\n  -- advocating for applications in various economic fields --: among all, it\ncharacterizes (i) Marshallian and Hicksian demands and their geometric duality,\n(ii) Slutsky-type properties for the transformation paths, (iii) Roy-type\nproperties for their elementary variations.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0901.2586v1", "link": "http://arxiv.org/abs/0901.2586v1"}, {"title": "Regression to the Mean's Impact on the Synthetic Control Method: Bias\n  and Sensitivity Analysis", "summary": "To make informed policy recommendations from observational data, we must be\nable to discern true treatment effects from random noise and effects due to\nconfounding. Difference-in-Difference techniques which match treated units to\ncontrol units based on pre-treatment outcomes, such as the synthetic control\napproach have been presented as principled methods to account for confounding.\nHowever, we show that use of synthetic controls or other matching procedures\ncan introduce regression to the mean (RTM) bias into estimates of the average\ntreatment effect on the treated. Through simulations, we show RTM bias can lead\nto inflated type I error rates as well as decreased power in typical policy\nevaluation settings. Further, we provide a novel correction for RTM bias which\ncan reduce bias and attain appropriate type I error rates. This correction can\nbe used to perform a sensitivity analysis which determines how results may be\naffected by RTM. We use our proposed correction and sensitivity analysis to\nreanalyze data concerning the effects of California's Proposition 99, a\nlarge-scale tobacco control program, on statewide smoking rates.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.04706v1", "link": "http://arxiv.org/abs/1909.04706v1"}, {"title": "Testing the performance of technical trading rules in the Chinese market", "summary": "Technical trading rules have a long history of being used by practitioners in\nfinancial markets. Their profitable ability and efficiency of technical trading\nrules are yet controversial. In this paper, we test the performance of more\nthan seven thousands traditional technical trading rules on the Shanghai\nSecurities Composite Index (SSCI) from May 21, 1992 through June 30, 2013 and\nShanghai Shenzhen 300 Index (SHSZ 300) from April 8, 2005 through June 30, 2013\nto check whether an effective trading strategy could be found by using the\nperformance measurements based on the return and Sharpe ratio. To correct for\nthe influence of the data-snooping effect, we adopt the Superior Predictive\nAbility test to evaluate if there exists a trading rule that can significantly\noutperform the benchmark. The result shows that for SSCI, technical trading\nrules offer significant profitability, while for SHSZ 300, this ability is\nlost. We further partition the SSCI into two sub-series and find that the\nefficiency of technical trading in sub-series, which have exactly the same\nspanning period as that of SHSZ 300, is severely weakened. By testing the\ntrading rules on both indexes with a five-year moving window, we find that the\nfinancial bubble from 2005 to 2007 greatly improve the effectiveness of\ntechnical trading rules. This is consistent with the predictive ability of\ntechnical trading rules which appears when the market is less efficient.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1504.06397v1", "link": "http://dx.doi.org/10.1016/j.physa.2015.07.029"}, {"title": "The first passage event for sums of dependent L\u00e9vy processes with\n  applications to insurance risk", "summary": "For the sum process $X=X^1+X^2$ of a bivariate L\\'evy process $(X^1,X^2)$\nwith possibly dependent components, we derive a quintuple law describing the\nfirst upwards passage event of $X$ over a fixed barrier, caused by a jump, by\nthe joint distribution of five quantities: the time relative to the time of the\nprevious maximum, the time of the previous maximum, the overshoot, the\nundershoot and the undershoot of the previous maximum. The dependence between\nthe jumps of $X^1$ and $X^2$ is modeled by a L\\'evy copula. We calculate these\nquantities for some examples, where we pay particular attention to the\ninfluence of the dependence structure. We apply our findings to the ruin event\nof an insurance risk process.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/0912.1925v1", "link": "http://dx.doi.org/10.1214/09-AAP601"}, {"title": "A stochastic Stefan-type problem under first-order boundary conditions", "summary": "Moving boundary problems allow to model systems with phase transition at an\ninner boundary. Driven by problems in economics and finance, in particular\nmodeling of limit order books, we consider a stochastic and non-linear\nextension of the classical Stefan-problem in one space dimension, where the\npaths of the moving interface might have unbounded variation. Working on the\ndistribution space, Ito-Wentzell formula for SPDEs allows to transform these\nmoving boundary problems into partial differential equations on fixed domains.\nRewriting the equations into the framework of stochastic evolution equations,\nwe apply results based on stochastic maximal $L^p$-regularity to obtain\nexistence, uniqueness and regularity of local solutions. Moreover, we observe\nthat explosion might take place due to the boundary interaction even when the\ncoefficients of the original problem have linear growths.", "category": ["math.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/1601.03968v3", "link": "http://dx.doi.org/10.1214/17-AAP1359"}, {"title": "The Shapley Value of Digraph Games", "summary": "In this paper the Shapley value of digraph (directed graph) games are\nconsidered. Digraph games are transferable utility (TU) games with limited\ncooperation among players, where players are represented by nodes. A\nrestrictive relation between two adjacent players is established by a directed\nline segment. Directed path, connecting the initial player with the terminal\nplayer, form the coalition among players. A dominance relation is established\nbetween players and this relation determines whether or not a player wants to\ncooperate. To cooperate, we assume that a player joins a coalition where he/she\nis not dominated by any other players.The Shapley value is defined as the\naverage of marginal contribution vectors corresponding to all permutations that\ndo not violate the subordination of players. The Shapley value for cyclic\ndigraph games is calculated and analyzed. For a given family of characteristic\nfunctions, a quick way to calculate Shapley values is formulated.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1701.01677v2", "link": "http://arxiv.org/abs/1701.01677v2"}, {"title": "Financial equilibrium with asymmetric information and random horizon", "summary": "We study in detail and explicitly solve the version of Kyle's model\nintroduced in a specific case in \\cite{BB}, where the trading horizon is given\nby an exponentially distributed random time. The first part of the paper is\ndevoted to the analysis of time-homogeneous equilibria using tools from the\ntheory of one-dimensional diffusions. It turns out that such an equilibrium is\nonly possible if the final payoff is Bernoulli distributed as in \\cite{BB}. We\nshow in the second part that the signal of the market makers use in the general\ncase is a time-changed version of the one that they would have used had the\nfinal payoff had a Bernoulli distribution. In both cases we characterise\nexplicitly the equilibrium price process and the optimal strategy of the\ninformed trader. Contrary to the original Kyle model it is found that the\nreciprocal of market's depth, i.e. Kyle's lambda, is a uniformly integrable\nsupermartingale. While Kyle's lambda is a potential, i.e. converges to $0$, for\nthe Bernoulli distributed final payoff, its limit in general is different than\n$0$.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1603.08828v3", "link": "http://arxiv.org/abs/1603.08828v3"}, {"title": "The nonlinear Bernstein-Schr\u007f\u00f6dinger equation in Economics", "summary": "In this paper we relate the Equilibrium Assignment Problem (EAP), which is\nunderlying in several economics models, to a system of nonlinear equations that\nwe call the \"nonlinear Bernstein-Schr\u007f\\\"odinger system\", which is well-known in\nthe linear case, but whose nonlinear extension does not seem to have been\nstudied. We apply this connection to derive an existence result for the EAP,\nand an efficient computational method.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1508.05114v1", "link": "http://arxiv.org/abs/1508.05114v1"}, {"title": "Optimal Combination of Arctic Sea Ice Extent Measures: A Dynamic Factor\n  Modeling Approach", "summary": "The diminishing extent of Arctic sea ice is a key indicator of climate change\nas well as an accelerant for future global warming. Since 1978, Arctic sea ice\nhas been measured using satellite-based microwave sensing; however, different\nmeasures of Arctic sea ice extent have been made available based on differing\nalgorithmic transformations of the raw satellite data. We propose and estimate\na dynamic factor model that combines four of these measures in an optimal way\nthat accounts for their differing volatility and cross-correlations. From this\nmodel, we extract an optimal combined measure of Arctic sea ice extent using\nthe Kalman smoother.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.14276v1", "link": "http://arxiv.org/abs/2003.14276v1"}, {"title": "Asymptotic Distribution and Simultaneous Confidence Bands for Ratios of\n  Quantile Functions", "summary": "Ratio of medians or other suitable quantiles of two distributions is widely\nused in medical research to compare treatment and control groups or in\neconomics to compare various economic variables when repeated cross-sectional\ndata are available. Inspired by the so-called growth incidence curves\nintroduced in poverty research, we argue that the ratio of quantile functions\nis a more appropriate and informative tool to compare two distributions. We\npresent an estimator for the ratio of quantile functions and develop\ncorresponding simultaneous confidence bands, which allow to assess significance\nof certain features of the quantile functions ratio. Derived simultaneous\nconfidence bands rely on the asymptotic distribution of the quantile functions\nratio and do not require re-sampling techniques. The performance of the\nsimultaneous confidence bands is demonstrated in simulations. Analysis of the\nexpenditure data from Uganda in years 1999, 2002 and 2005 illustrates the\nrelevance of our approach.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1710.09009v1", "link": "http://arxiv.org/abs/1710.09009v1"}, {"title": "A nonlinear optimisation model for constructing minimal drawdown\n  portfolios", "summary": "In this paper we consider the problem of minimising drawdown in a portfolio\nof financial assets. Here drawdown represents the relative opportunity cost of\nthe single best missed trading opportunity over a specified time period. We\nformulate the problem (minimising average drawdown, maximum drawdown, or a\nweighted combination of the two) as a nonlinear program and show how it can be\npartially linearised by replacing one of the nonlinear constraints by\nequivalent linear constraints.\n  Computational results are presented (generated using the nonlinear solver\nSCIP) for three test instances drawn from the EURO STOXX 50, the FTSE 100 and\nthe S&P 500 with daily price data over the period 2010-2016. We present results\nfor long-only drawdown portfolios as well as results for portfolios with both\nlong and short positions. These indicate that (on average) our minimal drawdown\nportfolios dominate the market indices in terms of return, Sharpe ratio,\nmaximum drawdown and average drawdown over the (approximately 1800 trading day)\nout-of-sample period.", "category": ["q-fin.RM", "q-fin.PM"], "id": "http://arxiv.org/abs/1908.08684v1", "link": "http://arxiv.org/abs/1908.08684v1"}, {"title": "Modelling Returns and Volatilities During Financial Crises: a Time\n  Varying Coefficient Approach", "summary": "We examine how the most prevalent stochastic properties of key financial time\nseries have been affected during the recent financial crises. In particular we\nfocus on changes associated with the remarkable economic events of the last two\ndecades in the mean and volatility dynamics, including the underlying\nvolatility persistence and volatility spillovers structure. Using daily data\nfrom several key stock market indices we find that stock market returns exhibit\ntime varying persistence in their corresponding conditional variances.\nFurthermore, the results of our bivariate GARCH models show the existence of\ntime varying correlations as well as time varying shock and volatility\nspillovers between the returns of FTSE and DAX, and those of NIKKEI and Hang\nSeng, which became more prominent during the recent financial crisis. Our\ntheoretical considerations on the time varying model which provides the\nplatform upon which we integrate our multifaceted empirical approaches are also\nof independent interest. In particular, we provide the general solution for low\norder time varying specifications, which is a long standing research topic.\nThis enables us to characterize these models by deriving, first, their\nmultistep ahead predictors, second, the first two time varying unconditional\nmoments, and third, their covariance structure.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1403.7179v1", "link": "http://arxiv.org/abs/1403.7179v1"}, {"title": "Mean-Reversion and Optimization", "summary": "The purpose of these notes is to provide a systematic quantitative framework\n- in what is intended to be a \"pedagogical\" fashion - for discussing\nmean-reversion and optimization. We start with pair trading and add complexity\nby following the sequence \"mean-reversion via demeaning -> regression ->\nweighted regression -> (constrained) optimization -> factor models\". We discuss\nin detail how to do mean-reversion based on this approach, including common\npitfalls encountered in practical applications, such as the difference between\nmaximizing the Sharpe ratio and minimizing an objective function when trading\ncosts are included. We also discuss explicit algorithms for optimization with\nlinear costs, constraints and bounds.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1408.2217v3", "link": "http://arxiv.org/abs/1408.2217v3"}, {"title": "Diversity-Weighted Portfolios with Negative Parameter", "summary": "We analyze a negative-parameter variant of the diversity-weighted portfolio\nstudied by Fernholz, Karatzas, and Kardaras (Finance Stoch 9(1):1-27, 2005),\nwhich invests in each company a fraction of wealth inversely proportional to\nthe company's market weight (the ratio of its capitalization to that of the\nentire market). We show that this strategy outperforms the market with\nprobability one, under a non-degeneracy assumption on the volatility structure\nand the assumption that the market weights admit a positive lower bound.\nSeveral modifications of this portfolio, which outperform the market under\nmilder versions of this \"no-failure\" condition, are put forward, one of which\nis rank-based. An empirical study suggests that such strategies as studied here\nhave indeed the potential to outperform the market and to be preferable\ninvestment opportunities, even under realistic proportional transaction costs.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1504.01026v2", "link": "http://dx.doi.org/10.1007/s10436-015-0263-3"}, {"title": "Estimation Error of Expected Shortfall", "summary": "The problem of estimation error of Expected Shortfall is analyzed, with a\nview of its introduction as a global regulatory risk measure.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1402.5534v1", "link": "http://arxiv.org/abs/1402.5534v1"}, {"title": "Risk measuring under liquidity risk", "summary": "We present a general framework for measuring the liquidity risk. The\ntheoretical framework defines a class of risk measures that incorporate the\nliquidity risk into the standard risk measures. We consider a one-period risk\nmeasurement model. The liquidity risk is defined as the risk that a given\nsecurity or a portfolio of securities cannot be easily sold or bought by the\nfinancial institutions without causing significant changes in prices. The new\nrisk measures present some differences with respect to the standard risk\nmeasures. In particular, they are increasing monotonic and convex cash\nsub-additive on long positions. The contrary, in certain situations, holds for\nthe sell positions. For the long positions case, we provide these new risk\nmeasures with a dual representation. In some specific cases also the sell\npositions can be equipped with a dual representation. We apply our framework to\nthe situation in which financial institutions break up large trades into many\nsmall ones. Dual representation results are also obtained. We give many\npractical examples of risk measures and derive for each of them the respective\ncapital requirement. As a particular example, we discuss the VaR measure.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1412.6745v1", "link": "http://arxiv.org/abs/1412.6745v1"}, {"title": "Portfolio optimization for heavy-tailed assets: Extreme Risk Index vs.\n  Markowitz", "summary": "Using daily returns of the S&P 500 stocks from 2001 to 2011, we perform a\nbacktesting study of the portfolio optimization strategy based on the extreme\nrisk index (ERI). This method uses multivariate extreme value theory to\nminimize the probability of large portfolio losses. With more than 400 stocks\nto choose from, our study seems to be the first application of extreme value\ntechniques in portfolio management on a large scale. The primary aim of our\ninvestigation is the potential of ERI in practice. The performance of this\nstrategy is benchmarked against the minimum variance portfolio and the equally\nweighted portfolio. These fundamental strategies are important benchmarks for\nlarge-scale applications. Our comparison includes annualized portfolio returns,\nmaximal drawdowns, transaction costs, portfolio concentration, and asset\ndiversity in the portfolio. In addition to that we study the impact of an\nalternative tail index estimator. Our results show that the ERI strategy\nsignificantly outperforms both the minimum-variance portfolio and the equally\nweighted portfolio on assets with heavy tails.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1505.04045v1", "link": "http://dx.doi.org/10.1016/j.jempfin.2015.03.003"}, {"title": "Research and Teaching Efficiencies of Turkish Universities with\n  Heterogeneity Considerations: Application of Multi-Activity DEA and DEA by\n  Sequential Exclusion of Alternatives Methods", "summary": "The research and teaching efficiencies of 45 Turkish state universities are\nevaluated by using Multi-Activity Data Envelopment Analysis (MA-DEA) model\ndeveloped by Beasley (1995). Universities are multi-purpose institutions,\ntherefore they face multiple production functions simultaneously associated\nwith research and teaching activities. MA-DEA allows assigning priorities and\nallocating shared resources to these activities.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1701.07318v1", "link": "http://arxiv.org/abs/1701.07318v1"}, {"title": "Asset pricing with random information flow", "summary": "In the information-based approach to asset pricing the market filtration is\nmodelled explicitly as a superposition of signals concerning relevant market\nfactors and independent noise. The rate at which the signal is revealed to the\nmarket then determines the overall magnitude of asset volatility. By letting\nthis information flow rate random, we obtain an elementary stochastic\nvolatility model within the information-based approach. Such an extension is\neconomically justified on account of the fact that in real markets information\nflow rates are rarely measurable. Effects of having a random information flow\nrate is investigated in detail in the context of a simple model setup.\nSpecifically, the price process of the asset is derived, and its characteristic\nbehaviours are revealed via simulation studies. The price of a European-style\noption is worked out, showing that the model has a sufficient flexibility to\nfit volatility surface. As an extension of the random information flow model,\nprice manipulation is considered. A simple model is used to show how the\nskewness of the manipulated and unmanipulated price processes take opposite\nsignature.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1009.3810v1", "link": "http://arxiv.org/abs/1009.3810v1"}, {"title": "Minimizing the expected market time to reach a certain wealth level", "summary": "In a financial market model, we consider variations of the problem of\nminimizing the expected time to upcross a certain wealth level. For exponential\nLevy markets, we show the asymptotic optimality of the growth-optimal portfolio\nfor the above problem and obtain tight bounds for the value function for any\nwealth level. In an Ito market, we employ the concept of market time, which is\na clock that runs according to the underlying market growth. We show the\noptimality of the growth-optimal portfolio for minimizing the expected market\ntime to reach any wealth level. This reveals a general definition of market\ntime which can be useful from an investor's point of view. We utilize this last\ndefinition to extend the previous results in a general semimartingale setting.", "category": ["q-fin.PM", "q-fin.TR"], "id": "http://arxiv.org/abs/0904.1903v1", "link": "http://arxiv.org/abs/0904.1903v1"}, {"title": "Analysis of a network structure of the foreign currency exchange market", "summary": "We analyze structure of the world foreign currency exchange (FX) market\nviewed as a network of interacting currencies. We analyze daily time series of\nFX data for a set of 63 currencies, including gold, silver and platinum. We\ngroup together all the exchange rates with a common base currency and study\neach group separately. By applying the methods of filtered correlation matrix\nwe identify clusters of closely related currencies. The clusters are formed\ntypically according to the economical and geographical factors. We also study\ntopology of weighted minimal spanning trees for different network\nrepresentations (i.e., for different base currencies) and find that in a\nmajority of representations the network has a hierarchical scale-free\nstructure. In addition, we analyze the temporal evolution of the network and\ndetect that its structure is not stable over time. A medium-term trend can be\nidentified which affects the USD node by decreasing its centrality. Our\nanalysis shows also an increasing role of euro in the world's currency market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0906.0480v1", "link": "http://arxiv.org/abs/0906.0480v1"}, {"title": "How the rich get richer", "summary": "In our model, $n$ traders interact with each other and with a central bank;\nthey are taxed on the money they make, some of which is dissipated away by\ncorruption. A generic feature of our model is that the richest trader always\nwins by 'consuming' all the others: another is the existence of a threshold\nwealth, below which all traders go bankrupt. The two-trader case is examined in\ndetail,in the socialist and capitalist limits, which generalise easily to\n$n>2$. In its mean-field incarnation, our model exhibits a two-time-scale\nglassy dynamics, as well as an astonishing universality.When preference is\ngiven to local interactions in finite neighbourhoods,a novel feature emerges:\ninstead of at most one overall winner in the system,finite numbers of winners\nemerge, each one the overlord of a particular region.The patterns formed by\nsuch winners (metastable states) are very much a consequence of initial\nconditions, so that the fate of the marketplace is ruled by its past history;\nhysteresis is thus also manifested.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0504121v1", "link": "http://arxiv.org/abs/physics/0504121v1"}, {"title": "Majority Orienting Model for the Oscillation of Market Price", "summary": "The present paper introduces a majority orienting model in which the dealers'\nbehavior changes based on the influence of the price to show the oscillation of\nstock price in the stock market. We show the oscillation of the price for the\nmodel by applying the van der Pol equation which is a deterministic\napproximation of our model.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/nlin/0403056v1", "link": "http://dx.doi.org/10.1140/epjb/e2004-00054-8"}, {"title": "Measuring Inaccuracy in Travel Demand Forecasting: Methodological\n  Considerations Regarding Ramp Up and Sampling", "summary": "Project promoters, forecasters, and managers sometimes object to two things\nin measuring inaccuracy in travel demand forecasting: (1) using the forecast\nmade at the time of making the decision to build as the basis for measuring\ninaccuracy and (2) using traffic during the first year of operations as the\nbasis for measurement. This paper presents the case against both objections.\nFirst, if one is interested in learning whether decisions about building\ntransport infrastructure are based on reliable information, then it is exactly\nthe traffic forecasted at the time of making the decision to build that is of\ninterest. Second, although ideally studies should take into account so-called\ndemand \"ramp up\" over a period of years, the empirical evidence and practical\nconsiderations do not support this ideal requirement, at least not for large-N\nstudies. Finally, the paper argues that large samples of inaccuracy in travel\ndemand forecasts are likely to be conservatively biased, i.e., accuracy in\ntravel demand forecasts estimated from such samples would likely be higher than\naccuracy in travel demand forecasts in the project population. This bias must\nbe taken into account when interpreting the results from statistical analyses\nof inaccuracy in travel demand forecasting.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1303.7401v1", "link": "http://dx.doi.org/10.1016/j.tra.2005.02.003"}, {"title": "An iterative algorithm for evaluating approximations to the optimal\n  exercise boundary for a nonlinear Black-Scholes equation", "summary": "The purpose of this paper is to analyze and compute the early exercise\nboundary for a class of nonlinear Black--Scholes equations with a nonlinear\nvolatility which can be a function of the second derivative of the option price\nitself. A motivation for studying the nonlinear Black--Scholes equation with a\nnonlinear volatility arises from option pricing models taking into account e.g.\nnontrivial transaction costs, investor's preferences, feedback and illiquid\nmarkets effects and risk from a volatile (unprotected) portfolio. We present a\nnew method how to transform the free boundary problem for the early exercise\nboundary position into a solution of a time depending nonlinear parabolic\nequation defined on a fixed domain. We furthermore propose an iterative\nnumerical scheme that can be used to find an approximation of the free\nboundary. We present results of numerical approximation of the early exercise\nboundary for various types of nonlinear Black--Scholes equations and we discuss\ndependence of the free boundary on various model parameters.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/0710.5301v1", "link": "http://arxiv.org/abs/0710.5301v1"}, {"title": "Power Law Distribution of the Frequency of Demises of U.S Firms", "summary": "Both theoretical and applied economics have a great deal to say about many\naspects of the firm, but the literature on the extinctions, or demises, of\nfirms is very sparse. We use a publicly available data base covering some 6\nmillion firms in the US and show that the underlying statistical distribution\nwhich characterises the frequency of firm demises - the disappearances of firms\nas autonomous entities - is closely approximated by a power law. The exponent\nof the power law is, intriguingly, close to that reported in the literature on\nthe extinction of biological species.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0212186v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01955-6"}, {"title": "Extreme prices in electricity balancing markets from an approach of\n  statistical physics", "summary": "An increase in energy production from renewable energy sources is viewed as a\ncrucial achievement in most industrialized countries.\n  The higher variability of power production via renewables leads to a rise in\nancillary service costs over the power system, in particular costs within the\nelectricity balancing markets, mainly due to an increased number of extreme\nprice spikes. This study focuses on forecasting the behavior of price and\nvolumes of the Italian balancing market in the presence of an increased share\nof renewable energy sources. Starting from configurations of load and power\nproduction, which guarantee a stable performance, we implement fluctuations in\nthe load and in renewables; in particular we artificially increase the\ncontribution of renewables as compared to conventional power sources to cover\nthe total load. We then forecast the amount of provided energy in the balancing\nmarket and its fluctuations, which are induced by production and consumption.\nWithin an approach of agent based modeling we estimate the resulting energy\nprices and costs. While their average values turn out to be only slightly\naffected by an increased contribution from renewables, the probability for\nextreme price events is shown to increase along with undesired peaks in the\ncosts.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1612.05525v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.09.001"}, {"title": "GDP growth rates as confined L\u00e9vy flights", "summary": "A new model that combines economic growth rate fluctuations at the\nmicroscopic and macroscopic level is presented. At the microscopic level, firms\nare growing at different rates while also being exposed to idiosyncratic shocks\nat the firm and sector level. We describe such fluctuations as independent\nL\\'evy-stable fluctuations, varying over multiple orders of magnitude. These\nfluctuations are aggregated and measured at the macroscopic level in averaged\neconomic output quantities such as GDP. A fundamental question is thereby to\nwhat extend individual firm size fluctuations can have a noticeable impact on\nthe overall economy. We argue that this question can be answered by considering\nthe L\\'evy fluctuations as embedded in a steep confining potential well,\nensuring nonlinear mean-reversal behavior, without having to rely on\nmicroscopic details of the system. The steepness of the potential well directly\ncontrols the extend towards which idiosyncratic shocks to firms and sectors are\ndamped at the level of the economy. Additionally, the theory naturally accounts\nfor business cycles, represented in terms of a bimodal economic output\ndistribution, and thus connects two so far unrelated fields in economics. By\nanalyzing 200 years of US GDP growth rates, we find that the model is in good\nagreement with the data.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1709.05594v1", "link": "http://dx.doi.org/10.1103/PhysRevE.97.012150"}, {"title": "Stochastic PDEs for large portfolios with general mean-reverting\n  volatility processes", "summary": "In this article we consider a large structural market model of defaultable\nassets, where the asset value processes are modelled by using stochastic\nvolatility models with default upon hitting a lower boundary. The volatility\nprocesses are picked from a class of general mean-reverting diffusions\nsatisfying certain regularity assumptions. The value processes and the\nvolatility processes are all correlated through systemic Brownian motions. We\nprove that our system converges as the portfolio becomes large, and the limit\nof the empirical measure process has a density which is the unique solution to\nan SPDE in the two-dimensional half-space with a Dirichlet boundary condition.\nWe use Malliavin calculus to establish the existence of a regular density for\nthe volatility component of the density satisfying our stochastic\ninitial-boundary value problem, and we improve an existing kernel smoothing\ntechnique to obtain higher regularity and uniqueness results.", "category": ["math.PR", "q-fin.PM", "q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1906.05898v1", "link": "http://arxiv.org/abs/1906.05898v1"}, {"title": "Machine Learning Treasury Yields", "summary": "We give explicit algorithms and source code for extracting factors underlying\nTreasury yields using (unsupervised) machine learning (ML) techniques, such as\nnonnegative matrix factorization (NMF) and (statistically deterministic)\nclustering. NMF is a popular ML algorithm (used in computer vision,\nbioinformatics/computational biology, document classification, etc.), but is\noften misconstrued and misused. We discuss how to properly apply NMF to\nTreasury yields. We analyze the factors based on NMF and clustering and their\ninterpretation. We discuss their implications for forecasting Treasury yields\nin the context of out-of-sample ML stability issues.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/2003.05095v1", "link": "http://arxiv.org/abs/2003.05095v1"}, {"title": "Effect of Franchised Business models on Fast Food Company Stock Prices\n  in Recession and Recovery with Weibull Analysis", "summary": "At the initial stages of this research, the assumption was that the\nfranchised businesses perhaps should not be affected much by recession as there\nare multiple cash pools available inherent to the franchised business model.\nHowever, after analyzing the available data, it indicated otherwise, the stock\nprice performance as discussed indicates a different pattern. The stock price\ndata is analyzed with an unconventional tool, Weibull distribution and\nobservations confirmed the presence of either a reverse trend in franchised\nbusiness than what is observed for non-franchised or the franchised stock\nfollowed large food suppliers. There is a layered ownership and cash flow in a\nfranchised business model. The parent company run by franchiser depends on the\nperformance of child companies run by franchisees. Both parent and child\ncompanies are run as independent businesses but only the parent company is\nlisted as a stock ticker in stock exchange. Does this double layer of vertical\noperation, cash reserve, and cash flow protect them better in recession? The\ndata analyzed in this paper indicates that the recession effect can be more\nsevere; and if it dives with the average market, expect a slower recovery of\nstock prices in a franchised business model. This paper characterizes the\ndifferences and explains the natural experiment with available financial data.", "category": ["econ.GN", "q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1912.12940v1", "link": "http://arxiv.org/abs/1912.12940v1"}, {"title": "Do wealth distributions follow power laws? Evidence from \"rich lists\"", "summary": "We use data on wealth of the richest persons taken from the \"rich lists\"\nprovided by business magazines like Forbes to verify if upper tails of wealth\ndistributions follow, as often claimed, a power-law behaviour. The data sets\nused cover the world's richest persons over 1996-2012, the richest Americans\nover 1988-2012, the richest Chinese over 2006-2012 and the richest Russians\nover 2004-2011. Using a recently introduced comprehensive empirical methodology\nfor detecting power laws, which allows for testing goodness of fit as well as\nfor comparing the power-law model with rival distributions, we find that a\npower-law model is consistent with data only in 35% of the analysed data sets.\nMoreover, even if wealth data are consistent with the power-law model, usually\nthey are also consistent with some rivals like the log-normal or stretched\nexponential distributions.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1304.0212v1", "link": "http://dx.doi.org/10.1016/j.physa.2014.03.052"}, {"title": "Minimax perfect stopping rules for selling an asset near its ultimate\n  maximum", "summary": "We study the problem of selling an asset near its ultimate maximum in the\nminimax setting. The regret-based notion of a perfect stopping time is\nintroduced. A perfect stopping time is uniquely characterized by its optimality\nproperties and has the following form: one should sell the asset if its price\ndeviates from the running maximum by a certain time-dependent quantity. The\nrelated selling rule improves any earlier one and cannot be improved by further\ndelay. The results, which are applicable to a quite general price model, are\nillustrated by several examples.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1601.00175v2", "link": "http://arxiv.org/abs/1601.00175v2"}, {"title": "Variance optimal hedging with application to Electricity markets", "summary": "In Electricity markets, illiquidity, transaction costs and market price\ncharacteristics prevent managers to replicate exactly contracts. A residual\nrisk is always present and the hedging strategy depends on a risk criterion\nchosen. We present an algorithm to hedge a position for a mean variance\ncriterion taking into account the transaction cost and the small depth of the\nmarket. We show its effectiveness on a typical problem coming from the field of\nelectricity markets.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1711.03733v2", "link": "http://arxiv.org/abs/1711.03733v2"}, {"title": "Defining and estimating stochastic rate change in a dynamic general\n  insurance portfolio", "summary": "Rate change calculations in the literature involve deterministic methods that\nmeasure the change in premium for a given policy. The definition of rate change\nas a statistical parameter is proposed to address the stochastic nature of the\npremium charged for a policy. It promotes the idea that rate change is a\nproperty of an asymptotic population to be estimated, not just a property to\nmeasure or monitor in the sample of observed policies that are written. Various\nmodels and techniques are given for estimating this stochastic rate change and\nquantifying the uncertainty in the estimates. The use of matched sampling is\nemphasized for rate change estimation, as it adjusts for changes in policy\ncharacteristics by directly searching for similar policies across policy years.\nThis avoids any of the assumptions and recipes that are required to re-rate\npolicies in years where they were not written, as is common with deterministic\nmethods. Such procedures can be subjective or implausible if the structure of\nrating algorithms change or there are complex and heterogeneous exposure bases\nand coverages. The methods discussed are applied to a motor premium database.\nThe application includes the use of a genetic algorithm with parallel\ncomputations to automatically optimize the matched sampling.", "category": ["q-fin.PM", "q-fin.GN"], "id": "http://arxiv.org/abs/1810.10970v1", "link": "http://arxiv.org/abs/1810.10970v1"}, {"title": "Losing money with a high Sharpe ratio", "summary": "A simple example shows that losing all money is compatible with a very high\nSharpe ratio (as computed after losing all money). However, the only way that\nthe Sharpe ratio can be high while losing money is that there is a period in\nwhich all or almost all money is lost. This note explores the best achievable\nSharpe and Sortino ratios for investors who lose money but whose one-period\nreturns are bounded below (or both below and above) by a known constant.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1109.0706v1", "link": "http://arxiv.org/abs/1109.0706v1"}, {"title": "On refined volatility smile expansion in the Heston model", "summary": "It is known that Heston's stochastic volatility model exhibits moment\nexplosion, and that the critical moment $s_+$ can be obtained by solving\n(numerically) a simple equation. This yields a leading order expansion for the\nimplied volatility at large strikes: $\\sigma_{BS}( k,T)^{2}T\\sim \\Psi (s_+-1)\n\\times k$ (Roger Lee's moment formula). Motivated by recent \"tail-wing\"\nrefinements of this moment formula, we first derive a novel tail expansion for\nthe Heston density, sharpening previous work of Dragulescu and Yakovenko\n[Quant. Finance 2, 6 (2002), 443--453], and then show the validity of a refined\nexpansion of the type $\\sigma_{BS}( k,T) ^{2}T=(\n\\beta_{1}k^{1/2}+\\beta_{2}+...)^{2}$, where all constants are explicitly known\nas functions of $s_+$, the Heston model parameters, spot vol and maturity $T$.\nIn the case of the \"zero-correlation\" Heston model such an expansion was\nderived by Gulisashvili and Stein [Appl. Math. Optim. 61, 3 (2010), 287--315].\nOur methods and results may prove useful beyond the Heston model: the entire\nquantitative analysis is based on affine principles: at no point do we need\nknowledge of the (explicit, but cumbersome) closed form expression of the\nFourier transform of $\\log S_{T}$\\ (equivalently: Mellin transform of $S_{T}$\n); what matters is that these transforms satisfy ordinary differential\nequations of Riccati type. Secondly, our analysis reveals a new parameter\n(\"critical slope\"), defined in a model free manner, which drives the second and\nhigher order terms in tail- and implied volatility expansions.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1001.3003v2", "link": "http://arxiv.org/abs/1001.3003v2"}, {"title": "Estimation and HAC-based Inference for Machine Learning Time Series\n  Regressions", "summary": "Time series regression analysis in econometrics typically involves a\nframework relying on a set of mixing conditions to establish consistency and\nasymptotic normality of parameter estimates and HAC-type estimators of the\nresidual long-run variances to conduct proper inference. This article\nintroduces structured machine learning regressions for high-dimensional time\nseries data using the aforementioned commonly used setting. To recognize the\ntime series data structures we rely on the sparse-group LASSO estimator. We\nderive a new Fuk-Nagaev inequality for a class of $\\tau$-dependent processes\nwith heavier than Gaussian tails, nesting $\\alpha$-mixing processes as a\nspecial case, and establish estimation, prediction, and inferential properties,\nincluding convergence rates of the HAC estimator for the long-run variance\nbased on LASSO residuals. An empirical application to nowcasting US GDP growth\nindicates that the estimator performs favorably compared to other alternatives\nand that the text data can be a useful addition to more traditional numerical\ndata.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.06307v1", "link": "http://arxiv.org/abs/1912.06307v1"}, {"title": "Delusion and Deception in Large Infrastructure Projects: Two Models for\n  Explaining and Preventing Executive Disaster", "summary": "The Economist recently reported that infrastructure spending is the largest\nit is ever been as a share of world GDP. With $22 trillion in projected\ninvestments over the next ten years in emerging economies alone, the magazine\ncalls it the \"biggest investment boom in history.\" The efficiency of\ninfrastructure planning and execution is therefore particularly important at\npresent. Unfortunately, the private sector, the public sector and\nprivate/public sector partnerships have a dismal record of delivering on large\ninfrastructure cost and performance promises. This paper explains why and how\nto solve the problem.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1303.7403v1", "link": "http://dx.doi.org/10.1225/CMR423"}, {"title": "From Disequilibrium Markets to Equilibrium", "summary": "The modeling of financial markets as disequilibrium models by ordinary\ndifferential equations has become a popular modeling tool. One famous example\nof such a model is the Beja-Goldman model(The Journal of Finance, 1980) which\nwe consider in this paper. We study the passage from disequilibrium dynamics to\nequilibrium. Mathematically, this limit corresponds to an asymptotic limit also\nknown as a Tikhonov-Fenichel reduction. Furthermore, we analyze the stability\nof the reduced equilibrium model and discuss the economic implications. We\nconduct several numerical examples to visualize and support our analysis.", "category": ["econ.GN", "q-fin.EC", "q-fin.MF", "q-fin.TR"], "id": "http://arxiv.org/abs/1912.09679v1", "link": "http://arxiv.org/abs/1912.09679v1"}, {"title": "Statistically validated lead-lag networks and inventory prediction in\n  the foreign exchange market", "summary": "We introduce a method to infer lead-lag networks of agents' actions in\ncomplex systems. These networks open the way to both microscopic and\nmacroscopic states prediction in such systems. We apply this method to\ntrader-resolved data in the foreign exchange market. We show that these\nnetworks are remarkably persistent, which explains why and how order flow\nprediction is possible from trader-resolved data. In addition, if traders'\nactions depend on past prices, the evolution of the average price paid by\ntraders may also be predictable. Using random forests, we verify that the\npredictability of both the sign of order flow and the direction of average\ntransaction price is strong for retail investors at an hourly time scale, which\nis of great relevance to brokers and order matching engines. Finally, we argue\nthat the existence of trader lead-lag networks explains in a self-referential\nway why a given trader becomes active, which is in line with the fact that most\ntrading activity has an endogenous origin.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1609.04640v3", "link": "http://arxiv.org/abs/1609.04640v3"}, {"title": "Non-Stationary Dividend-Price Ratios", "summary": "Dividend yields have been widely used in previous research to relate stock\nmarket valuations to cash flow fundamentals. However, this approach relies on\nthe assumption that dividend yields are stationary. Due to the failure to\nreject the hypothesis of a unit root in the classical dividend-price ratio for\nthe US stock market, Polimenis and Neokosmidis (2016) proposed the use of a\nmodified dividend price ratio (mdp) as the deviation between d and p from their\nlong run equilibrium, and showed that mdp provides substantially improved\nforecasting results over the classical dp ratio. Here, we extend that paper by\nperforming multivariate regressions based on the Campbell-Shiller\napproximation, by utilizing a dynamic econometric procedure to estimate the\nmodified dp, and by testing the modified ratios against reinvested\ndividend-yields. By comparing the performance of mdp and dp in the period after\n1965, we are not only able to enhance the robustness of the findings, but also\nto debunk a possible false explanation that the enhanced mdp performance in\npredicting future returns comes from a capacity to predict the risk-free return\ncomponent. Depending on whether one uses the recursive or population\nmethodology to measure the performance of mdp, the Out-of-Sample performance\ngain is between 30% to 50%.", "category": ["q-fin.PM", "q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1902.06053v1", "link": "http://dx.doi.org/10.1057/s41260-019-00143-3"}, {"title": "Calibration of Chaotic Models for Interest Rates", "summary": "In this paper we calibrate chaotic models for interest rates to market data\nusing a polynomial-exponential parametrization for the chaos coefficients. We\nidentify a subclass of one-variable models that allow us to introduce\ncomplexity from higher order chaos in a controlled way while retaining\nconsiderable analytic tractability. In particular we derive explicit\nexpressions for bond and option prices in a one-variable third chaos model in\nterms of elementary combinations of normal density and cumulative distribution\nfunctions. We then compare the calibration performance of chaos models with\nthat of well-known benchmark models. For term structure calibration we find\nthat chaos models are comparable to the Svensson model, with the advantage of\nguaranteed positivity and consistency with a dynamic stochastic evolution of\ninterest rates. For calibration to option data, chaos models outperform the\nHull and White and rational lognormal models and are comparable to LIBOR market\nmodels.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1106.2478v1", "link": "http://arxiv.org/abs/1106.2478v1"}, {"title": "Cross-correlations in Warsaw Stock Exchange", "summary": "We study the inter-stock correlations for the largest companies listed on\nWarsaw Stock Exchange and included in the WIG20 index. Our results from the\ncorrelation matrix analysis indicate that the Polish stock market can be well\ndescribed by a one factor model. We also show that the stock-stock correlations\ntend to increase with the time scale of returns and they approach a saturation\nlevel for the time scales of at least 200 min, i.e. an order of magnitude\nlonger than in the case of some developed markets. We also show that the\nstrength of correlations among the stocks crucially depends on their\ncapitalization. These results combined with our earlier findings together\nsuggest that now the Polish stock market situates itself somewhere between an\nemerging market phase and a mature market phase.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0803.0057v1", "link": "http://arxiv.org/abs/0803.0057v1"}, {"title": "Identification of Random Coefficient Latent Utility Models", "summary": "This paper provides nonparametric identification results for random\ncoefficient distributions in perturbed utility models. We cover discrete and\ncontinuous choice models. We establish identification using variation in mean\nquantities, and the results apply when an analyst observes aggregate demands\nbut not whether goods are chosen together. We require exclusion restrictions\nand independence between random slope coefficients and random intercepts. We do\nnot require regressors to have large supports or parametric assumptions.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2003.00276v1", "link": "http://arxiv.org/abs/2003.00276v1"}, {"title": "On Some Processes and Distributions in a Collective Model of Investors'\n  Behavior", "summary": "This article considers a model for alternative processes for securities\nprices and compares this model with actual return data of several securities.\nThe distributions of returns that appear in the model can be Gaussian as well\nas non-Gaussian; in particular they may have two peaks. We consider a discrete\nMarkov chain model. This model in some aspects is similar to well-known Ising\nmodel describing ferromagnetics. Namely we consider a set of N investors, each\nof whom has either bullish or bearish opinion, denoted by plus or minus\nrespectively. At every time step each of N investors can change his/her sign.\nThe probability of a plus becoming a minus and the probability of a minus\nbecoming a plus depends only on the bullish sentiment described as the number\nof bullish investors among the total of N investors. The number of bullish\ninvestors then forms a Markov chain whose transition matrix is calculated\nexplicitly. The transition matrix of that chain is ergodic and any initial\ndistribution of bullish investors converges to stationary. Stationary\ndistributions of bullish investors in this Markov chain model are similar to\ncontinuous distributions of the \"theory of social imitation\" of Callen and\nShapero. Distributions obtained this way can represent 3 types of market\nbehavior: one-peaked distribution that is close to Gaussian, transition market\n(flattening of the top), and two-peaked distribution.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/nlin/0506015v1", "link": "http://arxiv.org/abs/nlin/0506015v1"}, {"title": "Discrete Time Dynamic Programming with Recursive Preferences: Optimality\n  and Applications", "summary": "This paper provides an alternative approach to the theory of dynamic\nprogramming, designed to accommodate the kinds of recursive preference\nspecifications that have become popular in economic and financial analysis,\nwhile still supporting traditional additively separable rewards. The approach\nexploits the theory of monotone convex operators, which turns out to be well\nsuited to dynamic maximization. The intuition is that convexity is preserved\nunder maximization, so convexity properties found in preferences extend\nnaturally to the Bellman operator.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1812.05748v3", "link": "http://arxiv.org/abs/1812.05748v3"}, {"title": "Disentangling bipartite and core-periphery structure in financial\n  networks", "summary": "A growing number of systems are represented as networks whose architecture\nconveys significant information and determines many of their properties.\nExamples of network architecture include modular, bipartite, and core-periphery\nstructures. However inferring the network structure is a non trivial task and\ncan depend sometimes on the chosen null model. Here we propose a method for\nclassifying network structures and ranking its nodes in a statistically\nwell-grounded fashion. The method is based on the use of Belief Propagation for\nlearning through Entropy Maximization on both the Stochastic Block Model (SBM)\nand the degree-corrected Stochastic Block Model (dcSBM). As a specific\napplication we show how the combined use of the two ensembles -SBM and dcSBM-\nallows to disentangle the bipartite and the core-periphery structure in the\ncase of the e-MID interbank network. Specifically we find that, taking into\naccount the degree, this interbank network is better described by a bipartite\nstructure, while using the SBM the core-periphery structure emerges only when\ndata are aggregated for more than a week.", "category": ["q-fin.GN", "q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1511.08830v1", "link": "http://dx.doi.org/10.1016/j.chaos.2016.02.004"}, {"title": "Investment strategy based on a company growth model", "summary": "We first estimate the average growth of a company's annual income and its\nvariance by using both real company data and a numerical model which we already\nintroduced a couple of years ago. Investment strategies expecting for income\ngrowth is evaluated based on the numerical model. Our numerical simulation\nsuggests the possibility that an investment strategy focusing on the\nmedium-sized companies gives the best asset growth with relatively low risk.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/cond-mat/0303304v1", "link": "http://arxiv.org/abs/cond-mat/0303304v1"}, {"title": "Linear Stochastic Dividend Model", "summary": "In this paper we propose a new model for pricing stock and dividend\nderivatives. We jointly specify dynamics for the stock price and the dividend\nrate such that the stock price is positive and the dividend rate non-negative.\nIn its simplest form, the model features a dividend rate that is mean-reverting\naround a constant fraction of the stock price. The advantage of directly\nspecifying dynamics for the dividend rate, as opposed to the more common\napproach of modeling the dividend yield, is that it is easier to keep the\ndistribution of cumulative dividends tractable. The model is non-affine but\ndoes belong to the more general class of polynomial processes, which allows us\nto compute all conditional moments of the stock price and the cumulative\ndividends explicitly. In particular, we have closed-form expressions for the\nprices of stock and dividend futures. Prices of stock and dividend options are\naccurately approximated using a moment matching technique based on the\nprinciple of maximal entropy.", "category": ["q-fin.MF", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1908.05850v2", "link": "http://arxiv.org/abs/1908.05850v2"}, {"title": "A Production Function with Variable Elasticity of Factor Substitution", "summary": "The main aim of this paper is to prove the existence of a new production\nfunction with variable elasticity of factor substitution. This production\nfunction is a more general form which includes the Cobb-Douglas production\nfunction and the CES production function as particular cases. The econometric\nestimates presented in the paper confirm some other results and reinforces the\nconclusion that the sigma is well-below the Cobb-Douglas value of one.", "category": [], "id": "http://arxiv.org/abs/1907.12624v1", "link": "http://arxiv.org/abs/1907.12624v1"}, {"title": "The Naive Extrapolation Hypothesis and the Rosy-Gloomy Forecasts", "summary": "I study the behavior and the performance of the long-term forecasts issued by\nfinancial analysts with respect to the Extrapolation Hypothesis. That\nhypothesis states that investors, extrapolating from the firms' recent\nperformances, are too optimistic about growth and large firms and too\npessimistic about value and small firms. I find that the forecasting errors are\nhigher for the growth firms and large firms, thus providing support for the\nExtrapolation Hypothesis. However, in addition to the rosy picture of the\ngrowth and large firms, the forecasts of the value and small firms are not so\ngloomy in many cases. My analysis also reveals that expectations move together\nfor all categories of book-to-market and all sizes of firms. I proceed by\ninvestigating some common factors that may influence analysts' long-term\nforecasts, including co-movement and excessive optimism. I find that macro\nfactors beyond a firm's recent performance may influence the formation of\nexpectations.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1406.1733v1", "link": "http://arxiv.org/abs/1406.1733v1"}, {"title": "Model-independent no-arbitrage conditions on American put options", "summary": "We consider the pricing of American put options in a model-independent\nsetting: that is, we do not assume that asset prices behave according to a\ngiven model, but aim to draw conclusions that hold in any model. We incorporate\nmarket information by supposing that the prices of European options are known.\n  In this setting, we are able to provide conditions on the American Put prices\nwhich are necessary for the absence of arbitrage. Moreover, if we further\nassume that there are finitely many European and American options traded, then\nwe are able to show that these conditions are also sufficient. To show\nsufficiency, we construct a model under which both American and European\noptions are correctly priced at all strikes simultaneously. In particular, we\nneed to carefully consider the optimal stopping strategy in the construction of\nour process.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1301.5467v1", "link": "http://arxiv.org/abs/1301.5467v1"}, {"title": "A risk analysis for a system stabilized by a central agent", "summary": "We formulate and analyze a multi-agent model for the evolution of individual\nand systemic risk in which the local agents interact with each other through a\ncentral agent who, in turn, is influenced by the mean field of the local\nagents. The central agent is stabilized by a bistable potential, the only\nstabilizing force in the system. The local agents derive their stability only\nfrom the central agent. In the mean field limit of a large number of local\nagents we show that the systemic risk decreases when the strength of the\ninteraction of the local agents with the central agent increases. This means\nthat the probability of transition from one of the two stable quasi-equilibria\nto the other one decreases. We also show that the systemic risk increases when\nthe strength of the interaction of the central agent with the mean field of the\nlocal agents increases. Following the financial interpretation of such models\nand their behavior given in our previous paper (Garnier, Papanicolaou and Yang,\nSIAM J. Fin. Math. 4, 2013, 151-184), we may interpret the results of this\npaper in the following way. From the point of view of systemic risk, and while\nkeeping the perceived risk of the local agents approximately constant, it is\nbetter to strengthen the interaction of the local agents with the central agent\nthan the other way around.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1507.08333v2", "link": "http://arxiv.org/abs/1507.08333v2"}, {"title": "Local Parametric Estimation in High Frequency Data", "summary": "In this paper, we give a general time-varying parameter model, where the\nmultidimensional parameter possibly includes jumps. The quantity of interest is\ndefined as the integrated value over time of the parameter process $\\Theta =\nT^{-1} \\int_0^T \\theta_t^* dt$. We provide a local parametric estimator (LPE)\nof $\\Theta$ and conditions under which we can show the central limit theorem.\nRoughly speaking those conditions correspond to some uniform limit theory in\nthe parametric version of the problem. The framework is restricted to the\nspecific convergence rate $n^{1/2}$. Several examples of LPE are studied:\nestimation of volatility, powers of volatility, volatility when incorporating\ntrading information and time-varying MA(1).", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1603.05700v4", "link": "http://arxiv.org/abs/1603.05700v4"}, {"title": "Adapted Wasserstein Distances and Stability in Mathematical Finance", "summary": "Assume that an agent models a financial asset through a measure Q with the\ngoal to price / hedge some derivative or optimize some expected utility. Even\nif the model Q is chosen in the most skilful and sophisticated way, she is left\nwith the possibility that Q does not provide an \"exact\" description of reality.\nThis leads us to the following question: will the hedge still be somewhat\nmeaningful for models in the proximity of Q?\n  If we measure proximity with the usual Wasserstein distance (say), the answer\nis NO. Models which are similar w.r.t. Wasserstein distance may provide\ndramatically different information on which to base a hedging strategy.\n  Remarkably, this can be overcome by considering a suitable \"adapted\" version\nof the Wasserstein distance which takes the temporal structure of pricing\nmodels into account. This adapted Wasserstein distance is most closely related\nto the nested distance as pioneered by Pflug and Pichler\n\\cite{Pf09,PfPi12,PfPi14}. It allows us to establish Lipschitz properties of\nhedging strategies for semimartingale models in discrete and continuous time.\nNotably, these abstract results are sharp already for Brownian motion and\nEuropean call options.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1901.07450v3", "link": "http://arxiv.org/abs/1901.07450v3"}, {"title": "Fear Universality and Doubt in Asset price movements", "summary": "We take a look the changes of different asset prices over variable periods,\nusing both traditional and spectral methods, and discover universality\nphenomena which hold (in some cases) across asset classes.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1803.07138v1", "link": "http://arxiv.org/abs/1803.07138v1"}, {"title": "Capital distribution and portfolio performance in the mean-field Atlas\n  model", "summary": "We study a mean-field version of rank-based models of equity markets such as\nthe Atlas model introduced by Fernholz in the framework of Stochastic Portfolio\nTheory. We obtain an asymptotic description of the market when the number of\ncompanies grows to infinity. Then, we discuss the long-term capital\ndistribution. We recover the Pareto-like shape of capital distribution curves\nusually derived from empirical studies, and provide a new description of the\nphase transition phenomenon observed by Chatterjee and Pal. Finally, we address\nthe performance of simple portfolio rules and highlight the influence of the\nvolatility structure on the growth of portfolios.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1312.5660v2", "link": "http://arxiv.org/abs/1312.5660v2"}, {"title": "Sparse Approximate Factor Estimation for High-Dimensional Covariance\n  Matrices", "summary": "We propose a novel estimation approach for the covariance matrix based on the\n$l_1$-regularized approximate factor model. Our sparse approximate factor (SAF)\ncovariance estimator allows for the existence of weak factors and hence relaxes\nthe pervasiveness assumption generally adopted for the standard approximate\nfactor model. We prove consistency of the covariance matrix estimator under the\nFrobenius norm as well as the consistency of the factor loadings and the\nfactors.\n  Our Monte Carlo simulations reveal that the SAF covariance estimator has\nsuperior properties in finite samples for low and high dimensions and different\ndesigns of the covariance matrix. Moreover, in an out-of-sample portfolio\nforecasting application the estimator uniformly outperforms alternative\nportfolio strategies based on alternative covariance estimation approaches and\nmodeling strategies including the $1/N$-strategy.", "category": ["econ.EM", "q-fin.PM"], "id": "http://arxiv.org/abs/1906.05545v1", "link": "http://arxiv.org/abs/1906.05545v1"}, {"title": "Exact Smooth Term-Structure Estimation", "summary": "We present a non-parametric method to estimate the discount curve from market\nquotes based on the Moore-Penrose pseudoinverse. The discount curve reproduces\nthe market quotes perfectly, has maximal smoothness, and is given in\nclosed-form. The method is easy to implement and requires only basic linear\nalgebra operations. We provide a full theoretical framework as well as several\npractical applications.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1606.03899v4", "link": "http://arxiv.org/abs/1606.03899v4"}, {"title": "Fractional smoothness and applications in finance", "summary": "This overview article concerns the notion of fractional smoothness of random\nvariables of the form $g(X_T)$, where $X=(X_t)_{t\\in [0,T]}$ is a certain\ndiffusion process. We review the connection to the real interpolation theory,\ngive examples and applications of this concept. The applications in stochastic\nfinance mainly concern the analysis of discrete time hedging errors. We close\nthe review by indicating some further developments.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1004.3577v1", "link": "http://arxiv.org/abs/1004.3577v1"}, {"title": "Ensemble Forecasting for Intraday Electricity Prices: Simulating\n  Trajectories", "summary": "Recent studies concerning the point electricity price forecasting have shown\nevidence that the hourly German Intraday Continuous Market is weak-form\nefficient. Therefore, we take a novel, advanced approach to the problem. A\nprobabilistic forecasting of the hourly intraday electricity prices is\nperformed by simulating trajectories in every trading window to receive a\nrealistic ensemble to allow for more efficient intraday trading and redispatch.\nA generalized additive model is fitted to the price differences with the\nassumption that they follow a mixture of the Dirac and the Student's\nt-distributions. Moreover, the mixing term is estimated using a\nhigh-dimensional logistic regression with lasso penalty. We model the expected\nvalue and volatility of the series using i.a. autoregressive and no-trade\neffects or load, wind and solar generation forecasts and accounting for the\nnon-linearities in e.g. time to maturity. Both the in-sample characteristics\nand forecasting performance are analysed using a rolling window forecasting\nstudy. Multiple versions of the model are compared to several benchmark models.\nThe study aims to forecast the price distribution in the German Intraday\nContinuous Market in the last 3 hours of trading, but the approach allows for\napplication to other continuous markets. The results prove superiority of the\nmixture model over the benchmarks gaining the most from the modelling of the\nvolatility.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/2005.01365v1", "link": "http://arxiv.org/abs/2005.01365v1"}, {"title": "Talent management - an etymological study", "summary": "The current article unveils and analyzes important shades of meaning for the\nwidely discussed term talent management. It not only grounds the outlined\nperspectives in incremental formulation and elaboration of this construct, but\nalso is oriented to exploring the underlying reasons for the social actors,\nproposing new nuances. Thus, a mind map and a fish-bone diagram are constructed\nto depict effectively and efficiently the current state of development for\ntalent management and make easier the realizations of future research\nendeavours in this field.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.02615v1", "link": "http://dx.doi.org/10.5281/zenodo.1434892"}, {"title": "Factorising equity returns in an emerging market through exogenous\n  shocks and capital flows", "summary": "A technique from stochastic portfolio theory [Fernholz, 1998] is applied to\nanalyse equity returns of Small, Mid and Large cap portfolios in an emerging\nmarket through periods of growth and regional crises, up to the onset of the\nglobal financial crisis. In particular, we factorize portfolios in the South\nAfrican market in terms of distribution of capital, change of stock ranks in\nportfolios, and the effect due to dividends for the period Nov 1994 to May\n2007. We discuss the results in the context of broader economic thinking to\nconsider capital flows as risk factors, turning around more established\napproaches which use macroeconomic and socio-economic conditions to explain\nForeign Direct Investment (into the economy) and Net Portfolio Investment (into\nequity and bond markets).", "category": ["q-fin.GN", "q-fin.RM"], "id": "http://arxiv.org/abs/1306.5302v2", "link": "http://arxiv.org/abs/1306.5302v2"}, {"title": "The microstructure of high frequency markets", "summary": "We present a novel approach to describing the microstructure of high\nfrequency trading using two key elements. First we introduce a new notion of\ninformed trader which we starkly contrast to current informed trader models. We\ndescribe the exact nature of the `superior information' high frequency traders\nhave access to, and how these agents differ from the more standard `insider\ntraders' described in past papers. This then leads to a model and an empirical\nanalysis of the data which strongly supports our claims. The second key element\nis a rigorous description of clearing conditions on a limit order book and how\nto derive correct formulas for such a market. From a theoretical point of view,\nthis allows the exact identification of two frictions in the market, one of\nwhich is intimately linked to our notion of `superior information'.\nEmpirically, we show that ignoring these frictions can misrepresent the wealth\nexchanged on the market by 50%. Finally, we showcase two applications of our\napproach: we measure the profits made by high frequency traders on NASDAQ and\nre-visit the standard Black - Scholes model to determine how trading frictions\nalter the delta-hedging strategy.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1709.02015v1", "link": "http://arxiv.org/abs/1709.02015v1"}, {"title": "Exponential Martingales and Time integrals of Brownian Motion", "summary": "We find a simple expression for the probability density of $\\int \\exp (B_s -\ns/2) ds$ in terms of its distribution function and the distribution function\nfor the time integral of $\\exp (B_s + s/2)$. The relation is obtained with a\nchange of measure argument where expectations over events determined by the\ntime integral are replaced by expectations over the entire probability space.\nWe develop precise information concerning the lower tail probabilities for\nthese random variables as well as for time integrals of geometric Brownian\nmotion with arbitrary constant drift. In particular, $E[ \\exp\\big(\\theta / \\int\n\\exp (B_s)ds\\big) ]$ is finite iff $\\theta < 2$. We present a new formula for\nthe price of an Asian call option.", "category": ["math.PR", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0612034v2", "link": "http://arxiv.org/abs/math/0612034v2"}, {"title": "Optimising portfolio diversification and dimensionality", "summary": "A new framework for portfolio diversification is introduced which goes beyond\nthe classical mean-variance approach and portfolio allocation strategies such\nas risk parity. It is based on a novel concept called portfolio dimensionality\nthat connects diversification to the non-Gaussianity of portfolio returns and\ncan typically be defined in terms of the ratio of risk measures which are\nhomogenous functions of equal degree. The latter arises naturally due to our\nrequirement that diversification measures should be leverage invariant. We\nintroduce this new framework and argue the benefits relative to existing\nmeasures of diversification in the literature, before addressing the question\nof optimizing diversification or, equivalently, dimensionality. Maximising\nportfolio dimensionality leads to highly non-trivial optimization problems with\nobjective functions which are typically non-convex and potentially have\nmultiple local optima. Two complementary global optimization algorithms are\nthus presented. For problems of moderate size and more akin to asset allocation\nproblems, a deterministic Branch and Bound algorithm is developed, whereas for\nproblems of larger size a stochastic global optimization algorithm based on\nGradient Langevin Dynamics is given. We demonstrate analytically and through\nnumerical experiments that the framework reflects the desired properties often\ndiscussed in the literature.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1906.00920v3", "link": "http://arxiv.org/abs/1906.00920v3"}, {"title": "Corporate Governance, Noise Trading and Liquidity of Stocks", "summary": "Our main task is to study the effect of corporate governance on the market\nliquidity of listed companies' stocks. We establish a theoretical model that\ncontains the heterogeneity of investors' beliefs to explain the mechanisms by\nwhich corporate governance improves liquidity of the corporate stocks. In this\nprocess we found that the existence of noise traders who are semi-informed in\nthe market is an important condition for corporate governance to have the\neffect of improving liquidity of the stocks. We further find that the strength\nof this effect is affected by the degree of noise traders' participation in\nmarket transactions. Our model reveals that corporate governance and the degree\nof noise traders' participation in transactions have a synergistic effect on\nimproving the liquidity of the stocks.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/2001.06275v1", "link": "http://arxiv.org/abs/2001.06275v1"}, {"title": "Dynamic Model of the Price Dispersion of Homogeneous Goods", "summary": "Presented is an analytic microeconomic model of the temporal price dispersion\nof homogeneous goods in polypoly markets. This new approach is based on the\nidea that the price dispersion has its origin in the dynamics of the purchase\nprocess. The price dispersion is determined by the chance that demanded and\nsupplied product units meet in a given price interval. It can be characterized\nby a fat-tailed Laplace distribution for short and by a lognormal distribution\nfor long time horizons. Taking random temporal variations of demanded and\nsupplied units into account both the mean price and also the standard deviation\nof the price dispersion are governed by a lognormal distribution. A comparison\nwith empirical investigations confirms the model statements.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1509.01216v1", "link": "http://dx.doi.org/10.9734/BJEMT/2015/17849"}, {"title": "Precise asymptotics: robust stochastic volatility models", "summary": "We present a new methodology to analyze large classes of (classical and\nrough) stochastic volatility models, with special regard to short-time and\nsmall noise formulae for option prices. Our main tool is the theory of\nregularity structures, which we use in the form of [Bayer et al; A regularity\nstructure for rough volatility, 2017]. In essence, we implement a Laplace\nmethod on the space of models (in the sense of Hairer), which generalizes\nclassical works of Azencott and Ben Arous on path space and then Aida,\nInahama--Kawabi on rough path space. When applied to rough volatility models,\ne.g. in the setting of [Forde-Zhang, Asymptotics for rough stochastic\nvolatility models, 2017], one obtains precise asymptotic for European options\nwhich refine known large deviation asymptotics.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1811.00267v1", "link": "http://arxiv.org/abs/1811.00267v1"}, {"title": "Efficient Markets, Behavioral Finance and a Statistical Evidence of the\n  Validity of Technical Analysis", "summary": "This work tried to detect the existence of a relationship between the graphic\nsignals - or patterns - observed day by day in the Brazilian stock market and\nthe trends which happen after these signals, within a period of 8 years, for a\nnumber of securities. The results obtained from this study show evidence of the\nexistence of such a relationship, suggesting the validity of the Technical\nAnalysis as an instrument to predict the trend of security prices in the\nBrazilian stock market within that period.", "category": ["q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1302.1228v1", "link": "http://arxiv.org/abs/1302.1228v1"}, {"title": "Multinomial VaR Backtests: A simple implicit approach to backtesting\n  expected shortfall", "summary": "Under the Fundamental Review of the Trading Book (FRTB) capital charges for\nthe trading book are based on the coherent expected shortfall (ES) risk\nmeasure, which show greater sensitivity to tail risk. In this paper it is\nargued that backtesting of expected shortfall - or the trading book model from\nwhich it is calculated - can be based on a simultaneous multinomial test of\nvalue-at-risk (VaR) exceptions at different levels, an idea supported by an\napproximation of ES in terms of multiple quantiles of a distribution proposed\nin Emmer et al. (2015). By comparing Pearson, Nass and likelihood-ratio tests\n(LRTs) for different numbers of VaR levels $N$ it is shown in a series of\nsimulation experiments that multinomial tests with $N\\geq 4$ are much more\npowerful at detecting misspecifications of trading book loss models than\nstandard binomial exception tests corresponding to the case $N=1$. Each test\nhas its merits: Pearson offers simplicity; Nass is robust in its size\nproperties to the choice of $N$; the LRT is very powerful though slightly\nover-sized in small samples and more computationally burdensome. A\ntraffic-light system for trading book models based on the multinomial test is\nproposed and the recommended procedure is applied to a real-data example\nspanning the 2008 financial crisis.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1611.04851v1", "link": "http://arxiv.org/abs/1611.04851v1"}, {"title": "Gradient Boost with Convolution Neural Network for Stock Forecast", "summary": "Market economy closely connects aspects to all walks of life. The stock\nforecast is one of task among studies on the market economy. However,\ninformation on markets economy contains a lot of noise and uncertainties, which\nlead economy forecasting to become a challenging task. Ensemble learning and\ndeep learning are the most methods to solve the stock forecast task. In this\npaper, we present a model combining the advantages of two methods to forecast\nthe change of stock price. The proposed method combines CNN and GBoost. The\nexperimental results on six market indexes show that the proposed method has\nbetter performance against current popular methods.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1909.09563v1", "link": "http://arxiv.org/abs/1909.09563v1"}, {"title": "Learning from Neighbors about a Changing State", "summary": "Agents learn about a changing state using private signals and past actions of\nneighbors in a network. We characterize equilibrium learning and social\ninfluence in this setting. We then examine when agents can aggregate\ninformation well, responding quickly to recent changes. A key sufficient\ncondition for good aggregation is that each individual's neighbors have\nsufficiently different types of private information. In contrast, when signals\nare homogeneous, aggregation is suboptimal on any network. We also examine\nbehavioral versions of the model, and show that achieving good aggregation\nrequires a sophisticated understanding of correlations in neighbors' actions.\nThe model provides a Bayesian foundation for a tractable learning dynamic in\nnetworks, closely related to the DeGroot model, and offers new tools for\ncounterfactual and welfare analyses.", "category": [], "id": "http://arxiv.org/abs/1801.02042v6", "link": "http://arxiv.org/abs/1801.02042v6"}, {"title": "Analytical Pricing of Defaultable Bond with Stochastic Default Intensity", "summary": "We provide analytical pricing formula of corporate defaultable bond with both\nexpected and unexpected default in the case with stochastic default intensity.\nIn the case with constant short rate and exogenous default recovery using PDE\nmethod, we gave some pricing formula of the defaultable bond under the\nconditions that 1)expected default recovery is the same with unexpected default\nrecovery; 2) default intensity follows one of 3 special cases of Willmott\nmodel; 3) default intensity is uncorrelated with firm value. Then we derived a\npricing formula of a credit default swap. And in the case of stochastic short\nrate and exogenous default recovery using PDE method, we gave some pricing\nformula of the defaultable bond under the conditions that 1) expected default\nrecovery is the same with unexpected default recovery; 2) the short rate\nfollows Vasicek model; 3) default intensity follows one of 3 special cases of\nWillmott model; 4) default intensity is uncorrelated with firm value; 5)\ndefault intensity is uncorrelated with short rate. Then we derived a pricing\nformula of a credit default swap. We give some credit spread analysis, too.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1303.1298v3", "link": "http://dx.doi.org/10.2139/ssrn.723601"}, {"title": "A generalized reserving model: bridging the gap between pricing and\n  individual reserving", "summary": "Insurers record detailed information related to claims (e.g. the cause of the\nclaim) and policies (e.g. the value of the insured risk) for pricing insurance\ncontracts. However, this information is largely neglected when estimating the\nreserve for future liabilities originating from past exposures. We present a\nflexible, yet highly interpretable framework for including these claim and\npolicy-specific covariates in a reserving model. Our framework focuses on three\nbuilding blocks in the development process of a claim: the time to settlement,\nthe number of payments and the size of each payment. We carefully choose a\ngeneralized linear model (GLM) to model each of these stochastic building\nblocks in discrete time. Since GLMs are applied in the pricing of insurance\ncontracts, our project bridges the gap between pricing and reserving\nmethodology. We propose model selection techniques for GLMs adapted for\ncensored data to select the relevant covariates in these models and demonstrate\nhow the selected covariates determine the granularity of our reserving model.\nAt one extreme, including many covariates captures the heterogeneity in the\ndevelopment process of individual claims, while at the other extreme, including\nno covariates corresponds to specifying a model for data aggregated in\ntwo-dimensional contingency tables, similar to the run-off triangles\ntraditionally used by reserving actuaries. The set of selected covariates then\nnaturally determines the position the actuary should take in between those two\nextremes. We illustrate our method with case studies on real life insurance\ndata sets. These case studies provide new insights in the covariates driving\nthe development of claims and demonstrate the accuracy and robustness of the\nreserving methodology over time.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1910.12692v1", "link": "http://arxiv.org/abs/1910.12692v1"}, {"title": "Recovering Model Structures from Large Low Rank and Sparse Covariance\n  Matrix Estimation", "summary": "Many popular statistical models, such as factor and random effects models,\ngive arise a certain type of covariance structures that is a summation of low\nrank and sparse matrices. This paper introduces a penalized approximation\nframework to recover such model structures from large covariance matrix\nestimation. We propose an estimator based on minimizing a non-likelihood loss\nwith separable non-smooth penalty functions. This estimator is shown to recover\nexactly the rank and sparsity patterns of these two components, and thus\npartially recovers the model structures. Convergence rates under various matrix\nnorms are also presented. To compute this estimator, we further develop a\nfirst-order iterative algorithm to solve a convex optimization problem that\ncontains separa- ble non-smooth functions, and the algorithm is shown to\nproduce a solution within O(1/t^2) of the optimal, after any finite t\niterations. Numerical performance is illustrated using simulated data and stock\nportfolio selection on S&P 100.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1111.1133v2", "link": "http://arxiv.org/abs/1111.1133v2"}, {"title": "Maximum Likelihood Estimation of Stochastic Frontier Models with\n  Endogeneity", "summary": "We study a closed-form maximum likelihood estimator of stochastic frontier\nmodels with endogeneity in cross-section data when both error components may be\ncorrelated with inputs and environmental variables. We achieve identification\nusing a control function assumption. We show that the conditional distribution\nof the stochastic inefficiency term given the control functions is a folded\nnormal distribution, which reduces to the half-normal distribution when both\ninputs and environmental variables are independent of the stochastic\ninefficiency term. Hence, our framework is a natural generalization of the\nnormal half-normal stochastic frontier model with endogeneity. We further\nprovide a Battese-Coelli estimator of technical efficiency in this context. Our\nestimator is computationally fast and easy to implement. We showcase its finite\nsample properties in Monte-Carlo simulations and an empirical application to\nfarmers in Nepal.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.12369v2", "link": "http://arxiv.org/abs/2004.12369v2"}, {"title": "A Consistent Heteroskedasticity Robust LM Type Specification Test for\n  Semiparametric Models", "summary": "This paper develops a consistent heteroskedasticity robust Lagrange\nMultiplier (LM) type specification test for semiparametric conditional mean\nmodels. Consistency is achieved by turning a conditional moment restriction\ninto a growing number of unconditional moment restrictions using series\nmethods. The proposed test statistic is straightforward to compute and is\nasymptotically standard normal under the null. Compared with the earlier\nliterature on series-based specification tests in parametric models, I rely on\nthe projection property of series estimators and derive a different\nnormalization of the test statistic. Compared with the recent test in Gupta\n(2018), I use a different way of accounting for heteroskedasticity. I\ndemonstrate using Monte Carlo studies that my test has superior finite sample\nperformance compared with the existing tests. I apply the test to one of the\nsemiparametric gasoline demand specifications from Yatchew and No (2001) and\nfind no evidence against it.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.07620v3", "link": "http://arxiv.org/abs/1810.07620v3"}, {"title": "On a numerical approximation scheme for construction of the early\n  exercise boundary for a class of nonlinear Black-Scholes equations", "summary": "The purpose of this paper is to construct the early exercise boundary for a\nclass of nonlinear Black--Scholes equations with a nonlinear volatility\ndepending on the option price. We review a method how to transform the problem\ninto a solution of a time depending nonlinear parabolic equation defined on a\nfixed domain. Results of numerical computation of the early exercise boundary\nfor various nonlinear Black--Scholes equations are also presented.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1009.5973v2", "link": "http://arxiv.org/abs/1009.5973v2"}, {"title": "Statistical Facts of Artificial Stock Market", "summary": "The paper reports the construction of artificial stock market that emerges\nthe similar statistical facts with real data in Indonesian stock market. We use\nthe individual but dominant data, i.e.: PT TELKOM in hourly interval. The\nartificial stock market shows standard statistical facts, e.g.: volatility\nclustering, the excess kurtosis of the distribution of return, and the scaling\nproperties with its breakdown in the crossover of Levy distribution to the\nGaussian one. From this point, the artificial stock market will always be\nevaluated in order to have comprehension about market process in Indonesian\nstock market generally.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0408358v1", "link": "http://arxiv.org/abs/cond-mat/0408358v1"}, {"title": "Generalized Duality for Model-Free Superhedging given Marginals", "summary": "In a discrete-time financial market, a generalized duality is established for\nmodel-free superhedging, given marginal distributions of the underlying asset.\nContrary to prior studies, we do not require contingent claims to be upper\nsemicontinuous, allowing for upper semi-analytic ones. The generalized duality\nstipulates an extended version of risk-neutral pricing. To compute the\nmodel-free superhedging price, one needs to find the supremum of expected\nvalues of a contingent claim, evaluated not directly under martingale\n(risk-neutral) measures, but along sequences of measures that converge, in an\nappropriate sense, to martingale ones. To derive the main result, we first\nestablish a portfolio-constrained duality for upper semi-analytic contingent\nclaims, relying on Choquet's capacitability theorem. As we gradually fade out\nthe portfolio constraint, the generalized duality emerges through delicate\nprobabilistic estimations.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1909.06036v2", "link": "http://arxiv.org/abs/1909.06036v2"}, {"title": "Market Impact Paradoxes", "summary": "The market impact (MI) of Volume Weighted Average Price (VWAP) orders is a\nconvex function of a trading rate, but most empirical estimates of transaction\ncost are concave functions. How is this possible? We show that isochronic\n(constant trading time) MI is slightly convex, and isochoric (constant trading\nvolume) MI is concave. We suggest a model that fits all trading regimes and\nguarantees no-dynamic-arbitrage.", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1312.3349v1", "link": "http://arxiv.org/abs/1312.3349v1"}, {"title": "Optimal Investment and Premium Policies under Risk Shifting and Solvency\n  Regulation", "summary": "Limited liability creates a conflict of interests between policyholders and\nshareholders of insurance companies. It provides shareholders with incentives\nto increase the risk of the insurer's assets and liabilities which, in turn,\nmight reduce the value policyholders attach to and premiums they are willing to\npay for insurance coverage. We characterize Pareto optimal investment and\npremium policies in this context and provide necessary and sufficient\nconditions for their existence and uniqueness. We then identify investment and\npremium policies under the risk shifting problem if shareholders cannot\ncredibly commit to an investment strategy before policies are sold and premiums\nare paid. Last, we analyze the effect of solvency regulation, such as Solvency\nII or the Swiss Solvency Test, on the agency cost of the risk shifting problem\nand calibrate our model to a non-life insurer average portfolio.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1103.1729v1", "link": "http://arxiv.org/abs/1103.1729v1"}, {"title": "Ethnic Groups' Access to State Power and Group Size", "summary": "Many countries are ethnically diverse. However, despite the benefits of\nethnic heterogeneity, ethnic-based political inequality and discrimination are\npervasive. Why is this? This study suggests that part of the variation in\nethnic-based political inequality depends on the relative size of ethnic groups\nwithin each country. Using group-level data for 569 ethnic groups in 175\ncountries from 1946 to 2017, I find evidence of an inverted-U-shaped\nrelationship between an ethnic group's relative size and its access to power.\nThis single-peaked relationship is robust to many alternative specifications,\nand a battery of robustness checks suggests that relative size influences\naccess to power. Through a very simple model, I propose an explanation based on\nan initial high level of political inequality, and on the incentives that more\npowerful groups have to continue limiting other groups' access to power. This\nexplanation incorporates essential elements of several existing theories on the\nrelationship between group size and discrimination, and suggests a new\nempirical prediction: the single-peaked pattern should be weaker in countries\nwhere political institutions have historically been less open. This additional\nprediction is supported by the data.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.08064v1", "link": "http://arxiv.org/abs/2003.08064v1"}, {"title": "Weakly nonlinear analysis of the Hamilton-Jacobi-Bellman equation\n  arising from pension savings management", "summary": "The main purpose of this paper is to analyze solutions to a fully nonlinear\nparabolic equation arising from the problem of optimal portfolio construction.\nWe show how the problem of optimal stock to bond proportion in the management\nof pension fund portfolio can be formulated in terms of the solution to the\nHamilton-Jacobi-Bellman equation. We analyze the solution from qualitative as\nwell as quantitative point of view. We construct useful bounds of solution\nyielding estimates for the optimal value of the stock to bond proportion in the\nportfolio. Furthermore we construct asymptotic expansions of a solution in\nterms of a small model parameter. Finally, we perform sensitivity analysis of\nthe optimal solution with respect to various model parameters and compare\nanalytical results of this paper with the corresponding known results arising\nfrom time-discrete dynamic stochastic optimization model.", "category": ["q-fin.PM", "q-fin.CP"], "id": "http://arxiv.org/abs/0905.0155v2", "link": "http://arxiv.org/abs/0905.0155v2"}, {"title": "Stochastic Multiplicative Processes for Financial Markets", "summary": "We study a stochastic multiplicative system composed of finite asynchronous\nelements to describe the wealth evolution in financial markets. We find that\nthe wealth fluctuations or returns of this system can be described by a walk\nwith correlated step sizes obeying truncated Levy-like distribution, and the\ncross-correlation between relative updated wealths is the origin of the\nnontrivial properties of returns, including the power law distribution with\nexponent outside the stable Levy regime and the long-range persistence of\nvolatility correlations.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0110273v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)00519-8"}, {"title": "Wavelet Based Volatility Clustering Estimation of Foreign Exchange Rates", "summary": "We have presented a novel technique of detecting intermittencies in a\nfinancial time series of the foreign exchange rate data of U.S.- Euro\ndollar(US/EUR) using a combination of both statistical and spectral techniques.\nThis has been possible due to Continuous Wavelet Transform (CWT) analysis which\nhas been popularly applied to fluctuating data in various fields science and\nengineering and is also being tried out in finance and economics. We have been\nable to qualitatively identify the presence of nonlinearity and chaos in the\ntime series of the foreign exchange rates for US/EURO (United States dollar to\nEuro Dollar) and US/UK (United States dollar to United Kingdom Pound)\ncurrencies. Interestingly we find that for the US-INDIA(United States dollar to\nIndian Rupee) foreign exchange rates, no such chaotic dynamics is observed.\nThis could be a result of the government control over the foreign exchange\nrates, instead of the market controlling them.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0910.0087v1", "link": "http://arxiv.org/abs/0910.0087v1"}, {"title": "Stock price fluctuations and the mimetic behaviors of traders", "summary": "We give a stochastic microscopic modelling of stock markets driven by\ncontinuous double auction. If we take into account the mimetic behavior of\ntraders, when they place limit order, our virtual markets shows the power-law\ntail of the distribution of returns with the exponent outside the Levy stable\nregion, the short memory of returns and the long memory of volatilities. The\nHurst exponent of our model is asymptotically 1/2. An explanation is also given\nfor the profile of the autocorrelation function, which is responsible for the\nvalue of the Hurst exponent.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0607202v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.017"}, {"title": "Nonparametric Regression with Multiple Thresholds: Estimation and\n  Inference", "summary": "This paper examines nonparametric regression with an exogenous threshold\nvariable, allowing for an unknown number of thresholds. Given the number of\nthresholds and corresponding threshold values, we first establish the\nasymptotic properties of the local constant estimator for a nonparametric\nregression with multiple thresholds. However, the number of thresholds and\ncorresponding threshold values are typically unknown in practice. We then use\nour testing procedure to determine the unknown number of thresholds and derive\nthe limiting distribution of the proposed test. The Monte Carlo simulation\nresults indicate the adequacy of the modified test and accuracy of the\nsequential estimation of the threshold values. We apply our testing procedure\nto an empirical study of the 401(k) retirement savings plan with income\nthresholds.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1705.09418v2", "link": "http://arxiv.org/abs/1705.09418v2"}, {"title": "Turing's Children: Representation of Sexual Minorities in STEM", "summary": "We provide the first nationally representative estimates of sexual minority\nrepresentation in STEM fields by studying 142,641 men and women in same-sex\ncouples from the 2009-2018 American Community Surveys. These data indicate that\nmen in same-sex couples are 12 percentage points less likely to have completed\na bachelor's degree in a STEM field compared to men in different-sex couples;\nthere is no gap observed for women in same-sex couples compared to women in\ndifferent-sex couples. The STEM gap between men in same-sex and different-sex\ncouples is larger than the STEM gap between white and black men but is smaller\nthan the gender STEM gap. We also document a gap in STEM occupations between\nmen in same-sex and different-sex couples, and we replicate this finding using\nindependently drawn data from the 2013-2018 National Health Interview Surveys.\nThese differences persist after controlling for demographic characteristics,\nlocation, and fertility. Our findings further the call for interventions\ndesigned at increasing representation of sexual minorities in STEM.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.06664v1", "link": "http://arxiv.org/abs/2005.06664v1"}, {"title": "Local Volatility Pricing Models for Long-dated FX Derivatives", "summary": "We study the local volatility function in the Foreign Exchange market where\nboth domestic and foreign interest rates are stochastic. This model is suitable\nto price long-dated FX derivatives. We derive the local volatility function and\nobtain several results that can be used for the calibration of this local\nvolatility on the FX option's market. Then, we study an extension to obtain a\nmore general volatility model and propose a calibration method for the local\nvolatility associated to this model.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1204.0633v1", "link": "http://arxiv.org/abs/1204.0633v1"}, {"title": "Executing large orders in a microscopic market model", "summary": "In a recent paper, Alfonsi, Fruth and Schied (AFS) propose a simple order\nbook based model for the impact of large orders on stock prices. They use this\nmodel to derive optimal strategies for the execution of large orders. We apply\nthese strategies to an agent-based stochastic order book model that was\nrecently proposed by Bovier, \\v{C}ern\\'{y} and Hryniv, but already the\ncalibration fails. In particular, from our simulations the recovery speed of\nthe market after a large order is clearly dependent on the order size, whereas\nthe AFS model assumes a constant speed. For this reason, we propose a\ngeneralization of the AFS model, the GAFS model, that incorporates this\ndependency, and prove the optimal investment strategies. As a corollary, we\nfind that we can derive the ``correct'' constant resilience speed for the AFS\nmodel from the GAFS model such that the optimal strategies of the AFS and the\nGAFS model coincide. Finally, we show that the costs of applying the optimal\nstrategies of the GAFS model to the artificial market environment still differ\nsignificantly from the model predictions, indicating that even the improved\nmodel does not capture all of the relevant details of a real market.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/0904.4131v2", "link": "http://arxiv.org/abs/0904.4131v2"}, {"title": "Dual formulation of the utility maximization problem: the case of\n  nonsmooth utility", "summary": "We study the dual formulation of the utility maximization problem in\nincomplete markets when the utility function is finitely valued on the whole\nreal line. We extend the existing results in this literature in two directions.\n  First, we allow for nonsmooth utility functions, so as to include the\nshortfall minimization problems in our framework. Second, we allow for the\npresence of some given liability or a random endowment. In particular, these\nresults provide a dual formulation of the utility indifference valuation rule.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/math/0405290v1", "link": "http://dx.doi.org/10.1214/105051604000000062"}, {"title": "Robust replication of barrier-style claims on price and volatility", "summary": "We show how to price and replicate a variety of barrier-style claims written\non the $\\log$ price $X$ and quadratic variation $\\langle X \\rangle$ of a risky\nasset. Our framework assumes no arbitrage, frictionless markets and zero\ninterest rates. We model the risky asset as a strictly positive continuous\nsemimartingale with an independent volatility process. The volatility process\nmay exhibit jumps and may be non-Markovian. As hedging instruments, we use only\nthe underlying risky asset, zero-coupon bonds, and European calls and puts with\nthe same maturity as the barrier-style claim. We consider knock-in, knock-out\nand rebate claims in single and double barrier varieties.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1508.00632v3", "link": "http://arxiv.org/abs/1508.00632v3"}, {"title": "Singular Fourier-Pad\u00e9 Series Expansion of European Option Prices", "summary": "We apply a new numerical method, the singular Fourier-Pad\\'e (SFP) method\ninvented by Driscoll and Fornberg (2001, 2011), to price European-type options\nin L\\'evy and affine processes. The motivation behind this application is to\nreduce the inefficiency of current Fourier techniques when they are used to\napproximate piecewise continuous (non-smooth) probability density functions.\nWhen techniques such as fast Fourier transforms and Fourier series are applied\nto price and hedge options with non-smooth probability density functions, they\ncause the Gibbs phenomenon, accordingly, the techniques converge slowly for\ndensity functions with jumps in value or derivatives. This seriously adversely\naffects the efficiency and accuracy of these techniques. In this paper, we\nderive pricing formulae and their option Greeks using the SFP method to resolve\nthe Gibbs phenomenon and restore the global spectral convergence rate.\nMoreover, we show that our method requires a small number of terms to yield\nfast error convergence, and it is able to accurately price any European-type\noption deep in/out of the money and with very long/short maturities.\nFurthermore, we conduct an error-bound analysis of the SFP method in option\npricing. This new method performs favourably in numerical experiments compared\nwith existing techniques.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1706.06709v3", "link": "http://arxiv.org/abs/1706.06709v3"}, {"title": "Rational Inattention and Retirement Puzzles", "summary": "I present evidence incorporating costly thought solves three puzzles in the\nretirement literature. The first puzzle is, given incentives, the extent of\nbunching of labour market exits at legislated state pension ages (SPA) seems\nincompatible with rational expectations. Adding to the evidence for this\npuzzle, I include an empirical analysis focusing on whether liquidity\nconstraints can explain this bunching and find they cannot. The nature of this\npuzzle is clarified by exploring a life-cycle model with rational agents that\nmatches aggregate profiles. This model succeeds in matching aggregates by\noverestimating the impact of the SPA on poorer individuals whilst\nunderestimating its impact on wealthier people. The second puzzle is people are\noften mistaken about their own pension provisions. Concerning the second\npuzzle, I incorporate rational inattention to the SPA into the aforementioned\nlife-cycle model, allowing for mistaken beliefs. To the best of my knowledge,\nthis paper is the first not only to incorporate rational inattention into a\nlife-cycle model but also to assess a rationally inattentive model against\nnon-experimental individual choice data. This facilitates another important\ncontribution: discipling the cost of attention with subjective belief data.\nPreliminary results indicate rational inattention improves the aggregate fit\nand better matches the response of participation to the SPA across the wealth\ndistribution, hence offering a resolution to the first puzzle. The third puzzle\nis despite actuarially advantageous options to defer receipt of pension\nbenefits, take up is extremely low. An extension of the model generates an\nexplanation of this last puzzle: the actuarial calculations implying deferral\nis preferable ignore the utility cost of tracking your pension which can be\navoided by claiming. These puzzles are researched in the context of the reform\nto the UK female SPA.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1904.06520v1", "link": "http://arxiv.org/abs/1904.06520v1"}, {"title": "Avoiding zero probability events when computing Value at Risk\n  contributions: a Malliavin calculus approach", "summary": "This paper is concerned with the process of risk allocation for a generic\nmultivariate model when the risk measure is chosen as the Value-at-Risk (VaR).\nMaking use of Malliavin calculus, we recast the traditional Euler contributions\nfrom an expectation conditional to an event of zero probability to a ratio of\nconditional expectations, where both the numerator and the denominator's\nconditioning events have positive probability. For several different models we\nshow empirically that the estimator using this novel representation has no\nperceivable bias and variance smaller than a standard estimator used in\npractice.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/2004.13235v1", "link": "http://arxiv.org/abs/2004.13235v1"}, {"title": "Record statistics for biased random walks, with an application to\n  financial data", "summary": "We consider the occurrence of record-breaking events in random walks with\nasymmetric jump distributions. The statistics of records in symmetric random\nwalks was previously analyzed by Majumdar and Ziff and is well understood.\nUnlike the case of symmetric jump distributions, in the asymmetric case the\nstatistics of records depends on the choice of the jump distribution. We\ncompute the record rate $P_n(c)$, defined as the probability for the $n$th\nvalue to be larger than all previous values, for a Gaussian jump distribution\nwith standard deviation $\\sigma$ that is shifted by a constant drift $c$. For\nsmall drift, in the sense of $c/\\sigma \\ll n^{-1/2}$, the correction to\n$P_n(c)$ grows proportional to arctan$(\\sqrt{n})$ and saturates at the value\n$\\frac{c}{\\sqrt{2} \\sigma}$. For large $n$ the record rate approaches a\nconstant, which is approximately given by\n$1-(\\sigma/\\sqrt{2\\pi}c)\\textrm{exp}(-c^2/2\\sigma^2)$ for $c/\\sigma \\gg 1$.\nThese asymptotic results carry over to other continuous jump distributions with\nfinite variance. As an application, we compare our analytical results to the\nrecord statistics of 366 daily stock prices from the Standard & Poors 500\nindex. The biased random walk accounts quantitatively for the increase in the\nnumber of upper records due to the overall trend in the stock prices, and after\ndetrending the number of upper records is in good agreement with the symmetric\nrandom walk. However the number of lower records in the detrended data is\nsignificantly reduced by a mechanism that remains to be identified.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1103.0893v1", "link": "http://dx.doi.org/10.1103/PhysRevE.83.051109"}, {"title": "Systematic analysis of group identification in stock markets", "summary": "We propose improved methods to identify stock groups using the correlation\nmatrix of stock price changes. By filtering out the marketwide effect and the\nrandom noise, we construct the correlation matrix of stock groups in which\nnontrivial high correlations between stocks are found. Using the filtered\ncorrelation matrix, we successfully identify the multiple stock groups without\nany extra knowledge of the stocks by the optimization of the matrix\nrepresentation and the percolation approach to the correlation-based network of\nstocks. These methods drastically reduce the ambiguities while finding stock\ngroups using the eigenvectors of the correlation matrix.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0503076v3", "link": "http://dx.doi.org/10.1103/PhysRevE.72.046133"}, {"title": "On the Co-movement of Crude, Gold Prices and Stock Index in Indian\n  Market", "summary": "This non-linear relationship in the joint time-frequency domain has been\nstudied for the Indian National Stock Exchange (NSE) with the international\nGold price and WTI Crude Price being converted from Dollar to Indian National\nRupee based on that week's closing exchange rate. Though a good correlation was\nobtained during some period, but as a whole no such cointegration relation can\nbe found out. Using the \\textit{Discrete Wavelet Analysis}, the data was\ndecomposed and the presence of Granger Causal relations was tested.\nUnfortunately no significant relationships are being found. We then studied the\n\\textit{Wavelet Coherence} of the two pairs viz. NSE-Nifty \\& Gold and\nNSE-Nifty \\& Crude. For different frequencies, the coherence between the pairs\nhave been studied. At lower frequencies, some relatively good coherence have\nbeen found. In this paper, we report for the first time the co-movements\nbetween Crude Oil, Gold and Indian Stock Market Index using Wavelet Analysis\n(both Discrete and Continuous), a technique which is most sophisticated and\nrecent in market analysis. Thus for long term traders they can include gold\nand/or crude in their portfolio along with NSE-Nifty index in order to decrease\nthe risk(volatility) of the portfolio for Indian Market. But for short term\ntraders, it will not be effective, not to include all the three in their\nportfolio.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1904.05317v1", "link": "http://arxiv.org/abs/1904.05317v1"}, {"title": "Quantifying dynamics of the financial correlations", "summary": "A novel application of the correlation matrix formalism to study dynamics of\nthe financial evolution is presented. This formalism allows to quantify the\nmemory effects as well as some potential repeatable intradaily structures in\nthe financial time-series. The present study is based on the high-frequency\nDeutsche Aktienindex (DAX) data over the time-period between November 1997 and\nDecember 1999 and demonstrates a power of the method. In this way two\nsignificant new aspects of the DAX evolution are identified: (i) the memory\neffects turn out to be sizably shorter than what the standard autocorrelation\nfunction analysis seems to indicate and (ii) there exist short term repeatable\nstructures in fluctuations that are governed by a distinct dynamics. The former\nof these results may provide an argument in favour of the market efficiency\nwhile the later one may indicate origin of the difficulty in reaching a\nGaussian limit, expected from the central limit theorem, in the distribution of\nreturns on longer time-horizons.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0102402v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00289-8"}, {"title": "PAGAN: Portfolio Analysis with Generative Adversarial Networks", "summary": "Since decades, the data science community tries to propose prediction models\nof financial time series. Yet, driven by the rapid development of information\ntechnology and machine intelligence, the velocity of today's information leads\nto high market efficiency. Sound financial theories demonstrate that in an\nefficient marketplace all information available today, including expectations\non future events, are represented in today prices whereas future price trend is\ndriven by the uncertainty. This jeopardizes the efforts put in designing\nprediction models. To deal with the unpredictability of financial systems,\ntoday's portfolio management is largely based on the Markowitz framework which\nputs more emphasis in the analysis of the market uncertainty and less in the\nprice prediction. The limitation of the Markowitz framework stands in taking\nvery strong ideal assumptions about future returns probability distribution.\n  To address this situation we propose PAGAN, a pioneering methodology based on\ndeep generative models. The goal is modeling the market uncertainty that\nultimately is the main factor driving future trends. The generative model\nlearns the joint probability distribution of price trends for a set of\nfinancial assets to match the probability distribution of the real market. Once\nthe model is trained, a portfolio is optimized by deciding the best\ndiversification to minimize the risk and maximize the expected returns observed\nover the execution of several simulations. Applying the model for analyzing\npossible futures, is as simple as executing a Monte Carlo simulation, a\ntechnique very familiar to finance experts. The experimental results on\ndifferent portfolios representing different geopolitical areas and industrial\nsegments constructed using real-world public data sets demonstrate promising\nresults.", "category": ["q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/1909.10578v1", "link": "http://arxiv.org/abs/1909.10578v1"}, {"title": "Erratum for: Smile dynamics -- a theory of the implied leverage effect", "summary": "We correct a mistake in the published version of our paper. Our new\nconclusion is that the \"implied leverage effect\" for single stocks is\nunderestimated by option markets for short maturities and overestimated for\nlong maturities, while it is always overestimated for OEX options, except for\nthe shortest maturities where the revised theory and data match perfectly.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1105.5082v2", "link": "http://arxiv.org/abs/1105.5082v2"}, {"title": "A three dimensional stochastic Model for Claim Reserving", "summary": "Within the Solvency II framework the insurance industry requires a realistic\nmodelling of the risk processes relevant for its business. Every insurance\ncompany should be capable of running a holistic risk management process to meet\nthis challenge. For property and casualty (P&C) insurance companies the risk\nadequate modelling of the claim reserves is a very important topic as this\nliabilities determine up to 70% percent of the balance sum. We propose a three\ndimensional (3D) stochastic model for claim reserving. It delivers consistently\nthe reserve's distribution function as well as the distributions of all parts\nof it that are needed for accounting and controlling. The calibration methods\nfor the model are well known from data analysis and they are applicable in an\npractitioner environment. We evaluate the model numerically by the help of\nMonte Carlo (MC) simulation. Classical actuarial reserve models are two\ndimensional (2D). They lead to an estimation algorithm that is applied on a 2D\nmatrix, the run off triangle. Those methods (for instance the Chain - Ladder or\nthe Bornhuetter - Ferguson method) are widely used in practice nowadays and\ngive rise to several problems: They estimate the reserves' expectation and some\nof them - under very restriction assumptions - the variance. They provide no\ninformation about the tail of the reserve's distribution, what would be most\nimportant for risk calculation, for assessing the insurance company's financial\nstability and economic situation. Additionally, due to the projection of the\nclaim process into a two dimensional space the results are very often distorted\nand dependent on the kind of projection. Therefore we extend the classical 2D\nmodels to a 3D space because we find inconsistencies generated by inadequate\nprojections into the 2D spaces.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1009.4146v1", "link": "http://arxiv.org/abs/1009.4146v1"}, {"title": "On the Existence of Martingale Measures in Jump Diffusion Market Models", "summary": "In the context of jump-diffusion market models we construct examples that\nsatisfy the weaker no-arbitrage condition of NA1 (NUPBR), but not NFLVR. We\nshow that in these examples the only candidate for the density process of an\nequivalent local martingale measure is a supermartingale that is not a\nmartingale, not even a local martingale. This candidate is given by the\nsupermartingale deflator resulting from the inverse of the discounted growth\noptimal portfolio. In particular, we con- sider an example with constraints on\nthe portfolio that go beyond the standard ones for admissibility.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1511.08349v1", "link": "http://arxiv.org/abs/1511.08349v1"}, {"title": "The log-periodic-AR(1)-GARCH(1,1) model for financial crashes", "summary": "This paper intends to meet recent claims for the attainment of more rigorous\nstatistical methodology within the econophysics literature. To this end, we\nconsider an econometric approach to investigate the outcomes of the\nlog-periodic model of price movements, which has been largely used to forecast\nfinancial crashes. In order to accomplish reliable statistical inference for\nunknown parameters, we incorporate an autoregressive dynamic and a conditional\nheteroskedasticity structure in the error term of the original model, yielding\nthe log-periodic-AR(1)-GARCH(1,1) model. Both the original and the extended\nmodels are fitted to financial indices of U. S. market, namely S&P500 and\nNASDAQ. Our analysis reveal two main points: (i) the\nlog-periodic-AR(1)-GARCH(1,1) model has residuals with better statistical\nproperties and (ii) the estimation of the parameter concerning the time of the\nfinancial crash has been improved.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0801.4341v1", "link": "http://dx.doi.org/10.1140/epjb/e2008-00085-1"}, {"title": "An algorithm for calculating the set of superhedging portfolios in\n  markets with transaction costs", "summary": "We study the explicit calculation of the set of superhedging portfolios of\ncontingent claims in a discrete-time market model for d assets with\nproportional transaction costs. The set of superhedging portfolios can be\nobtained by a recursive construction involving set operations, going backward\nin the event tree. We reformulate the problem as a sequence of linear vector\noptimization problems and solve it by adapting known algorithms. The\ncorresponding superhedging strategy can be obtained going forward in the tree.\nExamples are given involving multiple correlated assets and basket options.\nFurthermore, we relate existing algorithms for the calculation of the scalar\nsuperhedging price to the set-valued algorithm by a recent duality theory for\nvector optimization problems. The main contribution of the paper is to\nestablish the connection to linear vector optimization, which allows to solve\nnumerically multi-asset superhedging problems under transaction costs.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1107.5720v3", "link": "http://arxiv.org/abs/1107.5720v3"}, {"title": "Correlation of financial markets in times of crisis", "summary": "Using the eigenvalues and eigenvectors of correlations matrices of some of\nthe main financial market indices in the world, we show that high volatility of\nmarkets is directly linked with strong correlations between them. This means\nthat markets tend to behave as one during great crashes. In order to do so, we\ninvestigate several financial market crises that occurred in the years 1987\n(Black Monday), 1989 (Russian crisis), 2001 (Burst of the dot-com bubble and\nSeptember 11), and 2008 (Subprime Mortgage Crisis), which mark some of the\nlargest downturns of financial markets in the last three decades.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1102.1339v2", "link": "http://dx.doi.org/10.1016/j.physa.2011.07.023"}, {"title": "Incentive compatibility in sender-receiver stopping games", "summary": "We introduce a model of sender-receiver stopping games, where the state of\nthe world follows an iid--process throughout the game. At each period, the\nsender observes the current state, and sends a message to the receiver,\nsuggesting either to stop or to continue. The receiver, only seeing the message\nbut not the state, decides either to stop the game, or to continue which takes\nthe game to the next period. The payoff to each player is a function of the\nstate when the receiver quits, with higher states leading to better payoffs.\nThe horizon of the game can be finite or infinite.\n  We prove existence and uniqueness of responsive (i.e. non-babbling) Perfect\nBayesian Equilibrium (PBE) under mild conditions on the game primitives in the\ncase where the players are sufficiently patient. The responsive PBE has a\nremarkably simple structure, which builds on the identification of an\neasy-to-implement and compute class of threshold strategies for the sender.\nWith the help of these threshold strategies, we derive simple expressions\ndescribing this PBE. It turns out that in this PBE the receiver obediently\nfollows the recommendations of the sender. Hence, surprisingly, the sender\nalone plays the decisive role, and regardless of the payoff function of the\nreceiver the sender always obtains the best possible payoff for himself.", "category": [], "id": "http://arxiv.org/abs/2004.01910v1", "link": "http://arxiv.org/abs/2004.01910v1"}, {"title": "Interdependence of sectors of economic activities for world countries\n  from the reduced Google matrix analysis of WTO data", "summary": "We apply the recently developed reduced Google matrix algorithm for the\nanalysis of the OECD-WTO world network of economic activities. This approach\nallows to determine interdependences and interactions of economy sectors of\nseveral countries, including China, Russia and USA, properly taking into\naccount the influence of all other world countries and their economic\nactivities. Within this analysis we also obtain the sensitivity of economy\nsectors and EU countries to petroleum activity sector. We show that this\napproach takes into account multiplicity of network links with economy\ninteractions between countries and activity sectors thus providing more rich\ninformation compared to the usual export-import analysis.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1905.06489v2", "link": "http://arxiv.org/abs/1905.06489v2"}, {"title": "A dynamical approach to Zipf's law", "summary": "The rank-size plots of very different systems are usually described in terms\nof Zipf's law, but the deep meaning of this scaling law is still controversial.\nWe show how usual criteria used to identify Zipf's scaling fail to capture an\nessential feature of this phenomenon, that is the coherent evolution of the\nsystem and the space in which it is embedded. We therefore identify a Zipfian\ndynamics in terms of dynamical constraints which relate in time the level of\nsampling and the parameters of the probability distribution of sizes. This\npermits to introduce the concepts of coherent evolution, which is the key\ningredient to develop a genuine Zipf's behaviour. We finally discuss a number\nof practical examples. For instance, earthquakes can evolve only incoherently\nand thus show only a spurious Zipf's law, while natural language dynamics is\nintrinsically Zipfian due to dynamical coherence imposed by grammar rules.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.04844v2", "link": "http://arxiv.org/abs/1911.04844v2"}, {"title": "Statistical properties of the Jakarta and Kuala Lumpur stock exchange\n  indices before and after crash", "summary": "Using the tools developed for statistical physics, we simultaneously analyze\nstatistical properties of the Jakarta and Kuala Lumpur Stock Exchange indices.\nIn spite of the small number of data used in the analysis, the result shows the\nuniversal behavior of complex systems previously found in the leading stock\nindices. We also analyze their features before and after the financial crisis.\nWe found that after the crisis both stocks do not show a same statistical\nbehavior. The impact of currency controls is observed in the distribution of\nindex returns.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0208574v1", "link": "http://arxiv.org/abs/cond-mat/0208574v1"}, {"title": "How Non-Arbitrage, Viability and Num\u00e9raire Portfolio are Related", "summary": "This paper proposes two approaches that quantify the exact relationship among\nthe viability, the absence of arbitrage, and/or the existence of the\nnum\\'eraire portfolio under minimal assumptions and for general continuous-time\nmarket models. Precisely, our first and principal contribution proves the\nequivalence among the No-Unbounded-Profit-with-Bounded-Risk condition (NUPBR\nhereafter), the existence of the num\\'eraire portfolio, and the existence of\nthe optimal portfolio under an equivalent probability measure for any \"nice\"\nutility and positive initial capital. Herein, a 'nice\" utility is any smooth\nvon Neumann-Morgenstern utility satisfying Inada's conditions and the\nelasticity assumptions of Kramkov and Schachermayer. Furthermore, the\nequivalent probability measure ---under which the utility maximization problems\nhave solutions--- can be chosen as close to the real-world probability measure\nas we want (but might not be equal). Without changing the underlying\nprobability measure and under mild assumptions, our second contribution proves\nthat the NUPBR is equivalent to the \"{\\it local}\" existence of the optimal\nportfolio. This constitutes an alternative to the first contribution, if one\ninsists on working under the real-world probability. These two contributions\nlead naturally to new types of viability that we call weak and local\nviabilities.", "category": ["q-fin.GN", "math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1211.4598v3", "link": "http://arxiv.org/abs/1211.4598v3"}, {"title": "The role of money and the financial sector in energy-economy models used\n  for assessing climate policy", "summary": "This paper outlines a critical gap in the assessment methodology used to\nestimate the macroeconomic costs and benefits of climate policy. It shows that\nthe vast majority of models used for assessing climate policy use assumptions\nabout the financial system that sit at odds with the observed reality. In\nparticular, the models' assumptions lead to `crowding out' of capital, which\ncause them to show negative impacts from climate policy in virtually all cases.\nWe compare this approach with that of the E3ME model, which follows\nnon-equilibrium economic theory and adopts a more empirical approach. While the\nnon-equilibrium model also has limitations, its treatment of the financial\nsystem is more consistent with reality and it shows that green investment need\nnot crowd out investment in other parts of the economy -- and may therefore\noffer an economic stimulus.\n  The implication of this finding is that standard CGE models consistently\nover-estimate the costs of climate policy in terms of GDP and welfare,\npotentially by a substantial amount. These findings overly restrict the range\nof possible emission pathways accessible using climate policy from the\nviewpoint of the decision-maker, and may also lead to misleading information\nused for policy making. Improvements in both modelling approaches should be\nsought with some urgency -- both to provide a better assessment of potential\nclimate policy and to improve understanding of the dynamics of the global\nfinancial system more generally.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1512.02912v1", "link": "http://dx.doi.org/10.1080/14693062.2016.1277685"}, {"title": "Equilibria and Systemic Risk in Saturated Networks", "summary": "We undertake a fundamental study of network equilibria modeled as solutions\nof fixed point of monotone linear functions with saturation nonlinearities. The\nconsidered model extends one originally proposed to study systemic risk in\nnetworks of financial institutions interconnected by mutual obligations and is\none of the simplest continuous models accounting for shock propagation\nphenomena and cascading failure effects. We first derive explicit expressions\nfor network equilibria and prove necessary and sufficient conditions for their\nuniqueness encompassing and generalizing several results in the literature.\nThen, we study jump discontinuities of the network equilibria when the\nexogenous flows cross a certain critical region consisting of the union of\nfinitely many linear submanifolds of co-dimension 1. This is of particular\ninterest in the financial systems context, as it shows that even small shocks\naffecting the values of the assets of few nodes, can trigger catastrophic\naggregated loss to the system and cause the default of several agents.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1912.04815v1", "link": "http://arxiv.org/abs/1912.04815v1"}, {"title": "A stochastic reachability approach to portfolio construction in finance\n  industry", "summary": "In finance industry portfolio construction deals with how to divide the\ninvestors' wealth across an asset-classes' menu in order to maximize the\ninvestors' gain. Main approaches in use at the present are based on variations\nof the classical Markowitz model. However, recent evolutions of the world\nmarket showed limitations of this method and motivated many researchers and\npractitioners to study alternative methodologies to portfolio construction. In\nthis paper we propose one approach to optimal portfolio construction based on\nrecent results on stochastic reachability, which overcome some of the limits of\ncurrent approaches. Given a sequence of target sets that the investors would\nlike their portfolio to stay within, the optimal portfolio allocation is\nsynthesized in order to maximize the joint probability for the portfolio value\nto fulfill the target sets requirements. A case study in the US market is given\nwhich shows benefits from the proposed methodology in portfolio construction. A\ncomparison with traditional approaches is included.", "category": ["q-fin.PM", "q-fin.CP"], "id": "http://arxiv.org/abs/0907.3301v1", "link": "http://arxiv.org/abs/0907.3301v1"}, {"title": "Aftershock prediction for high-frequency financial markets' dynamics", "summary": "The occurrence of aftershocks following a major financial crash manifests the\ncritical dynamical response of financial markets. Aftershocks put additional\nstress on markets, with conceivable dramatic consequences. Such a phenomenon\nhas been shown to be common to most financial assets, both at high and low\nfrequency. Its present-day description relies on an empirical characterization\nproposed by Omori at the end of 1800 for seismic earthquakes. We point out the\nlimited predictive power in this phenomenological approach and present a\nstochastic model, based on the scaling symmetry of financial assets, which is\npotentially capable to predict aftershocks occurrence, given the main shock\nmagnitude. Comparisons with S&P high-frequency data confirm this predictive\npotential.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1203.5893v2", "link": "http://dx.doi.org/10.1007/978-88-470-2553-0_4"}, {"title": "Dynamic Conic Finance: Pricing and Hedging in Market Models with\n  Transaction Costs via Dynamic Coherent Acceptability Indices", "summary": "In this paper we present a theoretical framework for determining dynamic ask\nand bid prices of derivatives using the theory of dynamic coherent\nacceptability indices in discrete time. We prove a version of the First\nFundamental Theorem of Asset Pricing using the dynamic coherent risk measures.\nWe introduce the dynamic ask and bid prices of a derivative contract in markets\nwith transaction costs. Based on these results, we derive a representation\ntheorem for the dynamic bid and ask prices in terms of dynamically consistent\nsequence of sets of probability measures and risk-neutral measures. To\nillustrate our results, we compute the ask and bid prices of some\npath-dependent options using the dynamic Gain-Loss Ratio.", "category": ["q-fin.RM", "math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1205.4790v3", "link": "http://dx.doi.org/10.1142/S0219024913500027"}, {"title": "Pricing bonds with optional sinking feature using Markov Decision\n  Processes", "summary": "An efficient method to price bonds with optional sinking feature is\npresented. Such instruments equip their issuer with the option (but not the\nobligation) to redeem parts of the notional prior to maturity, therefore the\nfuture cash flows are random. In a one-factor model for the issuer's default\nintensity we show that the pricing algorithm can be formulated as a Markov\nDecision Process, which is both accurate and quick. The method is demonstrated\nusing a 1.5-factor credit-equity model which defines the default intensity in a\nreciprocal relationship to the issuer's stock price process, termed\njump-to-default extended model with constant elasticity of variance (JDCEV).", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1305.5220v1", "link": "http://arxiv.org/abs/1305.5220v1"}, {"title": "The temporal evolution of venture investment strategies in sector space", "summary": "We analyze the sectoral dynamics of startup venture financing. Based on a\ndataset of 52000 start-ups and 110000 funding rounds in the United States from\n2000 to 2017, and by applying both Principal Component Analysis (PCA) and\nTensor Component Analysis (TCA) in sector space, we visualize and measure the\nevolution of the investment strategies of different classes of investors across\nsectors and over time. During the past decade, we observe a coherent evolution\nof early stage investments towards a lower-tech area in sector space,\nassociated with a marked increase in the concentration of investments and with\nthe emergence of a newer class of investors called accelerators. We provide\nevidence for a more recent shift of start-up venture financing away from the\nprevious one.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1906.01980v1", "link": "http://arxiv.org/abs/1906.01980v1"}, {"title": "A financial market with singular drift and no arbitrage", "summary": "We study a financial market where the risky asset is modelled by a geometric\nIt\\^o-L\\'evy process, with a singular drift term.This can for example model a\nsituation where the asset price is partially controlled by a company which\nintervenes when the price is reaching a certain lower barrier. See e.g. Jarrow\n& Protter {JP} for an explanation and discussion of this model in the Brownian\nmotion case. As already pointed out by Karatzas & Shreve {KS} (in the Brownian\nmotion case), this allows for arbitrages in the market. However, the situation\nin the case of jumps is not clear. Moreover, it is not clear what happens if\nthere is a delay in the system.\n  In this paper we consider a jump diffusion market model with a singular drift\nterm modelled as the local time of a given process, and with a delay \\theta> 0\nin the information flow available for the trader. Using white noise calculus we\ncompute explicitly the optimal consumption rate and portfolio in this case and\nwe show that the maximal value is finite as long as \\theta> 0. This implies\nthat there is no arbitrage in the market in that case. However, when \\theta\ngoes to 0, the value goes to infinity. This is in agreement with the above\nresult that is an arbitrage when there is no delay.\n  Our model is also relevant for high frequency trading issues. See e.g.\nLachapelle et al {LLLL} and the references therein.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1909.12578v3", "link": "http://arxiv.org/abs/1909.12578v3"}, {"title": "Determinants of occupational mobility within the social stratification\n  structure in India", "summary": "In this study, we make use of empirically observed occupational\nstratification patterns, in order to identify the relationship between\neducation and social mobility of individuals - the latter is approximated by\nthe social distance of an individual's occupation from his/her household's\ntraditional niche occupation. Our study draws upon a novel occupational network\nconstruction proposed in Lambert et.al (2018), with slight adjustments, to\nempirically identify social stratification patterns using cross sectional\nhousehold surveys available in the Indian context. We use IHDS-2 data-set for\nthe purpose of our study.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.06802v1", "link": "http://arxiv.org/abs/2005.06802v1"}, {"title": "Neural networks for option pricing and hedging: a literature review", "summary": "Neural networks have been used as a nonparametric method for option pricing\nand hedging since the early 1990s. Far over a hundred papers have been\npublished on this topic. This note intends to provide a comprehensive review.\nPapers are compared in terms of input features, output variables, benchmark\nmodels, performance measures, data partition methods, and underlying assets.\nFurthermore, related work and regularisation techniques are discussed.", "category": ["q-fin.CP", "q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1911.05620v2", "link": "http://arxiv.org/abs/1911.05620v2"}, {"title": "Robust Utility Maximizing Strategies under Model Uncertainty and their\n  Convergence", "summary": "In this paper we investigate a utility maximization problem with drift\nuncertainty in a continuous-time Black--Scholes type financial market. We\nimpose a constraint on the admissible strategies that prevents a pure bond\ninvestment and we include uncertainty by means of ellipsoidal uncertainty sets\nfor the drift. Our main results consist in finding an explicit representation\nof the optimal strategy and the worst-case parameter and proving a minimax\ntheorem that connects our robust utility maximization problem with the\ncorresponding dual problem. Moreover, we show that, as the degree of model\nuncertainty increases, the optimal strategy converges to a generalized uniform\ndiversification strategy.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1909.01830v1", "link": "http://arxiv.org/abs/1909.01830v1"}, {"title": "Spectrum-based estimators of the bivariate Hurst exponent", "summary": "We introduce two new estimators of the bivariate Hurst exponent in the\npower-law cross-correlations setting -- the cross-periodogram and local\n$X$-Whittle estimators -- as generalizations of their univariate counterparts.\nAs the spectrum-based estimators are dependent on a part of the spectrum taken\ninto consideration during estimation, a simulation study showing performance of\nthe estimators under varying bandwidth parameter as well as correlation between\nprocesses and their specification is provided as well. The newly introduced\nestimators are less biased than the already existent averaged periodogram\nestimator which, however, has slightly lower variance. The spectrum-based\nestimators can serve as a good complement to the popular time domain\nestimators.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1408.6637v2", "link": "http://dx.doi.org/10.1103/PhysRevE.90.062802"}, {"title": "The Wealth of Nations: Complexity Science for an Interdisciplinary\n  Approach in Economics", "summary": "Classic economic science is reaching the limits of its explanatory powers.\nComplexity science uses an increasingly larger set of different methods to\nanalyze physical, biological, cultural, social, and economic factors, providing\na broader understanding of the socio-economic dynamics involved in the\ndevelopment of nations worldwide. The use of tools developed in the natural\nsciences, such as thermodynamics, evolutionary biology, and analysis of complex\nsystems, help us to integrate aspects, formerly reserved to the social\nsciences, with the natural sciences. This integration reveals details of the\nsynergistic mechanisms that drive the evolution of societies. By doing so, we\nincrease the available alternatives for economic analysis and provide ways to\nincrease the efficiency of decision-making mechanisms in complex social\ncontexts. This interdisciplinary analysis seeks to deepen our understanding of\nwhy chronic poverty is still common, and how the emergence of prosperous\ntechnological societies can be made possible. This understanding should\nincrease the chances of achieving a sustainable, harmonious and prosperous\nfuture for humanity. The analysis evidences that complex fundamental economic\nproblems require multidisciplinary approaches and rigorous application of the\nscientific method if we want to advance significantly our understanding of\nthem. The analysis reveals viable routes for the generation of wealth and the\nreduction of poverty, but also reveals huge gaps in our knowledge about the\ndynamics of our societies and about the means to guide social development\ntowards a better future for all.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1707.02853v1", "link": "http://arxiv.org/abs/1707.02853v1"}, {"title": "A quantum mechanical model for the rate of return", "summary": "In their activity, the traders approximate the rate of return by integer\nmultiples of a minimal one. Therefore, it can be regarded as a quantized\nvariable. On the other hand, there is the impossibility of observing the rate\nof return and its instantaneous forward time derivative, even if we consider it\nas a continuous variable. We present a quantum model for the rate of return\nbased on the mathematical formalism used in the case of quantum systems with\nfinite-dimensional Hilbert space. The rate of return is described by a discrete\nwave function and its time evolution by a Schodinger type equation.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1211.1938v1", "link": "http://arxiv.org/abs/1211.1938v1"}, {"title": "False EUR exchange rates vs. DKK, CHF, JPY and USD. What is a strong\n  currency?", "summary": "The Euro (EUR) has been a currency introduced by the European Community on\nJan. 01, 1999. This implies eleven countries of the European Union which have\nbeen found to meet the five requirements of the Maastricht convergence\ncriteria. In order to test EUR behavior and understand various features, we\nhave extrapolated the EUR backwards and therefore have obtained a {\\it false\neuro} (FEUR) dating back to 1993. We have derived the exchange rates of the\nFEUR with respect to several currencies of interest not belonging to the EUR,\ni.e., Danish Kroner (DKK), Swiss Franc (CHF), Japanese Yen (JPY) and U.S.\nDollar (USD). We have first observed the distribution of fluctuations of the\nexchange rates. Within the {\\it Detrended Fluctuation Analysis} (DFA)\nstatistical method, we have calculated the power law behavior describing the\nroot-mean-square deviation of these exchange rate fluctuations as a function of\ntime, displaying in particular the JPY exchange rate case. In order to estimate\nthe role of each currency making the EUR and therefore in view of identifying\nwhether some of them mostly influences its behavior, we have compared the\ntime-dependent exponent of the exchange rate fluctuations for EUR with that for\nthe currencies that form the EUR. We have found that the German Mark (DEM) has\nbeen leading the fluctuations of EUR/JPY exchange rates, and Portuguese Escudo\n(PTE) is the farthest away currency from this point of view.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0103033v1", "link": "http://arxiv.org/abs/cond-mat/0103033v1"}, {"title": "Modelling Trading Networks and the Role of Trust", "summary": "We present a simple dynamical model for describing trading interactions\nbetween agents in a social network by considering only two dynamical variables,\nnamely money and goods or services, that are assumed conserved over the whole\ntime span of the agents' trading transactions. A key feature of the model is\nthat agent-to-agent transactions are governed by the price in units of money\nper goods, which is dynamically changing, and by a trust variable, which is\nrelated to the trading history of each agent. All agents are able to sell or\nbuy, and the decision to do either has to do with the level of trust the buyer\nhas in the seller, the price of the goods and the amount of money and goods at\nthe disposal of the buyer. Here we show the results of extensive numerical\ncalculations under various initial conditions in a random network of agents and\ncompare the results with the available related data. In most cases the\nagreement between the model results and real data turns out to be fairly good,\nwhich allow us to draw some general conclusions as how different trading\nstrategies could affect the distribution of wealth in different kinds of\nsocieties.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1605.08899v1", "link": "http://dx.doi.org/10.1016/j.physa.2016.11.144"}, {"title": "How should you discount your backtest PnL?", "summary": "In-sample overfitting is a drawback of any backtest-based investment\nstrategy. It is thus of paramount importance to have an understanding of why\nand how the in-sample overfitting occurs. In this article we propose a simple\nframework that allows one to model and quantify in-sample PnL overfitting. This\nallows us to compute the factor appropriate for discounting PnLs of in-sample\ninvestment strategies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1902.01802v1", "link": "http://arxiv.org/abs/1902.01802v1"}, {"title": "Modeling the relation between income and commuting distance", "summary": "We discuss the distribution of commuting distances and its relation to\nincome. Using data from Denmark, the UK, and the US, we show that the commuting\ndistance is (i) broadly distributed with a slow decaying tail that can be\nfitted by a power law with exponent $\\gamma \\approx 3$ and (ii) an average\ngrowing slowly as a power law with an exponent less than one that depends on\nthe country considered. The classical theory for job search is based on the\nidea that workers evaluate the wage of potential jobs as they arrive\nsequentially through time, and extending this model with space, we obtain\npredictions that are strongly contradicted by our empirical findings. We\npropose an alternative model that is based on the idea that workers evaluate\npotential jobs based on a quality aspect and that workers search for jobs\nsequentially across space. We also assume that the density of potential jobs\ndepends on the skills of the worker and decreases with the wage. The predicted\ndistribution of commuting distances decays as $1/r^{3}$ and is independent of\nthe distribution of the quality of jobs. We find our alternative model to be in\nagreement with our data. This type of approach opens new perspectives for the\nmodeling of mobility.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1602.01578v2", "link": "http://dx.doi.org/10.1098/rsif.2016.0306"}, {"title": "Nonparametric estimates of pricing functionals", "summary": "We analyze the empirical performance of several non-parametric estimators of\nthe pricing functional for European options, using historical put and call\nprices on the S&P500 during the year 2012. Two main families of estimators are\nconsidered, obtained by estimating the pricing functional directly, and by\nestimating the (Black-Scholes) implied volatility surface, respectively. In\neach case simple estimators based on linear interpolation are constructed, as\nwell as more sophisticated ones based on smoothing kernels, \\`a la\nNadaraya-Watson. The results based on the analysis of the empirical pricing\nerrors in an extensive out-of-sample study indicate that a simple approach\nbased on the Black-Scholes formula coupled with linear interpolation of the\nvolatility surface outperforms, both in accuracy and computational speed, all\nother methods.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1506.06568v2", "link": "http://arxiv.org/abs/1506.06568v2"}, {"title": "Dynamic risk measures", "summary": "This paper gives an overview of the theory of dynamic convex risk measures\nfor random variables in discrete time setting. We summarize robust\nrepresentation results of conditional convex risk measures, and we characterize\nvarious time consistency properties of dynamic risk measures in terms of\nacceptance sets, penalty functions, and by supermartingale properties of risk\nprocesses and penalty functions.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1002.3794v1", "link": "http://arxiv.org/abs/1002.3794v1"}, {"title": "Pathwise stochastic integrals for model free finance", "summary": "We present two different approaches to stochastic integration in frictionless\nmodel free financial mathematics. The first one is in the spirit of It\\^o's\nintegral and based on a certain topology which is induced by the outer measure\ncorresponding to the minimal superhedging price. The second one is based on the\ncontrolled rough path integral. We prove that every \"typical price path\" has a\nnaturally associated It\\^o rough path, and justify the application of the\ncontrolled rough path integral in finance by showing that it is the limit of\nnon-anticipating Riemann sums, a new result in itself. Compared to the first\napproach, rough paths have the disadvantage of severely restricting the space\nof integrands, but the advantage of being a Banach space theory. Both\napproaches are based entirely on financial arguments and do not require any\nprobabilistic structure.", "category": ["math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1311.6187v4", "link": "http://dx.doi.org/10.3150/15-BEJ735"}, {"title": "Calls, zonoids, peacocks and log-concavity", "summary": "The main results are two characterisations of log-concave densities in terms\nof the collection of lift zonoids corresponding to a peacock. These notions are\nrecalled and connected to arbitrage-free asset pricing in financial\nmathematics.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1610.09306v1", "link": "http://arxiv.org/abs/1610.09306v1"}, {"title": "Toward Economics as a New Complex System", "summary": "The 2015 Nobel Prize in Economic Sciences was awarded to Eugene Fama, Lars\nPeter Hansen and Robert Shiller for their contributions to the empirical\nanalysis of asset prices. Eugene Fama [1] is an advocate of the efficient\nmarket hypothesis. The efficient market hypothesis assumes that asset price is\ndetermined by using all available information and only reacts to new\ninformation not incorporated into the fundamentals. Thus, the movement of stock\nprices is unpredictable. Robert Shiller [2] has been studying the existence of\nirrational bubbles, which are defined as the long term deviations of asset\nprice from the fundamentals. This drives us to the unsettled question of how\nthe market actually works.\n  In this paper, I look back at the development of economics and consider the\ndirection in which we should move in order to truly understand the workings of\nan economic society.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1611.05280v1", "link": "http://dx.doi.org/10.1140/epjst/e2016-60161-x"}, {"title": "Market Impact: A Systematic Study of Limit Orders", "summary": "This paper is devoted to the important yet little explored subject of the\nmarket impact of limit orders. Our analysis is based on a proprietary database\nof metaorders - large orders that are split into smaller pieces before being\nsent to the market. We first address the case of aggressive limit orders and\nthen, that of passive limit orders. In both cases, we provide empirical\nevidence of a power law behaviour for the temporary market impact. The\nrelaxation of the price following the end of the metaorder is also studied, and\nthe long-term impact is shown to stabilize at a level of approximately\ntwo-thirds of the maximum impact. Finally, a fair pricing condition during the\nlife cycle of the metaorders is empirically validated.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1802.08502v4", "link": "http://arxiv.org/abs/1802.08502v4"}, {"title": "Robustness of mathematical models and technical analysis strategies", "summary": "The aim of this paper is to compare the performances of the optimal strategy\nunder parameters mis-specification and of a technical analysis trading\nstrategy. The setting we consider is that of a stochastic asset price model\nwhere the trend follows an unobservable Ornstein-Uhlenbeck process. For both\nstrategies, we provide the asymptotic expectation of the logarithmic return as\na function of the model parameters. Finally, numerical examples find that an\ninvestment strategy using the cross moving averages rule is more robust than\nthe optimal strategy under parameters mis-specification.", "category": ["q-fin.PM", "q-fin.MF", "q-fin.TR"], "id": "http://arxiv.org/abs/1605.00173v1", "link": "http://arxiv.org/abs/1605.00173v1"}, {"title": "Weekly idiosyncratic risk metrics and idiosyncratic momentum: Evidence\n  from the Chinese stock market", "summary": "This paper focuses on the weekly idiosyncratic momentum (IMOM) as well as its\nrisk-adjusted versions with respect to various idiosyncratic risk metrics.\nUsing the A-share individual stocks in the Chinese market from January 1997 to\nDecember 2017, we first evaluate the performance of the weekly momentum and\nidiosyncratic momentum based on raw returns and idiosyncratic returns,\nrespectively. After that the univariate portfolio analysis is conducted to\ninvestigate the return predictability with respect to various idiosyncratic\nrisk metrics. Further, we perform a comparative study on the performance of the\nIMOMportfolios with respect to various risk metrics. At last, we explore the\npossible explanations to the IMOM as well as risk-based IMOM portfolios. We\nfind that 1) there is a prevailing contrarian effect and a IMOM effect for the\nwhole sample; 2) a negative relation exists between most of the idiosyncratic\nrisk metrics and the cross-sectional returns, and better performance is found\nthat is linked to idiosyncratic volatility (IVol) and maximum drawdowns (IMDs);\n3) additionally, the IVol-based and IMD-based IMOM portfolios exhibit a better\nexplanatory power to the IMOM portfolios with respect to other risk metrics; 4)\nfinally, higher profitability of the IMOM as well as IVol-based and IMD-based\nIMOM portfolios is found to be related to upside market states, high levels of\nliquidity and high levels of investor sentiment.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1910.13115v1", "link": "http://arxiv.org/abs/1910.13115v1"}, {"title": "Consistent Calibration of Economic Scenario Generators: The Case for\n  Conditional Simulation", "summary": "Economic Scenario Generators (ESGs) simulate economic and financial variables\nforward in time for risk management and asset allocation purposes. It is often\nnot feasible to calibrate the dynamics of all variables within the ESG to\nhistorical data alone. Calibration to forward-information such as future\nscenarios and return expectations is needed for stress testing and portfolio\noptimization, but no generally accepted methodology is available. This paper\nintroduces the Conditional Scenario Simulator, which is a framework for\nconsistently calibrating simulations and projections of economic and financial\nvariables both to historical data and forward-looking information. The\nframework can be viewed as a multi-period, multi-factor generalization of the\nBlack-Litterman model, and can embed a wide array of financial and\nmacroeconomic models. Two practical examples demonstrate this in a frequentist\nand Bayesian setting.", "category": ["econ.EM", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/2004.09042v1", "link": "http://arxiv.org/abs/2004.09042v1"}, {"title": "On American VIX options under the generalized 3/2 and 1/2 models", "summary": "In this paper, we extend the 3/2-model for VIX studied by Goard and Mazur\n(2013) and introduce the generalized 3/2 and 1/2 classes of volatility\nprocesses. Under these models, we study the pricing of European and American\nVIX options and, for the latter, we obtain an early exercise premium\nrepresentation using a free-boundary approach and local time-space calculus.\nThe optimal exercise boundary for the volatility is obtained as the unique\nsolution to an integral equation of Volterra type.\n  We also consider a model mixing these two classes and formulate the\ncorresponding optimal stopping problem in terms of the observed factor process.\nThe price of an American VIX call is then represented by an early exercise\npremium formula. We show the existence of a pair of optimal exercise boundaries\nfor the factor process and characterize them as the unique solution to a system\nof integral equations.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1606.00530v5", "link": "http://dx.doi.org/10.1111/mafi.12153"}, {"title": "Reduction of systemic risk by means of Pigouvian taxation", "summary": "We analyze the possibility of reduction of systemic risk in financial markets\nthrough Pigouvian taxation of financial institutions which is used to support\nthe rescue fund. We introduce the concept of the cascade risk with a clear\noperational definition as a subclass and a network related measure of the\nsystemic risk. Using financial networks constructed from real Italian money\nmarket data and using realistic parameters, we show that the cascade risk can\nbe substantially reduced by a small rate of taxation and by means of a simple\nstrategy of the money transfer from the rescue fund to interbanking market\nsubjects. Furthermore, we show that while negative effects on the return on\ninvestment ($ROI$) are direct and certain, an overall positive effect on risk\nadjusted return on investments ($ROI^{RA}$) is visible. Please note that\n\\emph{the taxation} is introduced as a monetary/regulatory, not as a fiscal\nmeasure, as the term could suggest. \\emph{The rescue fund} is implemented in a\nform of a common reserve fund.", "category": ["q-fin.RM", "q-fin.GN", "q-fin.TR"], "id": "http://arxiv.org/abs/1406.5817v1", "link": "http://dx.doi.org/10.1371/journal.pone.0114928"}, {"title": "Health Care Expenditures, Financial Stability, and Participation in the\n  Supplemental Nutrition Assistance Program (SNAP)", "summary": "This paper examines the association between household healthcare expenses and\nparticipation in the Supplemental Nutrition Assistance Program (SNAP) when\nmoderated by factors associated with financial stability of households. Using a\nlarge longitudinal panel encompassing eight years, this study finds that an\ninter-temporal increase in out-of-pocket medical expenses increased the\nlikelihood of household SNAP participation in the current period. Financially\nstable households with precautionary financial assets to cover at least 6\nmonths worth of household expenses were significantly less likely to\nparticipate in SNAP. The low income households who recently experienced an\nincrease in out of pocket medical expenses but had adequate precautionary\nsavings were less likely than similar households who did not have precautionary\nsavings to participate in SNAP. Implications for economists, policy makers, and\nhousehold finance professionals are discussed.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.05421v1", "link": "http://arxiv.org/abs/1811.05421v1"}, {"title": "Large-Scale Dynamic Predictive Regressions", "summary": "We develop a novel \"decouple-recouple\" dynamic predictive strategy and\ncontribute to the literature on forecasting and economic decision making in a\ndata-rich environment. Under this framework, clusters of predictors generate\ndifferent latent states in the form of predictive densities that are later\nsynthesized within an implied time-varying latent factor model. As a result,\nthe latent inter-dependencies across predictive densities and biases are\nsequentially learned and corrected. Unlike sparse modeling and variable\nselection procedures, we do not assume a priori that there is a given subset of\nactive predictors, which characterize the predictive density of a quantity of\ninterest. We test our procedure by investigating the predictive content of a\nlarge set of financial ratios and macroeconomic variables on both the equity\npremium across different industries and the inflation rate in the U.S., two\ncontexts of topical interest in finance and macroeconomics. We find that our\npredictive synthesis framework generates both statistically and economically\nsignificant out-of-sample benefits while maintaining interpretability of the\nforecasting variables. In addition, the main empirical results highlight that\nour proposed framework outperforms both LASSO-type shrinkage regressions,\nfactor based dimension reduction, sequential variable selection, and\nequal-weighted linear pooling methodologies.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1803.06738v1", "link": "http://arxiv.org/abs/1803.06738v1"}, {"title": "Local risk-minimization under restricted information to asset prices", "summary": "In this paper we investigate the local risk-minimization approach for a\nsemimartingale financial market where there are restrictions on the available\ninformation to agents who can observe at least the asset prices. We\ncharacterize the optimal strategy in terms of suitable decompositions of a\ngiven contingent claim, with respect to a filtration representing the\ninformation level, even in presence of jumps. Finally, we discuss some\npractical examples in a Markovian framework and show that the computation of\nthe optimal strategy leads to filtering problems under the real-world\nprobability measure and under the minimalmartingale measure.", "category": ["math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1312.4385v2", "link": "http://arxiv.org/abs/1312.4385v2"}, {"title": "Estimating topological properties of weighted networks from limited\n  information", "summary": "A fundamental problem in studying and modeling economic and financial systems\nis represented by privacy issues, which put severe limitations on the amount of\naccessible information. Here we introduce a novel, highly nontrivial method to\nreconstruct the structural properties of complex weighted networks of this kind\nusing only partial information: the total number of nodes and links, and the\nvalues of the strength for all nodes. The latter are used as fitness to\nestimate the unknown node degrees through a standard configuration model. Then,\nthese estimated degrees and the strengths are used to calibrate an enhanced\nconfiguration model in order to generate ensembles of networks intended to\nrepresent the real system. The method, which is tested on real economic and\nfinancial networks, while drastically reducing the amount of information needed\nto infer network properties, turns out to be remarkably effective$-$thus\nrepresenting a valuable tool for gaining insights on privacy-protected\nsocioeconomic systems.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1409.6193v3", "link": "http://dx.doi.org/10.1103/PhysRevE.92.040802"}, {"title": "An Option Pricing Model with Memory", "summary": "We obtain option pricing formulas for stock price models in which the drift\nand volatility terms are functionals of a continuous history of the stock\nprices. That is, the stock dynamics follows a nonlinear stochastic functional\ndifferential equation. A model with full memory is obtained via approximation\nthrough a stock price model in which the continuous path dependence does not go\nup to the present: there is a memory gap. A strong solution is obtained by\nclosing the gap. Fair option prices are obtained through an equivalent (local)\nmartingale measure via Girsanov's Theorem and therefore are given in terms of a\nconditional expectation. The models maintain the completeness of the market and\nhave no arbitrage opportunities.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1709.00468v1", "link": "http://arxiv.org/abs/1709.00468v1"}, {"title": "The puzzle that just isn't", "summary": "In his stimulating article on the reasons for two puzzling observations about\nthe behaviour of interest rates, exchange rates and the rate of inflation,\nCharles Engel (2016) puts forward an explanation that rests on the concept of a\nnon-pecuniary liquidity return on assets. Albeit intriguing the analysis\nstruggles to account for a number of facts which are familiar to participants\nof the foreign exchange and bond markets. Reconciling these facts in\nconjunction with a careful dissection of the \"puzzle\" to begin with, shows that\nthe forward premium puzzle just does not exist, at least not in its canonical\nform.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1604.08895v1", "link": "http://arxiv.org/abs/1604.08895v1"}, {"title": "Economic Complexity: \"Buttarla in caciara\" vs a constructive approach", "summary": "This note is a contribution to the debate about the optimal algorithm for\nEconomic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2]\neventually agree that the ECI+ algorithm [1] consists just in a renaming of the\nFitness algorithm we introduced in 2012, as we explicitly showed in [3].\nHowever, they omit any comment on the fact that their extensive numerical tests\nclaimed to demonstrate that the same algorithm works well if they name it ECI+,\nbut not if its name is Fitness. They should realize that this eliminates any\ncredibility to their numerical methods and therefore also to their new\nanalysis, in which they consider many algorithms [2]. Since by their own\nadmission the best algorithm is the Fitness one, their new claim became that\nthe search for the best algorithm is pointless and all algorithms are alike.\nThis is exactly the opposite of what they claimed a few days ago and it does\nnot deserve much comments. After these clarifications we also present a\nconstructive analysis of the status of Economic Complexity, its algorithms, its\nsuccesses and its perspectives. For us the discussion closes here, we will not\nreply to further comments.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1709.05272v1", "link": "http://arxiv.org/abs/1709.05272v1"}, {"title": "Statistical inference for statistical decisions", "summary": "The Wald development of statistical decision theory addresses decision making\nwith sample data. Wald's concept of a statistical decision function (SDF)\nembraces all mappings of the form [data -> decision]. An SDF need not perform\nstatistical inference; that is, it need not use data to draw conclusions about\nthe true state of nature. Inference-based SDFs have the sequential form [data\n-> inference -> decision]. This paper motivates inference-based SDFs as\npractical procedures for decision making that may accomplish some of what Wald\nenvisioned. The paper first addresses binary choice problems, where all SDFs\nmay be viewed as hypothesis tests. It next considers as-if optimization, which\nuses a point estimate of the true state as if the estimate were accurate. It\nthen extends this idea to as-if maximin and minimax-regret decisions, which use\npoint estimates of some features of the true state as if they were accurate.\nThe paper primarily uses finite-sample maximum regret to evaluate the\nperformance of inference-based SDFs. To illustrate abstract ideas, it presents\nspecific findings concerning treatment choice and point prediction with sample\ndata.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.06853v1", "link": "http://arxiv.org/abs/1909.06853v1"}, {"title": "Behavioural present value", "summary": "Impact of chosen behavioural factors on imprecision of present value is\ndiscussed here. The formal model of behavioural present value is offered as a\nresult of this discussion. Behavioural present value is described here by fuzzy\nset. These considerations were illustrated by means of extensive numerical case\nstudy. Finally there are shown that in proposed model the return rate is given,\nas a fuzzy probabilistic set.", "category": ["q-fin.GN", "q-fin.PR"], "id": "http://arxiv.org/abs/1302.0539v1", "link": "http://dx.doi.org/10.2139/ssrn.1729351"}, {"title": "Compressing Over-the-Counter Markets", "summary": "Over-the-counter markets are at the center of the postcrisis global reform of\nthe financial system. We show how the size and structure of such markets can\nundergo rapid and extensive changes when participants engage in portfolio\ncompression, a post-trade netting technology. Tightly-knit and concentrated\ntrading structures, as featured by many large over-the-counter markets, are\nespecially susceptible to reductions of notional and reconfigurations of\nnetwork structure resulting from compression activities. Using\ntransaction-level data on credit-default-swaps markets, we estimate reduction\nlevels consistent with the historical development observed in these markets\nsince the Global Financial Crisis. Finally, we study the effect of a mandate to\ncentrally clear over-the-counter markets. When participants engage in both\ncentral clearing and portfolio compression, we find large netting failures if\nclearinghouses proliferate. Allowing for compression across clearinghouses\nby-and-large offsets this adverse effect.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1705.07155v2", "link": "http://arxiv.org/abs/1705.07155v2"}, {"title": "The Impacts of the Alaska Permanent Fund Dividend on High School Status\n  Completion Rates", "summary": "Direct cash transfer programs have shown success as poverty interventions in\nboth the developing and developed world, yet little research exists examining\nthe society-wide outcomes of an unconditional cash transfer program disbursed\nwithout means-testing. This paper attempts to determine the impact of direct\ncash transfers on educational outcomes in a developed society by investigating\nthe impacts of the Alaska Permanent Fund Dividend, which was launched in 1982\nand continues to be disbursed on an annual basis to every Alaskan. A synthetic\ncontrol model is deployed to examine the path of educational attainment among\nAlaskans between 1977 and 1991 in order to determine if high school status\ncompletion rates after the launch of the dividend diverge from the synthetic in\na manner suggestive of a treatment effect.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1910.04083v1", "link": "http://arxiv.org/abs/1910.04083v1"}, {"title": "Fibrations of financial events", "summary": "In this paper we shall prove that the plane of financial events, introduced\nand applied to financial problems by the author himself (see [2], [3] and [4])\ncan be considered as a fibration in two different ways. The first one, the\nnatural one, reveals itself to be isomorphic to the tangent- bundle of the real\nline, when the last one is considered as a differentiable manifold in the\nnatural way; the second one is a fibration induced by the status of compound\ninterest capitalization at a given rate i in the interval ] - 1, \\rightarrow [.\nMoreover, in the paper we define on the first fibration an affine connection,\nalso in this case induced by the status of compound interest at a given rate i.\nThe final goal of this paper is the awareness that all the effects determined\nby the status of compound interest are nothing but the consequences of the fact\nthat the space of financial events is a fibration endowed with a particular\naffine connection, so they are consequences of purely geometric properties, at\nlast, depending upon the curvature determined by the connection upon the\nfibration. A natural preorder upon the set of fibers of the second fibration is\nconsidered. Some remarks about the applicability to economics and finance of\nthe theories presented in the paper and about the possible developments are\nmade in the directions followed in papers [1], [5], [6], [7], [8] of the\nauthor.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1106.1774v1", "link": "http://arxiv.org/abs/1106.1774v1"}, {"title": "Eigenvalue density of empirical covariance matrix for correlated samples", "summary": "We describe a method to determine the eigenvalue density of empirical\ncovariance matrix in the presence of correlations between samples. This is a\nstraightforward generalization of the method developed earlier by the authors\nfor uncorrelated samples. The method allows for exact determination of the\nexperimental spectrum for a given covariance matrix and given correlations\nbetween samples in the limit of large N and N/T=r=const with N being the number\nof degrees of freedom and T being the number of samples. We discuss the effect\nof correlations on several examples.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0508451v1", "link": "http://arxiv.org/abs/cond-mat/0508451v1"}, {"title": "Emergence of statistically validated financial intraday lead-lag\n  relationships", "summary": "According to the leading models in modern finance, the presence of intraday\nlead-lag relationships between financial assets is negligible in efficient\nmarkets. With the advance of technology, however, markets have become more\nsophisticated. To determine whether this has resulted in an improved market\nefficiency, we investigate whether statistically significant lagged correlation\nrelationships exist in financial markets. We introduce a numerical method to\nstatistically validate links in correlation-based networks, and employ our\nmethod to study lagged correlation networks of equity returns in financial\nmarkets. Crucially, our statistical validation of lead-lag relationships\naccounts for multiple hypothesis testing over all stock pairs. In an analysis\nof intraday transaction data from the periods 2002--2003 and 2011--2012, we\nfind a striking growth in the networks as we increase the frequency with which\nwe sample returns. We compute how the number of validated links and the\nmagnitude of correlations change with increasing sampling frequency, and\ncompare the results between the two data sets. Finally, we compare topological\nproperties of the directed correlation-based networks from the two periods\nusing the in-degree and out-degree distributions and an analysis of three-node\nmotifs. Our analysis suggests a growth in both the efficiency and instability\nof financial markets over the past decade.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1401.0462v1", "link": "http://arxiv.org/abs/1401.0462v1"}, {"title": "Active Preference Learning for Personalized Portfolio Construction", "summary": "In financial asset management, choosing a portfolio requires balancing\nreturns, risk, exposure, liquidity, volatility and other factors. These\nconcerns are difficult to compare explicitly, with many asset managers using an\nintuitive or implicit sense of their interaction. We propose a mechanism for\nlearning someone's sense of distinctness between portfolios with the goal of\nbeing able to identify portfolios which are predicted to perform well but are\ndistinct from the perspective of the user. This identification occurs, e.g., in\nthe context of Bayesian optimization of a backtested performance metric.\nNumerical experiments are presented which show the impact of personal beliefs\nin informing the development of a diverse and high-performing portfolio.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1708.07567v1", "link": "http://arxiv.org/abs/1708.07567v1"}, {"title": "Mean-Field Games with Differing Beliefs for Algorithmic Trading", "summary": "Even when confronted with the same data, agents often disagree on a model of\nthe real-world. Here, we address the question of how interacting heterogenous\nagents, who disagree on what model the real-world follows, optimize their\ntrading actions. The market has latent factors that drive prices, and agents\naccount for the permanent impact they have on prices. This leads to a large\nstochastic game, where each agents' performance criteria are computed under a\ndifferent probability measure. We analyse the mean-field game (MFG) limit of\nthe stochastic game and show that the Nash equilibrium is given by the solution\nto a non-standard vector-valued forward-backward stochastic differential\nequation. Under some mild assumptions, we construct the solution in terms of\nexpectations of the filtered states. Furthermore, we prove the MFG strategy\nforms an $\\epsilon$-Nash equilibrium for the finite player game. Lastly, we\npresent a least-squares Monte Carlo based algorithm for computing the\nequilibria and show through simulations that increasing disagreement may\nincrease price volatility and trading activity.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1810.06101v2", "link": "http://arxiv.org/abs/1810.06101v2"}, {"title": "Privacy-Preserving Methods for Sharing Financial Risk Exposures", "summary": "Unlike other industries in which intellectual property is patentable, the\nfinancial industry relies on trade secrecy to protect its business processes\nand methods, which can obscure critical financial risk exposures from\nregulators and the public. We develop methods for sharing and aggregating such\nrisk exposures that protect the privacy of all parties involved and without the\nneed for a trusted third party. Our approach employs secure multi-party\ncomputation techniques from cryptography in which multiple parties are able to\ncompute joint functions without revealing their individual inputs. In our\nframework, individual financial institutions evaluate a protocol on their\nproprietary data which cannot be inverted, leading to secure computations of\nreal-valued statistics such a concentration indexes, pairwise correlations, and\nother single- and multi-point statistics. The proposed protocols are\ncomputationally tractable on realistic sample sizes. Potential financial\napplications include: the construction of privacy-preserving real-time indexes\nof bank capital and leverage ratios; the monitoring of delegated portfolio\ninvestments; financial audits; and the publication of new indexes of\nproprietary trading strategies.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1111.5228v2", "link": "http://arxiv.org/abs/1111.5228v2"}, {"title": "Revisiting Feller Diffusion: Derivation and Simulation", "summary": "We propose a simpler derivation of the probability density function of Feller\nDiffusion using the Fourier Transform and solving the resulting equation via\nthe Method of Characteristics. We also discuss simulation algorithms and\nconfirm key properties related to hitting time probabilities via the\nsimulation.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1905.10737v3", "link": "http://arxiv.org/abs/1905.10737v3"}, {"title": "The European debt crisis: Defaults and market equilibrium", "summary": "During the last two years, Europe has been facing a debt crisis, and Greece\nhas been at its center. In response to the crisis, drastic actions have been\ntaken, including the halving of Greek debt. Policy makers acted because\ninterest rates for sovereign debt increased dramatically. High interest rates\nimply that default is likely due to economic conditions. High interest rates\nalso increase the cost of borrowing and thus cause default to be likely. If\nthere is a departure from equilibrium, increasing interest rates may contribute\nto---rather than be caused by---default risk. Here we build a quantitative\nequilibrium model of sovereign default risk that, for the first time, is able\nto determine if markets are consistently set by economic conditions. We show\nthat over the period 2001-2012, the annually-averaged long-term interest rates\nof Greek debt are quantitatively related to the ratio of debt to GDP. The\nrelationship shows that the market consistently expects default to occur if the\nGreek debt reaches twice the GDP. Our analysis does not preclude\nnon-equilibrium increases in interest rates over shorter timeframes. We find\nevidence of such non-equilibrium fluctuations in a separate analysis. According\nto the equilibrium model, the date by which a half-default must occur is March\n2013, almost one year after the actual debt write-down. Any acceleration of\ndefault by non-equilibrium fluctuations is significant for national and\ninternational interventions. The need for austerity or bailout costs would be\nreduced if market regulations were implemented to increase market stability to\nprevent short term interest rate increases. We similarly evaluate the timing of\nprojected defaults without interventions for Portugal, Ireland, Spain and Italy\nto be March 2013, April 2014, May 2014, and July 2016, respectively. All\ndefaults are mitigated by planned interventions.", "category": ["q-fin.GN", "q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1209.6369v1", "link": "http://arxiv.org/abs/1209.6369v1"}, {"title": "Pricing without martingale measure", "summary": "For several decades, the no-arbitrage (NA) condition and the martingale\nmeasures have played a major role in the financial asset's pricing theory. We\npropose a new approach for estimating the super-replication cost based on\nconvex duality instead of martingale measures duality: Our prices will be\nexpressed using Fenchel conjugate and bi-conjugate. The super-hedging problem\nleads endogenously to a weak condition of NA called Absence of Immediate Profit\n(AIP). We propose several characterizations of AIP and study the relation with\nthe classical notions of no-arbitrage. We also give some promising numerical\nillustrations.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1807.04612v2", "link": "http://arxiv.org/abs/1807.04612v2"}, {"title": "A Dynamical Model for Operational Risk in Banks", "summary": "Operational risk is the risk relative to monetary losses caused by failures\nof bank internal processes due to heterogeneous causes. A dynamical model\nincluding both spontaneous generation of losses and generation via interactions\nbetween different processes is presented; the efforts made by the bank to avoid\nthe occurrence of losses is also taken into account. Under certain hypotheses,\nthe model can be exactly solved and, in principle, the solution can be\nexploited to estimate most of the model parameters from real data. The\nforecasting power of the model is also investigated and proved to be\nsurprisingly remarkable.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1207.6186v1", "link": "http://dx.doi.org/10.3254/978-1-61499-071-0-399"}, {"title": "Optimal Robust Mean-Variance Hedging in Incomplete Financial Markets", "summary": "Optimal B-robust estimate is constructed for multidimensional parameter in\ndrift coefficient of diffusion type process with small noise. Optimal\nmean-variance robust (optimal V -robust) trading strategy is find to hedge in\nmean-variance sense the contingent claim in incomplete financial market with\narbitrary information structure and misspecified volatility of asset price,\nwhich is modelled by multidimensional continuous semimartingale. Obtained\nresults are applied to stochastic volatility model, where the model of latent\nvolatility process contains unknown multidimensional parameter in drift\ncoefficient and small parameter in diffusion term.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/0805.0122v1", "link": "http://arxiv.org/abs/0805.0122v1"}, {"title": "Credit Risk: Simple Closed Form Approximate Maximum Likelihood Estimator", "summary": "We consider discrete default intensity based and logit type reduced form\nmodels for conditional default probabilities for corporate loans where we\ndevelop simple closed form approximations to the maximum likelihood estimator\n(MLE) when the underlying covariates follow a stationary Gaussian process. In a\npractically reasonable asymptotic regime where the default probabilities are\nsmall, say 1-3% annually, the number of firms and the time period of data\navailable is reasonably large, we rigorously show that the proposed estimator\nbehaves similarly or slightly worse than the MLE when the underlying model is\ncorrectly specified. For more realistic case of model misspecification, both\nestimators are seen to be equally good, or equally bad. Further, beyond a\npoint, both are more-or-less insensitive to increase in data. These conclusions\nare validated on empirical and simulated data. The proposed approximations\nshould also have applications outside finance, where logit-type models are used\nand probabilities of interest are small.", "category": ["econ.EM", "q-fin.PM"], "id": "http://arxiv.org/abs/1912.12611v1", "link": "http://arxiv.org/abs/1912.12611v1"}, {"title": "Characterization of arbitrage-free markets", "summary": "The present paper deals with the characterization of no-arbitrage properties\nof a continuous semimartingale. The first main result, Theorem\n\\refMainTheoremCharNA, extends the no-arbitrage criterion by Levental and\nSkorohod [Ann. Appl.\n  Probab. 5 (1995) 906-925] from diffusion processes to arbitrary continuous\nsemimartingales. The second main result, Theorem 2.4, is a characterization of\na weaker notion of no-arbitrage in terms of the existence of supermartingale\ndensities. The pertaining weaker notion of no-arbitrage is equivalent to the\nabsence of immediate arbitrage opportunities, a concept introduced by Delbaen\nand Schachermayer [Ann. Appl. Probab. 5 (1995) 926-945]. Both results are\nstated in terms of conditions for any semimartingales starting at arbitrary\nstopping times \\sigma. The necessity parts of both results are known for the\nstopping time \\sigma=0 from Delbaen and Schachermayer [Ann. Appl. Probab. 5\n(1995) 926-945]. The contribution of the present paper is the proofs of the\ncorresponding sufficiency parts.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/math/0503473v1", "link": "http://dx.doi.org/10.1214/105051604000000558"}, {"title": "Risk minimizing of derivatives via dynamic g-expectation and related\n  topics", "summary": "In this paper, we investigate risk minimization problem of derivatives based\non non-tradable underlyings by means of dynamic g-expectations which are slight\ndifferent from conditional g-expectations. In this framework, inspired by [1]\nand [16], we introduce risk indifference price, marginal risk price and\nderivative hedge and obtain their corresponding explicit expressions. The\ninteresting thing is that their expressions have nothing to do with nonlinear\ngenerator g, and one deep reason for this is due to the completeness of\nfinancial market. By giving three useful special risk minimization problems, we\nobtain the explicit optimal strategies with initial wealth involved,\ndemonstrate some qualitative analysis among optimal strategies, risk aversion\nparameter and market price of risk, together with some economic\ninterpretations.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1208.2068v1", "link": "http://arxiv.org/abs/1208.2068v1"}, {"title": "Generalized Gaussian Bridges", "summary": "A generalized bridge is the law of a stochastic process that is conditioned\non N linear functionals of its path. We consider two types of representations\nof such bridges: orthogonal and canonical.\n  The orthogonal representation is constructed from the entire path of the\nunderlying process. Thus, future knowledge of the path is needed. The\northogonal representation is provided for any continuous Gaussian process.\n  In the canonical representation the filtrations and the linear spaces\ngenerated by the bridge process and the underlying process coincide. Thus, no\nfuture information of the underlying process is needed. Also, in the\nsemimartingale case the canonical bridge representation is related to the\nenlargement of filtration and semimartingale decompositions. The canonical\nrepresentation is provided for the so-called prediction-invertible Gaussian\nprocesses. All martingales are trivially prediction-invertible. A typical\nnon-semimartingale example of a prediction-invertible Gaussian process is the\nfractional Brownian motion.\n  We apply the canonical bridges to insider trading.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1205.3405v2", "link": "http://arxiv.org/abs/1205.3405v2"}, {"title": "Naive Diversification Preferences and their Representation", "summary": "A widely applied diversification paradigm is the naive diversification choice\nheuristic. It stipulates that an economic agent allocates equal decision\nweights to given choice alternatives independent of their individual\ncharacteristics. This article provides mathematically and economically sound\nchoice theoretic foundations for the naive approach to diversification. We\naxiomatize naive diversification by defining it as a preference for equality\nover inequality and derive its relationship to the classical diversification\nparadigm. In particular, we show that (i) the notion of permutation invariance\nlies at the core of naive diversification and that an economic agent is a naive\ndiversifier if and only if his preferences are convex and permutation\ninvariant; (ii) Schur-concave utility functions capture the idea of being\ninequality averse on top of being risk averse; and (iii) the transformations,\nwhich rebalance unequal decision weights to equality, are characterized in\nterms of their implied turnover.", "category": ["q-fin.EC", "q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1611.01285v2", "link": "http://arxiv.org/abs/1611.01285v2"}, {"title": "New economic windows on income and wealth: The k-generalized family of\n  distributions", "summary": "Over the last decades, the distribution of income and wealth has been\ndeteriorating in many countries, leading to increased inequalities within and\nbetween societies. This tendency has revived the interest in the subject\ngreatly, yet it still receives very little attention within the realm of\nmainstream economic thinking. One reason for this is that the basic paradigm of\n\"standard economics\", the representative-agent General Equilibrium framework,\nis badly equipped to cope with distributional issues. Here we argue that when\nthe economy is treated as a complex system composed of many heterogeneous\ninteracting agents who give rise to emergent phenomena, to address the main\nstylized facts of income/wealth distribution requires leaving the toolbox of\nmainstream economics in favour of alternative approaches. The \"k-generalized\"\nfamily of income/wealth distributions, building on the categories of\ncomplexity, is an example of how advances in the field can be achieved within\nnew interdisciplinary research contexts.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1608.06076v1", "link": "http://arxiv.org/abs/1608.06076v1"}, {"title": "Multi Currency Credit Default Swaps Quanto effects and FX devaluation\n  jumps", "summary": "Credit Default Swaps (CDS) on a reference entity may be traded in multiple\ncurrencies, in that protection upon default may be offered either in the\ndomestic currency where the entity resides, or in a more liquid and global\nforeign currency. In this situation currency fluctuations clearly introduce a\nsource of risk on CDS spreads. For emerging markets, but in some cases even in\nwell developed markets, the risk of dramatic Foreign Exchange (FX) rate\ndevaluation in conjunction with default events is relevant. We address this\nissue by proposing and implementing a model that considers the risk of foreign\ncurrency devaluation that is synchronous with default of the reference entity.\n  Preliminary results indicate that perceived risks of devaluation can induce a\nsignificant basis across domestic and foreign CDS quotes. For the Republic of\nItaly, a USD CDS spread quote of 440 bps can translate into a EUR quote of 350\nbps in the middle of the Euro-debt crisis in the first week of May 2012. More\nrecently, from June 2013, the basis spreads between the EUR quotes and the USD\nquotes are in the range around 40 bps.\n  We explain in detail the sources for such discrepancies. Our modeling\napproach is based on the reduced form framework for credit risk, where the\ndefault time is modeled in a Cox process setting with explicit diffusion\ndynamics for default intensity/hazard rate and exponential jump to default. For\nthe FX part, we include an explicit default-driven jump in the FX dynamics. As\nour results show, such a mechanism provides a further and more effective way to\nmodel credit / FX dependency than the instantaneous correlation that can be\nimposed among the driving Brownian motions of default intensity and FX rates,\nas it is not possible to explain the observed basis spreads during the\nEuro-debt crisis by using the latter mechanism alone.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1512.07256v2", "link": "http://arxiv.org/abs/1512.07256v2"}, {"title": "Robust estimation of superhedging prices", "summary": "We consider statistical estimation of superhedging prices using historical\nstock returns in a frictionless market with d traded assets. We introduce a\nplugin estimator based on empirical measures and show it is consistent but\nlacks suitable robustness. To address this we propose novel estimators which\nuse a larger set of martingale measures defined through a tradeoff between the\nradius of Wasserstein balls around the empirical measure and the allowed norm\nof martingale densities. We establish consistency and robustness of these\nestimators and argue that they offer a superior performance relative to the\nplugin estimator. We generalise the results by replacing the superhedging\ncriterion with acceptance relative to a risk measure. We further extend our\nstudy, in part, to the case of markets with traded options, to a multiperiod\nsetting and to settings with model uncertainty. We also study convergence rates\nof estimators and convergence of superhedging strategies.", "category": ["q-fin.ST", "math.PR"], "id": "http://arxiv.org/abs/1807.04211v3", "link": "http://arxiv.org/abs/1807.04211v3"}, {"title": "Effect of segregation on inequality in kinetic models of wealth exchange", "summary": "Empirical distributions of wealth and income can be reproduced using\nsimplified agent-based models of economic interactions, analogous to\nmicroscopic collisions of gas particles. Building upon these models of freely\ninteracting agents, we explore the effect of a segregated economic network in\nwhich interactions are restricted to those between agents of similar wealth.\nAgents on a 2D lattice undergo kinetic exchanges with their nearest neighbours,\nwhile continuously switching places to minimize local wealth differences. A\nspatial concentration of wealth leads to a steady state with increased global\ninequality and a magnified distinction between local and global measures of\ncombatting poverty. Individual saving propensity proves ineffective in the\nsegregated economy, while redistributive taxation transcends the spatial\ninhomogeneity and greatly reduces inequality. Adding fluctuations to the\nsegregation dynamics, we observe a sharp phase transition to lower inequality\nat a critical temperature, accompanied by a sudden change in the distribution\nof the wealthy elite.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.04129v1", "link": "http://dx.doi.org/10.1140/epjb/e2020-100534-7"}, {"title": "Accelerated Share Repurchase: pricing and execution strategy", "summary": "In this article, we consider the optimal execution problem associated to\naccelerated share repurchase contracts. When firms want to repurchase their own\nshares, they often enter such a contract with a bank. The bank buys the shares\nfor the firm and is paid the average market price over the execution period,\nthe length of the period being decided upon by the bank during the buying\nprocess. Mathematically, the problem is new and related to both option pricing\n(Asian and Bermudan options) and optimal execution. We provide a model, along\nwith associated numerical methods, to determine the optimal stopping time and\nthe optimal buying strategy of the bank.", "category": ["q-fin.TR", "q-fin.PR"], "id": "http://arxiv.org/abs/1312.5617v4", "link": "http://arxiv.org/abs/1312.5617v4"}, {"title": "Nestedness in complex networks: Observation, emergence, and implications", "summary": "The observed architecture of ecological and socio-economic networks differs\nsignificantly from that of random networks. From a network science standpoint,\nnon-random structural patterns observed in real networks call for an\nexplanation of their emergence and an understanding of their potential systemic\nconsequences. This article focuses on one of these patterns: nestedness. Given\na network of interacting nodes, nestedness can be described as the tendency for\nnodes to interact with subsets of the interaction partners of better-connected\nnodes. Known since more than $80$ years in biogeography, nestedness has been\nfound in systems as diverse as ecological mutualistic organizations, world\ntrade, inter-organizational relations, among many others. This review article\nfocuses on three main pillars: the existing methodologies to observe nestedness\nin networks; the main theoretical mechanisms conceived to explain the emergence\nof nestedness in ecological and socio-economic networks; the implications of a\nnested topology of interactions for the stability and feasibility of a given\ninteracting system. We survey results from variegated disciplines, including\nstatistical physics, graph theory, ecology, and theoretical economics.\nNestedness was found to emerge both in bipartite networks and, more recently,\nin unipartite ones; this review is the first comprehensive attempt to unify\nboth streams of studies, usually disconnected from each other. We believe that\nthe truly interdisciplinary endeavour -- while rooted in a complex systems\nperspective -- may inspire new models and algorithms whose realm of application\nwill undoubtedly transcend disciplinary boundaries.", "category": [], "id": "http://arxiv.org/abs/1905.07593v1", "link": "http://dx.doi.org/10.1016/j.physrep.2019.04.001"}, {"title": "Multiple Wavelet Coherency Analysis and Forecasting of Metal Prices", "summary": "The assessment of co-movement among metals is crucial to better understand\nthe behaviors of the metal prices and the interactions with others that affect\nthe changes in prices. In this study, both Wavelet Analysis and VARMA (Vector\nAutoregressive Moving Average) models are utilized. First, Multiple Wavelet\nCoherence (MWC), where Wavelet Analysis is needed, is utilized to determine\ndynamic correlation time interval and scales. VARMA is then used for\nforecasting which results in reduced errors.\n  The daily prices of steel, aluminium, copper and zinc between 10.05.2010 and\n29.05.2014 are analyzed via wavelet analysis to highlight the interactions.\nResults uncover interesting dynamics between mentioned metals in the\ntime-frequency space. VARMA (1,1) model forecasting is carried out considering\nthe daily prices between 14.11.2011 and 16.11.2012 where the interactions are\nquite high and prediction errors are found quite limited with respect to\nARMA(1.1). It is shown that dynamic co-movement detection via four variables\nwavelet coherency analysis in the determination of VARMA time interval enables\nto improve forecasting power of ARMA by decreasing forecasting errors.", "category": ["q-fin.ST", "q-fin.MF"], "id": "http://arxiv.org/abs/1602.01960v1", "link": "http://arxiv.org/abs/1602.01960v1"}, {"title": "Investment Volatility: A Critique of Standard Beta Estimation and a\n  Simple Way Forward", "summary": "Beta is a widely used quantity in investment analysis. We review the common\ninterpretations that are applied to beta in finance and show that the standard\nmethod of estimation - least squares regression - is inconsistent with these\ninterpretations. We present the case for an alternative beta estimator which is\nmore appropriate, as well as being easier to understand and to calculate.\nUnlike regression, the line fit we propose treats both variables in the same\nway. Remarkably, it provides a slope that is precisely the ratio of the\nvolatility of the investment's rate of return to the volatility of the market\nindex rate of return (or the equivalent excess rates of returns). Hence, this\nline fitting method gives an alternative beta, which corresponds exactly to the\nrelative volatility of an investment - which is one of the usual\ninterpretations attached to beta.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1109.4422v1", "link": "http://arxiv.org/abs/1109.4422v1"}, {"title": "Portfolio Optimization in Fractional and Rough Heston Models", "summary": "We consider a fractional version of the Heston volatility model which is\ninspired by [16]. Within this model we treat portfolio optimization problems\nfor power utility functions. Using a suitable representation of the fractional\npart, followed by a reasonable approximation we show that it is possible to\ncast the problem into the classical stochastic control framework. This approach\nis generic for fractional processes. We derive explicit solutions and obtain as\na by-product the Laplace transform of the integrated volatility. In order to\nget rid of some undesirable features we introduce a new model for the rough\npath scenario which is based on the Marchaud fractional derivative. We provide\na numerical study to underline our results.", "category": ["q-fin.PM", "q-fin.MF"], "id": "http://arxiv.org/abs/1809.10716v2", "link": "http://arxiv.org/abs/1809.10716v2"}, {"title": "Information content of financial markets: a practical approach based on\n  Bohmian quantum mechanics", "summary": "The Bohmian quantum approach is implemented to analyze the financial markets.\nIn this approach, there is a wave function that leads to a quantum potential.\nThis potential can explain the relevance and entanglements of the agent's\nbehaviors with the past. The light is shed by considering the relevance of the\nmarket conditions with the previous market conditions enabling the conversion\nof the local concepts to the global ones. We have shown that there are two\npotential limits for each market. In essence, these potential limits act as a\nboundary which limits the return values inside it. By estimating the difference\nbetween these two limits in each market, it is found that the quantum\npotentials of the return time series in different time scales, possess a\nscaling behavior. The slopes of the scaling behaviors in mature, emerging and\ncommodity markets show different patterns. The emerge market having a slope\ngreater than 0.5, has a higher value compared to the corresponding values for\nthe mature and commodity markets which is less than 0.5. The cut-off observed\nin the curve of the commodity market indicates the threshold for the efficiency\nof the global effects. While before the cut-off, local effects in the market\nare dominant, as in the case of the mature markets. The findings could prove\nadequate for investors in different markets to invest in different time\nhorizons.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1212.4293v1", "link": "http://arxiv.org/abs/1212.4293v1"}, {"title": "Demand and Welfare Analysis in Discrete Choice Models with Social\n  Interactions", "summary": "Many real-life settings of consumer-choice involve social interactions,\ncausing targeted policies to have spillover-effects. This paper develops novel\nempirical tools for analyzing demand and welfare-effects of\npolicy-interventions in binary choice settings with social interactions.\nExamples include subsidies for health-product adoption and vouchers for\nattending a high-achieving school. We establish the connection between\neconometrics of large games and Brock-Durlauf-type interaction models, under\nboth I.I.D. and spatially correlated unobservables. We develop new convergence\nresults for associated beliefs and estimates of preference-parameters under\nincreasing-domain spatial asymptotics. Next, we show that even with fully\nparametric specifications and unique equilibrium, choice data, that are\nsufficient for counterfactual demand-prediction under interactions, are\ninsufficient for welfare-calculations. This is because distinct underlying\nmechanisms producing the same interaction coefficient can imply different\nwelfare-effects and deadweight-loss from a policy-intervention. Standard\nindex-restrictions imply distribution-free bounds on welfare. We illustrate our\nresults using experimental data on mosquito-net adoption in rural Kenya.", "category": ["econ.EM", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.04028v1", "link": "http://arxiv.org/abs/1905.04028v1"}, {"title": "The Future Has Thicker Tails than the Past: Model Error As Branching\n  Counterfactuals", "summary": "Ex ante forecast outcomes should be interpreted as counterfactuals (potential\nhistories), with errors as the spread between outcomes. Reapplying measurements\nof uncertainty about the estimation errors of the estimation errors of an\nestimation leads to branching counterfactuals. Such recursions of epistemic\nuncertainty have markedly different distributial properties from conventional\nsampling error. Nested counterfactuals of error rates invariably lead to fat\ntails, regardless of the probability distribution used, and to powerlaws under\nsome conditions. A mere .01% branching error rate about the STD (itself an\nerror rate), and .01% branching error rate about that error rate, etc.\n(recursing all the way) results in explosive (and infinite) higher moments than\n1. Missing any degree of regress leads to the underestimation of small\nprobabilities and concave payoffs (a standard example of which is Fukushima).\nThe paper states the conditions under which higher order rates of uncertainty\n(expressed in spreads of counterfactuals) alters the shapes the of final\ndistribution and shows which a priori beliefs about conterfactuals are needed\nto accept the reliability of conventional probabilistic methods (thin tails or\nmildly fat tails).", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1209.2298v1", "link": "http://arxiv.org/abs/1209.2298v1"}, {"title": "Stock mechanics: predicting recession in S&P500, DJIA, and NASDAQ", "summary": "An original method, assuming potential and kinetic energy for prices and\nconservation of their sum is developed for forecasting exchanges. Connections\nwith power law are shown. Semiempirical applications on S&P500, DJIA, and\nNASDAQ predict a coming recession in them. An emerging market, Istanbul Stock\nExchange index ISE-100 is found involving a potential to continue to rise.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0506098v1", "link": "http://dx.doi.org/10.1007/s11534-005-0006-6"}, {"title": "Optimal investment and consumption for Ornstein-Uhlenbeck spread\n  financial markets with logarithmic utility", "summary": "We consider a spread financial market defined by the multidimensional\nOrnstein--Uhlenbeck (OU) process. We study the optimal consumption/investment\nproblem for logarithmic utility functions in the base of stochastic dynamical\nprogramming method. We show a special Verification Theorem for this case. We\nfind the solution to the Hamilton--Jacobi--Bellman (HJB) equation in explicit\nform and as a consequence we construct the optimal financial strategies.\nMoreover, we study the constructed strategy by numerical simulations.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1809.08139v1", "link": "http://arxiv.org/abs/1809.08139v1"}, {"title": "Discovering East Africa's Industrial Opportunities", "summary": "What are East Africa's industrial opportunities? In this article we explore\nthis question by using the Product Space to study the productive structure of\nfive south-east African countries: Kenya, Mozambique, Rwanda, Tanzania and\nZambia. The Product Space is a network connecting products that tend to be\nexported by the same sets of countries. Since countries are more likely to\ndevelop products that are close by in the Product Space to the ones that they\nalready produce, the Product Space can be used to help anticipate a country's\nindustrial opportunities.\n  Our results suggest that the most natural avenue for future product\ndiversification for these five south-east African nations resides in the\nagricultural sector, since all of these nations appear to have productive\nstructures that are pre-adapted to the production of many agricultural products\nthat none of them are currently exporting.\n  We conclude this paper by exploring the potential benefits of further\nregional economic integration by doing an exercise in which we pull together\nthe productive structures of these five countries. This exercise shows that the\nproducts that become more accessible in the combined economy are once again\npredominantly agricultural. These results suggest that while diversification\ninto all sectors should remain an important long-term goal of the region, the\npath towards increased diversification in the near future may well lie in a\nmore empowered and diverse agricultural sector.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1203.0163v1", "link": "http://arxiv.org/abs/1203.0163v1"}, {"title": "Model Selection Techniques -- An Overview", "summary": "In the era of big data, analysts usually explore various statistical models\nor machine learning methods for observed data in order to facilitate scientific\ndiscoveries or gain predictive power. Whatever data and fitting procedures are\nemployed, a crucial step is to select the most appropriate model or method from\na set of candidates. Model selection is a key ingredient in data analysis for\nreliable and reproducible statistical inference or prediction, and thus central\nto scientific studies in fields such as ecology, economics, engineering,\nfinance, political science, biology, and epidemiology. There has been a long\nhistory of model selection techniques that arise from researches in statistics,\ninformation theory, and signal processing. A considerable number of methods\nhave been proposed, following different philosophies and exhibiting varying\nperformances. The purpose of this article is to bring a comprehensive overview\nof them, in terms of their motivation, large sample performance, and\napplicability. We provide integrated and practically relevant discussions on\ntheoretical properties of state-of- the-art model selection approaches. We also\nshare our thoughts on some controversial views on the practice of model\nselection.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.09583v1", "link": "http://dx.doi.org/10.1109/MSP.2018.2867638"}, {"title": "The value of forecasts: Quantifying the economic gains of accurate\n  quarter-hourly electricity price forecasts", "summary": "We propose a multivariate elastic net regression forecast model for German\nquarter-hourly electricity spot markets. While the literature is diverse on\nday-ahead prediction approaches, both the intraday continuous and intraday\ncall-auction prices have not been studied intensively with a clear focus on\npredictive power. Besides electricity price forecasting, we check for the\nimpact of early day-ahead (DA) EXAA prices on intraday forecasts. Another\nnovelty of this paper is the complementary discussion of economic benefits. A\nprecise estimation is worthless if it cannot be utilized. We elaborate possible\ntrading decisions based upon our forecasting scheme and analyze their monetary\neffects. We find that even simple electricity trading strategies can lead to\nsubstantial economic impact if combined with a decent forecasting technique.", "category": ["q-fin.ST", "econ.EM", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1811.08604v1", "link": "http://dx.doi.org/10.1016/j.eneco.2018.10.005"}, {"title": "Gas Storage valuation with regime switching", "summary": "In this paper we treat a gas storage valuation problem as a Markov Decision\nProcess. As opposed to existing literature we model the gas price process as a\nregime-switching model. Such a model has shown to fit market data quite well in\nChen and Forsyth (2010). Before we apply a numerical algorithm to solve the\nproblem, we first identify the structure of the optimal injection and withdraw\npolicy. This part extends results in Secomandi (2010). Knowing the structure\nreduces the complexity of the involved recursion in the algorithms by one\nvariable. We explain the usage and implementation of two algorithms: A\nMultinomial-Tree Algorithm and a Least-Square Monte Carlo Algorithm. Both\nalgorithms are shown to work for the regime-switching extension. In a numerical\nstudy we compare these two algorithms.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1412.1298v1", "link": "http://dx.doi.org/10.1007/s12667-015-0178-0"}, {"title": "International Stock Market Efficiency: A Non-Bayesian Time-Varying Model\n  Approach", "summary": "This paper develops a non-Bayesian methodology to analyze the time-varying\nstructure of international linkages and market efficiency in G7 countries. We\nconsider a non-Bayesian time-varying vector autoregressive (TV-VAR) model, and\napply it to estimate the joint degree of market efficiency in the sense of Fama\n(1970, 1991). Our empirical results provide a new perspective that the\ninternational linkages and market efficiency change over time and that their\nbehaviors correspond well to historical events of the international financial\nsystem.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1203.5176v14", "link": "http://dx.doi.org/10.1080/00036846.2014.909579"}, {"title": "Collectivised Post-Retirement Investment", "summary": "We quantify the benefit of collectivised investment funds, in which the\nassets of members who die are shared among the survivors. For our model, with\nrealistic parameter choices, an annuity or individual fund requires\napproximately 20\\% more initial capital to provide as good an outcome as a\ncollectivised investment fund. We demonstrate the importance of the new concept\nof pension adequacy in defining investor preferences and determining optimal\nfund management. We show how to manage heterogeneous funds of investors with\ndiverse needs. Our framework can be applied to existing pension products, such\nas Collective Defined Contribution schemes.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1909.12730v3", "link": "http://arxiv.org/abs/1909.12730v3"}, {"title": "Optimal life-cycle consumption and investment decisions under\n  age-dependent risk preferences", "summary": "In this article we solve the problem of maximizing the expected utility of\nfuture consumption and terminal wealth to determine the optimal pension or\nlife-cycle fund strategy for a cohort of pension fund investors. The setup is\nstrongly related to a DC pension plan where additionally (individual)\nconsumption is taken into account. The consumption rate is subject to a\ntime-varying minimum level and terminal wealth is subject to a terminal floor.\nMoreover, the preference between consumption and terminal wealth as well as the\nintertemporal coefficient of risk aversion are time-varying and therefore\ndepend on the age of the considered pension cohort. The optimal consumption and\ninvestment policies are calculated in the case of a Black-Scholes financial\nmarket framework and hyperbolic absolute risk aversion (HARA) utility\nfunctions. We generalize Ye (2008) (2008 American Control Conference, 356-362)\nby adding an age-dependent coefficient of risk aversion and extend Steffensen\n(2011) (Journal of Economic Dynamics and Control, 35(5), 659-667), Hentschel\n(2016) (Doctoral dissertation, Ulm University) and Aase (2017) (Stochastics,\n89(1), 115-141) by considering consumption in combination with terminal wealth\nand allowing for consumption and terminal wealth floors via an application of\nHARA utility functions. A case study on fitting several models to realistic,\ntime-dependent life-cycle consumption and relative investment profiles shows\nthat only our extended model with time-varying preference parameters provides\nsufficient flexibility for an adequate fit. This is of particular interest to\nlife-cycle products for (private) pension investments or pension insurance in\ngeneral.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1908.09976v1", "link": "http://arxiv.org/abs/1908.09976v1"}, {"title": "Inference on Auctions with Weak Assumptions on Information", "summary": "Given a sample of bids from independent auctions, this paper examines the\nquestion of inference on auction fundamentals (e.g. valuation distributions,\nwelfare measures) under weak assumptions on information structure. The question\nis important as it allows us to learn about the valuation distribution in a\nrobust way, i.e., without assuming that a particular information structure\nholds across observations. We leverage the recent contributions of\n\\cite{Bergemann2013} in the robust mechanism design literature that exploit the\nlink between Bayesian Correlated Equilibria and Bayesian Nash Equilibria in\nincomplete information games to construct an econometrics framework for\nlearning about auction fundamentals using observed data on bids. We showcase\nour construction of identified sets in private value and common value auctions.\nOur approach for constructing these sets inherits the computational simplicity\nof solving for correlated equilibria: checking whether a particular valuation\ndistribution belongs to the identified set is as simple as determining whether\na {\\it linear} program is feasible. A similar linear program can be used to\nconstruct the identified set on various welfare measures and counterfactual\nobjects. For inference and to summarize statistical uncertainty, we propose\nnovel finite sample methods using tail inequalities that are used to construct\nconfidence regions on sets. We also highlight methods based on Bayesian\nbootstrap and subsampling. A set of Monte Carlo experiments show adequate\nfinite sample properties of our inference procedures. We illustrate our methods\nusing data from OCS auctions.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1710.03830v2", "link": "http://arxiv.org/abs/1710.03830v2"}, {"title": "Optimizing the tie-breaker regression discontinuity design", "summary": "Motivated by customer loyalty plans, we study tie-breaker designs which are\nhybrids of randomized controlled trials (RCTs) and regression discontinuity\ndesigns (RDDs). We quantify the statistical efficiency of a tie-breaker design\nin which a proportion $\\Delta$ of observed customers are in the RCT. In a two\nline regression, statistical efficiency increases monotonically with $\\Delta$,\nso efficiency is maximized by an RCT. That same regression model quantifies the\nshort term value of the treatment allocation and this comparison favors smaller\n$\\Delta$ with the RDD being best. We solve for the optimal tradeoff between\nthese exploration and exploitation goals. The usual tie-breaker design\nexperiments on the middle $\\Delta$ subjects as ranked by the running variable.\nWe quantify the efficiency of other designs such as experimenting only in the\nsecond decile from the top. We also consider more general models such as\nquadratic regressions.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.07563v2", "link": "http://arxiv.org/abs/1808.07563v2"}, {"title": "The Choice of When to Buy and When To Sell", "summary": "A consumer who wants to consume a good at a particular period may\nnevertheless attempt to buy it earlier if he is concerned that the good will\notherwise be sold. We analyze the behavior of consumers in equilibrium and the\nprice a profit-maximizing firm would charge. We show that a firm profits by not\nselling early. If, however, the firm is obligated to also offer the good early,\nthen the firm may maximize profits by setting a price which induces consumers\nto all arrive early, or all arrive late, depending on the good's value to the\ncustomer.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.02869v1", "link": "http://arxiv.org/abs/1912.02869v1"}, {"title": "On the maximum drawdown during speculative bubbles", "summary": "A taxonomy of large financial crashes proposed in the literature locates the\nburst of speculative bubbles due to endogenous causes in the framework of\nextreme stock market crashes, defined as falls of market prices that are\noutlier with respect to the bulk of drawdown price movement distribution. This\npaper goes on deeper in the analysis providing a further characterization of\nthe rising part of such selected bubbles through the examination of drawdown\nand maximum drawdown movement of indices prices. The analysis of drawdown\nduration is also performed and it is the core of the risk measure estimated\nhere.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0607250v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.021"}, {"title": "Liability-side Pricing of Swaps and Coherent CVA and FVA by\n  Regression/Simulation", "summary": "An uncollateralized swap hedged back-to-back by a CCP swap is used to\nintroduce FVA. The open IR01 of FVA, however, is a sure sign of risk not being\nfully hedged, a theoretical no-arbitrage pricing concern, and a bait to lure\nmarket risk capital, a practical business concern. By dynamically trading the\nCCP swap, with the liability-side counterparty provides counterparty exposure\nhedge and swap funding, we find that the uncollateralized swap can be fully\nreplicated, leaving out no IR01 leakage. The fair value of the swap is obtained\nby applying to swap's net cash flows a discount rate switching to\ncounterparty's bond curve if the swap is a local asset or one's own curve if a\nliability, and the total valuation adjustment is the present value of cost of\nfunding the risk-free price discounted at the same switching rate. FVA is\nredefined as a liquidity or funding basis component of total valuation\nadjustment, coherent with CVA, the default risk component. A Longstaff-Schwartz\nstyle least-square regression and simulation is introduced to compute the\nrecursive fair value and adjustments. A separately developed finite difference\nscheme is used to test and find regression necessary to decouple the discount\nrate switch. Preliminary results show the impact of counterparty risk to swap\nhedge ratios, swap bid/ask spreads, and valuation adjustments, and considerable\nerrors of calculating CVA by discounting cash flow or potential future\nexposure.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1512.07340v1", "link": "http://arxiv.org/abs/1512.07340v1"}, {"title": "Efficient price dynamics in a limit order market: an utility\n  indifference approach", "summary": "We construct an utility-based dynamic asset pricing model for a limit order\nmarket. The price is nonlinear in volume and subject to market impact. We solve\nan optimal hedging problem under the market impact and derive the dynamics of\nthe efficient price, that is, the asset price when a representative liquidity\ndemander follows an optimal strategy. We show that a Pareto efficient\nallocation is achieved under a completeness condi- tion. We give an explicit\nrepresentation of the efficient price for several examples. In particular, we\nobserve that the volatility of the asset depends on the convexity of an initial\nendowment. Further, we observe that an asset price crash is invoked by an\nendowment shock. We establish a dynamic programming principle under an\nincomplete framework.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1410.8224v1", "link": "http://arxiv.org/abs/1410.8224v1"}, {"title": "How production networks amplify economic growth", "summary": "Technological improvement is the most important cause of long-term economic\ngrowth, but the factors that drive it are still not fully understood. In\nstandard growth models technology is treated in the aggregate, and a main goal\nhas been to understand how growth depends on factors such as knowledge\nproduction. But an economy can also be viewed as a network, in which producers\npurchase goods, convert them to new goods, and sell them to households or other\nproducers. Here we develop a simple theory that shows how the network\nproperties of an economy can amplify the effects of technological improvements\nas they propagate along chains of production. A key property of an industry is\nits output multiplier, which can be understood as the average number of\nproduction steps required to make a good. The model predicts that the output\nmultiplier of an industry predicts future changes in prices, and that the\naverage output multiplier of a country predicts future economic growth. We test\nthese predictions using data from the World Input Output Database and find\nresults in good agreement with the model. The results show how purely\nstructural properties of an economy, that have nothing to do with innovation or\nhuman creativity, can exert an important influence on long-term growth.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1810.07774v1", "link": "http://arxiv.org/abs/1810.07774v1"}, {"title": "Numerical pricing of American options under two stochastic factor models\n  with jumps using a meshless local Petrov-Galerkin method", "summary": "The most recent update of financial option models is American options under\nstochastic volatility models with jumps in returns (SVJ) and stochastic\nvolatility models with jumps in returns and volatility (SVCJ). To evaluate\nthese options, mesh-based methods are applied in a number of papers but it is\nwell-known that these methods depend strongly on the mesh properties which is\nthe major disadvantage of them. Therefore, we propose the use of the meshless\nmethods to solve the aforementioned options models, especially in this work we\nselect and analyze one scheme of them, named local radial point interpolation\n(LRPI) based on Wendland's compactly supported radial basis functions\n(WCS-RBFs) with C6, C4 and C2 smoothness degrees. The LRPI method which is a\nspecial type of meshless local Petrov-Galerkin method (MLPG), offers several\nadvantages over the mesh-based methods, nevertheless it has never been applied\nto option pricing, at least to the very best of our knowledge. These schemes\nare the truly meshless methods, because, a traditional non-overlapping\ncontinuous mesh is not required, neither for the construction of the shape\nfunctions, nor for the integration of the local sub-domains. In this work, the\nAmerican option which is a free boundary problem, is reduced to a problem with\nfixed boundary using a Richardson extrapolation technique. Then the\nimplicit-explicit (IMEX) time stepping scheme is employed for the time\nderivative which allows us to smooth the discontinuities of the options'\npayoffs. Stability analysis of the method is analyzed and performed. In fact,\naccording to an analysis carried out in the present paper, the proposed method\nis unconditionally stable. Numerical experiments are presented showing that the\nproposed approaches are extremely accurate and fast.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1412.6064v1", "link": "http://arxiv.org/abs/1412.6064v1"}, {"title": "Averaging plus Learning in financial markets", "summary": "This paper develops original models to study interacting agents in financial\nmarkets. The key feature of these models is how interactions are formulated and\nanalysed. Agents learn from their observations and learning ability to\ninterpret news or private information. Central limit theorems are developed but\nthey arise rather unexpectedly. Under certain type of conditions governing the\nlearning, agents beliefs converge in distribution that can be even fractal. The\nunderlying randomness in the systems is not restricted to be of a certain\nclass. Fresh insights are gained not only from developing new non-linear social\nlearning models but also from using different techniques to study discrete time\nrandom linear dynamical systems.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1904.08131v2", "link": "http://arxiv.org/abs/1904.08131v2"}, {"title": "Market states: A new understanding", "summary": "We present the clustering analysis of the financial markets of S&P 500 (USA)\nand Nikkei 225 (JPN) markets over a period of 2006-2019 as an example of a\ncomplex system. We investigate the statistical properties of correlation\nmatrices constructed from the sliding epochs. The correlation matrices can be\nclassified into different clusters, named as market states based on the\nsimilarity of correlation structures. We cluster the S&P 500 market into four\nand Nikkei 225 into six market states by optimizing the value of intracluster\ndistances. The market shows transitions between these market states and the\nstatistical properties of the transitions to critical market states can\nindicate likely precursors to the catastrophic events. We also analyze the same\nclustering technique on surrogate data constructed from average correlations of\nmarket states and the fluctuations arise due to the white noise of short time\nseries. We use the correlated Wishart orthogonal ensemble for the construction\nof surrogate data whose average correlation equals the average of the real\ndata.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/2003.07058v1", "link": "http://arxiv.org/abs/2003.07058v1"}, {"title": "Quantifying the Effects of the 2008 Recession using the Zillow Dataset", "summary": "This report explores the use of Zillow's housing metrics dataset to\ninvestigate the effects of the 2008 US subprime mortgage crisis on various US\nlocales. We begin by exploring the causes of the recession and the metrics\navailable to us in the dataset. We settle on using the Zillow Home Value Index\n(ZHVI) because it is seasonally adjusted and able to account for a variety of\ninventory factors. Then, we explore three methodologies for quantifying\nrecession impact: (a) Principal Components Analysis, (b) Area Under Baseline,\nand (c) ARIMA modeling and Confidence Intervals. While PCA does not yield\nuseable results, we ended up with six cities from both AUB and ARIMA analysis,\nthe top 3 \"losers\" and \"gainers\" of the 2008 recession, as determined by each\nanalysis. This gave us 12 cities in total. Finally, we tested the robustness of\nour analysis against three \"common knowledge\" metrics for the recession:\ngeographic clustering, population trends, and unemployment rate. While we did\nfind some overlap between the results of our analysis and geographic\nclustering, there was no positive regression outcome from comparing our\nmethodologies to population trends and the unemployment rate.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1912.11341v1", "link": "http://arxiv.org/abs/1912.11341v1"}, {"title": "The Internet as Quantitative Social Science Platform: Insights from a\n  Trillion Observations", "summary": "With the large-scale penetration of the internet, for the first time,\nhumanity has become linked by a single, open, communications platform.\nHarnessing this fact, we report insights arising from a unified internet\nactivity and location dataset of an unparalleled scope and accuracy drawn from\nover a trillion (1.5$\\times 10^{12}$) observations of end-user internet\nconnections, with temporal resolution of just 15min over 2006-2012. We first\napply this dataset to the expansion of the internet itself over 1,647 urban\nagglomerations globally. We find that unique IP per capita counts reach\nsaturation at approximately one IP per three people, and take, on average, 16.1\nyears to achieve; eclipsing the estimated 100- and 60- year saturation times\nfor steam-power and electrification respectively. Next, we use intra-diurnal\ninternet activity features to up-scale traditional over-night sleep\nobservations, producing the first global estimate of over-night sleep duration\nin 645 cities over 7 years. We find statistically significant variation between\ncontinental, national and regional sleep durations including some evidence of\nglobal sleep duration convergence. Finally, we estimate the relationship\nbetween internet concentration and economic outcomes in 411 OECD regions and\nfind that the internet's expansion is associated with negative or positive\nproductivity gains, depending strongly on sectoral considerations. To our\nknowledge, our study is the first of its kind to use online/offline activity of\nthe entire internet to infer social science insights, demonstrating the\nunparalleled potential of the internet as a social data-science platform.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1701.05632v1", "link": "http://arxiv.org/abs/1701.05632v1"}, {"title": "Information-theoretic approach to lead-lag effect on financial markets", "summary": "Recently the interest of researchers has shifted from the analysis of\nsynchronous relationships of financial instruments to the analysis of more\nmeaningful asynchronous relationships. Both of those analyses are concentrated\nonly on Pearson's correlation coefficient and thus intraday lead-lag\nrelationships associated with such. Under Efficient Market Hypothesis such\nrelationships are not possible as all information is embedded in the prices. In\nthis paper we analyse lead-lag relationships of financial instruments and\nextend known methodology by using mutual information instead of Pearson's\ncorrelation coefficient, which not only is a more general measure, sensitive to\nnon-linear dependencies, but also can lead to a simpler procedure of\nstatistical validation of links between financial instruments. We analyse\nlagged relationships using NYSE 100 data not only on intraday level but also\nfor daily stock returns, which has usually been ignored.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1402.3820v1", "link": "http://dx.doi.org/10.1140/epjb/e2014-50108-3"}, {"title": "Can social microblogging be used to forecast intraday exchange rates?", "summary": "The Efficient Market Hypothesis (EMH) is widely accepted to hold true under\ncertain assumptions. One of its implications is that the prediction of stock\nprices at least in the short run cannot outperform the random walk model. Yet,\nrecently many studies stressing the psychological and social dimension of\nfinancial behavior have challenged the validity of the EMH. Towards this aim,\nover the last few years, internet-based communication platforms and search\nengines have been used to extract early indicators of social and economic\ntrends. Here, we used Twitter's social networking platform to model and\nforecast the EUR/USD exchange rate in a high-frequency intradaily trading\nscale. Using time series and trading simulations analysis, we provide some\nevidence that the information provided in social microblogging platforms such\nas Twitter can in certain cases enhance the forecasting efficiency regarding\nthe very short (intradaily) forex.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1310.5306v1", "link": "http://dx.doi.org/10.1007/s11066-013-9079-3"}, {"title": "The structure of the climate debate", "summary": "First-best climate policy is a uniform carbon tax which gradually rises over\ntime. Civil servants have complicated climate policy to expand bureaucracies,\npoliticians to create rents. Environmentalists have exaggerated climate change\nto gain influence, other activists have joined the climate bandwagon. Opponents\nto climate policy have attacked the weaknesses in climate research. The climate\ndebate is convoluted and polarized as a result, and climate policy complex.\nClimate policy should become easier and more rational as the Paris Agreement\nhas shifted climate policy back towards national governments. Changing\npolitical priorities, austerity, and a maturing bureaucracy should lead to a\nmore constructive climate debate.", "category": ["q-fin.EC", "econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1608.05597v1", "link": "http://arxiv.org/abs/1608.05597v1"}, {"title": "Theoretical Sensitivity Analysis for Quantitative Operational Risk\n  Management", "summary": "We study the asymptotic behavior of the difference between the values at risk\nVaR(L) and VaR(L+S) for heavy tailed random variables L and S for application\nin sensitivity analysis of quantitative operational risk management within the\nframework of the advanced measurement approach of Basel II (and III). Here L\ndescribes the loss amount of the present risk profile and S describes the loss\namount caused by an additional loss factor. We obtain different types of\nresults according to the relative magnitudes of the thicknesses of the tails of\nL and S. In particular, if the tail of S is sufficiently thinner than the tail\nof L, then the difference between prior and posterior risk amounts VaR(L+S) -\nVaR(L) is asymptotically equivalent to the expectation (expected loss) of S.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1104.0359v5", "link": "http://dx.doi.org/10.1142/S0219024917500327"}, {"title": "Vast Volatility Matrix Estimation using High Frequency Data for\n  Portfolio Selection", "summary": "Portfolio allocation with gross-exposure constraint is an effective method to\nincrease the efficiency and stability of selected portfolios among a vast pool\nof assets, as demonstrated in Fan et al (2008). The required high-dimensional\nvolatility matrix can be estimated by using high frequency financial data. This\nenables us to better adapt to the local volatilities and local correlations\namong vast number of assets and to increase significantly the sample size for\nestimating the volatility matrix. This paper studies the volatility matrix\nestimation using high-dimensional high-frequency data from the perspective of\nportfolio selection. Specifically, we propose the use of \"pairwise-refresh\ntime\" and \"all-refresh time\" methods proposed by Barndorff-Nielsen et al (2008)\nfor estimation of vast covariance matrix and compare their merits in the\nportfolio selection. We also establish the concentration inequalities of the\nestimates, which guarantee desirable properties of the estimated volatility\nmatrix in vast asset allocation with gross exposure constraints. Extensive\nnumerical studies are made via carefully designed simulations. Comparing with\nthe methods based on low frequency daily data, our methods can capture the most\nrecent trend of the time varying volatility and correlation, hence provide more\naccurate guidance for the portfolio allocation in the next time period. The\nadvantage of using high-frequency data is significant in our simulation and\nempirical studies, which consist of 50 simulated assets and 30 constituent\nstocks of Dow Jones Industrial Average index.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1004.4956v1", "link": "http://arxiv.org/abs/1004.4956v1"}, {"title": "Hierarchical communities in the walnut structure of the Japanese\n  production network", "summary": "This paper studies the structure of the Japanese production network, which\nincludes one million firms and five million supplier-customer links. This study\nfinds that this network forms a tightly-knit structure with a core giant\nstrongly connected component (GSCC) surrounded by IN and OUT components\nconstituting two half-shells of the GSCC, which we call a\\textit{walnut}\nstructure because of its shape. The hierarchical structure of the communities\nis studied by the Infomap method, and most of the irreducible communities are\nfound to be at the second level. The composition of some of the major\ncommunities, including overexpressions regarding their industrial or regional\nnature, and the connections that exist between the communities are studied in\ndetail. The findings obtained here cause us to question the validity and\naccuracy of using the conventional input-output analysis, which is expected to\nbe useful when firms in the same sectors are highly connected to each other.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1808.10090v1", "link": "http://dx.doi.org/10.1371/journal.pone.0202739"}, {"title": "How Do Expectations Affect Learning About Fundamentals? Some\n  Experimental Evidence", "summary": "Individuals' output often depends not just on their ability and actions, but\nalso on external factors or fundamentals, whose effect they cannot separately\nidentify. At the same time, many individuals have incorrect beliefs about their\nown ability. Heidhues et al. (2018) characterise overconfident and\nunderconfident individuals' equilibrium beliefs and learning process in these\nsituations. They argue overconfident individuals will act sub-optimally because\nof how they learn. We carry out the first experimental test of their theory.\nSubjects take incorrectly marked tests, and we measure how they learn about the\nmarker's accuracy over time. We use machine learning to identify heterogeneous\neffects. Overconfident subjects have lower beliefs about the fundamental, as\nHeidhues et al. predict, and thus would make sub-optimal decisions. But we find\nno evidence it is because of how they learn.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.07229v2", "link": "http://arxiv.org/abs/2002.07229v2"}, {"title": "A construction of (t,s)-sequences with finite-row generating matrices\n  using global function fields", "summary": "For any prime power $q$ and any dimension $s \\ge 1$, we present a\nconstruction of $(t,s)$-sequences in base $q$ with finite-row generating\nmatrices such that, for fixed $q$, the quality parameter $t$ is asymptotically\noptimal as a function of $s$ as $s \\to \\infty$. This is the first construction\nof $(t,s)$-sequences that yields finite-row generating matrices and\nasymptotically optimal quality parameters at the same time. The construction is\nbased on global function fields. We put the construction into the framework of\n$(u,{\\bf e},s)$-sequences that was recently introduced by Tezuka. In this way\nwe obtain in many cases better discrepancy bounds for the constructed sequences\nthan by previous methods for bounding the discrepancy.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1210.5152v1", "link": "http://arxiv.org/abs/1210.5152v1"}, {"title": "Blackwell dominance in large samples", "summary": "We study repeated independent Blackwell experiments; standard examples\ninclude drawing multiple samples from a population, or performing a measurement\nin different locations. In the baseline setting of a binary state of nature, we\ncompare experiments in terms of their informativeness in large samples.\nAddressing a question due to Blackwell (1951) we show that generically, an\nexperiment is more informative than another in large samples if and only if it\nhas higher Renyi divergences. As an application, we show that every additive\ndivergence is an integral of Renyi divergences.", "category": ["math.PR"], "id": "http://arxiv.org/abs/1906.02838v4", "link": "http://arxiv.org/abs/1906.02838v4"}, {"title": "New class of distortion risk measures and their tail asymptotics with\n  emphasis on VaR", "summary": "Distortion risk measures are extensively used in finance and insurance\napplications because of their appealing properties. We present three methods to\nconstruct new class of distortion functions and measures. The approach involves\nthe composting methods, the mixing methods and the approach that based on the\ntheory of copula. Subadditivity is an important property when aggregating risks\nin order to preserve the benefits of diversification. However, Value at risk\n(VaR), as the most well-known example of distortion risk measure is not always\nglobally subadditive, except of elliptically distributed risks. In this paper,\ninstead of study subadditivity we investigate the tail subadditivity for VaR\nand other distortion risk measures. In particular, we demonstrate that VaR is\ntail subadditive for the case where the support of risk is bounded. Various\nexamples are also presented to illustrate the results.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1503.08586v2", "link": "http://arxiv.org/abs/1503.08586v2"}, {"title": "Second-Order, Dissipative T\u00e2tonnement: Economic Interpretation and\n  2-Point Limit Cycles", "summary": "This paper proposes an alternative to the classical price-adjustment\nmechanism (called \"t\\^{a}tonnement\" after Walras) that is second-order in time.\nThe proposed mechanism, an analogue to the damped harmonic oscillator, provides\na dynamic equilibration process that depends only on local information. We show\nhow such a process can result from simple behavioural rules. The discrete-time\nform of the model can result in two-step limit cycles, but as the distance\ncovered by the cycle depends on the size of the damping, the proposed mechanism\ncan lead to both highly unstable and relatively stable behaviour, as observed\nin real economies.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1108.0188v3", "link": "http://arxiv.org/abs/1108.0188v3"}, {"title": "A new proposal for the construction of a multi-period/multilateral price\n  index", "summary": "Price indexes in time and space is a most relevant topic in statistical\nanalysis from both the methodological and the application side. In this paper a\nprice index providing a novel and effective solution to price indexes over\nseveral periods and among several countries, that is in both a multi-period and\na multilateral framework, is devised. The reference basket of the devised index\nis the union of the intersections of the baskets of all periods/countries in\npairs. As such, it provides a broader coverage than usual indexes. Index\nclosed-form expressions and updating formulas are provided and properties\ninvestigated. Last, applications with real and simulated data provide evidence\nof the performance of the index at stake.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.00073v1", "link": "http://arxiv.org/abs/1910.00073v1"}, {"title": "A study of co-movements between USA and Latin American stock markets: a\n  cross-bicorrelations perspective", "summary": "In this paper we use the Brooks and Hinich cross-bicorrelation test in order\nto uncover nonlinear dependence periods between USA Standard and Poor 500\n(SP500), used as benchmark, and six Latin American stock markets indexes:\nMexico (BMV), Brazil (BOVESPA), Chile (IPSA), Colombia (COLCAP), Peru (IGBVL)\nand Argentina (MERVAL). We have found windows of nonlinear dependence and\nco-movement between the SP500 and the Latin American stock markets, some of\nwhich coincide with periods of crisis, giving way to a possible contagion or\ninterdependence interpretation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1503.06926v1", "link": "http://arxiv.org/abs/1503.06926v1"}, {"title": "Least Squares Importance Sampling for Libor Market Models", "summary": "A recently introduced Importance Sampling strategy based on a least squares\noptimization is applied to the Monte Carlo simulation of Libor Market Models.\nSuch Least Squares Importance Sampling (LSIS) allows the automatic optimization\nof the sampling distribution within a trial class by means of a quick\npresimulation algorithm of straightforward implementation. With several\nnumerical examples we show that LSIS can be extremely effective in reducing the\nvariance of Monte Carlo estimators often resulting, especially when combined\nwith stratified sampling, in computational speed-ups of orders of magnitude.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0711.0223v1", "link": "http://arxiv.org/abs/0711.0223v1"}, {"title": "Prudent case-based prediction when experience is lacking", "summary": "An inexperienced predictor is asked to qualitatively rank eventualities\naccording to their plausibility, given past cases. Inexperience means that,\nresampling past cases (with replacement) fails to generate a suitably diverse\nset of rankings. (4-diversity requires that each of the 4! strict rankings of\nfour eventualities arises for some sample.) Along with other essential\nconsistency requirements, 4-diversity yields a matrix representation that may\nbe viewed as an empirical likelihood function (Gilboa and Schmeidler, 2003). We\nimpose 2-diversity and derive a similar representation: provided the predictor\nis prudent enough to ensure that the arrival of novel cases will not force her\ninto being dogmatic, intransitive or into revising her existing rankings. We\nbuild on this to establish a formal tradeoff between inexperience and the\ncognitive or computational cost of more abstract resampling.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1904.02934v1", "link": "http://arxiv.org/abs/1904.02934v1"}, {"title": "The Kelly growth optimal strategy with a stop-loss rule", "summary": "From the Hamilton-Jacobi-Bellman equation for the value function we derive a\nnon-linear partial differential equation for the optimal portfolio strategy\n(the dynamic control). The equation is general in the sense that it does not\ndepend on the terminal utility and provides additional analytical insight for\nsome optimal investment problems with known solutions. Furthermore, when\nboundary conditions for the optimal strategy can be established independently,\nit is considerably simpler than the HJB to solve numerically. Using this method\nwe calculate the Kelly growth optimal strategy subject to a periodically reset\nstop-loss rule.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1311.2550v2", "link": "http://arxiv.org/abs/1311.2550v2"}, {"title": "calculation worst-case Value-at-Risk prediction using empirical data\n  under model uncertainty", "summary": "Quantification of risk positions under model uncertainty is of crucial\nimportance from both viewpoints of external regulation and internal management.\nThe concept of model uncertainty, sometimes also referred to as model\nambiguity. Although we know the family of models, we cannot precisely decide\nwhich one to use. Given the set $\\mathcal{P}$, the value of the risk measure\n$\\rho$ varies in a range over the set of all possible models. The largest value\nin such a range is referred to as a worst-case value, and the corresponding\nmodel is called a worst scenario. Value-at-Risk(VaR) has become a very popular\nrisk-measurement tool since it was first proposed. Naturally, WVaR(worst-case\nValue-at-Risk) attracts the attention of many researchers. Although many\nliteratures investigated WVaR, the implications for empirical data analysis\nremain rare. In this paper, we proposed a special model uncertainty market\nmodel to simply the $\\mathcal{P}$ to a set contain finite number of probability\ndistributions. The model has the structure of the two-layer mixed distribution\nmodel. We used change point detection method to divide the returns series and\nthen used EM algorithm to estimate the parameters. Finally, we calculated VaR,\nWVaR(worst-case Value-at-Risk) and BVaR(best-case Value-at-Risk) for four\nfinancial markets and then analyzed their different performance.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1908.00982v1", "link": "http://arxiv.org/abs/1908.00982v1"}, {"title": "Optimal Dynamic Allocation of Attention", "summary": "We consider a decision maker (DM) who, before taking an action, seeks\ninformation by allocating her limited attention dynamically over different news\nsources that are biased toward alternative actions. Endogenous choice of\ninformation generates rich dynamics: The chosen news source either reinforces\nor weakens the prior, shaping subsequent attention choices, belief updating,\nand the final action. The DM adopts a learning strategy biased toward the\ncurrent belief when the belief is extreme and against that belief when it is\nmoderate. Applied to consumption of news media, observed behavior exhibits an\n`echo-chamber' effect for partisan voters and a novel `anti echo-chamber'\neffect for moderates.", "category": [], "id": "http://arxiv.org/abs/1812.06967v1", "link": "http://arxiv.org/abs/1812.06967v1"}, {"title": "Bootstrap Consistency for Quadratic Forms of Sample Averages with\n  Increasing Dimension", "summary": "This paper establishes consistency of the weighted bootstrap for quadratic\nforms $\\left( n^{-1/2} \\sum_{i=1}^{n} Z_{i,n} \\right)^{T}\\left( n^{-1/2}\n\\sum_{i=1}^{n} Z_{i,n} \\right)$ where $(Z_{i,n})_{i=1}^{n}$ are mean zero,\nindependent $\\mathbb{R}^{d}$-valued random variables and $d=d(n)$ is allowed to\ngrow with the sample size $n$, slower than $n^{1/4}$. The proof relies on an\nadaptation of Lindeberg interpolation technique whereby we simplify the\noriginal problem to a Gaussian approximation problem. We apply our bootstrap\nresults to model-specification testing problems when the number of moments is\nallowed to grow with the sample size.", "category": ["econ.EM", "math.PR"], "id": "http://arxiv.org/abs/1411.2701v4", "link": "http://arxiv.org/abs/1411.2701v4"}, {"title": "Robust and Adaptive Algorithms for Online Portfolio Selection", "summary": "We present an online approach to portfolio selection. The motivation is\nwithin the context of algorithmic trading, which demands fast and recursive\nupdates of portfolio allocations, as new data arrives. In particular, we look\nat two online algorithms: Robust-Exponentially Weighted Least Squares (R-EWRLS)\nand a regularized Online minimum Variance algorithm (O-VAR). Our methods use\nsimple ideas from signal processing and statistics, which are sometimes\noverlooked in the empirical financial literature. The two approaches are\nevaluated against benchmark allocation techniques using 4 real datasets. Our\nmethods outperform the benchmark allocation techniques in these datasets, in\nterms of both computational demand and financial performance.", "category": ["q-fin.PM", "q-fin.CP"], "id": "http://arxiv.org/abs/1005.2979v1", "link": "http://arxiv.org/abs/1005.2979v1"}, {"title": "Deriving the factor endowment--commodity output relationship for\n  Thailand (1920-1927) using a three-factor two-good general equilibrium trade\n  model", "summary": "Feeny (1982, pp. 26-28) referred to a three-factor two-good general\nequilibrium trade model, when he explained the relative importance of trade and\nfactor endowments in Thailand 1880-1940. For example, Feeny (1982) stated that\nthe growth in labor stock would be responsible for a substantial increase in\nrice output relative to textile output. Is Feeny's statement plausible? The\npurpose of this paper is to derive the Rybczynski sign patterns, which express\nthe factor endowment--commodity output relationship, for Thailand during the\nperiod 1920 to 1927 using the EWS (economy-wide substitution)-ratio vector. A\n'strong Rybczynski result' necessarily holds. I derived three Rybczynski sign\npatterns. However, a more detailed estimate allowed a reduction from three\ncandidates to two. I restrict the analysis to the period 1920-1927 because of\ndata availability. The results imply that Feeny's statement might not\nnecessarily hold. Hence, labor stock might not affect the share of exportable\nsector in national income positively. Moreover, the percentage of Chinese\nimmigration in the total population growth was not as large as expected. This\nstudy will be useful when simulating real wage in Thailand.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.04819v1", "link": "http://arxiv.org/abs/1810.04819v1"}, {"title": "Existence of Equilibrium Prices: A Pedagogical Proof", "summary": "Under the same assumptions made by Mas-Colell et al. (1995), I develop a\nshort, simple, and complete proof of existence of equilibrium prices based on\nexcess demand functions. The result is obtained by applying the Brouwer fixed\npoint theorem to a trimmed simplex which does not contain prices equal to zero.\nThe mathematical techniques are based on some results obtained in Neuefeind\n(1980) and Geanakoplos (2003).", "category": [], "id": "http://arxiv.org/abs/1808.03129v2", "link": "http://arxiv.org/abs/1808.03129v2"}, {"title": "Asymptotic Optimal Strategy for Portfolio Optimization in a Slowly\n  Varying Stochastic Environment", "summary": "In this paper, we study the portfolio optimization problem with general\nutility functions and when the return and volatility of underlying asset are\nslowly varying. An asymptotic optimal strategy is provided within a specific\nclass of admissible controls under this problem setup. Specifically, we first\nestablish a rigorous first order approximation of the value function associated\nto a fixed zeroth order suboptimal trading strategy, which is given by the\nheuristic argument in [J.-P. Fouque, R. Sircar and T. Zariphopoulou, {\\it\nMathematical Finance}, 2016]. Then, we show that this zeroth order suboptimal\nstrategy is asymptotically optimal in a specific family of admissible trading\nstrategies. Finally, we show that our assumptions are satisfied by a particular\nfully solvable model.", "category": ["q-fin.MF", "math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1603.03538v2", "link": "http://arxiv.org/abs/1603.03538v2"}, {"title": "The network structure of city-firm relations", "summary": "How are economic activities linked to geographic locations? To answer this\nquestion, we use a data-driven approach that builds on the information about\nlocation, ownership and economic activities of the world's 3,000 largest firms\nand their almost one million subsidiaries. From this information we generate a\nbipartite network of cities linked to economic activities. Analysing the\nstructure of this network, we find striking similarities with nested networks\nobserved in ecology, where links represent mutualistic interactions between\nspecies. This motivates us to apply ecological indicators to identify the\nunbalanced deployment of economic activities. Such deployment can lead to an\nover-representation of specific economic sectors in a given city, and poses a\nsignificant thread for the city's future especially in times when the\nover-represented activities face economic uncertainties. If we compare our\nanalysis with external rankings about the quality of life in a city, we find\nthat the nested structure of the city-firm network also reflects such\ninformation about the quality of life, which can usually be assessed only via\ndedicated survey-based indicators.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1512.02859v1", "link": "http://arxiv.org/abs/1512.02859v1"}, {"title": "Downstream Effects of Affirmative Action", "summary": "We study a two-stage model, in which students are 1) admitted to college on\nthe basis of an entrance exam which is a noisy signal about their\nqualifications (type), and then 2) those students who were admitted to college\ncan be hired by an employer as a function of their college grades, which are an\nindependently drawn noisy signal of their type. Students are drawn from one of\ntwo populations, which might have different type distributions. We assume that\nthe employer at the end of the pipeline is rational, in the sense that it\ncomputes a posterior distribution on student type conditional on all\ninformation that it has available (college admissions, grades, and group\nmembership), and makes a decision based on posterior expectation. We then study\nwhat kinds of fairness goals can be achieved by the college by setting its\nadmissions rule and grading policy. For example, the college might have the\ngoal of guaranteeing equal opportunity across populations: that the probability\nof passing through the pipeline and being hired by the employer should be\nindependent of group membership, conditioned on type. Alternately, the college\nmight have the goal of incentivizing the employer to have a group blind hiring\nrule. We show that both goals can be achieved when the college does not report\ngrades. On the other hand, we show that under reasonable conditions, these\ngoals are impossible to achieve even in isolation when the college uses an\n(even minimally) informative grading policy.", "category": [], "id": "http://arxiv.org/abs/1808.09004v1", "link": "http://arxiv.org/abs/1808.09004v1"}, {"title": "A Primer on Portfolio Choice with Small Transaction Costs", "summary": "This survey is an introduction to asymptotic methods for portfolio-choice\nproblems with small transaction costs. We outline how to derive the\ncorresponding dynamic programming equations and simplify them in the small-cost\nlimit. This allows to obtain explicit solutions in a wide range of settings,\nwhich we illustrate for a model with mean-reverting expected returns and\nproportional transaction costs. For even more complex models, we present a\npolicy iteration scheme that allows to compute the solution numerically.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1612.01302v2", "link": "http://arxiv.org/abs/1612.01302v2"}, {"title": "Economics 2.0: The Natural Step towards A Self-Regulating, Participatory\n  Market Society", "summary": "Despite all our great advances in science, technology and financial\ninnovations, many societies today are struggling with a financial, economic and\npublic spending crisis, over-regulation, and mass unemployment, as well as lack\nof sustainability and innovation. Can we still rely on conventional economic\nthinking or do we need a new approach?\n  I argue that, as the complexity of socio-economic systems increases,\nnetworked decision-making and bottom-up self-regulation will be more and more\nimportant features. It will be explained why, besides the \"homo economicus\"\nwith strictly self-regarding preferences, natural selection has also created a\n\"homo socialis\" with other-regarding preferences. While the \"homo economicus\"\noptimizes the own prospects in separation, the decisions of the \"homo socialis\"\nare self-determined, but interconnected, a fact that may be characterized by\nthe term \"networked minds\". Notably, the \"homo socialis\" manages to earn higher\npayoffs than the \"homo economicus\".\n  I show that the \"homo economicus\" and the \"homo socialis\" imply a different\nkind of dynamics and distinct aggregate outcomes. Therefore, next to the\ntraditional economics for the \"homo economicus\" (\"economics 1.0\"), a\ncomplementary theory must be developed for the \"homo socialis\". This economic\ntheory might be called \"economics 2.0\" or \"socionomics\". The names are\njustified, because the Web 2.0 is currently promoting a transition to a new\nmarket organization, which benefits from social media platforms and could be\ncharacterized as \"participatory market society\". To thrive, the \"homo socialis\"\nrequires suitable institutional settings such a particular kinds of reputation\nsystems, which will be sketched in this paper. I also propose a new kind of\nmoney, so-called \"qualified money\", which may overcome some of the problems of\nour current financial system.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1305.4078v2", "link": "http://arxiv.org/abs/1305.4078v2"}, {"title": "Model Selection in Time Series Analysis: Using Information Criteria as\n  an Alternative to Hypothesis Testing", "summary": "The issue of model selection in applied research is of vital importance.\nSince the true model in such research is not known, which model should be used\nfrom among various potential ones is an empirical question. There might exist\nseveral competitive models. A typical approach to dealing with this is classic\nhypothesis testing using an arbitrarily chosen significance level based on the\nunderlying assumption that a true null hypothesis exists. In this paper we\ninvestigate how successful this approach is in determining the correct model\nfor different data generating processes using time series data. An alternative\napproach based on more formal model selection techniques using an information\ncriterion or cross-validation is suggested and evaluated in the time series\nenvironment via Monte Carlo experiments. This paper also explores the\neffectiveness of deciding what type of general relation exists between two\nvariables (e.g. relation in levels or relation in first differences) using\nvarious strategies based on hypothesis testing and on information criteria with\nthe presence or absence of unit roots.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1805.08991v1", "link": "http://arxiv.org/abs/1805.08991v1"}, {"title": "A Neural-embedded Choice Model: TasteNet-MNL Modeling Taste\n  Heterogeneity with Flexibility and Interpretability", "summary": "Discrete choice models (DCMs) and neural networks (NNs) can complement each\nother. We propose a neural network embedded choice model - TasteNet-MNL, to\nimprove the flexibility in modeling taste heterogeneity while keeping model\ninterpretability. The hybrid model consists of a TasteNet module: a\nfeed-forward neural network that learns taste parameters as flexible functions\nof individual characteristics; and a choice module: a multinomial logit model\n(MNL) with manually specified utility. TasteNet and MNL are fully integrated\nand jointly estimated. By embedding a neural network into a DCM, we exploit a\nneural network's function approximation capacity to reduce specification bias.\nThrough special structure and parameter constraints, we incorporate expert\nknowledge to regularize the neural network and maintain interpretability. On\nsynthetic data, we show that TasteNet-MNL can recover the underlying non-linear\nutility function, and provide predictions and interpretations as accurate as\nthe true model; while examples of logit or random coefficient logit models with\nmisspecified utility functions result in large parameter bias and low\npredictability. In the case study of Swissmetro mode choice, TasteNet-MNL\noutperforms benchmarking MNLs' predictability; and discovers a wider spectrum\nof taste variations within the population, and higher values of time on\naverage. This study takes an initial step towards developing a framework to\ncombine theory-based and data-driven approaches for discrete choice modeling.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.00922v1", "link": "http://arxiv.org/abs/2002.00922v1"}, {"title": "A pathwise approach to continuous-time trading", "summary": "This paper develops a mathematical framework for the analysis of\ncontinuous-time trading strategies which, in contrast to the classical setting\nof continuous-time mathematical finance, does not rely on stochastic integrals\nor other probabilistic notions. Our purely analytic framework allows for the\nderivation of a pathwise self-financial condition for continuous-time trading\nstrategies, which is consistent with the classical definition in case a\nprobability model is introduced. Our first proposition provides us with a\npathwise definition of the gain process for a large class of continuous-time,\npath-dependent, self-finacing trading strategies, including the important class\nof 'delta-hedging' strategies, and is based on the recently developed\n'non-anticipative functional calculus'. Two versions of the statement involve\nrespectively continuous and c\\`adl\\`ag price paths. The second proposition is a\npathwise replication result that generalizes the ones obtained in the classical\nframework of diffusion models. Moreover, it gives an explicit and purely\npathwise formula for the hedging error of delta-hedging strategies for\npath-dependent derivatives across a given set of scenarios. We also provide an\neconomic justification of our main assumption on price paths.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1602.04946v1", "link": "http://arxiv.org/abs/1602.04946v1"}, {"title": "Option pricing models without probability", "summary": "We describe the pricing and hedging practices refraining from the use of\nprobability. We encode volatility in an enhancement of the price trajectory and\nwe give pathwise presentations of the fundamental equations of Mathematical\nFinance. In particular this allows us to assess model misspecification,\ngeneralising the so-called fundamental theorem of derivative trading (see\nEllersgaard et al. 2017). Our pathwise integrals and equations exhibit the role\nof Greeks beyond the leading-order Delta, and makes explicit the role of Gamma\nsensitivities.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1808.09378v2", "link": "http://arxiv.org/abs/1808.09378v2"}, {"title": "Evidence for the Gompertz Curve in the Income Distribution of Brazil\n  1978-2005", "summary": "This work presents an empirical study of the evolution of the personal income\ndistribution in Brazil. Yearly samples available from 1978 to 2005 were studied\nand evidence was found that the complementary cumulative distribution of\npersonal income for 99% of the economically less favorable population is well\nrepresented by a Gompertz curve of the form $G(x)=\\exp [\\exp (A-Bx)]$, where\n$x$ is the normalized individual income. The complementary cumulative\ndistribution of the remaining 1% richest part of the population is well\nrepresented by a Pareto power law distribution $P(x)= \\beta x^{-\\alpha}$. This\nresult means that similarly to other countries, Brazil's income distribution is\ncharacterized by a well defined two class system. The parameters $A$, $B$,\n$\\alpha$, $\\beta$ were determined by a mixture of boundary conditions,\nnormalization and fitting methods for every year in the time span of this\nstudy. Since the Gompertz curve is characteristic of growth models, its\npresence here suggests that these patterns in income distribution could be a\nconsequence of the growth dynamics of the underlying economic system. In\naddition, we found out that the percentage share of both the Gompertzian and\nParetian components relative to the total income shows an approximate cycling\npattern with periods of about 4 years and whose maximum and minimum peaks in\neach component alternate at about every 2 years. This finding suggests that the\ngrowth dynamics of Brazil's economic system might possibly follow a\nGoodwin-type class model dynamics based on the application of the\nLotka-Volterra equation to economic growth and cycle.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/0812.2664v1", "link": "http://dx.doi.org/10.1140/epjb/e2008-00469-1"}, {"title": "Spatial interactions in agent-based modeling", "summary": "Agent Based Modeling (ABM) has become a widespread approach to model complex\ninteractions. In this chapter after briefly summarizing some features of ABM\nthe different approaches in modeling spatial interactions are discussed.\n  It is stressed that agents can interact either indirectly through a shared\nenvironment and/or directly with each other. In such an approach, higher-order\nvariables such as commodity prices, population dynamics or even institutions,\nare not exogenously specified but instead are seen as the results of\ninteractions. It is highlighted in the chapter that the understanding of\npatterns emerging from such spatial interaction between agents is a key problem\nas much as their description through analytical or simulation means.\n  The chapter reviews different approaches for modeling agents' behavior,\ntaking into account either explicit spatial (lattice based) structures or\nnetworks. Some emphasis is placed on recent ABM as applied to the description\nof the dynamics of the geographical distribution of economic activities, - out\nof equilibrium. The Eurace@Unibi Model, an agent-based macroeconomic model with\nspatial structure, is used to illustrate the potential of such an approach for\nspatial policy analysis.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1405.0733v1", "link": "http://arxiv.org/abs/1405.0733v1"}, {"title": "Does the price of strategic commodities respond to U.S. Partisan\n  Conflict?", "summary": "A noteworthy feature of U.S. politics in recent years is serious partisan\nconflict, which has led to intensifying polarization and exacerbating high\npolicy uncertainty. The US is a significant player in oil and gold markets. Oil\nand gold also form the basis of important strategic reserves in the US. We\ninvestigate whether U.S. partisan conflict affects the returns and price\nvolatility of oil and gold using a parametric test of Granger causality in\nquantiles. The empirical results suggest that U.S. partisan conflict has an\neffect on the returns of oil and gold, and the effects are concentrated at the\ntail of the conditional distribution of returns. More specifically, the\npartisan conflict mainly affects oil returns when the crude oil market is in a\nbearish state (lower quantiles). By contrast, partisan conflict matters for\ngold returns only when the gold market is in a bullish scenario (higher\nquantiles). In addition, for the volatility of oil and gold, the predictability\nof partisan conflict index virtually covers the entire distribution of\nvolatility.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.08396v2", "link": "http://arxiv.org/abs/1810.08396v2"}, {"title": "On the Normality of Negative Interest Rates", "summary": "We argue that a negative interest rate policy (NIRP) can be an effect tool\nfor macroeconomic stabilization. We first discuss how implementing negative\nrates on reserves held at a central bank does not pose any theoretical\ndifficulty, with a reduction in rates operating in exactly the same way when\nrates are positive or negative, and show that this is compatible with an\nendogenous money point of view. We then propose a simplified stock-flow\nconsistent macroeconomic model where rates are allowed to become arbitrarily\nnegative and present simulation evidence for their stabilizing effects. In\npractice, the existence of physical cash imposes a lower bound for interest\nrates, which in our view is the main reason for the lack of effectiveness of\nnegative interest rates in the countries that adopted them as part of their\nmonetary policy. We conclude by discussing alternative ways to overcome this\nlower bound , in particular the use of central bank digital currencies.", "category": ["econ.GN", "q-fin.EC", "q-fin.MF"], "id": "http://arxiv.org/abs/1808.07909v1", "link": "http://arxiv.org/abs/1808.07909v1"}, {"title": "Predicting market instability: New dynamics between volume and\n  volatility", "summary": "Econophysics and econometrics agree that there is a correlation between\nvolume and volatility in a time series. Using empirical data and their\ndistributions, we further investigate this correlation and discover new ways\nthat volatility and volume interact, particularly when the levels of both are\nhigh. We find that the distribution of the volume-conditional volatility is\nwell fit by a power-law function with an exponential cutoff. We find that the\nvolume-conditional volatility distribution scales with volume, and collapses\nthese distributions to a single curve. We exploit the characteristics of the\nvolume-volatility scatter plot to find a strong correlation between logarithmic\nvolume and a quantity we define as local maximum volatility (LMV), which\nindicates the largest volatility observed in a given range of trading volumes.\nThis finding supports our empirical analysis showing that volume is an\nexcellent predictor of the maximum value of volatility for both same-day and\nnear-future time periods. We also use a joint conditional probability that\nincludes both volatility and volume to demonstrate that invoking both allows us\nto better predict the largest next-day volatility than invoking either one\nalone.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1403.5193v1", "link": "http://arxiv.org/abs/1403.5193v1"}, {"title": "Parallel Algorithm for Approximating Nash Equilibrium in Multiplayer\n  Stochastic Games with Application to Naval Strategic Planning", "summary": "Many real-world domains contain multiple agents behaving strategically with\nprobabilistic transitions and uncertain (potentially infinite) duration. Such\nsettings can be modeled as stochastic games. While algorithms have been\ndeveloped for solving (i.e., computing a game-theoretic solution concept such\nas Nash equilibrium) two-player zero-sum stochastic games, research on\nalgorithms for non-zero-sum and multiplayer stochastic games is limited. We\npresent a new algorithm for these settings, which constitutes the first\nparallel algorithm for multiplayer stochastic games. We present experimental\nresults on a 4-player stochastic game motivated by a naval strategic planning\nscenario, showing that our algorithm is able to quickly compute strategies\nconstituting Nash equilibrium up to a very small degree of approximation error.", "category": [], "id": "http://arxiv.org/abs/1910.00193v4", "link": "http://arxiv.org/abs/1910.00193v4"}, {"title": "Gauge Invariance, Geometry and Arbitrage", "summary": "In this work, we identify the most general measure of arbitrage for any\nmarket model governed by It\\^o processes. We show that our arbitrage measure is\ninvariant under changes of num\\'{e}raire and equivalent probability. Moreover,\nsuch measure has a geometrical interpretation as a gauge connection. The\nconnection has zero curvature if and only if there is no arbitrage. We prove an\nextension of the Martingale pricing theorem in the case of arbitrage. In our\ncase, the present value of any traded asset is given by the expectation of\nfuture cash-flows discounted by a line integral of the gauge connection. We\ndevelop simple strategies to measure arbitrage using both simulated and real\nmarket data. We find that, within our limited data sample, the market is\nefficient at time horizons of one day or longer. However, we provide strong\nevidence for non-zero arbitrage in high frequency intraday data. Such events\nseem to have a decay time of the order of one minute.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0908.3043v1", "link": "http://arxiv.org/abs/0908.3043v1"}, {"title": "Multilevel estimation of expected exit times and other functionals of\n  stopped diffusions", "summary": "This paper proposes and analyses a new multilevel Monte Carlo method for the\nestimation of mean exit times for multi-dimensional Brownian diffusions, and\nassociated functionals which correspond to solutions to high-dimensional\nparabolic PDEs through the Feynman-Kac formula. In particular, it is proved\nthat the complexity to achieve an $\\varepsilon$ root-mean-square error is\n$O(\\varepsilon^{-2}\\, |\\!\\log \\varepsilon|^3)$.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1710.07492v2", "link": "http://arxiv.org/abs/1710.07492v2"}, {"title": "Stationarity of Bivariate Dynamic Contagion Processes", "summary": "The Bivariate Dynamic Contagion Processes (BDCP) are a broad class of\nbivariate point processes characterized by the intensities as a general class\nof piecewise deterministic Markov processes. The BDCP describes a rich dynamic\nstructure where the system is under the influence of both external and internal\nfactors modelled by a shot-noise Cox process and a generalized Hawkes process\nrespectively. In this paper we mainly address the stationarity issue for the\nBDCP, which is important in applications. We investigate the stationary\ndistribution by applying the the Markov theory on the branching system\napproximation representation of the BDCP. We find the condition under which\nthere exists a unique stationary distribution of the BDCP intensity and the\nresulting BDCP has stationary increments. Moments of the stationary intensity\nare provided by using the Markov property.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1405.5842v1", "link": "http://arxiv.org/abs/1405.5842v1"}, {"title": "Dynamic exponential utility indifference valuation", "summary": "We study the dynamics of the exponential utility indifference value process\nC(B;\\alpha) for a contingent claim B in a semimartingale model with a general\ncontinuous filtration. We prove that C(B;\\alpha) is (the first component of)\nthe unique solution of a backward stochastic differential equation with a\nquadratic generator and obtain BMO estimates for the components of this\nsolution. This allows us to prove several new results about C_t(B;\\alpha). We\nobtain continuity in B and local Lipschitz-continuity in the risk aversion\n\\alpha, uniformly in t, and we extend earlier results on the asymptotic\nbehavior as \\alpha\\searrow0 or \\alpha\\nearrow\\infty to our general setting.\nMoreover, we also prove convergence of the corresponding hedging strategies.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/math/0508489v1", "link": "http://dx.doi.org/10.1214/105051605000000395"}, {"title": "Optimal intervention in the foreign exchange market when interventions\n  affect market dynamics", "summary": "We address the problem of optimal Central Bank intervention in the exchange\nrate market when interventions create feedback in the rate dynamics. In\nparticular, we extend the work done on optimal impulse control by Cadenillas\nand Zapatero to incorporate temporary market reactions, of random duration and\nlevel, to Bank interventions, and to establish results for more general rate\nprocesses. We obtain new explicit optimal impulse control strategies that\naccount for these market reactions, and show that they cannot be obtained\nsimply by adjusting the intervention cost in a model without market reactions.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0909.1142v1", "link": "http://arxiv.org/abs/0909.1142v1"}, {"title": "Matching distributions: Recovery of implied physical densities from\n  option prices", "summary": "We introduce a non-parametric method to recover physical probability\ndistributions of asset returns based on their European option prices and some\nother sparse parametric information. Thus the main problem is similar to the\none considered foir instance in the Recovery Theorem by Ross (2015), except\nthat here we consider a non-dynamical setting. The recovery of the distribution\nis complete, instead of estimating merely a finite number of its parameters,\nsuch as implied volatility, skew or kurtosis. The technique is based on a\nreverse application of recently introduced Distribution Matching by the author\nand is related to the ideas in Distribution Pricing by Dybvig (1988) as well as\ncomonotonicity.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.03996v1", "link": "http://arxiv.org/abs/1803.03996v1"}, {"title": "Integration with respect to model-free price paths with jumps", "summary": "For every adapted, c\\`agl\\`ad process (strategy) $G$ and typical c\\`adl\\`ag\nprice paths whose jumps satisfy some mild growth condition we define integral\n$G\\cdot S$ as a limit of simple integrals.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1511.08194v3", "link": "http://arxiv.org/abs/1511.08194v3"}, {"title": "A Consistent Stochastic Model of the Term Structure of Interest Rates\n  for Multiple Tenors", "summary": "Explicitly taking into account the risk incurred when borrowing at a shorter\ntenor versus lending at a longer tenor (\"roll-over risk\"), we construct a\nstochastic model framework for the term structure of interest rates in which a\nfrequency basis (i.e. a spread applied to one leg of a swap to exchange one\nfloating interest rate for another of a different tenor in the same currency)\narises endogenously. This rollover risk consists of two components, a credit\nrisk component due to the possibility of being downgraded and thus facing a\nhigher credit spread when attempting to roll over short-term borrowing, and a\ncomponent reflecting the (systemic) possibility of being unable to roll over\nshort-term borrowing at the reference rate (e.g., LIBOR) due to an absence of\nliquidity in the market. The modelling framework is of \"reduced form\" in the\nsense that (similar to the credit risk literature) the source of credit risk is\nnot modelled (nor is the source of liquidity risk). However, the framework has\nmore structure than the literature seeking to simply model a different term\nstructure of interest rates for each tenor frequency, since relationships\nbetween rates for all tenor frequencies are established based on the modelled\nroll-over risk. We proceed to consider a specific case within this framework,\nwhere the dynamics of interest rate and roll-over risk are driven by a\nmultifactor Cox/Ingersoll/Ross-type process, show how such model can be\ncalibrated to market data, and used for relative pricing of interest rate\nderivatives, including bespoke tenor frequencies not liquidly traded in the\nmarket.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1809.06643v1", "link": "http://arxiv.org/abs/1809.06643v1"}, {"title": "Risk-minimization and hedging claims on a jump-diffusion market model,\n  Feynman-Kac Theorem and PIDE", "summary": "At first, we solve a problem of finding a risk-minimizing hedging strategy on\na general market with ratings. Next, we find a solution to this problem on\nMarkovian market with ratings on which prices are influenced by additional\nfactors and rating, and behavior of this system is described by SDE driven by\nWiener process and compensated Poisson random measure and claims depend on\nrating. To find a tool to calculate hedging strategy we prove a Feynman-Kac\ntype theorem. This result is of independent interest and has many applications,\nsince it enables to calculate some conditional expectations using related\nPIDE's. We illustrate our theory on two examples of market. The first is a\ngeneral exponential L\\'{e}vy model with stochastic volatility, and the second\nis a generalization of exponential L\\'{e}vy model with regime-switching.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1305.4132v2", "link": "http://arxiv.org/abs/1305.4132v2"}, {"title": "Optimal consumption and investment with bounded downside risk measures\n  for logarithmic utility functions", "summary": "We investigate optimal consumption problems for a Black-Scholes market under\nuniform restrictions on Value-at-Risk and Expected Shortfall for logarithmic\nutility functions. We find the solutions in terms of a dynamic strategy in\nexplicit form, which can be compared and interpreted. This paper continues our\nprevious work, where we solved similar problems for power utility functions.", "category": ["q-fin.PM", "math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1002.2486v1", "link": "http://arxiv.org/abs/1002.2486v1"}, {"title": "Value-Based Inventory Management", "summary": "The basic financial purpose of a firm is to maximize its value. An inventory\nmanagement system should also contribute to realization of this basic aim. Many\ncurrent asset management models currently found in financial management\nliterature were constructed with the assumption of book profit maximization as\nbasic aim. However these models could lack what relates to another aim, i.e.,\nmaximization of enterprise value. This article presents a modified value-based\ninventory management model.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1301.3826v1", "link": "http://arxiv.org/abs/1301.3826v1"}, {"title": "Uniform inference for bounds on the distribution and quantile functions\n  of treatment effects in randomized experiments", "summary": "This paper develops a novel approach to uniform inference for functions that\nbound the distribution and quantile functions of heterogeneous treatment\neffects in randomized experiments when only marginal treatment and control\ndistributions are observed and the joint distribution of outcomes is\nunobserved. These bounds are nonlinear maps of the marginal distribution\nfunctions of control and treatment outcomes, and statistical inference methods\nfor nonlinear maps usually rely on smoothness through a type of\ndifferentiability. We show that the maps from marginal distributions to bound\nfunctions are not differentiable, but uniform test statistics applied to the\nbound functions - such as Kolmogorov-Smirnov or Cram\\'er-von Mises - are\ndirectionally differentiable. We establish the consistency and weak convergence\nof nonparametric plug-in estimates of the test statistics and show how they can\nbe used to conduct inference for bounds uniformly over the distribution of\ntreatment effects. We also establish the directional differentiability of\nminimax operators applied to general - that is, not only convex-concave -\nfunctions, which may be of independent interest. In addition, we develop\ndetailed resampling techniques to conduct practical inference for the bounds or\nfor the true distribution or quantile function of the treatment effect\ndistribution. Finally, we apply our methods to the evaluation of a job training\nprogram.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1911.10215v1", "link": "http://arxiv.org/abs/1911.10215v1"}, {"title": "Crisis-Critical Intellectual Property: Findings from the COVID-19\n  Pandemic", "summary": "Within national and international innovation systems a pandemic calls for\nlarge-scale action by many actors across sectors, to mobilise resources,\ndeveloping and manufacturing Crisis-Critical Products (CC-Products) efficiently\nand in the huge quantities needed. Nowadays, this also includes digital\ninnovations from complex epidemiological models, AI, to open data platforms for\nprevention, diagnostic and treatment. Amongst the many challenges during a\npandemic, innovation and manufacturing stakeholders find themselves engaged in\nnew relationships, and are likely to face intellectual property (IP) related\nchallenges. This paper adopts an IP perspective on the COVID-19 pandemic to\nidentify pandemic related IP considerations and IP challenges. The focus is on\nchallenges related to research, development and urgent upscaling of capacity to\nmanufacture CC-Products in the huge volumes suddenly in demand. Its purpose is\nto provide a structure for steering clear of IP challenges to avoid delays in\nfighting a pandemic. We identify 4 stakeholder groups concerned with IP\nchallenges: (i) governments, (ii) organisations owning existing Crisis-Critical\nIP, described as incumbents in Crisis-Critical Sectors (CC-Sectors), (iii)\nmanufacturing firms from other sectors normally not producing CC-Products\nsuddenly rushing into CC-Sectors to support the manufacturing of CC-Products\n(new entrants), and (iv) voluntary grassroot initiatives that are formed during\na pandemic. This paper discusses IP challenges related to the development and\nmanufacturing of technologies and products for (i) prevention (of spread), (ii)\ndiagnosis of infected patients and (iii) the development of treatments. We\noffer an initial discussion of potential response measures to reduce IP\nassociated risks among industrial stakeholders during a pandemic.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.03715v2", "link": "http://dx.doi.org/10.17863/CAM.51142"}, {"title": "Factor endowment -- commodity output relationships in a three-factor,\n  two-good general equilibrium trade model", "summary": "We analyze the Rybczynski sign pattern, which expresses the factor endowment\n- commodity output relationships in a three-factor, two-good general\nequilibrium trade model. The relationship determines whether a strong\nRybczynski result holds. We search for a sufficient condition for each\nRybczynski sign pattern to hold in a systematic manner, which no other studies\nhave derived. We assume factor-intensity ranking is constant. We use the EWS\n(economy-wide substitution)-ratio vector and the Hadamard product in our\nanalysis. We show that the position of the EWS-ratio vector determines the\nRybczynski sign pattern. This article provides a basis for further\napplications.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1711.11429v1", "link": "http://arxiv.org/abs/1711.11429v1"}, {"title": "Semi Markov model for market microstructure", "summary": "We introduce a new model for describing the fluctuations of a tick-by-tick\nsingle asset price. Our model is based on Markov renewal processes. We consider\na point process associated to the timestamps of the price jumps, and marks\nassociated to price increments. By modeling the marks with a suitable Markov\nchain, we can reproduce the strong mean-reversion of price returns known as\nmicrostructure noise. Moreover, by using Markov renewal processes, we can model\nthe presence of spikes in intensity of market activity, i.e. the volatility\nclustering, and consider dependence between price increments and jump times. We\nalso provide simple parametric and nonparametric statistical procedures for the\nestimation of our model. We obtain closed-form formula for the mean signature\nplot, and show the diffusive behavior of our model at large scale limit. We\nillustrate our results by numerical simulations, and that our model is\nconsistent with empirical data on the Euribor future.", "category": ["q-fin.TR", "math.PR"], "id": "http://arxiv.org/abs/1305.0105v1", "link": "http://arxiv.org/abs/1305.0105v1"}, {"title": "Loss aversion and the welfare ranking of policy interventions", "summary": "In this paper we develop theoretical criteria and econometric methods to rank\npolicy interventions in terms of welfare when individuals are loss-averse. The\nnew criterion for \"loss aversion-sensitive dominance\" defines a weak partial\nordering of the distributions of policy-induced gains and losses. It applies to\nthe class of welfare functions which model individual preferences with\nnon-decreasing and loss-averse attitudes towards changes in outcomes. We also\ndevelop new statistical methods to test loss aversion-sensitive dominance in\npractice, using nonparametric plug-in estimates. We establish the limiting\ndistributions of uniform test statistics by showing that they are directionally\ndifferentiable. This implies that inference can be conducted by a special\nresampling procedure. Since point-identification of the distribution of\npolicy-induced gains and losses may require very strong assumptions, we also\nextend comparison criteria, test statistics, and resampling procedures to the\npartially-identified case. Finally, we illustrate our methods with an empirical\napplication to welfare comparison of two income support programs.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.08468v1", "link": "http://arxiv.org/abs/2004.08468v1"}, {"title": "Extended Gini-type measures of risk and variability", "summary": "The aim of this paper is to introduce a risk measure that extends the\nGini-type measures of risk and variability, the Extended Gini Shortfall, by\ntaking risk aversion into consideration. Our risk measure is coherent and\ncatches variability, an important concept for risk management. The analysis is\nmade under the Choquet integral representations framework. We expose results\nfor analytic computation under well-known distribution functions. Furthermore,\nwe provide a practical application.", "category": ["q-fin.RM", "q-fin.GN"], "id": "http://arxiv.org/abs/1707.07322v2", "link": "http://arxiv.org/abs/1707.07322v2"}, {"title": "On the Evolution of U.S. Temperature Dynamics", "summary": "Climate change is a multidimensional shift. While much research has\ndocumented rising mean temperature levels, we also examine range-based measures\nof daily temperature volatility. Specifically, using data for select U.S.\ncities over the past half-century, we compare the evolving time series dynamics\nof the average temperature level, AVG, and the diurnal temperature range, DTR\n(the difference between the daily maximum and minimum temperatures at a given\nlocation). We characterize trend and seasonality in these two series using\nlinear models with time-varying coefficients. These straightforward yet\nflexible approximations provide evidence of evolving DTR seasonality, stable\nAVG seasonality, and conditionally Gaussian but heteroskedastic innovations for\nboth DTR and AVG.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.06303v1", "link": "http://arxiv.org/abs/1907.06303v1"}, {"title": "On the valuation of compositions in L\u00e9vy term structure models", "summary": "We derive explicit valuation formulae for an exotic path-dependent interest\nrate derivative, namely an option on the composition of LIBOR rates. The\nformulae are based on Fourier transform methods for option pricing. We consider\ntwo models for the evolution of interest rates: an HJM-type forward rate model\nand a LIBOR-type forward price model. Both models are driven by a\ntime-inhomogeneous L\\'evy process.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0902.3456v1", "link": "http://arxiv.org/abs/0902.3456v1"}, {"title": "Multifractal modeling of short-term interest rates", "summary": "We propose a multifractal model for short-term interest rates. The model is a\nversion of the Markov-Switching Multifractal (MSM), which incorporates the\nwell-known level effect observed in interest rates. Unlike previously suggested\nmodels, the level-MSM model captures the power-law scaling of the structure\nfunctions and the slowly decaying dependency in the absolute value of returns.\nWe apply the model to the Norwegian Interbank Offered Rate with three months\nmaturity (NIBORM3) and the U.S. Treasury Bill with three months maturity\n(TBM3). The performance of the model is compared to level-GARCH models,\nlevel-EGARCH models and jump-diffusions. For the TBM3 data the multifractal\nout-performs all the alternatives considered.", "category": ["q-fin.ST", "q-fin.RM"], "id": "http://arxiv.org/abs/1111.5265v1", "link": "http://arxiv.org/abs/1111.5265v1"}, {"title": "Detrended cross-correlations between returns, volatility, trading\n  activity, and volume traded for the stock market companies", "summary": "We consider a few quantities that characterize trading on a stock market in a\nfixed time interval: logarithmic returns, volatility, trading activity (i.e.,\nthe number of transactions), and volume traded. We search for the power-law\ncross-correlations among these quantities aggregated over different time units\nfrom 1 min to 10 min. Our study is based on empirical data from the American\nstock market consisting of tick-by-tick recordings of 31 stocks listed in Dow\nJones Industrial Average during the years 2008-2011. Since all the considered\nquantities except the returns show strong daily patterns related to the\nvariable trading activity in different parts of a day, which are the best\nevident in the autocorrelation function, we remove these patterns by detrending\nbefore we proceed further with our study. We apply the multifractal detrended\ncross-correlation analysis with sign preserving (MFCCA) and show that the\nstrongest power-law cross-correlations exist between trading activity and\nvolume traded, while the weakest ones exist (or even do not exist) between the\nreturns and the remaining quantities. We also show that the strongest\ncross-correlations are carried by those parts of the signals that are\ncharacterized by large and medium variance. Our observation that the most\nconvincing power-law cross-correlations occur between trading activity and\nvolume traded reveals the existence of strong fractal-like coupling between\nthese quantities.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1510.04910v2", "link": "http://dx.doi.org/10.1209/0295-5075/112/48001"}, {"title": "Covariance matrix filtering with bootstrapped hierarchies", "summary": "Statistical inference of the dependence between objects often relies on\ncovariance matrices. Unless the number of features (e.g. data points) is much\nlarger than the number of objects, covariance matrix cleaning is necessary to\nreduce estimation noise. We propose a method that is robust yet flexible enough\nto account for fine details of the structure covariance matrix. Robustness\ncomes from using a hierarchical ansatz and dependence averaging between\nclusters; flexibility comes from a bootstrap procedure. This method finds\nseveral possible hierarchical structures in DNA microarray gene expression\ndata, and leads to lower realized risk in global minimum variance portfolios\nthan current filtering methods when the number of data points is relatively\nsmall.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/2003.05807v1", "link": "http://arxiv.org/abs/2003.05807v1"}, {"title": "On the Hedging of Options On Exploding Exchange Rates", "summary": "We study a novel pricing operator for complete, local martingale models. The\nnew pricing operator guarantees put-call parity to hold for model prices and\nthe value of a forward contract to match the buy-and-hold strategy, even if the\nunderlying follows strict local martingale dynamics. More precisely, we discuss\na change of num\\'eraire (change of currency) technique when the underlying is\nonly a local martingale modelling for example an exchange rate. The new pricing\noperator assigns prices to contingent claims according to the minimal cost for\nsuperreplication strategies that succeed with probability one for both\ncurrencies as num\\'eraire. Within this context, we interpret the lack of the\nmartingale property of an exchange-rate as a reflection of the possibility that\nthe num\\'eraire currency may devalue completely against the asset currency\n(hyperinflation).", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1202.6188v3", "link": "http://arxiv.org/abs/1202.6188v3"}, {"title": "Analysis of Ornstein-Uhlenbeck process stopped at maximum drawdown and\n  application to trading strategies with trailing stops", "summary": "We propose a strategy for automated trading, outline theoretical\njustification of the profitability of this strategy and overview the\nhypothetical results in application to currency pairs trading. The proposed\nmethodology relies on the assumption that processes reflecting the dynamics of\ncurrency exchange rates are in a certain sense similar to the class of\nOrnstein-Uhlenbeck processes and exhibits the mean reverting property. In order\nto describe the quantitative characteristics of the projected return of the\nstrategy, we derive the explicit expression for the running maximum of the\nOrnstein-Uhlenbeck process stopped at maximum drawdown and look at the\ncorrespondence between derived characteristics and the observed ones.", "category": ["q-fin.TR", "math.PR"], "id": "http://arxiv.org/abs/1507.01610v1", "link": "http://arxiv.org/abs/1507.01610v1"}, {"title": "A Relation between Short-Term and Long-Term Arbitrage", "summary": "In this work a relation between a measure of short-term arbitrage in the\nmarket and the excess growth of portfolios as a notion of long-term arbitrage\nis established. The former originates from \"Geometric Arbitrage Theory\" and the\nlatter from \"Stochastic Portfolio Theory\". Both aim to describe non-equilibrium\neffects in financial markets. Thereby, a connection between two different\ntheoretical frameworks of arbitrage is drawn.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1909.00570v1", "link": "http://arxiv.org/abs/1909.00570v1"}, {"title": "Stock Trading via Feedback Control: Stochastic Model Predictive or\n  Genetic?", "summary": "We seek a discussion about the most suitable feedback control structure for\nstock trading under the consideration of proportional transaction costs.\nSuitability refers to robustness and performance capability. Both are tested by\nconsidering different one-step ahead prediction qualities, including the ideal\ncase, correct prediction of the direction of change in daily stock prices and\nthe worst-case. Feedback control structures are partitioned into two general\nclasses: stochastic model predictive control (SMPC) and genetic. For the former\nclass three controllers are discussed, whereby it is distinguished between two\nMarkowitz- and one dynamic hedging-inspired SMPC formulation. For the latter\nclass five trading algorithms are disucssed, whereby it is distinguished\nbetween two different moving average (MA) based, two trading range (TR) based,\nand one strategy based on historical optimal (HistOpt) trajectories. This paper\nalso gives a preliminary discussion about how modified dynamic hedging-inspired\nSMPC formulations may serve as alternatives to Markowitz portfolio\noptimization. The combinations of all of the eight controllers with five\ndifferent one-step ahead prediction methods are backtested for daily trading of\nthe 30 components of the German stock market index DAX for the time period\nbetween November 27, 2015 and November 25, 2016.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1708.08857v2", "link": "http://arxiv.org/abs/1708.08857v2"}, {"title": "Cluster-Robust Standard Errors for Linear Regression Models with Many\n  Controls", "summary": "It is common practice in empirical work to employ cluster-robust standard\nerrors when using the linear regression model to estimate some\nstructural/causal effect of interest. Researchers also often include a large\nset of regressors in their model specification in order to control for observed\nand unobserved confounders. In this paper we develop inference methods for\nlinear regression models with many controls and clustering. We show that\ninference based on the usual cluster-robust standard errors by Liang and Zeger\n(1986) is invalid in general when the number of controls is a non-vanishing\nfraction of the sample size. We then propose a new clustered standard errors\nformula that is robust to the inclusion of many controls and allows to carry\nout valid inference in a variety of high-dimensional linear regression models,\nincluding fixed effects panel data models and the semiparametric partially\nlinear model. Monte Carlo evidence supports our theoretical results and shows\nthat our proposed variance estimator performs well in finite samples. The\nproposed method is also illustrated with an empirical application that\nre-visits Donohue III and Levitt's (2001) study of the impact of abortion on\ncrime.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1806.07314v3", "link": "http://arxiv.org/abs/1806.07314v3"}, {"title": "Managing Default Contagion in Inhomogeneous Financial Networks", "summary": "The aim of this paper is to quantify and manage systemic risk caused by\ndefault contagion in the interbank market. We model the market as a random\ndirected network, where the vertices represent financial institutions and the\nweighted edges monetary exposures between them. Our model captures the strong\ndegree of heterogeneity observed in empirical data and the parameters can\neasily be fitted to real data sets. One of our main results allows us to\ndetermine the impact of local shocks, where initially some banks default, to\nthe entire system and the wider economy. Here the impact is measured by some\nindex of total systemic importance of all eventually defaulted institutions. As\na central application, we characterize resilient and non-resilient cases. In\nparticular, for the prominent case where the network has a degree sequence\nwithout second moment, we show that a small number of initially defaulted banks\ncan trigger a substantial default cascade. Our results complement and extend\nsignificantly earlier findings derived in the configuration model where the\nexistence of a second moment of the degree distribution is assumed. As a second\nmain contribution, paralleling regulatory discussions, we determine minimal\ncapital requirements for financial institutions sufficient to make the network\nresilient to small shocks. An appealing feature of these capital requirements\nis that they can be determined locally by each institution without knowing the\ncomplete network structure as they basically only depend on the institution's\nexposures to its counterparties.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1610.09542v5", "link": "http://arxiv.org/abs/1610.09542v5"}, {"title": "Exact Pricing Asymptotics of Investment-Grade Tranches of Synthetic\n  CDO's Part I: A Large Homogeneous Pool", "summary": "We use the theory of large deviations to study the pricing of\ninvestment-grade tranches of synthetic CDO's. In this paper, we consider a\nsimplified model which will allow us to introduce some of the concepts and\ncalculations.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0903.4475v1", "link": "http://arxiv.org/abs/0903.4475v1"}, {"title": "Time-inconsistent Markovian control problems under model uncertainty\n  with application to the mean-variance portfolio selection", "summary": "In this paper we study a class of time-inconsistent terminal Markovian\ncontrol problems in discrete time subject to model uncertainty. We combine the\nconcept of the sub-game perfect strategies with the adaptive robust stochastic\nto tackle the theoretical aspects of the considered stochastic control problem.\nConsequently, as an important application of the theoretical results, by\napplying a machine learning algorithm we solve numerically the mean-variance\nportfolio selection problem under the model uncertainty.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2002.02604v1", "link": "http://arxiv.org/abs/2002.02604v1"}, {"title": "Dynamic Process of Money Transfer Models", "summary": "We have studied numerically the statistical mechanics of the dynamic\nphenomena, including money circulation and economic mobility, in some transfer\nmodels. The models on which our investigations were performed are the basic\nmodel proposed by A. Dragulescu and V. Yakovenko [1], the model with uniform\nsaving rate developed by A. Chakraborti and B.K. Chakrabarti [2], and its\nextended model with diverse saving rate [3]. The velocity of circulation is\nfound to be inversely related with the average holding time of money. In order\nto check the nature of money transferring process in these models, we\ndemonstrated the probability distributions of holding time. In the model with\nuniform saving rate, the distribution obeys exponential law, which indicates\nmoney transfer here is a kind of Poisson process. But when the saving rate is\nset diversely, the holding time distribution follows a power law. The velocity\ncan also be deduced from a typical individual's optimal choice. In this way, an\napproach for building the micro-foundation of velocity is provided. In order to\nexpose the dynamic mechanism behind the distribution in microscope, we examined\nthe mobility by collecting the time series of agents' rank and measured it by\nemploying an index raised by economists. In the model with uniform saving rate,\nthe higher saving rate, the slower agents moves in the economy. Meanwhile, all\nof the agents have the same chance to be the rich. However, it is not the case\nin the model with diverse saving rate, where the assumed economy falls into\nstratification. The volatility distribution of the agents' ranks are also\ndemonstrated to distinguish the differences among these models.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0507162v1", "link": "http://arxiv.org/abs/physics/0507162v1"}, {"title": "Modelling the health impact of food taxes and subsidies with price\n  elasticities: the case for additional scaling of food consumption using the\n  total food expenditure elasticity", "summary": "Background Food taxes and subsidies are one intervention to address poor\ndiets. Price elasticity (PE) matrices are commonly used to model the change in\nfood purchasing. Usually a PE matrix is generated in one setting then applied\nto another setting with differing starting consumption and prices of foods.\nThis violates econometric assumptions resulting in likely misestimation of\ntotal food consumption. We illustrate rescaling all consumption after applying\na PE matrix using a total food expenditure elasticity (TFEe, the expenditure\nelasticity for all food combined given the policy induced change in the total\nprice of food). We use case studies of NZ$2 per 100g saturated fat (SAFA) tax,\nNZ$0.4 per 100g sugar tax, and a 20% fruit and vegetable (F&V) subsidy. Methods\nWe estimated changes in food purchasing using a NZ PE matrix applied\nconventionally, then with TFEe adjustment. Impacts were quantified for total\nfood expenditure and health adjusted life years (HALYs) for the total NZ\npopulation alive in 2011 over the rest of their lifetime using a multistate\nlifetable model. Results Two NZ studies gave TFEes of 0.68 and 0.83, with\ninternational estimates ranging from 0.46 to 0.90. Without TFEe adjustment,\ntotal food expenditure decreased with the tax policies and increased with the\nF&V subsidy, implausible directions of shift given economic theory. After TFEe\nadjustment, HALY gains reduced by a third to a half for the two taxes and\nreversed from an apparent health loss to a health gain for the F&V subsidy.\nWith TFEe adjustment, HALY gains (in 1000s) were 1,805 (95% uncertainty\ninterval 1,337 to 2,340) for the SAFA tax, 1,671 (1,220 to 2,269) for the sugar\ntax, and 953 (453 to 1,308) for the F&V subsidy. Conclusions If PE matrices are\napplied in settings beyond where they were derived, additional scaling is\nlikely required. We suggest that the TFEe is a useful scalar.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.13179v1", "link": "http://arxiv.org/abs/1909.13179v1"}, {"title": "Robust Hedging with Proportional Transaction Costs", "summary": "Duality for robust hedging with proportional transaction costs of path\ndependent European options is obtained in a discrete time financial market with\none risky asset. Investor's portfolio consists of a dynamically traded stock\nand a static position in vanilla options which can be exercised at maturity.\nBoth the stock and the option trading is subject to proportional transaction\ncosts. The main theorem is duality between hedging and a Monge-Kantorovich type\noptimization problem. In this dual transport problem the optimization is over\nall the probability measures which satisfy an approximate martingale condition\nrelated to consistent price systems in addition to the usual marginal\nconstraints.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1302.0590v3", "link": "http://arxiv.org/abs/1302.0590v3"}, {"title": "A Subjective and Probabilistic Approach to Derivatives", "summary": "We propose a probabilistic framework for pricing derivatives, which\nacknowledges that information and beliefs are subjective. Market prices can be\ntranslated into implied probabilities. In particular, futures imply returns for\nthese implied probability distributions. We argue that volatility is not risk,\nbut uncertainty. Non-normal distributions combine the risk in the left tail\nwith the opportunities in the right tail -- unifying the \"risk premium\" with\nthe possible loss. Risk and reward must be part of the same picture and\nexpected returns must include possible losses due to risks. We reinterpret the\nBlack-Scholes pricing formulas as prices for maximum-entropy probability\ndistributions, illuminating their importance from a new angle. Using these\nideas we show how derivatives can be priced under \"uncertain uncertainty\" and\nhow this creates a skew for the implied volatilities. We argue that the current\nstandard approach based on stochastic modelling and risk-neutral pricing fails\nto account for subjectivity in markets and mistreats uncertainty as risk.\nFurthermore, it is founded on a questionable argument -- that uncertainty is\neliminated at all cost.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1001.1616v1", "link": "http://arxiv.org/abs/1001.1616v1"}, {"title": "Conformal Prediction Interval Estimations with an Application to\n  Day-Ahead and Intraday Power Markets", "summary": "We discuss a concept denoted as Conformal Prediction (CP) in this paper.\nWhile initially stemming from the world of machine learning, it was never\napplied or analyzed in the context of short-term electricity price forecasting.\nTherefore, we elaborate the aspects that render Conformal Prediction worthwhile\nto know and explain why its simple yet very efficient idea has worked in other\nfields of application and why its characteristics are promising for short-term\npower applications as well. We compare its performance with different\nstate-of-the-art electricity price forecasting models such as quantile\nregression averaging (QRA) in an empirical out-of-sample study for three\nshort-term electricity time series. We combine Conformal Prediction with\nvarious underlying point forecast models to demonstrate its versatility and\nbehavior under changing conditions. Our findings suggest that Conformal\nPrediction yields sharp and reliable prediction intervals in short-term power\nmarkets. We further inspect the effect each of Conformal Prediction's model\ncomponents has and provide a path-based guideline on how to find the best CP\nmodel for each market.", "category": ["econ.EM", "q-fin.PM", "q-fin.TR"], "id": "http://arxiv.org/abs/1905.07886v1", "link": "http://arxiv.org/abs/1905.07886v1"}, {"title": "Challenges in approximating the Black and Scholes call formula with\n  hyperbolic tangents", "summary": "In this paper we introduce the concept of standardized call function and we\nobtain a new approximating formula for the Black and Scholes call function\nthrough the hyperbolic tangent. This formula is useful for pricing and risk\nmanagement as well as for extracting the implied volatility from quoted\noptions. The latter is of particular importance since it indicates the risk of\nthe underlying and it is the main component of the option's price. Further we\nestimate numerically the approximating error of the suggested solution and, by\ncomparing our results in computing the implied volatility with the most common\nmethods available in literature we discuss the challenges of this approach.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1810.04623v1", "link": "http://arxiv.org/abs/1810.04623v1"}, {"title": "A General Equilibrium Theorem for the Economy of Giving", "summary": "In [1] we presented a model for transactions when goods are given away in the\nexpectation of a later settlement. In settings where people keep track of their\nsocial accounts we were able to redefine concepts like account balance, yield\ncurve and the law of diminishing returns. In this paper we establish a general\nequilibrium theorem, conjectured in [1], by developing sufficient conditions\nfor any instance of the standard model (or Gift Economy Model) to have a unique\nequilibrium. The convergence to that equilibrium is exponential and for each\npair of entities P and Q the total sum of yields from all mutual transactions\nis equal to zero.\n  [1] W.P. Weijland, Mathematical Foundations for the Economy of Giving, ArXiv\nCategories: q-fin.GN, Report 1401.4664, 2014.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1411.1929v1", "link": "http://arxiv.org/abs/1411.1929v1"}, {"title": "Value-at-Risk substitute for non-ruin capital is fallacious and\n  redundant", "summary": "This seemed impossible to use a theoretically adequate but too sophisticated\nrisk measure called non-ruin capital, whence its widespread (including\nregulatory documents) replacement with an inadequate, but simple risk measure\ncalled Value-at-Risk. Conflicting with the idea by Albert Einstein that\n\"everything should be made as simple as possible, but not simpler\", this led to\nfallacious, and even deceitful (but generally accepted) standards and\nrecommendations. Arguing from the standpoint of mathematical theory of risk, we\naim to break this impasse.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/2005.05428v1", "link": "http://arxiv.org/abs/2005.05428v1"}, {"title": "Quantum Econophysics", "summary": "The relationships between game theory and quantum mechanics let us propose\ncertain quantization relationships through which we could describe and\nunderstand not only quantum but also classical, evolutionary and the biological\nsystems that were described before through the replicator dynamics. Quantum\nmechanics could be used to explain more correctly biological and economical\nprocesses and even it could encloses theories like games and evolutionary\ndynamics. This could make quantum mechanics a more general theory that we had\nthought. Although both systems analyzed are described through two apparently\ndifferent theories (quantum mechanics and game theory) it is shown that both\nsystems are analogous and thus exactly equivalents. So, we can take some\nconcepts and definitions from quantum mechanics and physics for the best\nunderstanding of the behavior of economics and biology. Also, we could maybe\nunderstand nature like a game in where its players compete for a common welfare\nand the equilibrium of the system that they are members.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0609245v3", "link": "http://arxiv.org/abs/physics/0609245v3"}, {"title": "CORN: Correlation-Driven Nonparametric Learning Approach for Portfolio\n  Selection -- an Online Appendix", "summary": "This appendix proves CORN's universal consistency. One of Bin's PhD thesis\nexaminer (Special thanks to Vladimir Vovk from Royal Holloway, University of\nLondon) suggested that CORN is universal and provided sketch proof of Lemma\n1.6, which is the key of this proof. Based on the proof in Gy\\\"prfi et al.\n[2006], we thus prove CORN's universal consistency. Note that the notations in\nthis appendix follows Gy\\\"orfi et al. [2006].", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1306.1378v1", "link": "http://arxiv.org/abs/1306.1378v1"}, {"title": "Dynamic Competitive Persuasion", "summary": "We examine a dynamic game of competitive persuasion played between two\nlong-lived sellers over $T \\leq \\infty$ periods. Each period, each seller\nprovides information via a Blackwell experiment to a single short-lived buyer,\nwho buys from the seller whose product has the highest expected quality. We\nsolve for the unique subgame perfect equilibrium of this game, and conduct\ncomparative statics: in particular we find that long horizons lead to less\ninformation.", "category": ["math.PR", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.11664v4", "link": "http://arxiv.org/abs/1811.11664v4"}, {"title": "Scaling of the distribution of fluctuations of financial market indices", "summary": "We study the distribution of fluctuations over a time scale $\\Delta t$ (i.e.,\nthe returns) of the S&P 500 index by analyzing three distinct databases.\nDatabase (i) contains approximately 1 million records sampled at 1 min\nintervals for the 13-year period 1984-1996, database (ii) contains 8686 daily\nrecords for the 35-year period 1962-1996, and database (iii) contains 852\nmonthly records for the 71-year period 1926-1996. We compute the probability\ndistributions of returns over a time scale $\\Delta t$, where $\\Delta t$ varies\napproximately over a factor of 10^4 - from 1 min up to more than 1 month. We\nfind that the distributions for $\\Delta t \\leq$ 4 days (1560 mins) are\nconsistent with a power-law asymptotic behavior, characterized by an exponent\n$\\alpha \\approx 3$, well outside the stable L\\'evy regime $0 < \\alpha < 2$. To\ntest the robustness of the S&P result, we perform a parallel analysis on two\nother financial market indices. Database (iv) contains 3560 daily records of\nthe NIKKEI index for the 14-year period 1984-97, and database (v) contains 4649\ndaily records of the Hang-Seng index for the 18-year period 1980-97. We find\nestimates of $\\alpha$ consistent with those describing the distribution of S&P\n500 daily-returns. One possible reason for the scaling of these distributions\nis the long persistence of the autocorrelation function of the volatility. For\ntime scales longer than $(\\Delta t)_{\\times} \\approx 4$ days, our results are\nconsistent with slow convergence to Gaussian behavior.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9905305v1", "link": "http://dx.doi.org/10.1103/PhysRevE.60.5305"}, {"title": "Linkages and systemic risk in the European insurance sector: Some new\n  evidence based on dynamic spanning trees", "summary": "This paper is part of the research on the interlinkages between insurers and\ntheir contribution to systemic risk on the insurance market. Its main purpose\nis to present the results of the analysis of linkage dynamics and systemic risk\nin the European insurance sector which are obtained using correlation networks.\nThese networks are based on dynamic dependence structures modelled using a\ncopula. Then, we determine minimum spanning trees (MST). Finally, the linkage\ndynamics is described by means of selected topological network measures.", "category": ["q-fin.ST", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.01142v2", "link": "http://arxiv.org/abs/1908.01142v2"}, {"title": "On the existence of sure profits via flash strategies", "summary": "We introduce and study the notion of sure profit via flash strategy,\nconsisting of a high-frequency limit of buy-and-hold trading strategies. In a\nfully general setting, without imposing any semimartingale restriction, we\nprove that there are no sure profits via flash strategies if and only if asset\nprices do not exhibit predictable jumps. This result relies on the general\ntheory of processes and provides the most general formulation of the well-known\nfact that, in an arbitrage-free financial market, asset prices (including\ndividends) should not exhibit jumps of a predictable direction or magnitude at\npredictable times. We furthermore show that any price process is always\nright-continuous in the absence of sure profits. Our results are robust under\nsmall transaction costs and imply that, under minimal assumptions, price\nchanges occurring at scheduled dates should only be due to unanticipated\ninformation releases.", "category": ["q-fin.TR", "math.PR"], "id": "http://arxiv.org/abs/1708.03099v4", "link": "http://dx.doi.org/10.1017/jpr.2019.32"}, {"title": "Bubble Diagnosis and Prediction of the 2005-2007 and 2008-2009 Chinese\n  stock market bubbles", "summary": "By combining (i) the economic theory of rational expectation bubbles, (ii)\nbehavioral finance on imitation and herding of investors and traders and (iii)\nthe mathematical and statistical physics of bifurcations and phase transitions,\nthe log-periodic power law model has been developed as a flexible tool to\ndetect bubbles. The LPPL model considers the faster-than-exponential (power law\nwith finite-time singularity) increase in asset prices decorated by\naccelerating oscillations as the main diagnostic of bubbles. It embodies a\npositive feedback loop of higher return anticipations competing with negative\nfeedback spirals of crash expectations. We use the LPPL model in one of its\nincarnations to analyze two bubbles and subsequent market crashes in two\nimportant indexes in the Chinese stock markets between May 2005 and July 2009.\nBoth the Shanghai Stock Exchange Composite and Shenzhen Stock Exchange\nComponent indexes exhibited such behavior in two distinct time periods: 1) from\nmid-2005, bursting in Oct. 2007 and 2) from Nov. 2008, bursting in the\nbeginning of Aug. 2009. We successfully predicted time windows for both crashes\nin advance with the same methods used to successfully predict the peak in\nmid-2006 of the US housing bubble and the peak in July 2008 of the global oil\nbubble. The more recent bubble in the Chinese indexes was detected and its end\nor change of regime was predicted independently by two groups with similar\nresults, showing that the model has been well-documented and can be replicated\nby industrial practitioners. Here we present more detailed analysis of the\nindividual Chinese index predictions and of the methods used to make and test\nthem.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0909.1007v2", "link": "http://dx.doi.org/10.1016/j.jebo.2010.02.007"}, {"title": "Why Financial Markets Will Remain Marginally Inefficient?", "summary": "I summarize the recent work on market (in)efficiency, highlighting key\nelements why financial markets will never be made efficient. My approach is not\nby adding more empirical evidence, but giving plausible reasons as to where\ninefficiency arises and why it's not rational to arbitrage it away.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0105373v1", "link": "http://arxiv.org/abs/cond-mat/0105373v1"}, {"title": "Size matters: some stylized facts of the stock market revisited", "summary": "We reanalyze high resolution data from the New York Stock Exchange and find a\nmonotonic (but not power law) variation of the mean value per trade, the mean\nnumber of trades per minute and the mean trading activity with company\ncapitalization. We show that the second moment of the traded value distribution\nis finite. Consequently, the Hurst exponents for the corresponding time series\ncan be calculated. These are, however, non-universal: The persistence grows\nwith larger capitalization and this results in a logarithmically increasing\nHurst exponent. A similar trend is displayed by intertrade time intervals.\nFinally, we demonstrate that the distribution of the intertrade times is better\ndescribed by a multiscaling ansatz than by simple gap scaling.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0508156v4", "link": "http://dx.doi.org/10.1140/epjb/e2006-00189-6"}, {"title": "Option pricing under normal dynamics with stochastic volatility", "summary": "In this paper, we derive the price of a European call option of an asset\nfollowing a normal process assuming stochastic volatility. The volatility is\nassumed to follow the Cox Ingersoll Ross (CIR) process. We then use the fast\nFourier transform (FFT) to evaluate the option price given we know the\ncharacteristic function of the return analytically. We compare the results of\nfast Fourier transform with the Monte Carlo simulation results of our process.\nFurther, we present a numerical example to understand the normal implied\nvolatility of the model.", "category": ["q-fin.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1909.08047v3", "link": "http://arxiv.org/abs/1909.08047v3"}, {"title": "Proxy Controls and Panel Data", "summary": "We present a flexible approach to the identification and estimation of causal\nobjects in nonparametric, non-separable models with confounding. Key to our\nanalysis is the use of `proxy controls': covariates that do not satisfy a\nstandard `unconfoundedness' assumption but which are informative proxies for\nvariables that do. Our methods are presented in sufficient generality that they\napply to both cross-sectional and panel models. Our identification results\nmotivate an easy-to-implement nonparametric estimation method. Under our\nidentifying assumptions the estimation problem is `well-posed'. We describe the\nestimator and derive convergence rates. When applied in panel settings our\nmethods provide a novel approach to identification with non-separable general\nheterogeneity and a fixed time dimension. In the panel case, observations from\ndifferent periods serve as proxies for unobserved individual heterogeneity and\nour key identifying assumptions follow from restrictions on the serial\ndependence structure of the data and latent variables. We apply our methodology\nto two separate empirical settings that respectively showcase the\ncross-sectional and the panel implementations of our approach. We estimate\ncausal effects of grade retention on performance in a range of cognitive tests\nusing cross-sectional variation in the data and we estimate a structural Engel\ncurve for food using panel data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.00283v3", "link": "http://arxiv.org/abs/1810.00283v3"}, {"title": "Quantitative analysis of privatization", "summary": "In recent years, the economic policy of privatization, which is defined as\nthe transfer of property or responsibility from public sector to private\nsector, is one of the global phenomenon that increases use of markets to\nallocate resources. One important motivation for privatization is to help\ndevelop factor and product markets, as well as security markets. Progress in\nprivatization is correlated with improvements in perceived political and\ninvestment risk. Many emerging countries have gradually reduced their political\nrisks during the course of sustained privatization. In fact, most risk\nresolution seems to take place as privatization proceeds to its later stage.\nAlternative benefits of privatization are improved risk sharing and increased\nliquidity and activity of the market. One of the main methods to develop\nprivatization is entering a new stock to the markets for arising competition.\nHowever, attention to the capability of the markets to accept a new stock is\nsubstantial. Without considering the above statement, it is possible to reduce\nthe market's efficiency. In other words, introduction of a new stock to the\nmarket usually decreases the stage of development and activity and increases\nthe risk. Based on complexity theory, we quantify how the following factors:\nstage of development, activity, risk and investment horizons play roles in the\nprivatization.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0803.2388v1", "link": "http://arxiv.org/abs/0803.2388v1"}, {"title": "Randomised Mixture Models for Pricing Kernels", "summary": "Numerous kinds of uncertainties may affect an economy, e.g. economic,\npolitical, and environmental ones. We model the aggregate impact by the\nuncertainties on an economy and its associated financial market by randomised\nmixtures of L\\'evy processes. We assume that market participants observe the\nrandomised mixtures only through best estimates based on noisy market\ninformation. The concept of incomplete information introduces an element of\nstochastic filtering theory in constructing what we term \"filtered Esscher\nmartingales\". We make use of this family of martingales to develop pricing\nkernel models. Examples of bond price models are examined, and we show that the\nchoice of the random mixture has a significant effect on the model dynamics and\nthe types of movements observed in the associated yield curves. Parameter\nsensitivity is analysed and option price processes are derived. We extend the\nclass of pricing kernel models by considering a weighted heat kernel approach,\nand develop models driven by mixtures of Markov processes.", "category": ["q-fin.GN", "math.PR"], "id": "http://arxiv.org/abs/1112.2059v1", "link": "http://arxiv.org/abs/1112.2059v1"}, {"title": "A New Wald Test for Hypothesis Testing Based on MCMC outputs", "summary": "In this paper, a new and convenient $\\chi^2$ wald test based on MCMC outputs\nis proposed for hypothesis testing. The new statistic can be explained as MCMC\nversion of Wald test and has several important advantages that make it very\nconvenient in practical applications. First, it is well-defined under improper\nprior distributions and avoids Jeffrey-Lindley's paradox. Second, it's\nasymptotic distribution can be proved to follow the $\\chi^2$ distribution so\nthat the threshold values can be easily calibrated from this distribution.\nThird, it's statistical error can be derived using the Markov chain Monte Carlo\n(MCMC) approach. Fourth, most importantly, it is only based on the posterior\nMCMC random samples drawn from the posterior distribution. Hence, it is only\nthe by-product of the posterior outputs and very easy to compute. In addition,\nwhen the prior information is available, the finite sample theory is derived\nfor the proposed test statistic. At last, the usefulness of the test is\nillustrated with several applications to latent variable models widely used in\neconomics and finance.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1801.00973v1", "link": "http://arxiv.org/abs/1801.00973v1"}, {"title": "A Generalized Preferential Attachment Model for Business Firms Growth\n  Rates: II. Mathematical Treatment", "summary": "We present a preferential attachment growth model to obtain the distribution\n$P(K)$ of number of units $K$ in the classes which may represent business firms\nor other socio-economic entities. We found that $P(K)$ is described in its\ncentral part by a power law with an exponent $\\phi=2+b/(1-b)$ which depends on\nthe probability of entry of new classes, $b$. In a particular problem of city\npopulation this distribution is equivalent to the well known Zipf law. In the\nabsence of the new classes entry, the distribution $P(K)$ is exponential. Using\nanalytical form of $P(K)$ and assuming proportional growth for units, we derive\n$P(g)$, the distribution of business firm growth rates. The model predicts that\n$P(g)$ has a Laplacian cusp in the central part and asymptotic power-law tails\nwith an exponent $\\zeta=3$. We test the analytical expressions derived using\nheuristic arguments by simulations. The model might also explain the\nsize-variance relationship of the firm growth rates.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0609020v1", "link": "http://dx.doi.org/10.1140/epjb/e2007-00165-8"}, {"title": "Efficient Computation of the Quasi Likelihood function for Discretely\n  Observed Diffusion Processes", "summary": "We introduce a simple method for nearly simultaneous computation of all\nmoments needed for quasi maximum likelihood estimation of parameters in\ndiscretely observed stochastic differential equations commonly seen in finance.\nThe method proposed in this papers is not restricted to any particular dynamics\nof the differential equation and is virtually insensitive to the sampling\ninterval. The key contribution of the paper is that computational complexity is\nsublinear in the number of observations as we compute all moments through a\nsingle operation. Furthermore, that operation can be done offline. The\nsimulations show that the method is unbiased for all practical purposes for any\nsampling design, including random sampling, and that the computational cost is\ncomparable (actually faster for moderate and large data sets) to the simple,\noften severely biased, Euler-Maruyama approximation.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1509.07751v1", "link": "http://arxiv.org/abs/1509.07751v1"}, {"title": "Outperformance Portfolio Optimization via the Equivalence of Pure and\n  Randomized Hypothesis Testing", "summary": "We study the portfolio problem of maximizing the outperformance probability\nover a random benchmark through dynamic trading with a fixed initial capital.\nUnder a general incomplete market framework, this stochastic control problem\ncan be formulated as a composite pure hypothesis testing problem. We analyze\nthe connection between this pure testing problem and its randomized\ncounterpart, and from latter we derive a dual representation for the maximal\noutperformance probability. Moreover, in a complete market setting, we provide\na closed-form solution to the problem of beating a leveraged exchange traded\nfund. For a general benchmark under an incomplete stochastic factor model, we\nprovide the Hamilton-Jacobi-Bellman PDE characterization for the maximal\noutperformance probability.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1109.5316v6", "link": "http://dx.doi.org/10.1007/s00780-013-0213-8"}, {"title": "Game-Theoretic Optimal Portfolios for Jump Diffusions", "summary": "This paper studies a two-person trading game in continuous time that\ngeneralizes Garivaltis (2018) to allow for stock prices that both jump and\ndiffuse. Analogous to Bell and Cover (1988) in discrete time, the players start\nby choosing fair randomizations of the initial dollar, by exchanging it for a\nrandom wealth whose mean is at most 1. Each player then deposits the resulting\ncapital into some continuously-rebalanced portfolio that must be adhered to\nover $[0,t]$. We solve the corresponding `investment $\\phi$-game,' namely the\nzero-sum game with payoff kernel\n$\\mathbb{E}[\\phi\\{\\textbf{W}_1V_t(b)/(\\textbf{W}_2V_t(c))\\}]$, where\n$\\textbf{W}_i$ is player $i$'s fair randomization, $V_t(b)$ is the final wealth\nthat accrues to a one dollar deposit into the rebalancing rule $b$, and\n$\\phi(\\bullet)$ is any increasing function meant to measure relative\nperformance. We show that the unique saddle point is for both players to use\nthe (leveraged) Kelly rule for jump diffusions, which is ordinarily defined by\nmaximizing the asymptotic almost-sure continuously-compounded capital growth\nrate. Thus, the Kelly rule for jump diffusions is the correct behavior for\npractically anybody who wants to outperform other traders (on any time frame)\nwith respect to practically any measure of relative performance.", "category": ["econ.GN", "q-fin.EC", "q-fin.GN", "q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1812.04603v1", "link": "http://arxiv.org/abs/1812.04603v1"}, {"title": "Clearing price distributions in call auctions", "summary": "We propose a model for price formation in financial markets based on clearing\nof a standard call auction with random orders, and verify its validity for\nprediction of the daily closing price distribution statistically. The model\nconsiders random buy and sell orders, placed following demand- and supply-side\nvaluation distributions; an equilibrium equation then leads to a distribution\nfor clearing price and transacted volume. Bid and ask volumes are left as free\nparameters, permitting possibly heavy-tailed or very skewed order flow\nconditions. In highly liquid auctions, the clearing price distribution\nconverges to an asymptotically normal central limit, with mean and variance in\nterms of supply/demand-valuation distributions and order flow imbalance. By\nmeans of simulations, we illustrate the influence of variations in order flow\nand valuation distributions on price/volume, noting a distinction between high-\nand low-volume auction price variance. To verify the validity of the model\nstatistically, we predict a year's worth of daily closing price distributions\nfor 5 constituents of the Eurostoxx 50 index; Kolmogorov-Smirnov statistics and\nQQ-plots demonstrate with ample statistical significance that the model\npredicts closing price distributions accurately, and compares favourably with\nalternative methods of prediction.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1904.07583v2", "link": "http://arxiv.org/abs/1904.07583v2"}, {"title": "An indifference approach to the cost of capital constraints: KVA and\n  beyond", "summary": "The strengthening of capital requirements has induced banks and traders to\nconsider charging a so called capital valuation adjustment (KVA) to the clients\nin OTC transactions. This roughly corresponds to charge the clients ex-ante the\nprofit requirement that is asked to the trading desk. In the following we try\nto delineate a possible way to assess the impact of capital constraints in the\nvaluation of a deal. We resort to an optimisation stemming from an indifference\npricing approach, and we study both the linear problem from the point of view\nof the whole bank and the non-linear problem given by the viewpoint of\nshareholders. We also consider the case where one optimises the median rather\nthan the mean statistics of the profit and loss distribution.", "category": ["q-fin.RM", "q-fin.PR"], "id": "http://arxiv.org/abs/1708.05319v1", "link": "http://arxiv.org/abs/1708.05319v1"}, {"title": "Deep xVA solver -- A neural network based counterparty credit risk\n  management framework", "summary": "In this paper, we present a novel computational framework for portfolio-wide\nrisk management problems where the presence of a potentially large number of\nrisk factors makes traditional numerical techniques ineffective. The new method\nutilises a coupled system of BSDEs for the valuation adjustments (xVA) and\nsolves these by a recursive application of a neural network based BSDE solver.\nThis not only makes the computation of xVA for high-dimensional problems\nfeasible, but also produces hedge ratios and dynamic risk measures for xVA, and\nallows simulations of the collateral account.", "category": ["q-fin.MF", "q-fin.CP", "q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/2005.02633v1", "link": "http://arxiv.org/abs/2005.02633v1"}, {"title": "Different Cost Performance: Different Determinants? The Case of Cost\n  Overruns in Dutch Transportation Infrastructure Projects", "summary": "This paper examines three independent explanatory variables and their\nrelation with cost overrun in order to decide whether this is different for\nDutch infrastructure projects compared to worldwide findings. The three\nindependent variables are project type (road, rail, and fixed link projects),\nproject size (measured in terms of estimated costs) and the length of the\nproject implementation phase. For Dutch projects, average cost overrun is 10.6%\nfor rail, 18.6% for roads and 21.7% for fixed links. For project size, small\nDutch projects have the largest average percentage cost overruns but in terms\nof total overrun, large projects have a larger share. The length of the\nimplementation phase and especially the length of the pre-construction phase\nare important determinants of cost overruns in the Netherlands. With each\nadditional year of pre-construction, percentage cost overrun increases by five\npercentage points. In contrast, the length of the construction phase has hardly\nany influence on cost overruns. This is an important contribution to current\nknowledge about cost overruns, because the period in which projects are most\nprone to cost overruns is narrowed down considerably, at least in the\nNetherlands. This means that period can be focused on to determine the causes\nand cures of overruns.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1307.2179v2", "link": "http://dx.doi.org/10.1016/j.tranpol.2012.04.002"}, {"title": "Gains in evolutionary dynamics: A unifying and intuitive approach to\n  linking static and dynamic stability", "summary": "Static stability in an economic model typically means negative incentives for\ndeviation from equilibrium strategies, which we expect to assure a return to\nequilibrium, i.e., dynamic stability, as long as agents respond to incentives.\nThere have been many attempts to prove this link, especially in evolutionary\ngame theory, yielding both negative and positive results. This paper offers a\nuniversal and intuitive approach to this link. We prove static stability\nassures dynamic stability as long as agents' decisions of switching strategies\nare rationalizable by revealing costs and constraints behind distortions from\nexact optimization. This idea guides us to track the remaining expected maximal\npayoff gain from switches, after deducting the costs and to be maximized\nsubject to the constraints, as a disequilibrium index, namely, a Lyapunov\nfunction. While our analysis here is confined to myopic evolutionary dynamics\nin population games, our approach is applicable to more complex situations.", "category": [], "id": "http://arxiv.org/abs/1805.04898v6", "link": "http://arxiv.org/abs/1805.04898v6"}, {"title": "The Markowitz Category", "summary": "We give an algebraic definition of a Markowitz market and classify markets up\nto isomorphism. Given this classification, the theory of portfolio optimization\nin Markowitz markets without short selling constraints becomes trivial.\nConversely, this classification shows that, up to isomorphism, there is little\nthat can be said about a Markowitz market that is not already detected by the\ntheory of portfolio optimization. In particular, if one seeks to develop a\nsimplified low-dimensional model of a large financial market using\nmean--variance analysis alone, the resulting model can be at most\ntwo-dimensional.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1611.07741v2", "link": "http://dx.doi.org/10.1137/17M1155727"}, {"title": "Housing Search in the Age of Big Data: Smarter Cities or the Same Old\n  Blind Spots?", "summary": "Housing scholars stress the importance of the information environment in\nshaping housing search behavior and outcomes. Rental listings have increasingly\nmoved online over the past two decades and, in turn, online platforms like\nCraigslist are now central to the search process. Do these technology platforms\nserve as information equalizers or do they reflect traditional information\ninequalities that correlate with neighborhood sociodemographics? We synthesize\nand extend analyses of millions of US Craigslist rental listings and find they\nsupply significantly different volumes, quality, and types of information in\ndifferent communities. Technology platforms have the potential to broaden,\ndiversify, and equalize housing search information, but they rely on landlord\nbehavior and, in turn, likely will not reach this potential without a\nsignificant redesign or policy intervention. Smart cities advocates hoping to\nbuild better cities through technology must critically interrogate technology\nplatforms and big data for systematic biases.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.11585v1", "link": "http://dx.doi.org/10.1080/10511482.2019.1684336"}, {"title": "GDP Trend Deviations and the Yield Spread: the Case of Five E.U.\n  Countries", "summary": "Several studies have established the predictive power of the yield curve in\nterms of real economic activity. In this paper we use data for a variety of\nE.U. countries: both EMU (Germany, France, Italy) and non-EMU members (Sweden\nand the U.K.). The data used range from 1991:Q1 to 2009:Q1. For each country,\nwe extract the long run trend and the cyclical component of real economic\nactivity, while the corresponding interbank interest rates of long and short\nterm maturities are used for the calculation of the country specific yield\nspreads. We also augment the models tested with non monetary policy variables:\nthe countries' unemployment rates and stock indices. The methodology employed\nin the effort to forecast real output, is a probit model of the inverse\ncumulative distribution function of the standard distribution, using several\nformal forecasting and goodness of fit evaluation tests. The results show that\nthe yield curve augmented with the non-monetary variables has significant\nforecasting power in terms of real economic activity but the results differ\nqualitatively between the individual economies examined raising non-trivial\npolicy implications.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1005.1326v1", "link": "http://arxiv.org/abs/1005.1326v1"}, {"title": "Working Paper on Organizational Dynamics within Corporate Venture\n  Capital Firms", "summary": "Corporate venture capital is in the midst of a renaissance. The end of 2015\nmarked all-time highs both in the number of corporate firms participating in VC\ndeals and in the amount of capital being deployed by corporate VCs. This paper\nexplores, rather than defines, how these firms find success in the wake of this\nsudden influx of corporate investors. A series of interviews was conducted in\norder to capture the direct and indirect objectives, philosophies, and modes of\noperation within some of these corporate VC organizations. During the course of\nthis exploration, numerous operational coherency issues were discovered. Many\nfirms were implicitly incentivizing conflicting and inconsistent behavior among\ntheir investment team. Perhaps most surprising, the worst offenders were the\nmore mature corporate VCs who have been in the game for some time. As will be\ndiscussed, fundamental evidence suggests that this misalignment is due to lack\nof attention and commitment at the executive level as corporate strategy\nevolves.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1611.00970v1", "link": "http://arxiv.org/abs/1611.00970v1"}, {"title": "Towards identifying the world stock market cross-correlations: DAX\n  versus Dow Jones", "summary": "Effects connected with the world globalization affect also the financial\nmarkets. On a way towards quantifying the related characteristics we study the\nfinancial empirical correlation matrix of the 60 companies which both the\nDeutsche Aktienindex (DAX) and the Dow Jones (DJ) industrial average comprised\nduring the years 1990-1999. The time-dependence of the underlying\ncross-correlations is monitored using a time window of 60 trading days. Our\nstudy shows that if the time-zone delays are properly accounted for the two\ndistant markets largely merge into one. This effect is particularly visible\nduring the last few years. It is however the Dow Jones which dictates the\ntrend.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0011488v1", "link": "http://dx.doi.org/10.1016/S0378-4371(01)00119-4"}, {"title": "An operatorial approach to stock markets", "summary": "We propose and discuss some toy models of stock markets using the same\noperatorial approach adopted in quantum mechanics. Our models are suggested by\nthe discrete nature of the number of shares and of the cash which are exchanged\nin a real market, and by the existence of conserved quantities, like the total\nnumber of shares or some linear combination of cash and shares. The same\nframework as the one used in the description of a gas of interacting bosons is\nadopted.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0904.0896v1", "link": "http://dx.doi.org/10.1088/0305-4470/39/22/001"}, {"title": "A theory of stochastic integration for bond markets", "summary": "We introduce a theory of stochastic integration with respect to a family of\nsemimartingales depending on a continuous parameter, as a mathematical\nbackground to the theory of bond markets. We apply our results to the problem\nof super-replication and utility maximization from terminal wealth in a bond\nmarket. Finally, we compare our approach to those already existing in\nliterature.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/math/0602532v1", "link": "http://dx.doi.org/10.1214/105051605000000548"}, {"title": "Risk Aversion and Catastrophic Risks: the Pill Experiment", "summary": "This article focuses on the work of O. Chanel and G. Chichilnisky (2013) on\nthe flaws of expected utility theory while assessing the value of life.\nExpected utility is a fundamental tool in decision theory. However, it does not\nfit with the experimental results when it comes to catastrophic outcomes\n---see, for example, Chichilnisky (2009) for more details. In the experiments\nconducted by Olivier Chanel in 1998 and 2009, several subjects are ask to\nimagine they are presented 1 billion identical pills. They are paid \\$220,000\nto take and swallow one, knowing that one out of 1 billion is deadly. The\nobjective of this article is to show that risk aversion phenomenon cannot\nexplain the experimental results found. This is an additional reason why a new\nkind of utility function is necessary: the axioms proposed by Graciela\nChichilnisky will be briefly presented, and it will be shown that it better\nfits with experiments than any risk aversion utility function.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1604.05672v1", "link": "http://arxiv.org/abs/1604.05672v1"}, {"title": "Black-Scholes equation from Gauge Theory of Arbitrage", "summary": "We apply Gauge Theory of Arbitrage (GTA) {hep-th/9710148} to derivative\npricing. We show how the standard results of Black-Scholes analysis appear from\nGTA and derive correction to the Black-Scholes equation due to a virtual\narbitrage and speculators reaction on it. The model accounts for both violation\nof the no-arbitrage constraint and non-Brownian price walks which resemble real\nfinancial data. The correction is nonlocal and transform the differential\nBlack-Scholes equation to an integro-differential one.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/hep-th/9712034v2", "link": "http://arxiv.org/abs/hep-th/9712034v2"}, {"title": "Decomposition formula for rough Volterra stochastic volatility models", "summary": "The research presented in this article provides an alternative option pricing\napproach for a class of rough fractional stochastic volatility models. These\nmodels are increasingly popular between academics and practitioners due to\ntheir surprising consistency with financial markets. However, they bring\nseveral challenges alongside. Most noticeably, even simple non-linear financial\nderivatives as vanilla European options are typically priced by means of\nMonte-Carlo (MC) simulations which are more computationally demanding than\nsimilar MC schemes for standard stochastic volatility models.\n  In this paper, we provide a proof of the prediction law for general Gaussian\nVolterra processes. The prediction law is then utilized to obtain an adapted\nprojection of the future squared volatility -- a cornerstone of the proposed\npricing approximation. Firstly, a decomposition formula for European option\nprices under general Volterra volatility models is introduced. Then we focus on\nparticular models with rough fractional volatility and we derive an explicit\nsemi-closed approximation formula. Numerical properties of the approximation\nfor a popular model -- the rBergomi model -- are studied and we propose a\nhybrid calibration scheme which combines the approximation formula alongside MC\nsimulations. This scheme can significantly speed up the calibration to\nfinancial markets as illustrated on a set of AAPL options.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1906.07101v2", "link": "http://arxiv.org/abs/1906.07101v2"}, {"title": "Analitic approach to solve a degenerate parabolic PDE for the Heston\n  model", "summary": "We present an analytic approach to solve a degenerate parabolic problem\nassociated to the Heston model, which is widely used in mathematical finance to\nderive the price of an European option on an risky asset with stochastic\nvolatility. We give a variational formulation, involving weighted Sobolev\nspaces, of the second order degenerate elliptic operator of the parabolic PDE.\nWe use this approach to prove, under appropriate assumptions on some involved\nunknown parameters, the existence and uniqueness of weak solutions to the\nparabolic problem on unbounded subdomains of the half-plane.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1406.2292v1", "link": "http://arxiv.org/abs/1406.2292v1"}, {"title": "Arbitrage and utility maximization in market models with an insider", "summary": "We study arbitrage opportunities, market viability and utility maximization\nin market models with an insider. Assuming that an economic agent possesses\nfrom the beginning an additional information in the form of a random variable\nG, which only becomes known to the ordinary agents at date T, we give criteria\nfor the No Unbounded Profits with Bounded Risk property to hold, characterize\noptimal arbitrage strategies, and prove duality results for the utility\nmaximization problem faced by the insider. Examples of markets satisfying NUPBR\nyet admitting arbitrage opportunities are provided for both atomic and\ncontinuous random variables G.", "category": ["q-fin.RM", "q-fin.MF"], "id": "http://arxiv.org/abs/1608.02068v2", "link": "http://arxiv.org/abs/1608.02068v2"}, {"title": "A note on estimating stochastic volatility and its volatility: a new\n  simple method", "summary": "We present a new simple method of estimating stochastic volatility and its\nvolatility. This method is applicable to both cross-sectional and time-series\ndata. Moreover, this method does not require volatility data series.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1212.0380v1", "link": "http://arxiv.org/abs/1212.0380v1"}, {"title": "Dynamics of income inequality under disequilibrium: The case of India", "summary": "Income inequality is one of the most significant socio-economic challenges\nconfronting India, with potentially long-lasting implications for the future of\nits democracy and society. In this work, we explore income inequality in India,\nwithout assumptions of equilibrium, and illustrate the nature and direction of\nre-distribution within the income distribution in a dynamic sense. Given that\nboth mean income and income inequality show a rising trend post the Industrial\nRevolution, we argue that such a process is appropriately modeled using\nGeometric Brownian Motion (GBM). Specifically, we use the mechanism of GBM with\na reallocation parameter (which indicates the nature of re-distribution\noccurring in the income distribution) proposed by Berman et al. We find that\nsince the mid-1990s, reallocation is negative, meaning that incomes are\nexponentially diverging, indicating that there is a perverse re-distribution of\nresources from the poor to the rich. It has been well known that static\ninequality is rising in India, but the assumption has been that while the rich\nmay be benefiting more than proportionally from economic growth, the poor are\nalso better off than before. The surprising finding from our work is that the\nnature of income inequality is such that we have moved from a regime of\nprogressive to regressive re-distribution. Essentially, continued\nimpoverishment of the poor is directly spurring multiplicative income growth of\nthe rich. We characterize these findings in the context of increasing\ninformality of the workforce in the formal manufacturing and service sectors,\nas well as the possible evolution of negative net incomes of the agriculture\nworkforce in India. Significant structural changes may be required to address\nthis phenomenon.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.04452v2", "link": "http://arxiv.org/abs/1909.04452v2"}, {"title": "Generalized statistical arbitrage concepts and related gain strategies", "summary": "Generalized statistical arbitrage concepts are introduced corresponding to\ntrading strategies which yield positive gains on average in a class of\nscenarios rather than almost surely. The relevant scenarios or market states\nare specified via an information system given by a $\\sigma$-algebra and so this\nnotion contains classical arbitrage as a special case. It also covers the\nnotion of statistical arbitrage introduced in Bondarenko (2003).\n  Relaxing these notions further we introduce generalized profitable strategies\nwhich include also static or semi-static strategies. Under standard\nno-arbitrage there may exist generalized gain strategies yielding positive\ngains on average under the specified scenarios.\n  In the first part of the paper we characterize these generalized statistical\nno-arbitrage notions. In the second part of the paper we construct several\nprofitable generalized strategies with respect to various choices of the\ninformation system. In particular, we consider several forms of embedded\nbinomial strategies and follow-the-trend strategies as well as partition-type\nstrategies. We study and compare their behaviour on simulated data.\nAdditionally, we find good performance on market data of these simple\nstrategies which makes them profitable candidates for real applications.", "category": ["q-fin.MF", "q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1907.09218v2", "link": "http://arxiv.org/abs/1907.09218v2"}, {"title": "Optimal investment-consumption and life insurance with capital\n  constraints", "summary": "The aim of this paper is to solve an optimal investment, consumption and life\ninsurance problem when the investor is restricted to capital guarantee. We\nconsider an incomplete market described by a jump-diffusion model with\nstochastic volatility. Using the martingale approach, we prove the existence of\nthe optimal strategy and the optimal martingale measure and we obtain the\nexplicit solutions for the power utility functions.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1808.04613v1", "link": "http://arxiv.org/abs/1808.04613v1"}, {"title": "\"Chaos\" in energy and commodity markets: a controversial matter", "summary": "We test whether the futures prices of some commodity and energy markets are\ndetermined by stochastic rules or exhibit nonlinear deterministic endogenous\nfluctuations. As for the methodologies, we use the maximal Lyapunov exponents\n(MLE) and a determinism test, both based on the reconstruction of the phase\nspace. In particular, employing a recent methodology, we estimate a coefficient\n$\\kappa$ that describes the determinism rate of the analyzed time series. We\nfind that the underlying system for futures prices shows a reliability level\n$\\kappa$ near to $1$ while the MLE is positive for all commodity futures\nseries. Thus, the empirical evidence suggests that commodity and energy futures\nprices are the measured footprint of a nonlinear deterministic, rather than a\nstochastic, system.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1611.07432v2", "link": "http://arxiv.org/abs/1611.07432v2"}, {"title": "BSDEs with weak reflections and partial hedging of American options", "summary": "We introduce a new class of \\textit{Backward Stochastic Differential\nEquations with weak reflections} whose solution $(Y,Z)$ satisfies the weak\nconstraint $\\textbf{E}[\\Psi(\\theta,Y_\\theta)] \\geq m,$ for all stopping time\n$\\theta$ taking values between $0$ and a terminal time $T$, where $\\Psi$ is a\nrandom non-decreasing map and $m$ a given threshold. We study the wellposedness\nof such equations and show that the family of minimal time $t$-values $Y_t$ can\nbe aggregated by a right-continuous process. We give a nonlinear Mertens type\ndecomposition for lower reflected $g$-submartingales, which to the best of our\nknowledge, represents a new result in the literature. Using this decomposition,\nwe obtain a representation of the minimal time $t$-values process. We also show\nthat the minimal supersolution of a such equation can be written as a\n\\textit{stochastic control/optimal stopping game}, which is shown to admit,\nunder appropriate assumptions, a value and saddle points. From a financial\npoint of view, this problem is related to the approximative hedging for\nAmerican options.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1708.05957v1", "link": "http://arxiv.org/abs/1708.05957v1"}, {"title": "Trading on the Floor after Sweeping the Book", "summary": "Informed traders need to trade fast in order to profit from their private\ninformation before it becomes public. Fast electronic markets provide such\nliquidity. Slow markets provide execution in an auction based trading floor.\nHybrid markets combine both execution venues. In its main result, the paper\nshows that to compensate for their slow and risky executions, trading floors\nneed to be at least twice as deep as the sweeping facility. Furthermore, when a\nstand-alone trading floor is enhanced with the addition of a sweeping facility,\noverall informed trading will decline because it is easier for informed traders\nto extract the full value of their private info.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/2001.06445v1", "link": "http://arxiv.org/abs/2001.06445v1"}, {"title": "Financial density forecasts: A comprehensive comparison of risk-neutral\n  and historical schemes", "summary": "We investigate the forecasting ability of the most commonly used benchmarks\nin financial economics. We approach the usual caveats of probabilistic\nforecasts studies -small samples, limited models and non-holistic validations-\nby performing a comprehensive comparison of 15 predictive schemes during a time\nperiod of over 21 years. All densities are evaluated in terms of their\nstatistical consistency, local accuracy and forecasting errors. Using a new\ncomposite indicator, the Integrated Forecast Score (IFS), we show that\nrisk-neutral densities outperform historical-based predictions in terms of\ninformation content. We find that the Variance Gamma model generates the\nhighest out-of-sample likelihood of observed prices and the lowest predictive\nerrors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts\nacross the entire density range. In contrast, lognormal densities, the Heston\nmodel or the Breeden-Litzenberger formula yield biased predictions and are\nrejected in statistical tests.", "category": ["q-fin.RM", "math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1801.08007v2", "link": "http://dx.doi.org/10.1002/for.2521"}, {"title": "Statistical Arbitrage in the Black-Scholes Framework", "summary": "In this study we prove the existence of statistical arbitrage opportunities\nin the Black-Scholes framework by considering trading strategies that consists\nof borrowing from the risk free rate and taking a long position in the stock\nuntil it hits a deterministic barrier level. We derive analytical formulas for\nthe expected value, variance, and probability of loss for the discounted\ncumulative trading profits. No-statistical arbitrage condition is derived for\nthe Black-Scholes framework, which imposes a constraint on the Sharpe ratio of\nthe stock. Furthermore, we verify our theoretical results via extensive Monte\nCarlo simulations.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1406.5646v4", "link": "http://arxiv.org/abs/1406.5646v4"}, {"title": "Persuading a Consumer to Visit", "summary": "We consider a variation on the classic Weitzman search problem: competing\nfirms with products of unknown quality may design how much information a\nconsumer's visit will glean. After observing these information structures, the\nconsumer then decides how to search sequentially across the firms for a high\nvalue product. If there are no search frictions, then there is a unique\nsymmetric equilibrium in pure strategies, and the firms are not fully\ninformative. With search frictions, the information a visit will reveal depends\nin a systematic way on the ex-ante probability that a firm's product is high\nquality. When the expected quality of the product is sufficiently high, there\nis a unique symmetric equilibrium in which firms are fully informative. There,\na small search cost leads to the perfect competition level of information\nprovision--consumers gain when firms are forced to compete on information.\nConversely, in the low and medium expected quality cases there are no pure\nstrategy equilibria. Instead, firms mix over a continuum of levels of\ninformation: in the low expected quality case they provide full information\nwith probability zero; and in the medium expected quality case they provide\nfull information with positive probability. In both cases there are mixed\nstrategy equilibria in which the firms' realized information structures are\nBlackwell comparable. Moreover, though recall is permitted, in each case there\nare equilibria in which the consumer never returns.", "category": ["math.PR", "q-fin.EC"], "id": "http://arxiv.org/abs/1802.09396v4", "link": "http://arxiv.org/abs/1802.09396v4"}, {"title": "A Novel Twitter Sentiment Analysis Model with Baseline Correlation for\n  Financial Market Prediction with Improved Efficiency", "summary": "A novel social networks sentiment analysis model is proposed based on Twitter\nsentiment score (TSS) for real-time prediction of the future stock market price\nFTSE 100, as compared with conventional econometric models of investor\nsentiment based on closed-end fund discount (CEFD). The proposed TSS model\nfeatures a new baseline correlation approach, which not only exhibits a decent\nprediction accuracy, but also reduces the computation burden and enables a fast\ndecision making without the knowledge of historical data. Polynomial\nregression, classification modelling and lexicon-based sentiment analysis are\nperformed using R. The obtained TSS predicts the future stock market trend in\nadvance by 15 time samples (30 working hours) with an accuracy of 67.22% using\nthe proposed baseline criterion without referring to historical TSS or market\ndata. Specifically, TSS's prediction performance of an upward market is found\nfar better than that of a downward market. Under the logistic regression and\nlinear discriminant analysis, the accuracy of TSS in predicting the upward\ntrend of the future market achieves 97.87%.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2003.08137v2", "link": "http://dx.doi.org/10.1109/SNAMS.2019.8931720"}, {"title": "Are Bitcoins price predictable? Evidence from machine learning\n  techniques using technical indicators", "summary": "The uncertainties in future Bitcoin price make it difficult to accurately\npredict the price of Bitcoin. Accurately predicting the price for Bitcoin is\ntherefore important for decision-making process of investors and market players\nin the cryptocurrency market. Using historical data from 01/01/2012 to\n16/08/2019, machine learning techniques (Generalized linear model via penalized\nmaximum likelihood, random forest, support vector regression with linear\nkernel, and stacking ensemble) were used to forecast the price of Bitcoin. The\nprediction models employed key and high dimensional technical indicators as the\npredictors. The performance of these techniques were evaluated using mean\nabsolute percentage error (MAPE), root mean square error (RMSE), mean absolute\nerror (MAE), and coefficient of determination (R-squared). The performance\nmetrics revealed that the stacking ensemble model with two base learner (random\nforest and generalized linear model via penalized maximum likelihood) and\nsupport vector regression with linear kernel as meta-learner was the optimal\nmodel for forecasting Bitcoin price. The MAPE, RMSE, MAE, and R-squared values\nfor the stacking ensemble model were 0.0191%, 15.5331 USD, 124.5508 USD, and\n0.9967 respectively. These values show a high degree of reliability in\npredicting the price of Bitcoin using the stacking ensemble model. Accurately\npredicting the future price of Bitcoin will yield significant returns for\ninvestors and market players in the cryptocurrency market.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/1909.01268v1", "link": "http://arxiv.org/abs/1909.01268v1"}, {"title": "Structural social capital and health in Italy", "summary": "This paper presents the first empirical assessment of the causal relationship\nbetween social capital and health in Italy. The analysis draws on the 2000 wave\nof the Multipurpose Survey on Household conducted by the Italian Institute of\nStatistics on a representative sample of the population (n = 46,868). Our\nmeasure of social capital is the frequency of meetings with friends. Based on\nIV and bivariate probit estimates, we find that individuals who meet friends\nevery day or at least two times a week are approximately 11% to 16% more likely\nto report good health.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1408.1671v1", "link": "http://arxiv.org/abs/1408.1671v1"}, {"title": "Normal Approximation in Large Network Models", "summary": "We develop a methodology for proving central limit theorems in network models\nwith strategic interactions and homophilous agents. We consider an asymptotic\nframework in which the size of the network tends to infinity, which is useful\nfor inference in the typical setting in which the sample consists of a single\nlarge network. In the presence of strategic interactions, network moments are\ngenerally complex functions of network components, where a node's component\nconsists of all alters to which it is directly or indirectly connected. We find\nthat a modification of \"exponential stabilization\" conditions from the\nstochastic geometry literature provides a useful formulation of weak dependence\nfor moments of this type. Our first contribution is to prove a CLT for a large\nclass of network moments satisfying stabilization and a moment condition. Our\nsecond contribution is a methodology for deriving primitive sufficient\nconditions for stabilization using results in branching process theory. We\napply the methodology to static and dynamic models of network formation and\ndiscuss how it can be used more broadly.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1904.11060v2", "link": "http://arxiv.org/abs/1904.11060v2"}, {"title": "Financial Time Series Prediction Using Deep Learning", "summary": "In this work we present a data-driven end-to-end Deep Learning approach for\ntime series prediction, applied to financial time series. A Deep Learning\nscheme is derived to predict the temporal trends of stocks and ETFs in NYSE or\nNASDAQ. Our approach is based on a neural network (NN) that is applied to raw\nfinancial data inputs, and is trained to predict the temporal trends of stocks\nand ETFs. In order to handle commission-based trading, we derive an investment\nstrategy that utilizes the probabilistic outputs of the NN, and optimizes the\naverage return. The proposed scheme is shown to provide statistically\nsignificant accurate predictions of financial market trends, and the investment\nstrategy is shown to be profitable under this challenging setup. The\nperformance compares favorably with contemporary benchmarks along two-years of\nback-testing.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1711.04174v1", "link": "http://arxiv.org/abs/1711.04174v1"}, {"title": "A Dynamic Correlation Modelling Framework with Consistent Stochastic\n  Recovery", "summary": "This paper describes a flexible and tractable bottom-up dynamic correlation\nmodelling framework with a consistent stochastic recovery specification. The\nstochastic recovery specification only models the first two moments of the spot\nrecovery rate as its higher moments have almost no contribution to the loss\ndistribution and CDO tranche pricing. Observing that only the joint\ndistribution of default indicators is needed to build the portfolio loss\ndistribution, we propose a generic class of default indicator copulas to model\nCDO tranches, which can be easily calibrated to index tranche prices across\nmultiple maturities. This correlation modelling framework has the unique\nadvantage that the joint distribution of default time and other dynamic\nproperties of the model can be changed separately from the loss distribution\nand tranche prices. After calibrating the model to index tranche prices,\nexisting top-down methods can be applied to the common factor process to\nconstruct very flexible systemic dynamics without changing the already\ncalibrated tranche prices. This modelling framework therefore combines the best\nfeatures of the bottom-up and top-down models: it is fully consistent with all\nthe single name market information and it admits very rich and flexible spread\ndynamics. Numerical results from a non-parametric implementation of this\nmodelling framework are also presented. The non-parametric implementation\nachieved fast and accurate calibration to the index tranches across multiple\nmaturities even under extreme market conditions. A conditional Markov chain\nmethod is also proposed to construct the systemic dynamics, which supports an\nefficient lattice pricing method for dynamic spread instruments. We also showed\nhow to price tranche options as an example of this fast lattice method.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1004.3758v1", "link": "http://arxiv.org/abs/1004.3758v1"}, {"title": "Stochastic Utilities With a Given Optimal Portfolio : Approach by\n  Stochastic Flows", "summary": "The paper generalizes the construction by stochastic flows of consistent\nutility processes introduced by M. Mrad and N. El Karoui in (2010). The\nutilities random fields are defined from a general class of processes denoted\nby $\\GX$. Making minimal assumptions and convex constraints on test-processes,\nwe construct by composing two stochastic flows of homeomorphisms, all the\nconsistent stochastic utilities whose the optimal-benchmark process is given,\nstrictly increasing in its initial condition. Proofs are essentially based on\nstochastic change of variables techniques.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/1004.5192v2", "link": "http://arxiv.org/abs/1004.5192v2"}, {"title": "Defaultable Bonds via HKA", "summary": "To construct a no-arbitrage defaultable bond market, we work on the state\nprice density framework. Using the heat kernel approach (HKA for short) with\nthe killing of a Markov process, we construct a single defaultable bond market\nthat enables an explicit expression of a defaultable bond and credit spread\nunder quadratic Gaussian settings. Some simulation results show that the model\nis not only tractable but realistic.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1103.4541v1", "link": "http://arxiv.org/abs/1103.4541v1"}, {"title": "Machine Learning Risk Models", "summary": "We give an explicit algorithm and source code for constructing risk models\nbased on machine learning techniques. The resultant covariance matrices are not\nfactor models. Based on empirical backtests, we compare the performance of\nthese machine learning risk models to other constructions, including\nstatistical risk models, risk models based on fundamental industry\nclassifications, and also those utilizing multilevel clustering based industry\nclassifications.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1903.06334v2", "link": "http://arxiv.org/abs/1903.06334v2"}, {"title": "Extensions of Random Orthogonal Matrix Simulation for Targetting Kollo\n  Skewness", "summary": "Modelling multivariate systems is important for many applications in\nengineering and operational research. The multivariate distributions under\nscrutiny usually have no analytic or closed form. Therefore their modelling\nemploys a numerical technique, typically multivariate simulations, which can\nhave very high dimensions. Random Orthogonal Matrix (ROM) simulation is a\nmethod that has gained some popularity because of the absence of certain\nsimulation errors. Specifically, it exactly matches a target mean, covariance\nmatrix and certain higher moments with every simulation. This paper extends the\nROM simulation algorithm presented by Hanke et al. (2017), hereafter referred\nto as HPSW, which matches the target mean, covariance matrix and Kollo skewness\nvector exactly. Our first contribution is to establish necessary and sufficient\nconditions for the HPSW algorithm to work. Our second contribution is to\ndevelop a general approach for constructing admissible values in the HPSW. Our\nthird theoretical contribution is to analyse the effect of multivariate sample\nconcatenation on the target Kollo skewness. Finally, we illustrate the\nextensions we develop here using a simulation study.", "category": ["q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/2004.06586v1", "link": "http://arxiv.org/abs/2004.06586v1"}, {"title": "Consistent iterated simulation of multi-variate default times: a\n  Markovian indicators characterization", "summary": "We investigate under which conditions a single simulation of joint default\ntimes at a final time horizon can be decomposed into a set of simulations of\njoint defaults on subsequent adjacent sub-periods leading to that final\nhorizon. Besides the theoretical interest, this is also a practical problem as\npart of the industry has been working under the misleading assumption that the\ntwo approaches are equivalent for practical purposes. As a reasonable trade-off\nbetween realistic stylized facts, practical demands, and mathematical\ntractability, we propose models leading to a Markovian multi-variate\nsurvival--indicator process, and we investigate two instances of static models\nfor the vector of default times from the statistical literature that fall into\nthis class. On the one hand, the \"looping default\" case is known to be equipped\nwith this property, and we point out that it coincides with the classical\n\"Freund distribution\" in the bivariate case. On the other hand, if all\nsub-vectors of the survival indicator process are Markovian, this constitutes a\nnew characterization of the Marshall--Olkin distribution, and hence of\nmulti-variate lack-of-memory. A paramount property of the resulting model is\nstability of the type of multi-variate distribution with respect to elimination\nor insertion of a new marginal component with marginal distribution from the\nsame family. The practical implications of this \"nested margining\" property are\nenormous. To implement this distribution we present an efficient and unbiased\nsimulation algorithm based on the L\\'evy-frailty construction. We highlight\ndifferent pitfalls in the simulation of dependent default times and examine,\nwithin a numerical case study, the effect of inadequate simulation practices.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1306.0887v3", "link": "http://arxiv.org/abs/1306.0887v3"}, {"title": "Subgeometrically ergodic autoregressions", "summary": "In this paper we discuss how the notion of subgeometric ergodicity in Markov\nchain theory can be exploited to study stationarity and ergodicity of nonlinear\ntime series models. Subgeometric ergodicity means that the transition\nprobability measures converge to the stationary measure at a rate slower than\ngeometric. Specifically, we consider suitably defined higher-order nonlinear\nautoregressions that behave similarly to a unit root process for large values\nof the observed series but we place almost no restrictions on their dynamics\nfor moderate values of the observed series. Results on the subgeometric\nergodicity of nonlinear autoregressions have previously appeared only in the\nfirst-order case. We provide an extension to the higher-order case and show\nthat the autoregressions we consider are, under appropriate conditions,\nsubgeometrically ergodic. As useful implications we also obtain stationarity\nand $\\beta$-mixing with subgeometrically decaying mixing coefficients.", "category": ["econ.EM", "math.PR"], "id": "http://arxiv.org/abs/1904.07089v3", "link": "http://arxiv.org/abs/1904.07089v3"}, {"title": "A Doubly Corrected Robust Variance Estimator for Linear GMM", "summary": "We propose a new finite sample corrected variance estimator for the linear\ngeneralized method of moments (GMM) including the one-step, two-step, and\niterated estimators. Our formula additionally corrects for the\nover-identification bias in variance estimation on top of the commonly used\nfinite sample correction of Windmeijer (2005) which corrects for the bias from\nestimating the efficient weight matrix, so is doubly corrected. Formal\nstochastic expansions are derived to show the proposed double correction\nestimates the variance of some higher-order terms in the expansion. In\naddition, the proposed double correction provides robustness to\nmisspecification of the moment condition. In contrast, the conventional\nvariance estimator and the Windmeijer correction are inconsistent under\nmisspecification. That is, the proposed double correction formula provides a\nconvenient way to obtain improved inference under correct specification and\nrobustness against misspecification at the same time.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.07821v1", "link": "http://arxiv.org/abs/1908.07821v1"}, {"title": "The Quantum Black-Scholes Equation", "summary": "Motivated by the work of Segal and Segal on the Black-Scholes pricing formula\nin the quantum context, we study a quantum extension of the Black-Scholes\nequation within the context of Hudson-Parthasarathy quantum stochastic\ncalculus. Our model includes stock markets described by quantum Brownian motion\nand Poisson process.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0706.1300v1", "link": "http://arxiv.org/abs/0706.1300v1"}, {"title": "Mathematical Foundations of Realtime Equity Trading. Liquidity Deficit\n  and Market Dynamics. Automated Trading Machines", "summary": "We postulates, and then show experimentally, that liquidity deficit is the\ndriving force of the markets. In the first part of the paper a kinematic of\nliquidity deficit is developed. The calculus-like approach, which is based on\nRadon--Nikodym derivatives and their generalization, allows us to calculate\nimportant characteristics of observable market dynamics. In the second part of\nthe paper this calculus is used in an attempt to build a dynamic equation in\nthe form: future price tend to the value maximizing the number of shares traded\nper unit time. To build a practical automated trading machine P&L dynamics\ninstead of price dynamics is considered. This allows a trading automate\nresilient to catastrophic P&L drains to be built. The results are very\npromising, yet when all the fees and trading commissions are taken into\naccount, are close to breakeven. In the end of the paper important criteria for\nautomated trading systems are presented. We list the system types that can and\ncannot make money on the market. These criteria can be successfully applied not\nonly by automated trading machines, but also by a human trader.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1510.05510v5", "link": "http://arxiv.org/abs/1510.05510v5"}, {"title": "A New Methodology for Estimating Internal Credit Risk and Bankruptcy\n  Prediction under Basel II Regime", "summary": "Credit estimation and bankruptcy prediction methods have been utilizing\nAltman's $z$ score method for the last several years. It is reported in many\nstudies that $z$ score is sensitive to changes in accounting figures.\nResearches have proposed different variations to conventional $z$ score that\ncan improve the prediction accuracy. In this paper we develop a new\nmultivariate non-linear model for computing the $z$ score. In addition we\ndevelop a new credit risk index by fitting a Pearson type-III distribution to\nthe transformed financial ratios. The results from our study have shown that\nthe new $z$ score can predict the bankruptcy with an accuracy of $98.6\\%$ as\ncompared to $93.5\\%$ by the Altman's $z$ score. Also, the discriminate analysis\nrevealed that the new transformed financial ratios could predict the bankruptcy\nprobability with an accuracy of $93.0\\%$ as compared to $87.4\\%$ using the\nweights of Altman's $z$ score.", "category": ["q-fin.EC", "q-fin.RM"], "id": "http://arxiv.org/abs/1502.00882v1", "link": "http://dx.doi.org/10.1007/s10614-014-9452-9"}, {"title": "Optimal Order Scheduling for Deterministic Liquidity Patterns", "summary": "We consider a broker who has to place a large order which consumes a sizable\npart of average daily trading volume. The broker's aim is thus to minimize\nexecution costs he incurs from the adverse impact of his trades on market\nprices. By contrast to the previous literature, see, e.g., Obizhaeva and Wang\n(2005), Predoiu, Shaikhet, and Shreve (2011), we allow the liquidity parameters\nof market depth and resilience to vary deterministically over the course of the\ntrading period. The resulting singular optimal control problem is shown to be\ntractable by methods from convex analysis and, under minimal assumptions, we\nconstruct an explicit solution to the scheduling problem in terms of some\nconcave envelope of the resilience adjusted market depth.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1310.3077v1", "link": "http://arxiv.org/abs/1310.3077v1"}, {"title": "Offline Multi-Action Policy Learning: Generalization and Optimization", "summary": "In many settings, a decision-maker wishes to learn a rule, or policy, that\nmaps from observable characteristics of an individual to an action. Examples\ninclude selecting offers, prices, advertisements, or emails to send to\nconsumers, as well as the problem of determining which medication to prescribe\nto a patient. While there is a growing body of literature devoted to this\nproblem, most existing results are focused on the case where data comes from a\nrandomized experiment, and further, there are only two possible actions, such\nas giving a drug to a patient or not. In this paper, we study the offline\nmulti-action policy learning problem with observational data and where the\npolicy may need to respect budget constraints or belong to a restricted policy\nclass such as decision trees. We build on the theory of efficient\nsemi-parametric inference in order to propose and implement a policy learning\nalgorithm that achieves asymptotically minimax-optimal regret. To the best of\nour knowledge, this is the first result of this type in the multi-action setup,\nand it provides a substantial performance improvement over the existing\nlearning algorithms. We then consider additional computational challenges that\narise in implementing our method for the case where the policy is restricted to\ntake the form of a decision tree. We propose two different approaches, one\nusing a mixed integer program formulation and the other using a tree-search\nbased algorithm.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1810.04778v2", "link": "http://arxiv.org/abs/1810.04778v2"}, {"title": "Exploring the Interconnectedness of Cryptocurrencies using Correlation\n  Networks", "summary": "Correlation networks were used to detect characteristics which, although\nfixed over time, have an important influence on the evolution of prices over\ntime. Potentially important features were identified using the websites and\nwhitepapers of cryptocurrencies with the largest userbases. These were assessed\nusing two datasets to enhance robustness: one with fourteen cryptocurrencies\nbeginning from 9 November 2017, and a subset with nine cryptocurrencies\nstarting 9 September 2016, both ending 6 March 2018. Separately analysing the\nsubset of cryptocurrencies raised the number of data points from 115 to 537,\nand improved robustness to changes in relationships over time. Excluding USD\nTether, the results showed a positive association between different\ncryptocurrencies that was statistically significant. Robust, strong positive\nassociations were observed for six cryptocurrencies where one was a fork of the\nother; Bitcoin / Bitcoin Cash was an exception. There was evidence for the\nexistence of a group of cryptocurrencies particularly associated with Cardano,\nand a separate group correlated with Ethereum. The data was not consistent with\na token's functionality or creation mechanism being the dominant determinants\nof the evolution of prices over time but did suggest that factors other than\nspeculation contributed to the price.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1806.06632v1", "link": "http://arxiv.org/abs/1806.06632v1"}, {"title": "Regularity of the Exercise Boundary for American Put Options on Assets\n  with Discrete Dividends", "summary": "We analyze the regularity of the optimal exercise boundary for the American\nPut option when the underlying asset pays a discrete dividend at a known time\n$t_d$ during the lifetime of the option. The ex-dividend asset price process is\nassumed to follow Black-Scholes dynamics and the dividend amount is a\ndeterministic function of the ex-dividend asset price just before the dividend\ndate. The solution to the associated optimal stopping problem can be\ncharacterised in terms of an optimal exercise boundary which, in contrast to\nthe case when there are no dividends, may no longer be monotone. In this paper\nwe prove that when the dividend function is positive and concave, then the\nboundary is non-increasing in a left-hand neighbourhood of $t_d$, and tends to\n$0$ as time tends to $t_d^-$ with a speed that we can characterize. When the\ndividend function is linear in a neighbourhood of zero, then we show continuity\nof the exercise boundary and a high contact principle in the left-hand\nneighbourhood of $t_d$. When it is globally linear, then right-continuity of\nthe boundary and the high contact principle are proved to hold globally.\nFinally, we show how all the previous results can be extended to multiple\ndividend payment dates in that case.", "category": ["q-fin.CP", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/0911.5117v2", "link": "http://arxiv.org/abs/0911.5117v2"}, {"title": "On pricing rules and optimal strategies in general Kyle-Back models", "summary": "The folk result in Kyle-Back models states that the value function of the\ninsider remains unchanged when her admissible strategies are restricted to\nabsolutely continuous ones. In this paper we show that, for a large class of\npricing rules used in current literature, the value function of the insider can\nbe finite when her strategies are restricted to be absolutely continuous and\ninfinite when this restriction is not imposed. This implies that the folk\nresult doesn't hold for those pricing rules and that they are not consistent\nwith equilibrium. We derive the necessary conditions for a pricing rule to be\nconsistent with equilibrium and prove that, when a pricing rule satisfies these\nnecessary conditions, the insider's optimal strategy is absolutely continuous,\nthus obtaining the classical result in a more general setting.\n  This, furthermore, allows us to justify the standard assumption of absolute\ncontinuity of insider's strategies since one can construct a pricing rule\nsatisfying the necessary conditions derived in the paper that yield the same\nprice process as the pricing rules employed in the modern literature when\ninsider's strategies are absolutely continuous.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1812.07529v1", "link": "http://arxiv.org/abs/1812.07529v1"}, {"title": "Intra-Horizon Expected Shortfall and Risk Structure in Models with Jumps", "summary": "The present article deals with intra-horizon risk in models with jumps. Our\ngeneral understanding of intra-horizon risk is along the lines of the approach\ntaken in Boudoukh, Richardson, Stanton and Whitelaw (2004), Rossello (2008),\nBhattacharyya, Misra and Kodase (2009), Bakshi and Panayotov (2010), and\nLeippold and Vasiljevi\\'c (2019). In particular, we believe that quantifying\nmarket risk by strictly relying on point-in-time measures cannot be deemed a\nsatisfactory approach in general. Instead, we argue that complementing this\napproach by studying measures of risk that capture the magnitude of losses\npotentially incurred at any time of a trading horizon is necessary when dealing\nwith (m)any financial position(s). To address this issue, we propose an\nintra-horizon analogue of the expected shortfall for general profit and loss\nprocesses and discuss its key properties. Our intra-horizon expected shortfall\nis well-defined for (m)any popular class(es) of L\\'evy processes encountered\nwhen modeling market dynamics and constitutes a coherent measure of risk, as\nintroduced in Cheridito, Delbaen and Kupper (2004). On the computational side,\nwe provide a simple method to derive the intra-horizon risk inherent to popular\nL\\'evy dynamics. Our general technique relies on results for\nmaturity-randomized first-passage probabilities and allows for a derivation of\ndiffusion and single jump risk contributions. These theoretical results are\ncomplemented with an empirical analysis, where popular L\\'evy dynamics are\ncalibrated to S&P 500 index data and an analysis of the resulting intra-horizon\nrisk is presented.", "category": ["q-fin.MF", "q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/2002.04675v1", "link": "http://arxiv.org/abs/2002.04675v1"}, {"title": "Evolution of community structure in the world trade web", "summary": "In this note we study the bilateral merchandise trade flows between 186\ncountries over the 1948-2005 period using data from the International Monetary\nFund. We use Pajek to identify network structure and behavior across thresholds\nand over time. In particular, we focus on the evolution of trade \"islands\" in\nthe a world trade network in which countries are linked with directed edges\nweighted according to fraction of total dollars sent from one country to\nanother. We find mixed evidence for globalization.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0709.2630v1", "link": "http://dx.doi.org/10.1140/epjb/e2008-00181-2"}, {"title": "Correlation of coming limit price with order book in stock markets", "summary": "We examine the correlation of the limit price with the order book, when a\nlimit order comes. We analyzed the Rebuild Order Book of Stock Exchange\nElectronic Trading Service, which is the centralized order book market of\nLondon Stock Exchange. As a result, the limit price is broadly distributed\naround the best price according to a power-law, and it isn't randomly drawn\nfrom the distribution, but has a strong correlation with the size of cumulative\nunexecuted limit orders on the price. It was also found that the limit price,\non the coarse-grained price scale, tends to gather around the price which has a\nlarge size of cumulative unexecuted limit orders.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/physics/0702029v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.04.091"}, {"title": "Microstructure Effects on Daily Return Volatility in Financial Markets", "summary": "We simulate a series of daily returns from intraday price movements initiated\nby microstructure elements. Significant evidence is found that daily returns\nand daily return volatility exhibit first order autocorrelation, but trading\nvolume and daily return volatility are not correlated, while intraday\nvolatility is. We also consider GARCH effects in daily return series and show\nthat estimates using daily returns are biased from the influence of the level\nof prices. Using daily price changes instead, we find evidence of a significant\nGARCH component. These results suggest that microstructure elements have a\nconsiderable influence on the return generating process.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0011295v1", "link": "http://dx.doi.org/10.1142/S0219024903002171"}, {"title": "Collective Philanthropy: Describing and Modeling the Ecology of Giving", "summary": "Reflective of income and wealth distributions, philanthropic gifting appears\nto follow an approximate power-law size distribution as measured by the size of\ngifts received by individual institutions. We explore the ecology of gifting by\nanalysing data sets of individual gifts for a diverse group of institutions\ndedicated to education, medicine, art, public support, and religion. We find\nthat the detailed forms of gift-size distributions differ across but are\nrelatively constant within charity categories. We construct a model for how a\ndonor's income affects their giving preferences in different charity\ncategories, offering a mechanistic explanation for variations in institutional\ngift-size distributions. We discuss how knowledge of gift-sized distributions\nmay be used to assess an institution's gift-giving profile, to help set\nfundraising goals, and to design an institution-specific giving pyramid.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1307.2278v3", "link": "http://dx.doi.org/10.1371/journal.pone.0098876"}, {"title": "High-dimensional macroeconomic forecasting using message passing\n  algorithms", "summary": "This paper proposes two distinct contributions to econometric analysis of\nlarge information sets and structural instabilities. First, it treats a\nregression model with time-varying coefficients, stochastic volatility and\nexogenous predictors, as an equivalent high-dimensional static regression\nproblem with thousands of covariates. Inference in this specification proceeds\nusing Bayesian hierarchical priors that shrink the high-dimensional vector of\ncoefficients either towards zero or time-invariance. Second, it introduces the\nframeworks of factor graphs and message passing as a means of designing\nefficient Bayesian estimation algorithms. In particular, a Generalized\nApproximate Message Passing (GAMP) algorithm is derived that has low\nalgorithmic complexity and is trivially parallelizable. The result is a\ncomprehensive methodology that can be used to estimate time-varying parameter\nregressions with arbitrarily large number of exogenous predictors. In a\nforecasting exercise for U.S. price inflation this methodology is shown to work\nvery well.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/2004.11485v1", "link": "http://dx.doi.org/10.1080/07350015.2019.1677472"}, {"title": "A limit order book model for latency arbitrage", "summary": "We consider a single security market based on a limit order book and two\ninvestors, with different speeds of trade execution. If the fast investor can\nfront-run the slower investor, we show that this allows the fast trader to\nobtain risk free profits, but that these profits cannot be scaled. We derive\nthe fast trader's optimal behaviour when she has only distributional knowledge\nof the slow trader's actions, with few restrictions on the possible prior\ndistributions. We also consider the slower trader's response to the presence of\na fast trader in a market, and the effects of the introduction of a `Tobin tax'\non financial transactions. We show that such a tax can lead to the elimination\nof profits from front-running strategies. Consequently, a Tobin tax can both\nincrease market efficiency and attract traders to a market.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1110.4811v1", "link": "http://arxiv.org/abs/1110.4811v1"}, {"title": "Decomposition of order statistics of semimartingales using local times", "summary": "In a recent work \\cite{BG}, given a collection of continuous semimartingales,\nauthors derive a semimartingale decomposition from the corresponding ranked\nprocesses in the case that the ranked processes can meet more than two original\nprocesses at the same time. This has led to a more general decomposition of\nranked processes. In this paper, we derive a more general result for\nsemimartingales (not necessarily continuous) using a simpler approach.\nFurthermore, we also give a generalization of Ouknine \\cite{O1, O2} and Yan's\n\\cite{Y1} formula for local times of ranked processes", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/0807.5001v1", "link": "http://arxiv.org/abs/0807.5001v1"}, {"title": "On using shadow prices in portfolio optimization with transaction costs", "summary": "In frictionless markets, utility maximization problems are typically solved\neither by stochastic control or by martingale methods. Beginning with the\nseminal paper of Davis and Norman [Math. Oper. Res. 15 (1990) 676--713],\nstochastic control theory has also been used to solve various problems of this\ntype in the presence of proportional transaction costs. Martingale methods, on\nthe other hand, have so far only been used to derive general structural\nresults. These apply the duality theory for frictionless markets typically to a\nfictitious shadow price process lying within the bid-ask bounds of the real\nprice process. In this paper, we show that this dual approach can actually be\nused for both deriving a candidate solution and verification in Merton's\nproblem with logarithmic utility and proportional transaction costs. In\nparticular, we determine the shadow price process.", "category": ["q-fin.CP", "math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1010.4989v1", "link": "http://dx.doi.org/10.1214/09-AAP648"}, {"title": "Portfolio Insurance under a risk-measure constraint", "summary": "We study the problem of portfolio insurance from the point of view of a fund\nmanager, who guarantees to the investor that the portfolio value at maturity\nwill be above a fixed threshold. If, at maturity, the portfolio value is below\nthe guaranteed level, a third party will refund the investor up to the\nguarantee. In exchange for this protection, the third party imposes a limit on\nthe risk exposure of the fund manager, in the form of a convex monetary risk\nmeasure. The fund manager therefore tries to maximize the investor's utility\nfunction subject to the risk measure constraint.We give a full solution to this\nnonconvex optimization problem in the complete market setting and show in\nparticular that the choice of the risk measure is crucial for the optimal\nportfolio to exist. Explicit results are provided for the entropic risk measure\n(for which the optimal portfolio always exists) and for the class of spectral\nrisk measures (for which the optimal portfolio may fail to exist in some\ncases).", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1102.4489v1", "link": "http://arxiv.org/abs/1102.4489v1"}, {"title": "CAP and Monetary Policy", "summary": "Despite the importance of CAP-related agricultural market regulation\nmechanisms within Europe, the agricultural sectors in European countries retain\na degree of sensitivity to macroeconomic activity and policies. This reality\nnow raises the question of the effects to be expected from the implementation\nof the single monetary policy on these agricultural sectors within the Monetary\nUnion.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.09475v1", "link": "http://arxiv.org/abs/1807.09475v1"}, {"title": "Thermodynamics of firms' growth", "summary": "The distribution of firms' growth and firms' sizes is a topic under intense\nscrutiny. In this paper we show that a thermodynamic model based on the Maximum\nEntropy Principle, with dynamical prior information, can be constructed that\nadequately describes the dynamics and distribution of firms' growth. Our\ntheoretical framework is tested against a comprehensive data-base of Spanish\nfirms, which covers to a very large extent Spain's economic activity with a\ntotal of 1,155,142 firms evolving along a full decade. We show that the\nempirical exponent of Pareto's law, a rule often observed in the rank\ndistribution of large-size firms, is explained by the capacity of the economic\nsystem for creating/destroying firms, and can be used to measure the health of\na capitalist-based economy. Indeed, our model predicts that when the exponent\nis larger that 1, creation of firms is favored; when it is smaller that 1,\ndestruction of firms is favored instead; and when it equals 1 (matching Zipf's\nlaw), the system is in a full macroeconomic equilibrium, entailing \"free\"\ncreation and/or destruction of firms. For medium and smaller firm-sizes, the\ndynamical regime changes; the whole distribution can no longer be fitted to a\nsingle simple analytic form and numerical prediction is required. Our model\nconstitutes the basis of a full predictive framework for the economic evolution\nof an ensemble of firms that can be potentially used to develop simulations and\ntest hypothetical scenarios, as economic crisis or the response to specific\npolicy measures.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1504.07666v1", "link": "http://arxiv.org/abs/1504.07666v1"}, {"title": "On Binscatter", "summary": "Binscatter is very popular in applied microeconomics. It provides a flexible,\nyet parsimonious way of visualizing and summarizing large data sets in\nregression settings, and it is often used for informal evaluation of\nsubstantive hypotheses such as linearity or monotonicity of the regression\nfunction. This paper presents a foundational, thorough analysis of binscatter:\nwe give an array of theoretical and practical results that aid both in\nunderstanding current practices (i.e., their validity or lack thereof) and in\noffering theory-based guidance for future applications. Our main results\ninclude principled number of bins selection, confidence intervals and bands,\nhypothesis tests for parametric and shape restrictions of the regression\nfunction, and several other new methods, applicable to canonical binscatter as\nwell as higher-order polynomial, covariate-adjusted and smoothness-restricted\nextensions thereof. In particular, we highlight important methodological\nproblems related to covariate adjustment methods used in current practice. We\nalso discuss extensions to clustered data. Our results are illustrated with\nsimulated and real data throughout. Companion general-purpose software packages\nfor \\texttt{Stata} and \\texttt{R} are provided. Finally, from a technical\nperspective, new theoretical results for partitioning-based series estimation\nare obtained that may be of independent interest.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1902.09608v1", "link": "http://arxiv.org/abs/1902.09608v1"}, {"title": "Singular recursive utility", "summary": "We introduce the concept of singular recursive utility. This leads to a kind\nof singular BSDE which, to the best of our knowledge, has not been studied\nbefore. We show conditions for existence and uniqueness of a solution for this\nkind of singular BSDE. Furthermore, we analyze the problem of maximizing the\nsingular recursive utility. We derive sufficient and necessary maximum\nprinciples for this problem, and connect it to the Skorohod reflection problem.\nFinally, we apply our results to a specific cash flow. In this case, we find\nthat the optimal consumption rate is given by the solution to the corresponding\nSkorohod reflection problem.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1504.08170v2", "link": "http://arxiv.org/abs/1504.08170v2"}, {"title": "Projection pursuit based generalized betas accounting for higher order\n  co-moment effects in financial market analysis", "summary": "Betas are possibly the most frequently applied tool to analyze how securities\nrelate to the market. While in very widespread use, betas only express dynamics\nderived from second moment statistics. Financial returns data often deviate\nfrom normal assumptions in the sense that they have significant third and\nfourth order moments and contain outliers. This paper targets to introduce a\nway to calculate generalized betas that also account for higher order moment\neffects, while maintaining the conceptual simplicity and interpretability of\nbetas. Thereunto, the co-moment analysis projection index (CAPI) is introduced.\nWhen applied as a projection index in the projection pursuit (PP) framework,\ngeneralized betas are obtained as the directions optimizing the CAPI objective.\nA version of CAPI based on trimmed means is introduced as well, which is more\nstable in the presence of outliers. Simulation results underpin the statistical\nproperties of all projections and a small, yet highly illustrative example is\npresented.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1908.00141v1", "link": "http://arxiv.org/abs/1908.00141v1"}, {"title": "Stability of Utility Maximization in Nonequivalent Markets", "summary": "Stability of the utility maximization problem with random endowment and\nindifference prices is studied for a sequence of financial markets in an\nincomplete Brownian setting. Our novelty lies in the nonequivalence of markets,\nin which the volatility of asset prices (as well as the drift) varies.\nDegeneracies arise from the presence of nonequivalence. In the positive real\nline utility framework, a counterexample is presented showing that the expected\nutility maximization problem can be unstable. A positive stability result is\nproven for utility functions on the entire real line.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1410.0915v2", "link": "http://arxiv.org/abs/1410.0915v2"}, {"title": "Climate Events and Insurance Demand - The effect of potentially\n  catastrophic events on insurance demand in Italy", "summary": "Climate extreme events are constantly increasing. What is the effect of these\npotentially catastrophic events on insurance demand in Italy, with particular\nreference to the economic activities? Extreme precipitation events over most of\nthe midlatitude land masses and over wet tropical regions will very likely\nbecome more intense and more frequent by the end of this century, as global\nmean surface temperature increases. If we look to Italy, examination of the\nprecipitation time series shows a sensitive and highly significant decrease in\nthe total number of precipitation events in Italy, with a trend of events\nintense dissimilar as regards to low and high intensity, with a decline of\nfirsts and an increase of seconds. The risk related to hydrological natural\ndisasters is in Italy one of the most important problem for both damage and\nnumber of victims. How evolves the ability to pay for damages, with a view to\nsafeguarding work and economic activities, and employment protection?", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1406.4114v1", "link": "http://arxiv.org/abs/1406.4114v1"}, {"title": "Consequences of increased longevity for wealth, fertility, and\n  population growth", "summary": "We present, solve and numerically simulate a simple model that describes the\nconsequences of increased longevity on fertility rates, population growth and\nthe distribution of wealth in developed societies. We look at the consequences\nof the repeated use of life extension techniques and show that they represent a\nnovel commodity whose introduction will profoundly influence key aspects of\neconomy and society in general. In particular, we uncover two phases within our\nsimplified model, labeled as 'mortal' and 'immortal'. Within the life extension\nscenario it is possible to have sustainable economic growth in a population of\nstable size, as a result of dynamical equilibrium between the two phases.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0806.2964v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.09.004"}, {"title": "On the equivalence between Value-at-Risk and Expected Shortfall in\n  non-concave optimization", "summary": "This paper studies a non-concave optimization problem under a Value-at-Risk\n(VaR) or an Expected Shortfall (ES) constraint. The non-concavity of the\nproblem stems from the non-linear payoff structure of the optimizing investor.\nWe obtain the closed-form optimal wealth with an ES constraint as well as with\na VaR constraint respectively, and explicitly calculate the optimal trading\nstrategy for constant relative risk aversion (CRRA) utility functions. In our\nnon-concave optimization problem, we find that with a not too strict regulation\nfor any VaR-constraint with an arbitrary risk level, there exists an\nES-constraint leading to the same investment strategy, which shows on some\nlevel the ineffectiveness of the ES-based regulation. This differs from the\nconclusion drawn in Basak and Shapiro (2001) for the concave optimization\nproblem, where VaR and ES lead to different solutions and ES provides a better\nloss protection.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/2002.02229v2", "link": "http://arxiv.org/abs/2002.02229v2"}, {"title": "A possible alternative evaluation method for the non-use and nonmarket\n  values of ecosystem services", "summary": "Monetization of the non-use and nonmarket values of ecosystem services is\nimportant especially in the areas of environmental cost-benefit analysis,\nmanagement and environmental impact assessment. However, the reliability of\nvaluation estimations has been criticized due to the biases that associated\nwith methods like the popular contingent valuation method (CVM). In order to\nprovide alternative valuation results for comparison purpose, we proposed the\npossibility of using a method that incorporates fact-based costs and contingent\npreferences for evaluating non-use and nonmarket values, which we referred to\nas value allotment method (VAM). In this paper, we discussed the economic\nprinciples of VAM, introduced the performing procedure, analyzed assumptions\nand potential biases that associated with the method and compared VAM with CVM\nthrough a case study in Guangzhou, China. The case study showed that the VAM\ngave more conservative estimates than the CVM, which could be a merit since CVM\noften generates overestimated values. We believe that this method can be used\nat least as a referential alternative to CVM and might be particularly useful\nin assessing the non-use and nonmarket values of ecosystem services from\nhuman-invested ecosystems, such as restored ecosystems, man-made parks and\ncroplands.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.08376v1", "link": "http://arxiv.org/abs/1811.08376v1"}, {"title": "On the properties of the Lambda value at risk: robustness, elicitability\n  and consistency", "summary": "Recently, financial industry and regulators have enhanced the debate on the\ngood properties of a risk measure. A fundamental issue is the evaluation of the\nquality of a risk estimation. On the one hand, a backtesting procedure is\ndesirable for assessing the accuracy of such an estimation and this can be\nnaturally achieved by elicitable risk measures. For the same objective, an\nalternative approach has been introduced by Davis (2016) through the so-called\nconsistency property. On the other hand, a risk estimation should be less\nsensitive with respect to small changes in the available data set and exhibit\nqualitative robustness. A new risk measure, the Lambda value at risk (Lambda\nVaR), has been recently proposed by Frittelli et al. (2014), as a\ngeneralization of VaR with the ability to discriminate the risk among P&L\ndistributions with different tail behaviour. In this article, we show that\nLambda VaR also satisfies the properties of robustness, elicitability and\nconsistency under some conditions.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1603.09491v2", "link": "http://arxiv.org/abs/1603.09491v2"}, {"title": "Implied correlation from VaR", "summary": "Value at risk (VaR) is a risk measure that has been widely implemented by\nfinancial institutions. This paper measures the correlation among asset price\nchanges implied from VaR calculation. Empirical results using US and UK equity\nindexes show that implied correlation is not constant but tends to be higher\nfor events in the left tails (crashes) than in the right tails (booms).", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1103.5655v1", "link": "http://arxiv.org/abs/1103.5655v1"}, {"title": "A Class of Solvable Optimal Stopping Problems of Spectrally Negative\n  Jump Diffusions", "summary": "We consider the optimal stopping of a class of spectrally negative jump\ndiffusions. We state a set of conditions under which the value is shown to have\na representation in terms of an ordinary nonlinear programming problem. We\nestablish a connection between the considered problem and a stopping problem of\nan associated continuous diffusion process and demonstrate how this connection\nmay be applied for characterizing the stopping policy and its value. We also\nestablish a set of typically satisfied conditions under which increased\nvolatility as well as higher jump-intensity decelerates rational exercise by\nincreasing the value and expanding the continuation region.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1302.4181v1", "link": "http://arxiv.org/abs/1302.4181v1"}, {"title": "Stock mechanics: theory of conservation of total energy and predictions\n  of coming short-term fluctuations of Dow Jones Industrials Average (DJIA)", "summary": "Predicting absolute magnitude of fluctuations of price, even if their sign\nremains unknown, is important for risk analysis and for option prices. In the\npresent work, we display our predictions about absolute magnitude of daily\nfluctuations of the Dow Jones Industrials Average (DJIA), utilizing the\noriginal theory of conservation of total energy, for the coming 500 days.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0602055v1", "link": "http://arxiv.org/abs/physics/0602055v1"}, {"title": "Closed-form Solutions of Relativistic Black-Scholes Equations", "summary": "Drawing insights from the triumph of relativistic over classical mechanics\nwhen velocities approach the speed of light, we explore a similar improvement\nto the seminal Black-Scholes (Black and Scholes (1973)) option pricing formula\nby considering a relativist version of it, and then finding a respective\nsolution. We show that our solution offers a significant improvement over\ncompeting solutions (e.g., Romero and Zubieta-Martinez (2016)), and obtain a\nnew closed-form option pricing formula, containing the speed limit of\ninformation transfer c as a new parameter. The new formula is rigorously shown\nto converge to the Black-Scholes formula as c goes to infinity. When c is\nfinite, the new formula can flatten the standard volatility smile which is more\nconsistent with empirical observations. In addition, an alternative family of\ndistributions for stock prices arises from our new formula, which offer a\nbetter fit, are shown to converge to lognormal, and help to better explain the\nvolatility skew.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1711.04219v1", "link": "http://arxiv.org/abs/1711.04219v1"}, {"title": "A fractional reaction-diffusion description of supply and demand", "summary": "We suggest that the broad distribution of time scales in financial markets\ncould be a crucial ingredient to reproduce realistic price dynamics in stylised\nAgent-Based Models. We propose a fractional reaction-diffusion model for the\ndynamics of latent liquidity in financial markets, where agents are very\nheterogeneous in terms of their characteristic frequencies. Several features of\nour model are amenable to an exact analytical treatment. We find in particular\nthat the impact is a concave function of the transacted volume (aka the\n\"square-root impact law\"), as in the normal diffusion limit. However, the\nimpact kernel decays as $t^{-\\beta}$ with $\\beta=1/2$ in the diffusive case,\nwhich is inconsistent with market efficiency. In the sub-diffusive case the\ndecay exponent $\\beta$ takes any value in $[0,1/2]$, and can be tuned to match\nthe empirical value $\\beta \\approx 1/4$. Numerical simulations confirm our\ntheoretical results. Several extensions of the model are suggested.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1704.02638v2", "link": "http://dx.doi.org/10.1140/epjb/e2017-80246-9"}, {"title": "Monte Carlo Confidence Sets for Identified Sets", "summary": "In complicated/nonlinear parametric models, it is generally hard to know\nwhether the model parameters are point identified. We provide computationally\nattractive procedures to construct confidence sets (CSs) for identified sets of\nfull parameters and of subvectors in models defined through a likelihood or a\nvector of moment equalities or inequalities. These CSs are based on level sets\nof optimal sample criterion functions (such as likelihood or optimally-weighted\nor continuously-updated GMM criterions). The level sets are constructed using\ncutoffs that are computed via Monte Carlo (MC) simulations directly from the\nquasi-posterior distributions of the criterions. We establish new Bernstein-von\nMises (or Bayesian Wilks) type theorems for the quasi-posterior distributions\nof the quasi-likelihood ratio (QLR) and profile QLR in partially-identified\nregular models and some non-regular models. These results imply that our MC CSs\nhave exact asymptotic frequentist coverage for identified sets of full\nparameters and of subvectors in partially-identified regular models, and have\nvalid but potentially conservative coverage in models with reduced-form\nparameters on the boundary. Our MC CSs for identified sets of subvectors are\nshown to have exact asymptotic coverage in models with singularities. We also\nprovide results on uniform validity of our CSs over classes of DGPs that\ninclude point and partially identified models. We demonstrate good\nfinite-sample coverage properties of our procedures in two simulation\nexperiments. Finally, our procedures are applied to two non-trivial empirical\nexamples: an airline entry game and a model of trade flows.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1605.00499v3", "link": "http://arxiv.org/abs/1605.00499v3"}, {"title": "Selectivity correction in discrete-continuous models for the willingness\n  to work as crowd-shippers and travel time tolerance", "summary": "The objective of this study is to understand the different behavioral\nconsiderations that govern the choice of people to engage in a crowd-shipping\nmarket. Using novel data collected by the researchers in the US, we develop\ndiscrete-continuous models. A binary logit model has been used to estimate\ncrowd-shippers' willingness to work, and an ordinary least-square regression\nmodel has been employed to calculate crowd-shippers' maximum tolerance for\nshipping and delivery times. A selectivity-bias term has been included in the\nmodel to correct for the conditional relationships of the crowd-shipper's\nwillingness to work and their maximum travel time tolerance. The results show\nsocio-demographic characteristics (e.g. age, gender, race, income, and\neducation level), transporting freight experience, and number of social media\nusages significant influence the decision to participate in the crowd-shipping\nmarket. In addition, crowd-shippers pay expectations were found to be\nreasonable and concurrent with the literature on value-of-time. Findings from\nthis research are helpful for crowd-shipping companies to identify and attract\npotential shippers. In addition, an understanding of crowd-shippers - their\nbehaviors, perceptions, demographics, pay expectations, and in which contexts\nthey are willing to divert from their route - are valuable to the development\nof business strategies such as matching criteria and compensation schemes for\ndriver-partners.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.00985v1", "link": "http://arxiv.org/abs/1810.00985v1"}, {"title": "Modeling electricity spot prices using mean-reverting multifractal\n  processes", "summary": "We discuss stochastic modeling of volatility persistence and\nanti-correlations in electricity spot prices, and for this purpose we present\ntwo mean-reverting versions of the multifractal random walk (MRW). In the first\nmodel the anti-correlations are modeled in the same way as in an\nOrnstein-Uhlenbeck process, i.e. via a drift (damping) term, and in the second\nmodel the anti-correlations are included by letting the innovations in the MRW\nmodel be fractional Gaussian noise with H < 1/2. For both models we present\napproximate maximum likelihood methods, and we apply these methods to estimate\nthe parameters for the spot prices in the Nordic electricity market. The\nmaximum likelihood estimates show that electricity spot prices are\ncharacterized by scaling exponents that are significantly different from the\ncorresponding exponents in stock markets, confirming the exceptional nature of\nthe electricity market. In order to compare the damped MRW model with the\nfractional MRW model we use ensemble simulations and wavelet-based variograms,\nand we observe that certain features of the spot prices are better described by\nthe damped MRW model. The characteristic correlation time is estimated to\napproximately half a year.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1201.6137v1", "link": "http://dx.doi.org/10.1016/j.physa.2012.08.004"}, {"title": "Multifractal Analysis and Local Hoelder Exponents Approach to Detecting\n  Stock Markets Crashes", "summary": "This paper is devoted to problem of detecting critical events at finiacial\nmarkets using methods of multifractal analysis. Namely, the local regularity of\ntime-series is studied. As a result, one can find out a special behavior or\nsignal of regularity before crashes. This spesial behaviour of local Hoelder\nexponents inherent in financial time series can be used in detecting critcal\nevents or crashes at financial markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0407603v1", "link": "http://arxiv.org/abs/cond-mat/0407603v1"}, {"title": "On Origins of Bubbles", "summary": "We discuss - in what is intended to be a pedagogical fashion - a criterion,\nwhich is a lower bound on a certain ratio, for when a stock (or a similar\ninstrument) is not a good investment in the long term, which can happen even if\nthe expected return is positive. The root cause is that prices are positive and\nhave skewed, long-tailed distributions, which coupled with volatility results\nin a long-run asymmetry. This relates to bubbles in stock prices, which we\ndiscuss using a simple binomial tree model, without resorting to the stochastic\ncalculus machinery. We illustrate empirical properties of the aforesaid ratio.\nLog of market cap and sectors appear to be relevant explanatory variables for\nthis ratio, while price-to-book ratio (or its log) is not. We also discuss a\nshort-term effect of volatility, to wit, the analog of Heisenberg's uncertainty\nprinciple in finance and a simple derivation thereof using a binary tree.", "category": ["q-fin.RM", "q-fin.MF"], "id": "http://arxiv.org/abs/1610.03769v2", "link": "http://arxiv.org/abs/1610.03769v2"}, {"title": "Economic inequality and mobility in kinetic models for social sciences", "summary": "Statistical evaluations of the economic mobility of a society are more\ndifficult than measurements of the income distribution, because they require to\nfollow the evolution of the individuals' income for at least one or two\ngenerations. In micro-to-macro theoretical models of economic exchanges based\non kinetic equations, the income distribution depends only on the asymptotic\nequilibrium solutions, while mobility estimates also involve the detailed\nstructure of the transition probabilities of the model, and are thus an\nimportant tool for assessing its validity. Empirical data show a remarkably\ngeneral negative correlation between economic inequality and mobility, whose\nexplanation is still unclear. It is therefore particularly interesting to study\nthis correlation in analytical models. In previous work we investigated the\nbehavior of the Gini inequality index in kinetic models in dependence on\nseveral parameters which define the binary interactions and the taxation and\nredistribution processes: saving propensity, taxation rates gap, tax evasion\nrate, welfare means-testing etc. Here, we check the correlation of mobility\nwith inequality by analyzing the mobility dependence from the same parameters.\nAccording to several numerical solutions, the correlation is confirmed to be\nnegative.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1504.03232v1", "link": "http://dx.doi.org/10.1140/epjst/e2015-50117-8"}, {"title": "On return-volatility correlation in financial dynamics", "summary": "With the daily and minutely data of the German DAX and Chinese indices, we\ninvestigate how the return-volatility correlation originates in financial\ndynamics. Based on a retarded volatility model, we may eliminate or generate\nthe return-volatility correlation of the time series, while other\ncharacteristics, such as the probability distribution of returns and long-range\ntime-correlation of volatilities etc., remain essentially unchanged. This\nsuggests that the leverage effect or anti-leverage effect in financial markets\narises from a kind of feedback return-volatility interactions, rather than the\nlong-range time-correlation of volatilities and asymmetric probability\ndistribution of returns. Further, we show that large volatilities dominate the\nreturn-volatility correlation in financial dynamics.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1202.0342v1", "link": "http://dx.doi.org/10.1209/0295-5075/88/28003"}, {"title": "Percolation-Based Model of New-Product Diffusion with Macroscopic\n  Feedback Effects", "summary": "This paper proposes a percolation-based model of new-product diffusion in the\nspirit of Solomon et al. (2000) and Goldenberg et al. (2000). A consumer buys\nthe new product if she has formed her individual valuation of the product\n(reservation price) and if this valuation is greater or equal than the price of\nthe product announced by the firm in a given period. Our model differs from\nprevious percolation-based models of new-product diffusion in two respects.\nFirst, we consider macroscopic feedback effects affecting the supply or the\ndemand side of the market (or both). Second, a consumer who did not buy the\nproduct in the period in which her valuation was formed remains a potential\nbuyer and buys in some later period if and when her individual valuation equals\nor exceeds the price of the product. Unlike most previous models of new-product\ndiffusion, our framework accounts for the empirical finding of long tails\ncharacteristic for early stages of innovation diffusion.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0308358v1", "link": "http://arxiv.org/abs/cond-mat/0308358v1"}, {"title": "The effects of non-tariff measures on agri-food trade: a review and\n  meta-analysis of empirical evidence", "summary": "The increasing policy interests and the vivid academic debate on non-tariff\nmeasures (NTMs) has stimulated a growing literature on how NTMs affect agrifood\ntrade. The empirical literature provides contrasting and heterogeneous\nevidence, with some studies supporting the standards as catalysts view, and\nothers favouring the standards as barriers explanation. To the extent that NTMs\ncan influence trade, understanding the prevailing effect, and the motivations\nbehind one effect or the other, is a pressing issue. We review a large body of\nempirical evidence on the effect of NTMs on agri-food trade and conduct a\nmeta-analysis to disentangle potential determinants of heterogeneity in\nestimates. Our findings show the role played by the publication process and by\nstudy-specific assumptions. Some characteristics of the studies are correlated\nwith positive significant estimates, others covary with negative significant\nestimates. Overall, we found that the effects of NTMs vary across types of\nNTMs, proxy for NTMs, and levels of details of studies. Not negligible is the\ninfluence of methodological issues and publication process.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1811.06323v1", "link": "http://dx.doi.org/10.1111/1477-9552.12316"}, {"title": "Generalized Expected Discounted Penalty Function at General Drawdown for\n  L\u00e9vy Risk Processes", "summary": "This paper considers an insurance surplus process modeled by a spectrally\nnegative L\\'{e}vy process. Instead of the time of ruin in the traditional\nsetting, we apply the time of drawdown as the risk indicator in this paper. We\nstudy the joint distribution of the time of drawdown, the running maximum at\ndrawdown, the last minimum before drawdown, the surplus before drawdown and the\nsurplus at drawdown (may not be deficit in this case), which generalizes the\nknown results on the classical expected discounted penalty function in Gerber\nand Shiu (1998). The results have semi-explicit expressions in terms of the\n$q$-scale functions and the L\\'{e}vy measure associated with the L\\'{e}vy\nprocess. As applications, the obtained result is applied to recover results in\nthe literature and to obtain new results for the Gerber-Shiu function at ruin\nfor risk processes embedded with a loss-carry-forward taxation system or a\nbarrier dividend strategy. Moreover, numerical examples are provided to\nillustrate the results.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1906.01449v1", "link": "http://arxiv.org/abs/1906.01449v1"}, {"title": "A note on super-hedging for investor-producers", "summary": "We study the situation of an agent who can trade on a financial market and\ncan also transform some assets into others by means of a production system, in\norder to price and hedge derivatives on produced goods. This framework is\nmotivated by the case of an electricity producer who wants to hedge a position\non the electricity spot price and can trade commodities which are inputs for\nhis system. This extends the essential results of Bouchard & Nguyen Huu (2011)\nto continuous time markets. We introduce the generic concept of conditional\nsure profit along the idea of the no sure profit condition of R\\`asonyi (2009).\nThe condition allows one to provide a closedness property for the set of\nsuper-hedgeable claims in a very general financial setting. Using standard\nseparation arguments, we then deduce a dual characterization of the latter and\nprovide an application to power futures pricing.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1112.4740v3", "link": "http://arxiv.org/abs/1112.4740v3"}, {"title": "Modeling Institutional Credit Risk with Financial News", "summary": "Credit risk management, the practice of mitigating losses by understanding\nthe adequacy of a borrower's capital and loan loss reserves, has long been\nimperative to any financial institution's long-term sustainability and growth.\nMassMutual is no exception. The company is keen on effectively monitoring\ndowngrade risk, or the risk associated with the event when credit rating of a\ncompany deteriorates. Current work in downgrade risk modeling depends on\nmultiple variations of quantitative measures provided by third-party rating\nagencies and risk management consultancy companies. As these structured\nnumerical data become increasingly commoditized among institutional investors,\nthere has been a wide push into using alternative sources of data, such as\nfinancial news, earnings call transcripts, or social media content, to possibly\ngain a competitive edge in the industry. The volume of qualitative information\nor unstructured text data has exploded in the past decades and is now available\nfor due diligence to supplement quantitative measures of credit risk. This\npaper proposes a predictive downgrade model using solely news data represented\nby neural network embeddings. The model standalone achieves an Area Under the\nReceiver Operating Characteristic Curve (AUC) of more than 80 percent. The\noutput probability from this news model, as an additional feature, improves the\nperformance of our benchmark model using only quantitative measures by more\nthan 5 percent in terms of both AUC and recall rate. A qualitative evaluation\nalso indicates that news articles related to our predicted downgrade events are\nspecially relevant and high-quality in our business context.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/2004.08204v1", "link": "http://arxiv.org/abs/2004.08204v1"}, {"title": "A note on arbitrage, approximate arbitrage and the fundamental theorem\n  of asset pricing", "summary": "We provide a critical analysis of the proof of the fundamental theorem of\nasset pricing given in the paper \"Arbitrage and approximate arbitrage: the\nfundamental theorem of asset pricing\" by B. Wong and C.C. Heyde (Stochastics,\n2010) in the context of incomplete It\\^o-process models. We show that their\napproach can only work in the known case of a complete financial market model\nand give an explicit counterexample.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1311.7027v1", "link": "http://dx.doi.org/10.1080/17442508.2014.895358"}, {"title": "Stochastic Cellular Automata Model for Stock Market Dynamics", "summary": "In the present work we introduce a stochastic cellular automata model in\norder to simulate the dynamics of the stock market. A direct percolation method\nis used to create a hierarchy of clusters of active traders on a two\ndimensional grid. Active traders are characterised by the decision to buy,\n(+1), or sell, (-1), a stock at a certain discrete time step. The remaining\ncells are inactive,(0). The trading dynamics is then determined by the\nstochastic interaction between traders belonging to the same cluster. Most of\nthe stylized aspects of the financial market time series are reproduced by the\nmodel.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0311372v2", "link": "http://dx.doi.org/10.1103/PhysRevE.69.046112"}, {"title": "Maximising Survival, Growth, and Goal Reaching Under Borrowing\n  Constraints", "summary": "In this paper, we consider three problems related to survival, growth, and\ngoal reaching maximization of an investment portfolio with proportional net\ncash flow. We solve the problems in a market constrained due to borrowing\nprohibition. To solve the problems, we first construct an auxiliary market and\nthen apply the dynamic programming approach. Via our solutions, an alternative\napproach is introduced in order to solve the problems defined under an\nauxiliary market.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1209.6385v1", "link": "http://arxiv.org/abs/1209.6385v1"}, {"title": "Stock market integration in the Latin American markets: further evidence\n  from nonlinear modeling", "summary": "This article studies the financial integration between the six main Latin\nAmerican markets and the US market in a nonlinear framework. Using the\nthreshold cointegration techniques of Hansen and Seo (2002), we show\nsignificant threshold stock market linkages between Mexico, Chile and the US.\nThus, the dynamics of these markets depends simultaneously on local and global\nrisk factors. More importantly, our results show an on-off threshold financial\nintegration process that is activated only when the stock price adjustment\nexceeds some level.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0905.3874v1", "link": "http://arxiv.org/abs/0905.3874v1"}, {"title": "Option pricing with linear market impact and non-linear Black and\n  Scholes equations", "summary": "We consider a model of linear market impact, and address the problem of\nreplicating a contingent claim in this framework. We derive a non-linear\nBlack-Scholes Equation that provides an exact replication strategy.\n  This equation is fully non-linear and singular, but we show that it is well\nposed, and we prove existence of smooth solutions for a large class of final\npayoffs, both for constant and local volatility. To obtain regularity of the\nsolutions, we develop an original method based on Legendre transforms.\n  The close connections with the problem of hedging with it gamma constraints\nstudied by Cheridito, Soner and Touzi and with the problem of hedging under it\nliquidity costs are discussed.\n  We also derive a modified Black-Scholes formula valid for asymptotically\nsmall impact parameter, and finally provide numerical simulations as an\nillustration.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1301.6252v3", "link": "http://arxiv.org/abs/1301.6252v3"}, {"title": "Generalized Autoregressive Score Models in R: The GAS Package", "summary": "This paper presents the R package GAS for the analysis of time series under\nthe Generalized Autoregressive Score (GAS) framework of Creal et al. (2013) and\nHarvey (2013). The distinctive feature of the GAS approach is the use of the\nscore function as the driver of time-variation in the parameters of nonlinear\nmodels. The GAS package provides functions to simulate univariate and\nmultivariate GAS processes, estimate the GAS parameters and to make time series\nforecasts. We illustrate the use of the GAS package with a detailed case study\non estimating the time-varying conditional densities of a set of financial\nassets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1609.02354v1", "link": "http://arxiv.org/abs/1609.02354v1"}, {"title": "Bias and Consistency in Three-way Gravity Models", "summary": "We study the incidental parameter problem in \"three-way\" Poisson\nPseudo-Maximum Likelihood (\"PPML\") gravity models recently recommended for\nidentifying the effects of trade policies and in other network panel data\nsettings. Despite the number and variety of fixed effects this model entails,\nwe confirm it is consistent for small $T$ and we show it is in fact the only\nestimator among a wide range of PML gravity estimators that is generally\nconsistent in this context when $T$ is small. At the same time, asymptotic\nconfidence intervals in fixed-$T$ panels are not correctly centered at the true\npoint estimates, and cluster-robust variance estimates used to construct\nstandard errors are generally biased as well. We characterize each of these\nbiases analytically and show both numerically and empirically that they are\nsalient even for real-data settings with a large number of countries. We also\noffer practical remedies that can be used to obtain more reliable inferences of\nthe effects of trade policies and other time-varying gravity variables.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.01327v3", "link": "http://arxiv.org/abs/1909.01327v3"}, {"title": "The Financial Bubble Experiment: advanced diagnostics and forecasts of\n  bubble terminations", "summary": "On 2 November 2009, the Financial Bubble Experiment was launched within the\nFinancial Crisis Observatory (FCO) at ETH Zurich\n(\\url{http://www.er.ethz.ch/fco/}). In that initial report, we diagnosed and\nannounced three bubbles on three different assets. In this latest release of 23\nDecember 2009 in this ongoing experiment, we add a diagnostic of a new bubble\ndeveloping on a fourth asset.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0911.0454v4", "link": "http://arxiv.org/abs/0911.0454v4"}, {"title": "Significance of log-periodic signatures in cumulative noise", "summary": "Using methods introduced by Scargle in 1978 we derive a cumulative version of\nthe Lomb periodogram that exhibits frequency independent statistics when\napplied to cumulative noise. We show how this cumulative Lomb periodogram\nallows us to estimate the significance of log-periodic signatures in the S&P\n500 anti-bubble that started in August 2000.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0302507v3", "link": "http://dx.doi.org/10.1088/1469-7688/3/5/303"}, {"title": "Weighted Envy-Freeness in Indivisible Item Allocation", "summary": "We introduce and analyze new envy-based fairness concepts for agents with\nweights that quantify their entitlements in the allocation of indivisible\nitems. We propose two variants of weighted envy-freeness up to one item (WEF1):\nstrong, where the envy can be eliminated by removing an item from the envied\nagent's bundle, and weak, where the envy can be eliminated either by removing\nan item as in the strong version or by replicating an item from the envied\nagent's bundle in the envying agent's bundle. We prove that for additive\nvaluations, an allocation that is both Pareto optimal and strongly WEF1 always\nexists; however, an allocation that maximizes the weighted Nash social welfare\nmay not be strongly WEF1 but always satisfies the weak version of the property.\nMoreover, we establish that a generalization of the round-robin picking\nsequence algorithm produces in polynomial time a strongly WEF1 allocation for\nan arbitrary number of agents; for two agents, we can efficiently achieve both\nstrong WEF1 and Pareto optimality by adapting the adjusted winner procedure.\nOur work exhibits several aspects in which weighted fair division is richer and\nmore challenging than its unweighted counterpart.", "category": [], "id": "http://arxiv.org/abs/1909.10502v5", "link": "http://arxiv.org/abs/1909.10502v5"}, {"title": "Historical risk measures on stock market indices and energy markets", "summary": "In this paper we look at the efficacy of different risk measures on energy\nmarkets and across several different stock market indices. We use both the\nValue at Risk and the Tail Conditional Expectation on each of these data sets.\nWe also consider several different durations and levels for historical risk\nmeasures. Through our results we make some recommendations for a robust risk\nmanagement strategy that involves historical risk measures.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1111.4421v1", "link": "http://arxiv.org/abs/1111.4421v1"}, {"title": "Costs Models in Design and Manufacturing of Sand Casting Products", "summary": "In the early phases of the product life cycle, the costs controls became a\nmajor decision tool in the competitiveness of the companies due to the world\ncompetition. After defining the problems related to this control difficulties,\nwe will present an approach using a concept of cost entity related to the\ndesign and realization activities of the product. We will try to apply this\napproach to the fields of the sand casting foundry. This work will highlight\nthe enterprise modelling difficulties (limits of a global cost modelling) and\nsome specifics limitations of the tool used for this development. Finally we\nwill discuss on the limits of a generic approach.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1011.5716v1", "link": "http://arxiv.org/abs/1011.5716v1"}, {"title": "The Economic Complexity of US Metropolitan Areas", "summary": "We calculate measures of economic complexity for US metropolitan areas for\nthe years 2007-2015 based on industry employment data. We show that the concept\nof economic complexity translates well from the cross-country to the regional\nsetting, and is able to incorporate local as well as traded industries. The\nlargest cities and the Northeast of the US have the highest average complexity,\nwhile traded industries are more complex than local-serving ones on average,\nbut with some exceptions. On average, regions with higher complexity have a\nhigher income per capita, but those regions also were more affected by the\nfinancial crisis. Finally, economic complexity is a significant predictor of\nwithin-decreases in income per capita and population. Our findings highlight\nthe importance of subnational regions, and particularly metropolitan areas, as\nunits of economic geography.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.08112v1", "link": "http://arxiv.org/abs/1901.08112v1"}, {"title": "Optimal High Frequency Trading with limit and market orders", "summary": "We propose a framework for studying optimal market making policies in a limit\norder book (LOB). The bid-ask spread of the LOB is modelled by a Markov chain\nwith finite values, multiple of the tick size, and subordinated by the Poisson\nprocess of the tick-time clock. We consider a small agent who continuously\nsubmits limit buy/sell orders and submits market orders at discrete dates. The\nobjective of the market maker is to maximize her expected utility from revenue\nover a short term horizon by a tradeoff between limit and market orders, while\ncontrolling her inventory position. This is formulated as a mixed regime\nswitching regular/ impulse control problem that we characterize in terms of\nquasi-variational system by dynamic programming methods. In the case of a\nmean-variance criterion with martingale reference price or when the asset price\nfollows a Levy process and with exponential utility criterion, the dynamic\nprogramming system can be reduced to a system of simple equations involving\nonly the inventory and spread variables. Calibration procedures are derived for\nestimating the transition matrix and intensity parameters for the spread and\nfor Cox processes modelling the execution of limit orders. Several\ncomputational tests are performed both on simulated and real data, and\nillustrate the impact and profit when considering execution priority in limit\norders and market orders", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1106.5040v1", "link": "http://arxiv.org/abs/1106.5040v1"}, {"title": "Application of noise level estimation for portfolio optimization", "summary": "Time changes of noise level at Warsaw Stock Market are analyzed using a\nrecently developed method basing on properties of the coarse grained entropy.\nThe condition of the minimal noise level is used to build an efficient\nportfolio. Our noise level approach seems to be a much better tool for risk\nestimations than standard volatility parameters. Implementation of a\ncorresponding threshold investment strategy gives positive returns for\nhistorical data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0503242v1", "link": "http://arxiv.org/abs/physics/0503242v1"}, {"title": "Studies of the limit order book around large price changes", "summary": "We study the dynamics of the limit order book of liquid stocks after\nexperiencing large intra-day price changes. In the data we find large\nvariations in several microscopical measures, e.g., the volatility the bid-ask\nspread, the bid-ask imbalance, the number of queuing limit orders, the activity\n(number and volume) of limit orders placed and canceled, etc. The relaxation of\nthe quantities is generally very slow that can be described by a power law of\nexponent $\\approx0.4$. We introduce a numerical model in order to understand\nthe empirical results better. We find that with a zero intelligence deposition\nmodel of the order flow the empirical results can be reproduced qualitatively.\nThis suggests that the slow relaxations might not be results of agents'\nstrategic behaviour. Studying the difference between the exponents found\nempirically and numerically helps us to better identify the role of strategic\nbehaviour in the phenomena.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0901.0495v2", "link": "http://dx.doi.org/10.1140/epjb/e2009-00297-9"}, {"title": "Nonparametric instrumental variable regression and quantile regression\n  with full independence", "summary": "The problem of endogeneity in statistics and econometrics is often handled by\nintroducing instrumental variables (IV) which are assumed to be mean\nindependent of some regressors or other observables. When full independence of\nIV's and observables is assumed, nonparametric IV regression models and\nnonparametric demand models lead to nonlinear integral equations with unknown\nintegral kernels. We prove convergence rates for the mean integrated square\nerror of the iteratively regularized Newton method applied to these problems.\nCompared to related results we derive stronger convergence results that rely on\nweaker nonlinearity restrictions. We demonstrate in numerical simulations for a\nnonparametric IV regression that the method produces better results than the\nstandard model.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1511.03977v2", "link": "http://arxiv.org/abs/1511.03977v2"}, {"title": "Tail behavior of sums and differences of log-normal random variables", "summary": "We present sharp tail asymptotics for the density and the distribution\nfunction of linear combinations of correlated log-normal random variables, that\nis, exponentials of components of a correlated Gaussian vector. The asymptotic\nbehavior turns out to depend on the correlation between the components, and the\nexplicit solution is found by solving a tractable quadratic optimization\nproblem. These results can be used either to approximate the probability of\ntail events directly, or to construct variance reduction procedures to estimate\nthese probabilities by Monte Carlo methods. In particular, we propose an\nefficient importance sampling estimator for the left tail of the distribution\nfunction of the sum of log-normal variables. As a corollary of the tail\nasymptotics, we compute the asymptotics of the conditional law of a Gaussian\nrandom vector given a linear combination of exponentials of its components. In\nrisk management applications, this finding can be used for the systematic\nconstruction of stress tests, which the financial institutions are required to\nconduct by the regulators. We also characterize the asymptotic behavior of the\nValue at Risk for log-normal portfolios in the case where the confidence level\ntends to one.", "category": ["math.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1309.3057v2", "link": "http://dx.doi.org/10.3150/14-BEJ665"}, {"title": "Modeling Spatial Equilibrium in Cities: the Isobenefit Lines", "summary": "I propose and briefly define the concept of Urban Isobenefit Lines by using\nfunctions as easy as efficient, whose results can offer a rich tool to use into\nspatial equilibrium analysis involving cities. They are line joining urban\npoints with equal level of positional advantage from city amenities. The\nresults which one obtain by implementing a chosen function, gave specific\nscenarios: numerically described by indicators and graphically visualized by\nefficient city matrix views. This is also a theoretical concept for the Urban\nEconomics theory and Spatial Equilibrium analysis in cities.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1210.4461v3", "link": "http://arxiv.org/abs/1210.4461v3"}, {"title": "Natural Experiments", "summary": "The term natural experiment is used inconsistently. In one interpretation, it\nrefers to an experiment where a treatment is randomly assigned by someone other\nthan the researcher. In another interpretation, it refers to a study in which\nthere is no controlled random assignment, but treatment is assigned by some\nexternal factor in a way that loosely resembles a randomized experiment---often\ndescribed as an \"as if random\" assignment. In yet another interpretation, it\nrefers to any non-randomized study that compares a treatment to a control\ngroup, without any specific requirements on how the treatment is assigned. I\nintroduce an alternative definition that seeks to clarify the integral features\nof natural experiments and at the same time distinguish them from randomized\ncontrolled experiments. I define a natural experiment as a research study where\nthe treatment assignment mechanism (i) is neither designed nor implemented by\nthe researcher, (ii) is unknown to the researcher, and (iii) is probabilistic\nby virtue of depending on an external factor. The main message of this\ndefinition is that the difference between a randomized controlled experiment\nand a natural experiment is not a matter of degree, but of essence, and thus\nconceptualizing a natural experiment as a research design akin to a randomized\nexperiment is neither rigorous nor a useful guide to empirical analysis. Using\nmy alternative definition, I discuss how a natural experiment differs from a\ntraditional observational study, and offer practical recommendations for\nresearchers who wish to use natural experiments to study causal effects.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.00202v1", "link": "http://arxiv.org/abs/2002.00202v1"}, {"title": "A New Characterization of Comonotonicity and its Application in\n  Behavioral Finance", "summary": "It is well-known that an $\\mathbb{R}$-valued random vector $(X_1, X_2,\n\\cdots, X_n)$ is comonotonic if and only if $(X_1, X_2, \\cdots, X_n)$ and\n$(Q_1(U), Q_2(U),\\cdots, Q_n(U))$ coincide \\emph{in distribution}, for\n\\emph{any} random variable $U$ uniformly distributed on the unit interval\n$(0,1)$, where $Q_k(\\cdot)$ are the quantile functions of $X_k$, $k=1,2,\\cdots,\nn$. It is natural to ask whether $(X_1, X_2, \\cdots, X_n)$ and $(Q_1(U),\nQ_2(U),\\cdots, Q_n(U))$ can coincide \\emph{almost surely} for \\emph{some}\nspecial $U$. In this paper, we give a positive answer to this question by\nconstruction. We then apply this result to a general behavioral investment\nmodel with a law-invariant preference measure and develop a universal framework\nto link the problem to its quantile formulation. We show that any optimal\ninvestment output should be anti-comonotonic with the market pricing kernel.\nUnlike previous studies, our approach avoids making the assumption that the\npricing kernel is atomless, and consequently, we overcome one of the major\ndifficulties encountered when one considers behavioral economic equilibrium\nmodels in which the pricing kernel is a yet-to-be-determined unknown random\nvariable. The method is applicable to many other models such as risk sharing\nmodel.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1311.6080v7", "link": "http://arxiv.org/abs/1311.6080v7"}, {"title": "De-biased Machine Learning for Compliers", "summary": "Instrumental variable identification is a concept in causal statistics for\nestimating the counterfactual effect of treatment D on output Y controlling for\ncovariates X using observational data. Even when measurements of (Y,D) are\nconfounded, the treatment effect on the subpopulation of compliers can\nnonetheless be identified if an instrumental variable Z is available, which is\nindependent of (Y,D) conditional on X and the unmeasured confounder. We\nintroduce a de-biased machine learning (DML) approach to estimating complier\nparameters with high-dimensional data. Complier parameters include local\naverage treatment effect, average complier characteristics, and complier\ncounterfactual outcome distributions. In our approach, the de-biasing is itself\nperformed by machine learning, a variant called de-biased machine learning via\nregularized Riesz representers (DML-RRR). We prove our estimator is consistent,\nasymptotically normal, and semi-parametrically efficient. In experiments, our\nestimator outperforms state of the art alternatives. We use it to estimate the\neffect of 401(k) participation on the distribution of net financial assets.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.05244v1", "link": "http://arxiv.org/abs/1909.05244v1"}, {"title": "An Optimal Extraction Problem with Price Impact", "summary": "A price-maker company extracts an exhaustible commodity from a reservoir, and\nsells it instantaneously in the spot market. In absence of any actions of the\ncompany, the commodity's spot price evolves either as a drifted Brownian motion\nor as an Ornstein-Uhlenbeck process. While extracting, the company affects the\nmarket price of the commodity, and its actions have an impact on the dynamics\nof the commodity's spot price. The company aims at maximizing the total\nexpected profits from selling the commodity, net of the total expected\nproportional costs of extraction. We model this problem as a two-dimensional\ndegenerate singular stochastic control problem with finite fuel. To determine\nits solution, we construct an explicit solution to the associated\nHamilton-Jacobi-Bellman equation, and then verify its actual optimality through\na verification theorem. On the one hand, when the (uncontrolled) price is a\ndrifted Brownian motion, it is optimal to extract whenever the current price\nlevel is larger or equal than an endogenously determined constant threshold. On\nthe other hand, when the (uncontrolled) price evolves as an Ornstein-Uhlenbeck\nprocess, we show that the optimal extraction rule is triggered by a curve\ndepending on the current level of the reservoir. Such a curve is a strictly\ndecreasing $C^{\\infty}$-function for which we are able to provide an explicit\nexpression. Finally, our study is complemented by a theoretical and numerical\nanalysis of the dependency of the optimal extraction strategy and value\nfunction on the model's parameters.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1812.01270v1", "link": "http://arxiv.org/abs/1812.01270v1"}, {"title": "Some Statistical Problems with High Dimensional Financial data", "summary": "For high dimensional data, some of the standard statistical techniques do not\nwork well. So modification or further development of statistical methods are\nnecessary. In this paper, we explore these modifications. We start with the\nimportant problem of estimating high dimensional covariance matrix. Then we\nexplore some of the important statistical techniques such as high dimensional\nregression, principal component analysis, multiple testing problems and\nclassification. We describe some of the fast algorithms that can be readily\napplied in practice.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1808.02953v1", "link": "http://arxiv.org/abs/1808.02953v1"}, {"title": "Dynamic instability in a phenomenological model of correlated assets", "summary": "We show that financial correlations exhibit a non-trivial dynamic behavior.\nWe introduce a simple phenomenological model of a multi-asset financial market,\nwhich takes into account the impact of portfolio investment on price dynamics.\nThis captures the fact that correlations determine the optimal portfolio but\nare affected by investment based on it. We show that such a feedback on\ncorrelations gives rise to an instability when the volume of investment exceeds\na critical value. Close to the critical point the model exhibits dynamical\ncorrelations very similar to those observed in real markets. Maximum likelihood\nestimates of the model's parameter for empirical data indeed confirm this\nconclusion, thus suggesting that real markets operate close to a dynamically\nunstable point.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0508159v3", "link": "http://dx.doi.org/10.1088/1742-5468/2006/08/L08001"}, {"title": "Sparse Bayesian time-varying covariance estimation in many dimensions", "summary": "We address the curse of dimensionality in dynamic covariance estimation by\nmodeling the underlying co-volatility dynamics of a time series vector through\nlatent time-varying stochastic factors. The use of a global-local shrinkage\nprior for the elements of the factor loadings matrix pulls loadings on\nsuperfluous factors towards zero. To demonstrate the merits of the proposed\nframework, the model is applied to simulated data as well as to daily\nlog-returns of 300 S&P 500 members. Our approach yields precise correlation\nestimates, strong implied minimum variance portfolio performance and superior\nforecasting accuracy in terms of log predictive scores when compared to typical\nbenchmarks.", "category": ["econ.EM", "q-fin.PM"], "id": "http://arxiv.org/abs/1608.08468v3", "link": "http://dx.doi.org/10.1016/j.jeconom.2018.11.007"}, {"title": "Forecasts with Bayesian vector autoregressions under real time\n  conditions", "summary": "This paper investigates the sensitivity of forecast performance measures to\ntaking a real time versus pseudo out-of-sample perspective. We use monthly\nvintages for the United States (US) and the Euro Area (EA) and estimate a set\nof vector autoregressive (VAR) models of different sizes with constant and\ntime-varying parameters (TVPs) and stochastic volatility (SV). Our results\nsuggest differences in the relative ordering of model performance for point and\ndensity forecasts depending on whether real time data or truncated final\nvintages in pseudo out-of-sample simulations are used for evaluating forecasts.\nNo clearly superior specification for the US or the EA across variable types\nand forecast horizons can be identified, although larger models featuring TVPs\nappear to be affected the least by missing values and data revisions. We\nidentify substantial differences in performance metrics with respect to whether\nforecasts are produced for the US or the EA.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.04984v1", "link": "http://arxiv.org/abs/2004.04984v1"}, {"title": "Fractional trends and cycles in macroeconomic time series", "summary": "We develop a generalization of correlated trend-cycle decompositions that\navoids prior assumptions about the long-run dynamic characteristics by\nmodelling the permanent component as a fractionally integrated process and\nincorporating a fractional lag operator into the autoregressive polynomial of\nthe cyclical component. We relate the model to the Beveridge-Nelson\ndecomposition and derive a modified Kalman filter estimator for the fractional\ncomponents. Identification and consistency of the maximum likelihood estimator\nare shown. For US macroeconomic data we demonstrate that, unlike non-fractional\ncorrelated unobserved components models, the new model estimates a smooth trend\ntogether with a cycle hitting all NBER recessions.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2005.05266v1", "link": "http://arxiv.org/abs/2005.05266v1"}, {"title": "Tracking the circulation routes of fresh coins in Bitcoin: A way of\n  identifying coin miners with transaction network structural properties", "summary": "Bitcoin draws the highest degree of attention among cryptocurrencies, while\ncoin mining is one of the most important fashion of profiting in the Bitcoin\necosystem. This paper constructs fresh coin circulation networks by tracking\nthe fresh coin transfer routes with transaction referencing in Bitcoin\nblockchain. This paper proposes a heuristic algorithm to identifying coin\nminers by comparing coin circulation networks from different mining pools and\nthereby inferring the common profit distribution schemes of Bitcoin mining\npools. Furthermore, this paper characterizes the increasing trend of Bitcoin\nminer numbers during recent years.", "category": ["q-fin.GN", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.06400v1", "link": "http://dx.doi.org/10.13878/j.cnki.jnuist.2018.04.009"}, {"title": "Testing the weak-form efficiency of the WTI crude oil futures market", "summary": "We perform detrending moving average analysis (DMA) and detrended fluctuation\nanalysis (DFA) of the WTI crude oil futures prices (1983-2012) to investigate\nits efficiency. We further put forward a strict statistical test in the spirit\nof bootstrapping to verify the weak-form market efficiency hypothesis by\nemploying the DMA (or DFA) exponent as the statistic. We verify the weak-form\nefficiency of the crude oil futures market when the whole period is considered.\nWhen we break the whole series into three sub-series separated by the outbreaks\nof the Gulf War and the Iraq War, our statistical tests uncover that only the\nGulf War has the impact of reducing the efficiency of the crude oil market. If\nwe split the whole time series into two sub-series based on the signing date of\nthe North American Free Trade Agreement, we find that the market is inefficient\nin the sub-periods during which the Gulf War broke out. We also perform the\nsame analysis on short time series in moving windows and find that the market\nis inefficient only when some turbulent events occur, such as the oil price\ncrash in 1985, the Gulf war, and the oil price crash in 2008. Our analysis may\noffer a new understanding of the efficiency of the crude oil futures market and\nshed new lights on the investigation of the efficiency in other financial\nmarkets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1211.4686v1", "link": "http://dx.doi.org/10.1016/j.physa.2014.02.042"}, {"title": "Cumulant Approach of Arbitrary Truncated Levy Flight", "summary": "The problem of an arbitrary truncated Levy flight description using the\nmethod of cumulant approach has been solved. The set of cumulants of the\ntruncated Levy distribution given the assumption of arbitrary truncation has\nbeen found. The influence of truncation shape on the truncated Levy flight\nproperties in the Gaussian and the Levy regimes has been investigated.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1006.2489v5", "link": "http://dx.doi.org/10.1016/j.physa.2010.09.014"}, {"title": "Markets are efficient if and only if P = NP", "summary": "I prove that if markets are weak-form efficient, meaning current prices fully\nreflect all information available in past prices, then P = NP, meaning every\ncomputational problem whose solution can be verified in polynomial time can\nalso be solved in polynomial time. I also prove the converse by showing how we\ncan \"program\" the market to solve NP-complete problems. Since P probably does\nnot equal NP, markets are probably not efficient. Specifically, markets become\nincreasingly inefficient as the time series lengthens or becomes more frequent.\nAn illustration by way of partitioning the excess returns to momentum\nstrategies based on data availability confirms this prediction.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1002.2284v2", "link": "http://arxiv.org/abs/1002.2284v2"}, {"title": "Exact fit of simple finite mixture models", "summary": "How to forecast next year's portfolio-wide credit default rate based on last\nyear's default observations and the current score distribution? A classical\napproach to this problem consists of fitting a mixture of the conditional score\ndistributions observed last year to the current score distribution. This is a\nspecial (simple) case of a finite mixture model where the mixture components\nare fixed and only the weights of the components are estimated. The optimum\nweights provide a forecast of next year's portfolio-wide default rate. We point\nout that the maximum-likelihood (ML) approach to fitting the mixture\ndistribution not only gives an optimum but even an exact fit if we allow the\nmixture components to vary but keep their density ratio fix. From this\nobservation we can conclude that the standard default rate forecast based on\nlast year's conditional default rates will always be located between last\nyear's portfolio-wide default rate and the ML forecast for next year. As an\napplication example, then cost quantification is discussed. We also discuss how\nthe mixture model based estimation methods can be used to forecast total loss.\nThis involves the reinterpretation of an individual classification problem as a\ncollective quantification problem.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1406.6038v2", "link": "http://dx.doi.org/10.3390/jrfm7040150"}, {"title": "Optimal excess-of-loss reinsurance for stochastic factor risk models", "summary": "We study the optimal excess-of-loss reinsurance problem when both the\nintensity of the claims arrival process and the claim size distribution are\ninfluenced by an exogenous stochastic factor. We assume that the insurer's\nsurplus is governed by a marked point process with dual-predictable projection\naffected by an environmental factor and that the insurance company can borrow\nand invest money at a constant real-valued risk-free interest rate $r$. Our\nmodel allows for stochastic risk premia, which take into account risk\nfluctuations. Using stochastic control theory based on the\nHamilton-Jacobi-Bellman equation, we analyze the optimal reinsurance strategy\nunder the criterion of maximizing the expected exponential utility of the\nterminal wealth. A verification theorem for the value function in terms of\nclassical solutions of a backward partial differential equation is provided.\nFinally, some numerical results are discussed.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1904.05422v1", "link": "http://arxiv.org/abs/1904.05422v1"}, {"title": "Global Value Trees", "summary": "The fragmentation of production across countries has become an important\nfeature of the globalization in recent decades and is often conceptualized by\nthe term, global value chains (GVCs). When empirically investigating the GVCs,\nprevious studies are mainly interested in knowing how global the GVCs are\nrather than how the GVCs look like. From a complex networks perspective, we use\nthe World Input-Output Database (WIOD) to study the global production system.\nWe find that the industry-level GVCs are indeed not chain-like but are better\ncharacterized by the tree topology. Hence, we compute the global value trees\n(GVTs) for all the industries available in the WIOD. Moreover, we compute an\nindustry importance measure based on the GVTs and compare it with other network\ncentrality measures. Finally, we discuss some future applications of the GVTs.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1410.4694v2", "link": "http://dx.doi.org/10.1371/journal.pone.0126699"}, {"title": "Wavelet-based discrimination of isolated singularities masquerading as\n  multifractals in detrended fluctuation analyses", "summary": "The robustness of two widespread multifractal analysis methods, one based on\ndetrended fluctuation analysis and one on wavelet leaders, is discussed in the\ncontext of time-series containing non-uniform structures with only isolated\nsingularities. Signals generated by simulated and experimentally-realized chaos\ngenerators, together with synthetic data addressing particular aspects, are\ntaken into consideration. The results reveal essential limitations affecting\nthe ability of both methods to correctly infer the non-multifractal nature of\nsignals devoid of a cascade-like hierarchy of singularities. Namely, signals\nharboring only isolated singularities are found to artefactually give rise to\nbroad multifractal spectra, resembling those expected in the presence of a\nwell-developed underlying multifractal structure. Hence, there is a real risk\nof incorrectly inferring multifractality due to isolated singularities. The\ncareful consideration of local scaling properties and the distribution of\nH\\\"older exponent obtained, for example, through wavelet analysis, is\nindispensable for rigorously assessing the presence or absence of\nmultifractality.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2004.03319v1", "link": "http://dx.doi.org/10.1007/s11071-020-05581-y"}, {"title": "The AI Economist: Improving Equality and Productivity with AI-Driven Tax\n  Policies", "summary": "Tackling real-world socio-economic challenges requires designing and testing\neconomic policies. However, this is hard in practice, due to a lack of\nappropriate (micro-level) economic data and limited opportunity to experiment.\nIn this work, we train social planners that discover tax policies in dynamic\neconomies that can effectively trade-off economic equality and productivity. We\npropose a two-level deep reinforcement learning approach to learn dynamic tax\npolicies, based on economic simulations in which both agents and a government\nlearn and adapt. Our data-driven approach does not make use of economic\nmodeling assumptions, and learns from observational data alone. We make four\nmain contributions. First, we present an economic simulation environment that\nfeatures competitive pressures and market dynamics. We validate the simulation\nby showing that baseline tax systems perform in a way that is consistent with\neconomic theory, including in regard to learned agent behaviors and\nspecializations. Second, we show that AI-driven tax policies improve the\ntrade-off between equality and productivity by 16% over baseline policies,\nincluding the prominent Saez tax framework. Third, we showcase several emergent\nfeatures: AI-driven tax policies are qualitatively different from baselines,\nsetting a higher top tax rate and higher net subsidies for low incomes.\nMoreover, AI-driven tax policies perform strongly in the face of emergent\ntax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are\nalso effective when used in experiments with human participants. In experiments\nconducted on MTurk, an AI tax policy provides an equality-productivity\ntrade-off that is similar to that provided by the Saez framework along with\nhigher inverse-income weighted social welfare.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.13332v1", "link": "http://arxiv.org/abs/2004.13332v1"}, {"title": "Modeling of waiting times and price changes in currency exchange data", "summary": "A theory which describes the share price evolution at financial markets as a\ncontinuous-time random walk has been generalized in order to take into account\nthe dependence of waiting times t on price returns x. A joint probability\ndensity function (pdf) which uses the concept of a L\\'{e}vy stable distribution\nis worked out. The theory is fitted to high-frequency US$/Japanese Yen exchange\nrate and low-frequency 19th century Irish stock data. The theory has been\nfitted both to price return and to waiting time data and the adherence to data,\nin terms of the chi-squared test statistic, has been improved when compared to\nthe old theory.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0310351v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.06.162"}, {"title": "Bayesian shrinkage in mixture of experts models: Identifying robust\n  determinants of class membership", "summary": "A method for implicit variable selection in mixture of experts frameworks is\nproposed. We introduce a prior structure where information is taken from a set\nof independent covariates. Robust class membership predictors are identified\nusing a normal gamma prior. The resulting model setup is used in a finite\nmixture of Bernoulli distributions to find homogenous clusters of women in\nMozambique based on their information sources on HIV. Fully Bayesian inference\nis carried out via the implementation of a Gibbs sampler.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1809.04853v2", "link": "http://arxiv.org/abs/1809.04853v2"}, {"title": "Stock Price Prediction Using Convolutional Neural Networks on a\n  Multivariate Timeseries", "summary": "Prediction of future movement of stock prices has been a subject matter of\nmany research work. In this work, we propose a hybrid approach for stock price\nprediction using machine learning and deep learning-based methods. We select\nthe NIFTY 50 index values of the National Stock Exchange of India, over a\nperiod of four years, from January 2015 till December 2019. Based on the NIFTY\ndata during the said period, we build various predictive models using machine\nlearning approaches, and then use those models to predict the Close value of\nNIFTY 50 for the year 2019, with a forecast horizon of one week. For predicting\nthe NIFTY index movement patterns, we use a number of classification methods,\nwhile for forecasting the actual Close values of NIFTY index, various\nregression models are built. We, then, augment our predictive power of the\nmodels by building a deep learning-based regression model using Convolutional\nNeural Network with a walk-forward validation. The CNN model is fine-tuned for\nits parameters so that the validation loss stabilizes with increasing number of\niterations, and the training and validation accuracies converge. We exploit the\npower of CNN in forecasting the future NIFTY index values using three\napproaches which differ in number of variables used in forecasting, number of\nsub-models used in the overall models and, size of the input data for training\nthe models. Extensive results are presented on various metrics for all\nclassification and regression models. The results clearly indicate that\nCNN-based multivariate forecasting model is the most effective and accurate in\npredicting the movement of NIFTY index values with a weekly forecast horizon.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/2001.09769v1", "link": "http://arxiv.org/abs/2001.09769v1"}, {"title": "Yield to maturity modelling and a Monte Carlo Technique for pricing\n  Derivatives on Constant Maturity Treasury (CMT) and Derivatives on forward\n  Bonds", "summary": "This paper proposes a Monte Carlo technique for pricing the forward yield to\nmaturity, when the volatility of the zero-coupon bond is known. We make the\nassumption of deterministic default intensity (Hazard Rate Function). We make\nno assumption on the volatility of the yield. We actually calculate the initial\nvalue of the forward yield, we calculate the volatility of the yield, and we\nwrite the diffusion of the yield. As direct application we price options on\nConstant Maturity Treasury (CMT) in the Hull and White Model for the short\ninterest rate. Tests results with Caps and Floors on 10 years constant maturity\ntreasury (CMT10) are satisfactory. This work can also be used for pricing\noptions on bonds or forward bonds.", "category": ["q-fin.CP", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1204.4631v1", "link": "http://arxiv.org/abs/1204.4631v1"}, {"title": "Concentration of dynamic risk measures in a Brownian filtration", "summary": "Motivated by liquidity risk in mathematical finance, D. Lacker introduced\nconcentration inequalities for risk measures, i.e. upper bounds on the\n\\emph{liquidity risk profile} of a financial loss. We derive these inequalities\nin the case of time-consistent dynamic risk measures when the filtration is\nassumed to carry a Brownian motion. The theory of backward stochastic\ndifferential equations (BSDEs) and their dual formulation plays a crucial role\nin our analysis. Natural by-products of concentration of risk measures are a\ndescription of the tail behavior of the financial loss and transport-type\ninequalities in terms of the generator of the BSDE, which in the present case\ncan grow arbitrarily fast.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1805.09014v1", "link": "http://arxiv.org/abs/1805.09014v1"}, {"title": "Nash equilibria for game contingent claims with utility-based hedging", "summary": "Game contingent claims (GCCs) generalize American contingent claims by\nallowing the writer to recall the option as long as it is not exercised, at the\nprice of paying some penalty. In incomplete markets, an appealing approach is\nto analyze GCCs like their European and American counterparts by solving option\nholder's and writer's optimal investment problems in the underlying securities.\nBy this, partial hedging opportunities are taken into account. We extend\nresults in the literature by solving the stochastic game corresponding to GCCs\nwith both continuous time stopping and trading. Namely, we construct Nash\nequilibria by rewriting the game as a non-zero-sum stopping game in which\nplayers compare payoffs in terms of their exponential utility indifference\nvalues. As a by-product, we also obtain an existence result for the optimal\nexercise time of an American claim under utility indifference valuation by\nrelating it to the corresponding nonlinear Snell envelope.", "category": ["math.PR", "q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1707.09351v2", "link": "http://dx.doi.org/10.1137/17M1141059"}, {"title": "Implications of Correlated Default For Portfolio Allocation To Corporate\n  Bonds", "summary": "This article deals with the problem of optimal allocation of capital to\ncorporate bonds in fixed income portfolios when there is the possibility of\ncorrelated defaults. Using a multivariate normal Copula function for the joint\ndefault probabilities we show that retaining the first few moments of the\nportfolio default loss distribution gives an extremely good approximation to\nthe full solution of the asset allocation problem. We provide detailed results\non the convergence of the moment expansion and explore how the optimal\nportfolio allocation depends on recovery fractions, level of diversification\nand investment time horizon. Numerous numerical illustrations exhibit the\nresults for simple portfolios and utility functions.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/nlin/0209010v1", "link": "http://arxiv.org/abs/nlin/0209010v1"}, {"title": "Variational Bayesian Inference for Mixed Logit Models with Unobserved\n  Inter- and Intra-Individual Heterogeneity", "summary": "Variational Bayes (VB), a method originating from machine learning, enables\nfast and scalable estimation of complex probabilistic models. Thus far,\napplications of VB in discrete choice analysis have been limited to mixed logit\nmodels with unobserved inter-individual taste heterogeneity. However, such a\nmodel formulation may be too restrictive in panel data settings, since tastes\nmay vary both between individuals as well as across choice tasks encountered by\nthe same individual. In this paper, we derive a VB method for posterior\ninference in mixed logit models with unobserved inter- and intra-individual\nheterogeneity. In a simulation study, we benchmark the performance of the\nproposed VB method against maximum simulated likelihood (MSL) and Markov chain\nMonte Carlo (MCMC) methods in terms of parameter recovery, predictive accuracy\nand computational efficiency. The simulation study shows that VB can be a fast,\nscalable and accurate alternative to MSL and MCMC estimation, especially in\napplications in which fast predictions are paramount. VB is observed to be\nbetween 2.8 and 17.7 times faster than the two competing methods, while\naffording comparable or superior accuracy. Besides, the simulation study\ndemonstrates that a parallelised implementation of the MSL estimator with\nanalytical gradients is a viable alternative to MCMC in terms of both\nestimation accuracy and computational efficiency, as the MSL estimator is\nobserved to be between 0.9 and 2.1 times faster than MCMC.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.00419v3", "link": "http://arxiv.org/abs/1905.00419v3"}, {"title": "A Socioeconomic Well-Being Index", "summary": "An annual well-being index constructed from thirteen socioeconomic factors is\nproposed in order to dynamically measure the mood of the US citizenry.\nEconometric models are fitted to the log-returns of the index in order to\nquantify its tail risk and perform option pricing and risk budgeting. By\nproviding a statistically sound assessment of socioeconomic content, the index\nis consistent with rational finance theory, enabling the construction and\nvaluation of insurance-type financial instruments to serve as contracts written\nagainst it. Endogenously, the VXO volatility measure of the stock market\nappears to be the greatest contributor to tail risk. Exogenously,\n\"stress-testing\" the index against the politically important factors of trade\nimbalance and legal immigration, quantify the systemic risk. For probability\nlevels in the range of 5% to 10%, values of trade below these thresholds are\nassociated with larger downward movements of the index than for immigration at\nthe same level. The main intent of the index is to provide early-warning for\nnegative changes in the mood of citizens, thus alerting policy makers and\nprivate agents to potential future market downturns.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.01036v1", "link": "http://arxiv.org/abs/2001.01036v1"}, {"title": "Hedging of claims with physical delivery under convex transaction costs", "summary": "We study superhedging of contingent claims with physical delivery in a\ndiscrete-time market model with convex transaction costs. Our model extends\nKabanov's currency market model by allowing for nonlinear illiquidity effects.\nWe show that an appropriate generalization of Schachermayer's robust no\narbitrage condition implies that the set of claims hedgeable with zero cost is\nclosed in probability. Combined with classical techniques of convex analysis,\nthe closedness yields a dual characterization of premium processes that are\nsufficient to superhedge a given claim process. We also extend the fundamental\ntheorem of asset pricing for general conical models.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0810.2016v1", "link": "http://arxiv.org/abs/0810.2016v1"}, {"title": "Investment under uncertainty, competition and regulation", "summary": "We investigate a randomization procedure undertaken in real option games\nwhich can serve as a basic model of regulation in a duopoly model of preemptive\ninvestment. We recall the rigorous framework of [M. Grasselli, V. Lecl\\`ere and\nM. Ludkovsky, Priority Option: the value of being a leader, International\nJournal of Theoretical and Applied Finance, 16, 2013], and extend it to a\nrandom regulator. This model generalizes and unifies the different competitive\nframeworks proposed in the literature, and creates a new one similar to a\nStackelberg leadership. We fully characterize strategic interactions in the\nseveral situations following from the parametrization of the regulator.\nFinally, we study the effect of the coordination game and uncertainty of\noutcome when agents are risk-averse, providing new intuitions for the standard\ncase.", "category": ["q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1309.1844v3", "link": "http://arxiv.org/abs/1309.1844v3"}, {"title": "A Self-Attention Network for Hierarchical Data Structures with an\n  Application to Claims Management", "summary": "Insurance companies must manage millions of claims per year. While most of\nthese claims are non-fraudulent, fraud detection is core for insurance\ncompanies. The ultimate goal is a predictive model to single out the fraudulent\nclaims and pay out the non-fraudulent ones immediately. Modern machine learning\nmethods are well suited for this kind of problem. Health care claims often have\na data structure that is hierarchical and of variable length. We propose one\nmodel based on piecewise feed forward neural networks (deep learning) and\nanother model based on self-attention neural networks for the task of claim\nmanagement. We show that the proposed methods outperform bag-of-words based\nmodels, hand designed features, and models based on convolutional neural\nnetworks, on a data set of two million health care claims. The proposed\nself-attention method performs the best.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.10543v1", "link": "http://arxiv.org/abs/1808.10543v1"}, {"title": "Conditional Density Estimation with Neural Networks: Best Practices and\n  Benchmarks", "summary": "Given a set of empirical observations, conditional density estimation aims to\ncapture the statistical relationship between a conditional variable\n$\\mathbf{x}$ and a dependent variable $\\mathbf{y}$ by modeling their\nconditional probability $p(\\mathbf{y}|\\mathbf{x})$. The paper develops best\npractices for conditional density estimation for finance applications with\nneural networks, grounded on mathematical insights and empirical evaluations.\nIn particular, we introduce a noise regularization and data normalization\nscheme, alleviating problems with over-fitting, initialization and\nhyper-parameter sensitivity of such estimators. We compare our proposed\nmethodology with popular semi- and non-parametric density estimators, underpin\nits effectiveness in various benchmarks on simulated and Euro Stoxx 50 data and\nshow its superior performance. Our methodology allows to obtain high-quality\nestimators for statistical expectations of higher moments, quantiles and\nnon-linear return transformations, with very little assumptions about the\nreturn dynamic.", "category": ["q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/1903.00954v2", "link": "http://arxiv.org/abs/1903.00954v2"}, {"title": "A Note on Gale, Kuhn, and Tucker's Reductions of Zero-Sum Games", "summary": "Gale, Kuhn and Tucker (1950) introduced two ways to reduce a zero-sum game by\npackaging some strategies with respect to a probability distribution on them.\nIn terms of value, they gave conditions for a desirable reduction. We show that\na probability distribution for a desirable reduction relies on optimal\nstrategies in the original game. Also, we correct an improper example given by\nthem to show that the reverse of a theorem does not hold.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1710.02326v1", "link": "http://arxiv.org/abs/1710.02326v1"}, {"title": "Decreasing market value of variable renewables is a result of policy,\n  not variability", "summary": "Although recent studies have shown that electricity systems with shares of\nwind and solar above 80% can be affordable, economists have raised concerns\nabout market integration. Correlated generation from variable renewable sources\ndepresses market prices, which can cause wind and solar to cannibalize their\nown revenues and prevent them from covering their costs from the market. This\ncannibalization appears to set limits on the integration of wind and solar, and\nthus contradict studies that show that high shares are cost effective. Here we\nshow from theory and with numerical examples how policies interact with prices,\nrevenue and costs for renewable electricity systems. The decline in average\nrevenue seen in some recent literature is due to an implicit policy assumption\nthat technologies are forced into the system, whether it be with subsidies or\nquotas. If instead the driving policy is a carbon dioxide cap or tax, wind and\nsolar shares can rise without cannibalising their own market revenue, even at\npenetrations of wind and solar above 80%. Policy is thus the primary factor\ndriving lower market values; the variability of wind and solar is only a\nsecondary factor that accelerates the decline if they are subsidised. The\nstrong dependence of market value on the policy regime means that market value\nneeds to be used with caution as a measure of market integration.", "category": ["q-fin.GN", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.05209v1", "link": "http://arxiv.org/abs/2002.05209v1"}, {"title": "Long Run Feedback in the Broker Call Money Market", "summary": "I unravel the basic long run dynamics of the broker call money market, which\nis the pile of cash that funds margin loans to retail clients (read: continuous\ntime Kelly gamblers). Call money is assumed to supply itself perfectly\ninelastically, and to continuously reinvest all principal and interest. I show\nthat the relative size of the money market (that is, relative to the Kelly\nbankroll) is a martingale that nonetheless converges in probability to zero.\nThe margin loan interest rate is a submartingale that converges in mean square\nto the choke price $r_\\infty:=\\nu-\\sigma^2/2$, where $\\nu$ is the asymptotic\ncompound growth rate of the stock market and $\\sigma$ is its annual volatility.\nIn this environment, the gambler no longer beats the market asymptotically a.s.\nby an exponential factor (as he would under perfectly elastic supply). Rather,\nhe beats the market asymptotically with very high probability (think 98%) by a\nfactor (say 1.87, or 87% more final wealth) whose mean cannot exceed what the\nleverage ratio was at the start of the model (say, $2:1$). Although the ratio\nof the gambler's wealth to that of an equivalent buy-and-hold investor is a\nsubmartingale (always expected to increase), his realized compound growth rate\nconverges in mean square to $\\nu$. This happens because the equilibrium\nleverage ratio converges to $1:1$ in lockstep with the gradual rise of margin\nloan interest rates.", "category": ["econ.GN", "q-fin.CP", "q-fin.EC", "q-fin.GN", "q-fin.PM"], "id": "http://arxiv.org/abs/1906.10084v1", "link": "http://arxiv.org/abs/1906.10084v1"}, {"title": "A hybrid approach for the implementation of the Heston model", "summary": "We propose a hybrid tree-finite difference method in order to approximate the\nHeston model. We prove the convergence by embedding the procedure in a\nbivariate Markov chain and we study the convergence of European and American\noption prices. We finally provide numerical experiments that give accurate\noption prices in the Heston model, showing the reliability and the efficiency\nof the algorithm.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1307.7178v4", "link": "http://dx.doi.org/10.1093/imaman/dpv032"}, {"title": "Statistical properties and multifractality of Bitcoin", "summary": "Using 1-min returns of Bitcoin prices, we investigate statistical properties\nand multifractality of a Bitcoin time series. We find that the 1-min return\ndistribution is fat-tailed, and kurtosis largely deviates from the Gaussian\nexpectation. Although for large sampling periods, kurtosis is anticipated to\napproach the Gaussian expectation, we find that convergence to that is very\nslow. Skewness is found to be negative at time scales shorter than one day and\nbecomes consistent with zero at time scales longer than about one week. We also\ninvestigate daily volatility-asymmetry by using GARCH, GJR, and RGARCH models,\nand find no evidence of it. On exploring multifractality using multifractal\ndetrended fluctuation analysis, we find that the Bitcoin time series exhibits\nmultifractality. The sources of multifractality are investigated, confirming\nthat both temporal correlation and the fat-tailed distribution contribute to\nit. The influence of \"Brexit\" on June 23, 2016 to GBP--USD exchange rate and\nBitcoin is examined in multifractal properties. We find that, while Brexit\ninfluenced the GBP--USD exchange rate, Bitcoin was robust to Brexit.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1707.07618v3", "link": "http://dx.doi.org/10.1016/j.physa.2018.04.046"}, {"title": "Scoring Functions for Multivariate Distributions and Level Sets", "summary": "Interest in predicting multivariate probability distributions is growing due\nto the increasing availability of rich datasets and computational developments.\nScoring functions enable the comparison of forecast accuracy, and can\npotentially be used for estimation. A scoring function for multivariate\ndistributions that has gained some popularity is the energy score. This is a\ngeneralization of the continuous ranked probability score (CRPS), which is\nwidely used for univariate distributions. A little-known, alternative\ngeneralization is the multivariate CRPS (MCRPS). We propose a theoretical\nframework for scoring functions for multivariate distributions, which\nencompasses the energy score and MCRPS, as well as the quadratic score, which\nhas also received little attention. We demonstrate how this framework can be\nused to generate new scores. For univariate distributions, it is\nwell-established that the CRPS can be expressed as the integral over a quantile\nscore. We show that, in a similar way, scoring functions for multivariate\ndistributions can be \"disintegrated\" to obtain scoring functions for level\nsets. Using this, we present scoring functions for different types of level\nset, including those for densities and cumulative distributions. To compute the\nscoring functions, we propose a simple numerical algorithm. We illustrate our\nproposals using simulated and stock returns data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2002.09578v3", "link": "http://arxiv.org/abs/2002.09578v3"}, {"title": "The Local Fractal Properties of the Financial Time Series on the Polish\n  Stock Exchange Market", "summary": "We investigate the local fractal properties of the financial time series\nbased on the evolution of the Warsaw Stock Exchange Index (WIG) connected with\nthe largest developing financial market in Europe. Calculating the local Hurst\nexponent for the WIG time series we find an interesting dependence between the\nbehavior of the local fractal properties of the WIG time series and the crashes\nappearance on the financial market.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0708.0353v1", "link": "http://arxiv.org/abs/0708.0353v1"}, {"title": "k-price auctions and Combination auctions", "summary": "We provide an exact analytical solution of the Nash equilibrium for $k$-\nprice auctions. We also introduce a new type of auction and demonstrate that it\nhas fair solutions other than the second price auctions, therefore paving the\nway for replacing second price auctions.", "category": ["q-fin.MF", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.03494v3", "link": "http://arxiv.org/abs/1810.03494v3"}, {"title": "Predictive regressions for macroeconomic data", "summary": "Researchers have constantly asked whether stock returns can be predicted by\nsome macroeconomic data. However, it is known that macroeconomic data may\nexhibit nonstationarity and/or heavy tails, which complicates existing testing\nprocedures for predictability. In this paper we propose novel empirical\nlikelihood methods based on some weighted score equations to test whether the\nmonthly CRSP value-weighted index can be predicted by the log dividend-price\nratio or the log earnings-price ratio. The new methods work well both\ntheoretically and empirically regardless of the predicting variables being\nstationary or nonstationary or having an infinite variance.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1404.7642v1", "link": "http://dx.doi.org/10.1214/13-AOAS708"}, {"title": "Hedging Effectiveness under Conditions of Asymmetry", "summary": "We examine whether hedging effectiveness is affected by asymmetry in the\nreturn distribution by applying tail specific metrics to compare the hedging\neffectiveness of short and long hedgers using crude oil futures contracts. The\nmetrics used include Lower Partial Moments (LPM), Value at Risk (VaR) and\nConditional Value at Risk (CVAR). Comparisons are applied to a number of\nhedging strategies including OLS and both Symmetric and Asymmetric GARCH\nmodels. Our findings show that asymmetry reduces in-sample hedging performance\nand that there are significant differences in hedging performance between short\nand long hedgers. Thus, tail specific performance metrics should be applied in\nevaluating hedging effectiveness. We also find that the Ordinary Least Squares\n(OLS) model provides consistently good performance across different measures of\nhedging effectiveness and estimation methods irrespective of the\ncharacteristics of the underlying distribution.", "category": ["q-fin.CP", "q-fin.RM"], "id": "http://arxiv.org/abs/1103.5411v1", "link": "http://arxiv.org/abs/1103.5411v1"}, {"title": "Methodological provisions for conducting empirical research of the\n  availability and implementation of the consumers socially responsible\n  intentions", "summary": "Social responsibility of consumers is one of the main conditions for the\nrecoupment of enterprises expenses associated with the implementation of social\nand ethical marketing tasks. Therefore, the enterprises, which plan to act on\nterms of social and ethical marketing, should monitor the social responsibility\nof consumers in the relevant markets. At the same time, special attention\nshould be paid to the analysis of factors that prevent consumers from\nimplementing their socially responsible intentions in the regions with a low\nlevel of social activity of consumers. The purpose of the article is to develop\nmethodological guidelines that determine the tasks and directions of conducting\nempirical studies aimed at assessing the gap between the socially responsible\nintentions of consumers and the actual implementation of these intentions, as\nwell as to identify the causes of this gap. An empirical survey of the sampled\nconsumers in Kharkiv was carried out in terms of the proposed methodological\nprovisions. It revealed a rather high level of respondents' willingness to\nsupport socially responsible enterprises and a rather low level of\nimplementation of these intentions due to the lack of consumers awareness. To\ntest the proposed methodological guidelines, an empirical study of the\nconsumers social responsibility was conducted in 2017 on a sample of students\nand professors of the Semen Kuznets Kharkiv National University of Economics\n(120 people). Questioning of the respondents was carried out using the Google\nForms. The finding allowed to make conclusion for existence of a high level of\nrespondents' willingness to support socially responsible and socially active\nenterprises. However, the study also revealed the existence of a significant\ngap between the intentions and actions of consumers, caused by the lack of\nawareness.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.00191v1", "link": "http://dx.doi.org/10.21272/mmi.2018.3-11"}, {"title": "Optimal VWAP execution under transient price impact", "summary": "We solve the problem of optimal liquidation with volume weighted average\nprice (VWAP) benchmark when the market impact is linear and transient. Our\nsetting is indeed more general as it considers the case when the trading\ninterval is not necessarily coincident with the benchmark interval:\nImplementation Shortfall and Target Close execution are shown to be particular\ncases of our setting. We find explicit solutions in continuous and discrete\ntime considering risk averse investors having a CARA utility function. Finally,\nwe show that, contrary to what is observed for Implementation Shortfall, the\noptimal VWAP solution contains both buy and sell trades also when the decay\nkernel is convex.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1901.02327v2", "link": "http://arxiv.org/abs/1901.02327v2"}, {"title": "Finding a promising venture capital project with todim under\n  probabilistic hesitant fuzzy circumstance", "summary": "Considering the risk aversion for gains and the risk seeking for losses of\nventure capitalists, the TODIM has been chosen as the decision-making method.\nMoreover, group decision is an available way to avoid the limited ability and\nknowledge etc. of venture capitalists.Simultaneously, venture capitalists may\nbe hesitant among several assessed values with different probabilities to\nexpress their real perceptionbecause of the uncertain decision-making\nenvironment. However, the probabilistic hesitant fuzzy information can solve\nsuch problems effectively. Therefore, the TODIM has been extended to\nprobabilistic hesitant fuzzy circumstance for the sake of settling the\ndecision-making problem of venture capitalists in this paper. Moreover, due to\nthe uncertain investment environment, the criteria weights are considered as\nprobabilistic hesitant fuzzyinformation as well. Then, a case study has been\nused to verify the feasibility and validity of the proposed TODIM.Also, the\nTODIM with hesitant fuzzy information has been carried out to analysis the same\ncase.From the comparative analysis, the superiority of the proposed TODIM in\nthis paper has already appeared.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1809.00128v1", "link": "http://arxiv.org/abs/1809.00128v1"}, {"title": "A Nonlocal Approach to The Quantum Kolmogorov Backward Equation and\n  Links to Noncommutative Geometry", "summary": "The Accardi-Boukas quantum Black-Scholes equation can be used as an\nalternative to the classical approach to finance, and has been found to have a\nnumber of useful benefits. The quantum Kolmogorov backward equations, and\nassociated quantum Fokker-Planck equations, that arise from this general\nframework, are derived using the Hudson-Parthasarathy quantum stochastic\ncalculus. In this paper we show how these equations can be derived using a\nnonlocal approach to quantum mechanics. We show how nonlocal diffusions, and\nquantum stochastic processes can be linked, and discuss how moment matching can\nbe used for deriving solutions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1905.07257v1", "link": "http://arxiv.org/abs/1905.07257v1"}, {"title": "Estimation of the yield curve for Costa Rica using combinatorial\n  optimization metaheuristics applied to nonlinear regression", "summary": "The term structure of interest rates or yield curve is a function relating\nthe interest rate with its own term. Nonlinear regression models of\nNelson-Siegel and Svensson were used to estimate the yield curve using a sample\nof historical data supplied by the National Stock Exchange of Costa Rica. The\noptimization problem involved in the estimation process of model parameters is\naddressed by the use of four well known combinatorial optimization\nmetaheuristics: Ant colony optimization, Genetic algorithm, Particle swarm\noptimization and Simulated annealing. The aim of the study is to improve the\nlocal minima obtained by a classical quasi-Newton optimization method using a\ndescent direction. Good results with at least two metaheuristics are achieved,\nParticle swarm optimization and Simulated annealing. Keywords: Yield curve,\nnonlinear regression, Nelson-", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/2001.00920v1", "link": "http://arxiv.org/abs/2001.00920v1"}, {"title": "On fitting the Pareto-Levy distribution to stock market index data:\n  selecting a suitable cutoff value", "summary": "The so-called Pareto-Levy or power-law distribution has been successfully\nused as a model to describe probabilities associated to extreme variations of\nworldwide stock markets indexes data and it has the form $Pr(X>x) ~ x**(-alpha)\nfor gamma< x <infinity. The selection of the threshold parameter gamma$ from\nempirical data and consequently, the determination of the exponent alpha, is\noften is done by using a simple graphical method based on a log-log scale,\nwhere a power-law probability plot shows a straight line with slope equal to\nthe exponent of the power-law distribution. This procedure can be considered\nsubjective, particularly with regard to the choice of the threshold or cutoff\nparameter gamma. In this work is presented a more objective procedure, based on\na statistical measure of discrepancy between the empirical and the Pareto-Levy\ndistribution. The technique is illustrated for data sets from the New York\nStock Exchange Index and the Mexican Stock Market Index (IPC).", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0411161v1", "link": "http://dx.doi.org/10.1016/j.physa.2005.03.001"}, {"title": "Econophysics deserves a revamping", "summary": "The paper argues that attracting more economists and adopting a more-precise\ndefinition of dynamic complexity might help econophysics acquire more attention\nin the economics community and bring new lymph to economic research. It may be\nnecessary to concentrate less on the applications than on the basics of\neconomic complexity, beginning with expansion and deepening of the study of\nsmall systems with few interacting components, while until thus far complexity\nhas been assumed to be a prerogative of complicated systems only. It is\npossible that without a thorough analysis at that level, the understanding of\nsystems that are at the same time complex and complicated will continue to\nelude economics and econophysics research altogether. To that purpose, the\npaper initiates and frames a definition of dynamic complexity grounded on the\nconcept of non-linear dynamical system.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1911.05814v2", "link": "http://arxiv.org/abs/1911.05814v2"}, {"title": "Signs of dependence and heavy tails in non-life insurance data", "summary": "In this paper we study data from the yearly reports the four major Swedish\nnon-life insurers have sent to the Swedish Financial Supervisory Authority\n(FSA). We aim at finding marginal distributions of, and dependence between,\nlosses on the five largest lines of business (LoBs) in order to create models\nfor Solvency Capital Requirement (SCR) calculation. We try to use data in an\noptimal way by sensibly defining an accounting year loss in terms of actuarial\nliability predictions, and by pooling observations from several companies when\npossible to decrease the uncertainty about the underlying distributions and\ntheir parameters. We find that dependence between LoBs is weaker in our data\nthan what is assumed in the Solvency II standard formula. We also find\ndependence between companies that may affect financial stability, and must be\ntaken into account when estimating loss distribution parameters. Moreover, we\ndiscuss under what circumstances an insurer is better (or worse) off using an\ninternal model for SCR calculation instead of the standard formula.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1501.00833v1", "link": "http://arxiv.org/abs/1501.00833v1"}, {"title": "Deep Factor Model", "summary": "We propose to represent a return model and risk model in a unified manner\nwith deep learning, which is a representative model that can express a\nnonlinear relationship. Although deep learning performs quite well, it has\nsignificant disadvantages such as a lack of transparency and limitations to the\ninterpretability of the prediction. This is prone to practical problems in\nterms of accountability. Thus, we construct a multifactor model by using\ninterpretable deep learning. We implement deep learning as a return model to\npredict stock returns with various factors. Then, we present the application of\nlayer-wise relevance propagation (LRP) to decompose attributes of the predicted\nreturn as a risk model. By applying LRP to an individual stock or a portfolio\nbasis, we can determine which factor contributes to prediction. We call this\nmodel a deep factor model. We then perform an empirical analysis on the\nJapanese stock market and show that our deep factor model has better predictive\ncapability than the traditional linear model or other machine learning methods.\nIn addition , we illustrate which factor contributes to prediction.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1810.01278v1", "link": "http://arxiv.org/abs/1810.01278v1"}, {"title": "Dual method for continuous-time Markowitz's Problems with nonlinear\n  wealth equations", "summary": "Continuous-time mean-variance portfolio selection model with nonlinear wealth\nequations and bankruptcy prohibition is investigated by the dual method. A\nnecessary and sufficient condition which the optimal terminal wealth satisfies\nis obtained through a terminal perturbation technique. It is also shown that\nthe optimal wealth and portfolio is the solution of a forward-backward\nstochastic differential equation with constraints.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/0806.4834v1", "link": "http://arxiv.org/abs/0806.4834v1"}, {"title": "Power Law Tails in the Italian Personal Income Distribution", "summary": "We investigate the shape of the Italian personal income distribution using\nmicrodata from the Survey on Household Income and Wealth, made publicly\navailable by the Bank of Italy for the years 1977--2002. We find that the upper\ntail of the distribution is consistent with a Pareto-power law type\ndistribution, while the rest follows a two-parameter lognormal distribution.\nThe results of our analysis show a shift of the distribution and a change of\nthe indexes specifying it over time. As regards the first issue, we test the\nhypothesis that the evolution of both gross domestic product and personal\nincome is governed by similar mechanisms, pointing to the existence of\ncorrelation between these quantities. The fluctuations of the shape of income\ndistribution are instead quantified by establishing some links with the\nbusiness cycle phases experienced by the Italian economy over the years covered\nby our dataset.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0408067v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.11.038"}, {"title": "Dynamic structural and topological phase transitions on the Warsaw Stock\n  Exchange: A phenomenological approach", "summary": "We study the crash dynamics of the Warsaw Stock Exchange (WSE) by using the\nMinimal Spanning Tree (MST) networks. We find the transition of the complex\nnetwork during its evolution from a (hierarchical) power law MST network,\nrepresenting the stable state of WSE before the recent worldwide financial\ncrash, to a superstar-like (or superhub) MST network of the market decorated by\na hierarchy of trees (being, perhaps, an unstable, intermediate market state).\nSubsequently, we observed a transition from this complex tree to the topology\nof the (hierarchical) power law MST network decorated by several star-like\ntrees or hubs. This structure and topology represent, perhaps, the WSE after\nthe worldwide financial crash, and could be considered to be an aftershock. Our\nresults can serve as an empirical foundation for a future theory of dynamic\nstructural and topological phase transitions on financial markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1301.6506v1", "link": "http://arxiv.org/abs/1301.6506v1"}, {"title": "Pricing and Hedging Long-Term Options", "summary": "In this article, we investigate the behavior of long-term options. In many\ncases, option prices follow an exponential decay (or growth) rate for further\nmaturity dates. We determine under what conditions option prices are\ncharacterized by this property. To see this, we use the martingale extraction\nmethod through which a pricing operator is transformed into a semigroup\noperator, which is easier to address.\n  We also explore notions of hedging long-term options. Hedging is an attempt\nto reduce market risks, and we investigate the price sensitivities (Greeks)\nwith respect to such risks, which are typically repre- sented by variations in\nthe underlying process of an option. We combine the Malliavin calculus with the\nmartingale extraction method to analyze Greeks. We see that the ratios between\nGreeks and the option price are expressed in a simple form in the long term.", "category": ["q-fin.MF", "q-fin.PR"], "id": "http://arxiv.org/abs/1410.8160v3", "link": "http://arxiv.org/abs/1410.8160v3"}, {"title": "Can Google Trends search queries contribute to risk diversification?", "summary": "Portfolio diversification and active risk management are essential parts of\nfinancial analysis which became even more crucial (and questioned) during and\nafter the years of the Global Financial Crisis. We propose a novel approach to\nportfolio diversification using the information of searched items on Google\nTrends. The diversification is based on an idea that popularity of a stock\nmeasured by search queries is correlated with the stock riskiness. We penalize\nthe popular stocks by assigning them lower portfolio weights and we bring\nforward the less popular, or peripheral, stocks to decrease the total riskiness\nof the portfolio. Our results indicate that such strategy dominates both the\nbenchmark index and the uniformly weighted portfolio both in-sample and\nout-of-sample.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1310.1444v1", "link": "http://dx.doi.org/10.1038/srep02713"}, {"title": "Quasi Maximum Likelihood Estimation of Non-Stationary Large Approximate\n  Dynamic Factor Models", "summary": "This paper considers estimation of large dynamic factor models with common\nand idiosyncratic trends by means of the Expectation Maximization algorithm,\nimplemented jointly with the Kalman smoother. We show that, as the\ncross-sectional dimension $n$ and the sample size $T$ diverge to infinity, the\ncommon component for a given unit estimated at a given point in time is\n$\\min(\\sqrt n,\\sqrt T)$-consistent. The case of local levels and/or local\nlinear trends trends is also considered. By means of a MonteCarlo simulation\nexercise, we compare our approach with estimators based on principal component\nanalysis.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.09841v1", "link": "http://arxiv.org/abs/1910.09841v1"}, {"title": "Explicit implied volatilities for multifactor local-stochastic\n  volatility models", "summary": "We consider an asset whose risk-neutral dynamics are described by a general\nclass of local-stochastic volatility models and derive a family of asymptotic\nexpansions for European-style option prices and implied volatilities. Our\nimplied volatility expansions are explicit; they do not require any special\nfunctions nor do they require numerical integration. To illustrate the accuracy\nand versatility of our method, we implement it under five different model\ndynamics: CEV local volatility, quadratic local volatility, Heston stochastic\nvolatility, $3/2$ stochastic volatility, and SABR local-stochastic volatility.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1306.5447v4", "link": "http://arxiv.org/abs/1306.5447v4"}, {"title": "The economic default time and the Arcsine law", "summary": "This paper develops a structural credit risk model to characterize the\ndifference between the economic and recorded default times for a firm. Recorded\ndefault occurs when default is recorded in the legal system. The economic\ndefault time is the last time when the firm is able to pay off its debt prior\nto the legal default time. It has been empirically documented that these two\ntimes are distinct (see Guo, Jarrow, and Lin (2008)). In our model, the\nprobability distribution for the time span between economic and recorded\ndefaults follows a mixture of Arcsine Laws, which is consistent with the\nresults contained in Guo, Jarrow, and Lin. In addition, we show that the\nclassical structural model is a limiting case of our model as the time period\nbetween debt repayment dates goes to zero. As a corollary, we show how the firm\nvalue process's parameters can be estimated using the tail index and\ncorrelation structure of the firm's return.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1012.0843v2", "link": "http://arxiv.org/abs/1012.0843v2"}, {"title": "Jump-Diffusion Risk-Sensitive Asset Management", "summary": "This paper considers a portfolio optimization problem in which asset prices\nare represented by SDEs driven by Brownian motion and a Poisson random measure,\nwith drifts that are functions of an auxiliary diffusion 'factor' process. The\ncriterion, following earlier work by Bielecki, Pliska, Nagai and others, is\nrisk-sensitive optimization (equivalent to maximizing the expected growth rate\nsubject to a constraint on variance.) By using a change of measure technique\nintroduced by Kuroda and Nagai we show that the problem reduces to solving a\ncertain stochastic control problem in the factor process, which has no jumps.\nThe main result of the paper is that the Hamilton-Jacobi-Bellman equation for\nthis problem has a classical solution. The proof uses Bellman's \"policy\nimprovement\" method together with results on linear parabolic PDEs due to\nLadyzhenskaya et al.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/0905.4740v2", "link": "http://arxiv.org/abs/0905.4740v2"}, {"title": "Bootstrap-Assisted Unit Root Testing With Piecewise Locally Stationary\n  Errors", "summary": "In unit root testing, a piecewise locally stationary process is adopted to\naccommodate nonstationary errors that can have both smooth and abrupt changes\nin second- or higher-order properties. Under this framework, the limiting null\ndistributions of the conventional unit root test statistics are derived and\nshown to contain a number of unknown parameters. To circumvent the difficulty\nof direct consistent estimation, we propose to use the dependent wild bootstrap\nto approximate the non-pivotal limiting null distributions and provide a\nrigorous theoretical justification for bootstrap consistency. The proposed\nmethod is compared through finite sample simulations with the recolored wild\nbootstrap procedure, which was developed for errors that follow a\nheteroscedastic linear process. Further, a combination of autoregressive sieve\nrecoloring with the dependent wild bootstrap is shown to perform well. The\nvalidity of the dependent wild bootstrap in a nonstationary setting is\ndemonstrated for the first time, showing the possibility of extensions to other\ninference problems associated with locally stationary processes.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1802.05333v1", "link": "http://arxiv.org/abs/1802.05333v1"}, {"title": "Towards more effective consumer steering via network analysis", "summary": "Increased data gathering capacity, together with the spread of data analytics\ntechniques, has prompted an unprecedented concentration of information related\nto the individuals' preferences in the hands of a few gatekeepers. In the\npresent paper, we show how platforms' performances still appear astonishing in\nrelation to some unexplored data and networks properties, capable to enhance\nthe platforms' capacity to implement steering practices by means of an\nincreased ability to estimate individuals' preferences. To this end, we rely on\nnetwork science whose analytical tools allow data representations capable of\nhighlighting relationships between subjects and/or items, extracting a great\namount of information. We therefore propose a measure called Network\nInformation Patrimony, considering the amount of information available within\nthe system and we look into how platforms could exploit data stemming from\nconnected profiles within a network, with a view to obtaining competitive\nadvantages. Our measure takes into account the quality of the connections among\nnodes as the one of a hypothetical user in relation to its neighbourhood,\ndetecting how users with a good neighbourhood -- hence of a superior\nconnections set -- obtain better information. We tested our measures on\nAmazons' instances, obtaining evidence which confirm the relevance of\ninformation extracted from nodes' neighbourhood in order to steer targeted\nusers.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1903.11469v2", "link": "http://arxiv.org/abs/1903.11469v2"}, {"title": "Solvable Stochastic Dealer Models for Financial Markets", "summary": "We introduce solvable stochastic dealer models, which can reproduce basic\nempirical laws of financial markets such as the power law of price change.\nStarting from the simplest model that is almost equivalent to a Poisson random\nnoise generator, the model becomes fairly realistic by adding only two effects,\nthe self-modulation of transaction intervals and a forecasting tendency, which\nuses a moving average of the latest market price changes. Based on the present\nmicroscopic model of markets, we find a quantitative relation with market\npotential forces, which has recently been discovered in the study of market\nprice modeling based on random walks.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0809.0481v2", "link": "http://dx.doi.org/10.1103/PhysRevE.79.051120"}, {"title": "Forward equations for option prices in semimartingale models", "summary": "We derive a forward partial integro-differential equation for prices of call\noptions in a model where the dynamics of the underlying asset under the pricing\nmeasure is described by a -possibly discontinuous- semimartingale. A uniqueness\ntheorem is given for the solutions of this equation. This result generalizes\nDupire's forward equation to a large class of non-Markovian models with jumps.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1001.1380v4", "link": "http://dx.doi.org/10.1007/s00780-015-0265-z"}, {"title": "A comparison among some Hurst exponent approaches to predict nascent\n  bubbles in $500$ company stocks", "summary": "In this paper, three approaches to calculate the self-similarity exponent of\na time series are compared in order to determine which one performs best to\nidentify the transition from random efficient market behavior (EM) to herding\nbehavior (HB) and hence, to find out the beginning of a market bubble. In\nparticular, classical Detrended Fluctuation Analysis (DFA), Generalized Hurst\nExponent (GHE) and GM2 (one of Geometric Method-based algorithms) were applied\nfor self-similarity exponent calculation purposes. Traditionally, researchers\nhave been focused on identifying the beginning of a crash. Instead of this, we\nare pretty interested in identifying the beginning of the transition process\nfrom EM to a market bubble onset, what we consider could be more interesting.\nThe relevance of self-similarity index in such a context lies on the fact that\nit becomes a suitable indicator which allows to identify the raising of HB in\nfinancial markets. Overall, we could state that the greater the self-similarity\nexponent in financial series, the more likely the transition process to HB\ncould start. This fact is illustrated through actual S\\&P500 stocks.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1601.04188v1", "link": "http://dx.doi.org/10.1142/S0218348X17500062"}, {"title": "Homogenization and asymptotics for small transaction costs", "summary": "We consider the classical Merton problem of lifetime consumption-portfolio\noptimization problem with small proportional transaction costs. The first order\nterm in the asymptotic expansion is explicitly calculated through a singular\nergodic control problem which can be solved in closed form in the\none-dimensional case. Unlike the existing literature, we consider a general\nutility function and general dynamics for the underlying assets. Our arguments\nare based on ideas from the homogenization theory and use the convergence tools\nfrom the theory of viscosity solutions. The multidimensional case is studied in\nour accompanying paper using the same approach.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1202.6131v4", "link": "http://arxiv.org/abs/1202.6131v4"}, {"title": "A Classification Framework for Stablecoin Designs", "summary": "Stablecoins promise to bridge fiat currencies with the world of\ncryptocurrencies. They provide a way for users to take advantage of the\nbenefits of digital currencies, such as ability to transfer assets over the\ninternet, provide assurance on minting schedules and scarcity, and enable new\nasset classes, while also partially mitigating their volatility risks. In this\npaper, we systematically discuss general design, decompose existing stablecoins\ninto various component design elements, explore their strengths and drawbacks,\nand identify future directions.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1910.10098v1", "link": "http://arxiv.org/abs/1910.10098v1"}, {"title": "Some stylized facts of the Bitcoin market", "summary": "In recent years a new type of tradable assets appeared, generically known as\ncryptocurrencies. Among them, the most widespread is Bitcoin. Given its\nnovelty, this paper investigates some statistical properties of the Bitcoin\nmarket. This study compares Bitcoin and standard currencies dynamics and\nfocuses on the analysis of returns at different time scales. We test the\npresence of long memory in return time series from 2011 to 2017, using\ntransaction data from one Bitcoin platform. We compute the Hurst exponent by\nmeans of the Detrended Fluctuation Analysis method, using a sliding window in\norder to measure long range dependence. We detect that Hurst exponents changes\nsignificantly during the first years of existence of Bitcoin, tending to\nstabilize in recent times. Additionally, multiscale analysis shows a similar\nbehavior of the Hurst exponent, implying a self-similar process.", "category": ["q-fin.ST", "q-fin.GN"], "id": "http://arxiv.org/abs/1708.04532v1", "link": "http://dx.doi.org/10.1016/j.physa.2017.04.159"}, {"title": "The k-generalized distribution: A new descriptive model for the size\n  distribution of incomes", "summary": "This paper proposes the k-generalized distribution as a model for describing\nthe distribution and dispersion of income within a population. Formulas for the\nshape, moments and standard tools for inequality measurement - such as the\nLorenz curve and the Gini coefficient - are given. A method for parameter\nestimation is also discussed. The model is shown to fit extremely well the data\non personal income distribution in Australia and the United States.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0710.3645v4", "link": "http://dx.doi.org/10.1016/j.physa.2008.01.109"}, {"title": "What shakes the FX tree? Understanding currency dominance, dependence\n  and dynamics", "summary": "There is intense interest in understanding the stochastic and dynamical\nproperties of the global Foreign Exchange (FX) market, whose daily transactions\nexceed one trillion US dollars. This is a formidable task since the FX market\nis characterized by a web of fluctuating exchange rates, with subtle\ninter-dependencies which may change in time. In practice, traders talk of\nparticular currencies being 'in play' during a particular period of time -- yet\nthere is no established machinery for detecting such important information.\nHere we apply the construction of Minimum Spanning Trees (MSTs) to the FX\nmarket, and show that the MST can capture important features of the global FX\ndynamics. Moreover, we show that the MST can help identify momentarily dominant\nand dependent currencies.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0503014v1", "link": "http://dx.doi.org/10.1117/12.618875"}, {"title": "A Non-Gaussian Option Pricing Model with Skew", "summary": "Closed form option pricing formulae explaining skew and smile are obtained\nwithin a parsimonious non-Gaussian framework. We extend the non-Gaussian option\npricing model of L. Borland (Quantitative Finance, {\\bf 2}, 415-431, 2002) to\ninclude volatility-stock correlations consistent with the leverage effect. A\ngeneralized Black-Scholes partial differential equation for this model is\nobtained, together with closed-form approximate solutions for the fair price of\na European call option. In certain limits, the standard Black-Scholes model is\nrecovered, as is the Constant Elasticity of Variance (CEV) model of Cox and\nRoss. Alternative methods of solution to that model are thereby also discussed.\nThe model parameters are partially fit from empirical observations of the\ndistribution of the underlying. The option pricing model then predicts European\ncall prices which fit well to empirical market data over several maturities.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/0403022v2", "link": "http://arxiv.org/abs/cond-mat/0403022v2"}, {"title": "A New Solution to Market Definition: An Approach Based on\n  Multi-dimensional Substitutability Statistics", "summary": "Market definition is an important component in the premerger investigation,\nbut the models used in the market definition have not developed much in the\npast three decades since the Critical Loss Analysis (CLA) was proposed in 1989.\nThe CLA helps the Hypothetical Monopolist Test to determine whether the\nhypothetical monopolist is going to profit from the small but significant and\nnon-transitory increase in price (SSNIP). However, the CLA has long been\ncriticized by academic scholars for its tendency to conclude a narrow market.\nAlthough the CLA was adopted by the 2010 Horizontal Merger Guidelines (the 2010\nGuidelines), the criticisms are likely still valid. In this dissertation, we\ndiscussed the mathematical deduction of CLA, the data used, and the SSNIP\ndefined by the Agencies. Based on our research, we concluded that the narrow\nmarket conclusion was due to the incorrect implementation of the CLA; not the\nmodel itself. On the other hand, there are other unresolvable problems in the\nCLA and the Hypothetical Monopolist Test. The SSNIP test and the CLA are bright\nresolutions for market definition problem during their time, but we have more\nadvanced tools to solve the task nowadays. In this dissertation, we propose a\nmodel which is based directly on the multi-dimensional substitutability between\nthe products and is capable of maximizing the substitutability of product\nfeatures within each group. Since the 2010 Guidelines does not exclude the use\nof models other than the ones mentioned by the Guidelines, our method can\nhopefully supplement the current models to show a better picture of the\nsubstitutive relations and provide a more stable definition of the market.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1906.10030v1", "link": "http://arxiv.org/abs/1906.10030v1"}, {"title": "On the Multi-Dimensional Controller and Stopper Games", "summary": "We consider a zero-sum stochastic differential controller-and-stopper game in\nwhich the state process is a controlled diffusion evolving in a\nmulti-dimensional Euclidean space. In this game, the controller affects both\nthe drift and the volatility terms of the state process. Under appropriate\nconditions, we show that the game has a value and the value function is the\nunique viscosity solution to an obstacle problem for a Hamilton-Jacobi-Bellman\nequation.", "category": ["math.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1009.0932v8", "link": "http://arxiv.org/abs/1009.0932v8"}, {"title": "AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy\n  using Interpretable Deep Reinforcement Attention Networks", "summary": "Recent years have witnessed the successful marriage of finance innovations\nand AI techniques in various finance applications including quantitative\ntrading (QT). Despite great research efforts devoted to leveraging deep\nlearning (DL) methods for building better QT strategies, existing studies still\nface serious challenges especially from the side of finance, such as the\nbalance of risk and return, the resistance to extreme loss, and the\ninterpretability of strategies, which limit the application of DL-based\nstrategies in real-life financial markets. In this work, we propose AlphaStock,\na novel reinforcement learning (RL) based investment strategy enhanced by\ninterpretable deep attention networks, to address the above challenges. Our\nmain contributions are summarized as follows: i) We integrate deep attention\nnetworks with a Sharpe ratio-oriented reinforcement learning framework to\nachieve a risk-return balanced investment strategy; ii) We suggest modeling\ninterrelationships among assets to avoid selection bias and develop a\ncross-asset attention mechanism; iii) To our best knowledge, this work is among\nthe first to offer an interpretable investment strategy using deep\nreinforcement learning models. The experiments on long-periodic U.S. and\nChinese markets demonstrate the effectiveness and robustness of AlphaStock over\ndiverse market states. It turns out that AlphaStock tends to select the stocks\nas winners with high long-term growth, low volatility, high intrinsic value,\nand being undervalued recently.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/1908.02646v1", "link": "http://dx.doi.org/10.1145/3292500.3330647"}, {"title": "Intra-day Equity Price Prediction using Deep Learning as a Measure of\n  Market Efficiency", "summary": "In finance, the weak form of the Efficient Market Hypothesis asserts that\nhistoric stock price and volume data cannot inform predictions of future\nprices. In this paper we show that, to the contrary, future intra-day stock\nprices could be predicted effectively until 2009. We demonstrate this using two\ndifferent profitable machine learning-based trading strategies. However, the\neffectiveness of both approaches diminish over time, and neither of them are\nprofitable after 2009. We present our implementation and results in detail for\nthe period 2003-2017 and propose a novel idea: the use of such flexible machine\nlearning methods as an objective measure of relative market efficiency. We\nconclude with a candidate explanation, comparing our returns over time with\nhigh-frequency trading volume, and suggest concrete steps for further\ninvestigation.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1908.08168v1", "link": "http://arxiv.org/abs/1908.08168v1"}, {"title": "Proportional Dynamics in Exchange Economies", "summary": "We study the Proportional Response dynamic in exchange economies, where each\nplayer starts with some amount of money and a good. Every day, the players\nbring one unit of their good and submit bids on goods they like, each good gets\nallocated in proportion to the bid amounts, and each seller collects the bids\nreceived. Then every player updates the bids proportionally to the contribution\nof each good in their utility. This dynamic models a process of learning how to\nbid and has been studied in a series of papers on Fisher and production\nmarkets, but not in exchange economies. Our main results are as follows:\n  - For linear utilities, the dynamic converges to market equilibrium utilities\nand allocations, while the bids and prices may cycle. We give a combinatorial\ncharacterization of limit cycles for prices and bids.\n  - We introduce a lazy version of the dynamic, where players may save money\nfor later, and show this converges in everything: utilities, allocations, and\nprices.\n  - For CES utilities in the substitute range $[0,1)$, the dynamic converges\nfor all parameters.\n  This answers an open question about exchange economies with linear utilities,\nwhere tatonnement does not converge to market equilibria, and no natural\nprocess leading to equilibria was known. We also note that proportional\nresponse is a process where the players exchange goods throughout time (in\nout-of-equilibrium states), while tatonnement only explains how exchange\nhappens in the limit.", "category": [], "id": "http://arxiv.org/abs/1907.05037v2", "link": "http://arxiv.org/abs/1907.05037v2"}, {"title": "A model for a large investor trading at market indifference prices. I:\n  single-period case", "summary": "We develop a single-period model for a large economic agent who trades with\nmarket makers at their utility indifference prices. A key role is played by a\npair of conjugate saddle functions associated with the description of Pareto\noptimal allocations in terms of the utility function of a representative market\nmaker.", "category": ["q-fin.TR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1110.3224v3", "link": "http://dx.doi.org/10.1007/s00780-015-0258-y"}, {"title": "Optimal Consumption in the Stochastic Ramsey Problem without Boundedness\n  Constraints", "summary": "This paper investigates optimal consumption in the stochastic Ramsey problem\nwith the Cobb-Douglas production function. Contrary to prior studies, we allow\nfor general consumption processes, without any a priori boundedness constraint.\nA non-standard stochastic differential equation, with neither Lipschitz\ncontinuity nor linear growth, specifies the dynamics of the controlled state\nprocess. A mixture of probabilistic arguments are used to construct the state\nprocess, and establish its non-explosiveness and strict positivity. This leads\nto the optimality of a feedback consumption process, defined in terms of the\nvalue function and the state process. Based on additional viscosity solutions\ntechniques, we characterize the value function as the unique classical solution\nto a nonlinear elliptic equation, among an appropriate class of functions. This\ncharacterization involves a condition on the limiting behavior of the value\nfunction at the origin, which is the key to dealing with unbounded\nconsumptions. Finally, relaxing the boundedness constraint is shown to\nincrease, strictly, the expected utility at all wealth levels.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1805.07532v2", "link": "http://arxiv.org/abs/1805.07532v2"}, {"title": "Predictability Hidden by Anomalous Observations", "summary": "Testing procedures for predictive regressions with lagged autoregressive\nvariables imply a suboptimal inference in presence of small violations of ideal\nassumptions. We propose a novel testing framework resistant to such violations,\nwhich is consistent with nearly integrated regressors and applicable to\nmulti-predictor settings, when the data may only approximately follow a\npredictive regression model. The Monte Carlo evidence demonstrates large\nimprovements of our approach, while the empirical analysis produces a strong\nrobust evidence of market return predictability hidden by anomalous\nobservations, both in- and out-of-sample, using predictive variables such as\nthe dividend yield or the volatility risk premium.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1612.05072v1", "link": "http://arxiv.org/abs/1612.05072v1"}, {"title": "Bankruptcy Risk Induced by Career Concerns of Regulators", "summary": "We introduce a model in which a regulator employs mechanism design to embed\nher human capital beta signal(s) in a firm's capital structure, in order to\nenhance the value of her post career change indexed executive stock option\ncontract with the firm. We prove that the agency cost of this revolving door\nbehavior increases the firm's financial leverage, bankruptcy risk, and affects\nestimation of firm value at risk (VaR).", "category": ["q-fin.RM", "q-fin.GN"], "id": "http://arxiv.org/abs/1312.7346v1", "link": "http://arxiv.org/abs/1312.7346v1"}, {"title": "Option Pricing Model for Incomplete Market", "summary": "The problem of determining the European-style option price in the incomplete\nmarket has been examined within the framework of stochastic optimization. An\nanalytic method based on the discrete dynamic programming equation (Bellman\nequation) has been developed that gives the general formalism for determining\nthe option price and the optimal trading strategy (optimal control policy) that\nreduces total risk inherent in writing the option.\n  The basic purpose of paper is to present an effective algorithm that can be\nused in practice.\n  Keywords: option pricing, incomplete market, transaction costs, stochastic\noptimization, Bellman equation.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/cond-mat/9807397v2", "link": "http://arxiv.org/abs/cond-mat/9807397v2"}, {"title": "Gaussian Approximation of a Risk Model with Non-Stationary Hawkes\n  Arrivals of Claims", "summary": "We consider a classical risk process with arrival of claims following a\nnon-stationary Hawkes process. We study the asymptotic regime when the premium\nrate and the baseline intensity of the claims arrival process are large, and\nclaim size is small. The main goal of the article is to establish a diffusion\napproximation by verifying a functional central limit theorem and to compute\nthe ruin probability in finite-time horizon. Numerical results will also be\ngiven.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1801.07595v2", "link": "http://dx.doi.org/10.1007/s11009-019-09722-8"}, {"title": "Belief-Averaged Relative Utilitarianism", "summary": "We study preference aggregation under uncertainty when individual and\ncollective preferences are based on subjective expected utility. A natural\nprocedure for determining the collective preferences of a group then is to\naverage its members' beliefs and add up their $(0,1)$-normalized utility\nfunctions. This procedure extends the well-known relative utilitarianism to\ndecision making under uncertainty. We show that it is the only aggregation\nfunction that gives tie-breaking rights to agents who join a group and\nsatisfies an independence condition in the spirit of Arrow's independence of\nirrelevant alternatives as well as four undiscriminating axioms.", "category": [], "id": "http://arxiv.org/abs/2005.03693v1", "link": "http://arxiv.org/abs/2005.03693v1"}, {"title": "Optimal Equilibria for Time-Inconsistent Stopping Problems in Continuous\n  Time", "summary": "For an infinite-horizon continuous-time optimal stopping problem under\nnon-exponential discounting, we look for an optimal equilibrium, which\ngenerates larger values than any other equilibrium does on the entire state\nspace. When the discount function is log sub-additive and the state process is\none-dimensional, an optimal equilibrium is constructed in a specific form,\nunder appropriate regularity and integrability conditions. While there may\nexist other optimal equilibria, we show that they can differ from the\nconstructed one in very limited ways. This leads to a sufficient condition for\nthe uniqueness of optimal equilibria, up to some closedness condition. To\nillustrate our theoretic results, comprehensive analysis is carried out for\nthree specific stopping problems, concerning asset liquidation and real options\nvaluation. For each one of them, an optimal equilibrium is characterized\nthrough an explicit formula.", "category": ["q-fin.EC", "q-fin.MF"], "id": "http://arxiv.org/abs/1712.07806v2", "link": "http://arxiv.org/abs/1712.07806v2"}, {"title": "The Role of Uncertainty in Controlling Climate Change", "summary": "Integrated Assessment Models (IAMs) of the climate and economy aim to analyze\nthe impact and efficacy of policies that aim to control climate change, such as\ncarbon taxes and subsidies. A major characteristic of IAMs is that their\ngeophysical sector determines the mean surface temperature increase over the\npreindustrial level, which in turn determines the damage function. Most of the\nexisting IAMs are perfect-foresight forward-looking models, assuming that we\nknow all of the future information. However, there are significant\nuncertainties in the climate and economic system, including parameter\nuncertainty, model uncertainty, climate tipping risks, economic risks, and\nambiguity. For example, climate damages are uncertain: some researchers assume\nthat climate damages are proportional to instantaneous output, while others\nassume that climate damages have a more persistent impact on economic growth.\nClimate tipping risks represent (nearly) irreversible climate events that may\nlead to significant changes in the climate system, such as the Greenland ice\nsheet collapse, while the conditions, probability of tipping, duration, and\nassociated damage are also uncertain. Technological progress in carbon capture\nand storage, adaptation, renewable energy, and energy efficiency are uncertain\ntoo. In the face of these uncertainties, policymakers have to provide a\ndecision that considers important factors such as risk aversion, inequality\naversion, and sustainability of the economy and ecosystem. Solving this problem\nmay require richer and more realistic models than standard IAMs, and advanced\ncomputational methods. The recent literature has shown that these uncertainties\ncan be incorporated into IAMs and may change optimal climate policies\nsignificantly.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.01615v1", "link": "http://arxiv.org/abs/2003.01615v1"}, {"title": "Optimal Execution Strategy Under Price and Volume Uncertainty", "summary": "In the seminal paper on optimal execution of portfolio transactions, Almgren\nand Chriss (2001) define the optimal trading strategy to liquidate a fixed\nvolume of a single security under price uncertainty. Yet there exist\nsituations, such as in the power market, in which the volume to be traded can\nonly be estimated and becomes more accurate when approaching a specified\ndelivery time. In this paper, we develop a model that accounts for volume\nuncertainty and we show that a risk-averse trader has benefit in delaying their\ntrades. More precisely, we argue that the optimal strategy is a trade-off\nbetween early and late trades in order to balance risk associated with both\nprice and volume. By incorporating a risk term related to the volume to trade,\nthe static optimal strategies suggested by our model avoid the explosion in the\nalgorithmic complexity usually associated with dynamic programming solutions,\nall the while yielding competitive performance.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1810.11454v3", "link": "http://arxiv.org/abs/1810.11454v3"}, {"title": "Kinetic models for optimal control of wealth inequalities", "summary": "We introduce and discuss optimal control strategies for kinetic models for\nwealth distribution in a simple market economy, acting to minimize the variance\nof the wealth density among the population. Our analysis is based on a finite\ntime horizon approximation, or model predictive control, of the corresponding\ncontrol problem for the microscopic agents' dynamic and results in an\nalternative theoretical approach to the taxation and redistribution policy at a\nglobal level. It is shown that in general the control is able to modify the\nPareto index of the stationary solution of the corresponding Boltzmann kinetic\nequation, and that this modification can be exactly quantified. Connections\nbetween previous Fokker-Planck based models and taxation-redistribution\npolicies and the present approach are also discussed.", "category": ["econ.GN", "q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.02171v2", "link": "http://dx.doi.org/10.1140/epjb/e2018-90138-1"}, {"title": "Does Environmental Economics lead to patentable research?", "summary": "In this feasibility study, the impact of academic research from social\nsciences and humanities on technological innovation is explored through a study\nof citations patterns of journal articles in patents. Specifically we focus on\ncitations of journals from the field of environmental economics in patents\nincluded in an American patent database (USPTO). Three decades of patents have\nled to a small set of journal articles (85) that are being cited from the field\nof environmental economics. While this route of measuring how academic research\nis validated through its role in stimulating technological progress may be\nrather limited (based on this first exploration), it may still point to a\nvaluable and interesting topic for further research.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1905.02875v1", "link": "http://arxiv.org/abs/1905.02875v1"}, {"title": "On the transition to efficiency in Minority Games", "summary": "The existence of a phase transition with diverging susceptibility in batch\nMinority Games (MGs) is the mark of informationally efficient regimes and is\nlinked to the specifics of the agents' learning rules. Here we study how the\nstandard scenario is affected in a mixed population game in which agents with\nthe `optimal' learning rule (i.e. the one leading to efficiency) coexist with\nones whose adaptive dynamics is sub-optimal. Our generic finding is that any\nnon-vanishing intensive fraction of optimal agents guarantees the existence of\nan efficient phase. Specifically, we calculate the dependence of the critical\npoint on the fraction $q$ of `optimal' agents focusing our analysis on three\ncases: MGs with market impact correction, grand-canonical MGs and MGs with\nheterogeneous comfort levels.", "category": ["q-fin.GN", "q-fin.TR"], "id": "http://arxiv.org/abs/0712.0337v1", "link": "http://dx.doi.org/10.1088/1751-8113/41/32/324003"}, {"title": "Tax Bond Creation Using a Structural Model and its Extensions", "summary": "This article describes and explores taxes and debt in finance. Here a\nsituation is thought about, where tax payments would qualify to be considered\nas debt. Using this principle we can infer that it is possible to create and\nprice a type of bond (Tax Normalization Guarantee) for companies, which would\nallow them to enter in temporary tax breaks to allow them to free capital.\nFinally it is explored a way to structure these bonds in financial products and\nvaluate them.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1509.01218v1", "link": "http://arxiv.org/abs/1509.01218v1"}, {"title": "Risk- and ambiguity-averse portfolio optimization with quasiconcave\n  utility functionals", "summary": "Motivated by recent axiomatic developments, we study the risk- and\nambiguity-averse investment problem where trading takes place over a fixed\nfinite horizon and terminal payoffs are evaluated according to a criterion\ndefined in terms of a quasiconcave utility functional. We extend to the present\nsetting certain existence and duality results established for the so-called\nvariational preferences by Schied (2007). The results are proven by building on\nexisting results for the classical utility maximization problem.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1311.7419v1", "link": "http://arxiv.org/abs/1311.7419v1"}, {"title": "Transversality Conditions for Stochastic Higher-Order Optimality:\n  Continuous and Discrete Time Problems", "summary": "Higher-order optimization problems naturally appear when investigating the\neffects of a patent with finite length, as in the pioneering work of Futagami\nand Iwaisako (2007). In this paper, we establish the Euler equations and\ntransversality conditions necessary for analyzing such higher-order\noptimization problems. We develop our results for stochastic general\nreduced-form models and consider cases of both continuous and discrete time. We\nemploy our results to establish the Euler equations and transversality\nconditions for the simplified household maximization problem in Futagami and\nIwaisako (2007).", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1203.3869v1", "link": "http://arxiv.org/abs/1203.3869v1"}, {"title": "Oscillations in the Tsallis income distribution", "summary": "Oscillations in the complementary cumulative distribution function (CCDF) of\nindividual income data have been found in the data of various countries studied\nby different authors at different time periods, but the dynamical origins of\nthis behavior are currently unknown. Although these datasets can be fitted by\ndifferent functions at different income ranges, the Tsallis distribution has\nrecently been found capable of fitting the whole distribution by means of only\ntwo parameters. This procedure showed clearly such oscillatory feature in the\nentire income range feature, but made it particularly visible at the tail of\nthe distribution. Although log-periodic functions fitted to the data are\ncapable of describing this behavior, a different approach to naturally disclose\nsuch oscillatory characteristics is to allow the Tsallis $q$-parameter to\nbecome complex. In this paper we use this idea in order to describe the\nbehavior of the CCDF of the Brazilian personal income recently studied\nempirically by Soares et al.\\ (2016). Typical elements of periodic motion, such\nas amplitude and angular frequency coupled to this income analysis, were\nobtained by means of this approach. A highly non-linear function for the CCDF\nwas obtained through this methodology and a numerical test showed it capable of\nrecovering the main oscillatory feature of the original CCDF of the personal\nincome data of Brazil.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1706.10141v2", "link": "http://dx.doi.org/10.1016/j.physa.2019.121967"}, {"title": "Optimal Timing to Trade Along a Randomized Brownian Bridge", "summary": "This paper studies an optimal trading problem that incorporates the trader's\nmarket view on the terminal asset price distribution and uninformative noise\nembedded in the asset price dynamics. We model the underlying asset price\nevolution by an exponential randomized Brownian bridge (rBb) and consider\nvarious prior distributions for the random endpoint. We solve for the optimal\nstrategies to sell a stock, call, or put, and analyze the associated delayed\nliquidation premia. We solve for the optimal trading strategies numerically and\ncompare them across different prior beliefs. Among our results, we find that\ndisconnected continuation/exercise regions arise when the trader prescribe a\ntwo-point discrete distribution and double exponential distribution.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1801.00372v2", "link": "http://arxiv.org/abs/1801.00372v2"}, {"title": "Deep Recurrent Factor Model: Interpretable Non-Linear and Time-Varying\n  Multi-Factor Model", "summary": "A linear multi-factor model is one of the most important tools in equity\nportfolio management. The linear multi-factor models are widely used because\nthey can be easily interpreted. However, financial markets are not linear and\ntheir accuracy is limited. Recently, deep learning methods were proposed to\npredict stock return in terms of the multi-factor model. Although these methods\nperform quite well, they have significant disadvantages such as a lack of\ntransparency and limitations in the interpretability of the prediction. It is\nthus difficult for institutional investors to use black-box-type machine\nlearning techniques in actual investment practice because they should show\naccountability to their customers. Consequently, the solution we propose is\nbased on LSTM with LRP. Specifically, we extend the linear multi-factor model\nto be non-linear and time-varying with LSTM. Then, we approximate and linearize\nthe learned LSTM models by LRP. We call this LSTM+LRP model a deep recurrent\nfactor model. Finally, we perform an empirical analysis of the Japanese stock\nmarket and show that our recurrent model has better predictive capability than\nthe traditional linear model and fully-connected deep learning methods.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1901.11493v1", "link": "http://arxiv.org/abs/1901.11493v1"}, {"title": "Effects of the Affordable Care Act Dependent Coverage Mandate on Health\n  Insurance Coverage for Individuals in Same-Sex Couples", "summary": "A large body of research documents that the 2010 dependent coverage mandate\nof the Affordable Care Act was responsible for significantly increasing health\ninsurance coverage among young adults. No prior research has examined whether\nsexual minority young adults also benefitted from the dependent coverage\nmandate, despite previous studies showing lower health insurance coverage among\nsexual minorities and the fact that their higher likelihood of strained\nrelationships with their parents might predict a lower ability to use parental\ncoverage. Our estimates from the American Community Surveys using\ndifference-in-differences and event study models show that men in same-sex\ncouples age 21-25 were significantly more likely to have any health insurance\nafter 2010 compared to the associated change for slightly older 27 to\n31-year-old men in same-sex couples. This increase is concentrated among\nemployer-sponsored insurance, and it is robust to permutations of time periods\nand age groups. Effects for women in same-sex couples and men in different-sex\ncouples are smaller than the associated effects for men in same-sex couples.\nThese findings confirm the broad effects of expanded dependent coverage and\nsuggest that eliminating the federal dependent mandate could reduce health\ninsurance coverage among young adult sexual minorities in same-sex couples.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2004.02296v1", "link": "http://arxiv.org/abs/2004.02296v1"}, {"title": "Incremental computation of block triangular matrix exponentials with\n  application to option pricing", "summary": "We study the problem of computing the matrix exponential of a block\ntriangular matrix in a peculiar way: Block column by block column, from left to\nright. The need for such an evaluation scheme arises naturally in the context\nof option pricing in polynomial diffusion models. In this setting a\ndiscretization process produces a sequence of nested block triangular matrices,\nand their exponentials are to be computed at each stage, until a dynamically\nevaluated criterion allows to stop. Our algorithm is based on scaling and\nsquaring. By carefully reusing certain intermediate quantities from one step to\nthe next, we can efficiently compute such a sequence of matrix exponentials.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1703.00182v3", "link": "http://arxiv.org/abs/1703.00182v3"}, {"title": "Multifractal returns and Hierarchical Portfolio Theory", "summary": "We extend and test empirically the multifractal model of asset returns based\non a multiplicative cascade of volatilities from large to small time scales.\nThe multifractal description of asset fluctuations is generalized into a\nmultivariate framework to account simultaneously for correlations across times\nscales and between a basket of assets. The reported empirical results show that\nthis extension is pertinent for financial modelling. The second part of the\npaper applies this theory to portfolio optimisation. Our multi-scale\ndescription allows us to characterize the portfolio return distribution at all\ntime scales simultaneously. The portfolio composition is predicted to change\nwith the investment time horizon (i.e., the time scale) in a way that can be\nfully determined once an adequate measure of risk is chosen. We discuss the use\nof the fourth-order cumulant and of utility functions. While the portfolio\nvolatility can be optimized in some cases for all time horizons, the kurtosis\nand higher normalized cumulants cannot be simultaneously optimized. For a fixed\ninvestment horizon, we study in details the influence of the number of periods,\ni.e., of the number of rebalancing of the portfolio. For the large risks\nquantified by the cumulants of order larger than two, the number of periods has\na non-trivial influence, in contrast with Tobin's result valid in the\nmean-variance framework. This theory provides a fundamental framework for the\nconflicting optimization involved in the different time horizons and quantifies\nsystematically the trade-offs for an optimal inter-temporal portfolio\noptimization.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/cond-mat/0008069v2", "link": "http://arxiv.org/abs/cond-mat/0008069v2"}, {"title": "Atomic Swaptions: Cryptocurrency Derivatives", "summary": "The atomic swap protocol allows for the exchange of cryptocurrencies on\ndifferent blockchains without the need to trust a third-party. However, market\nparticipants who desire to hold derivative assets such as options or futures\nwould also benefit from trustless exchange. In this paper I propose the atomic\nswaption, which extends the atomic swap to allow for such exchanges. Crucially,\natomic swaptions do not require the use of oracles. I also introduce the margin\ncontract, which provides the ability to create leveraged and short positions.\nLastly, I discuss how atomic swaptions may be routed on the Lightning Network.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1807.08644v2", "link": "http://arxiv.org/abs/1807.08644v2"}, {"title": "Robust valuation and risk measurement under model uncertainty", "summary": "Model uncertainty is a type of inevitable financial risk. Mistakes on the\nchoice of pricing model may cause great financial losses. In this paper we\ninvestigate financial markets with mean-volatility uncertainty. Models for\nstock markets and option markets with uncertain prior distribution are\nestablished by Peng's G-stochastic calculus. The process of stock price is\ndescribed by generalized geometric G-Brownian motion in which the mean\nuncertainty may move together with or regardless of the volatility uncertainty.\nOn the hedging market, the upper price of an (exotic) option is derived\nfollowing the Black-Scholes-Barenblatt equation. It is interesting that the\ncorresponding Barenblatt equation does not depend on the risk preference of\ninvestors and the mean-uncertainty of underlying stocks. Hence under some\nappropriate sublinear expectation, neither the risk preference of investors nor\nthe mean-uncertainty of underlying stocks pose effects on our super and\nsubhedging strategies. Appropriate definitions of arbitrage for super and\nsub-hedging strategies are presented such that the super and sub-hedging prices\nare reasonable. Especially the condition of arbitrage for sub-hedging strategy\nfills the gap of the theory of arbitrage under model uncertainty. Finally we\nshow that the term $K$ of finite-variance arising in the super-hedging strategy\nis interpreted as the max Profit\\&Loss of being short a delta-hedged option.\nThe ask-bid spread is in fact the accumulation of summation of the superhedging\n$P\\&L$ and the subhedging $P\\&L $.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1407.8024v1", "link": "http://arxiv.org/abs/1407.8024v1"}, {"title": "k-price auctions and Combination auctions", "summary": "We provide an exact analytical solution of the Nash equilibrium for $k$-\nprice auctions. We also introduce a new type of auction and demonstrate that it\nhas fair solutions other than the second price auctions, therefore paving the\nway for replacing second price auctions.", "category": ["q-fin.MF", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.03494v3", "link": "http://arxiv.org/abs/1810.03494v3"}, {"title": "An exact formula for default swaptions' pricing in the SSRJD stochastic\n  intensity model", "summary": "We develop and test a fast and accurate semi-analytical formula for\nsingle-name default swaptions in the context of a shifted square root jump\ndiffusion (SSRJD) default intensity model. The model can be calibrated to the\nCDS term structure and a few default swaptions, to price and hedge other credit\nderivatives consistently. We show with numerical experiments that the model\nimplies plausible volatility smiles.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0812.4199v1", "link": "http://arxiv.org/abs/0812.4199v1"}, {"title": "Asymptotic properties of maximum likelihood estimator for the growth\n  rate for a jump-type CIR process based on continuous time observations", "summary": "We consider a jump-type Cox--Ingersoll--Ross (CIR) process driven by a\nstandard Wiener process and a subordinator, and we study asymptotic properties\nof the maximum likelihood estimator (MLE) for its growth rate. We distinguish\nthree cases: subcritical, critical and supercritical. In the subcritical case\nwe prove weak consistency and asymptotic normality, and, under an additional\nmoment assumption, strong consistency as well. In the supercritical case, we\nprove strong consistency and mixed normal (but non-normal) asymptotic behavior,\nwhile in the critical case, weak consistency and non-standard asymptotic\nbehavior are described. We specialize our results to so-called basic affine\njump-diffusions as well. Concerning the asymptotic behavior of the MLE in the\nsupercritical case, we derive a stochastic representation of the limiting mixed\nnormal distribution, where the almost sure limit of an appropriately scaled\njump-type supercritical CIR process comes into play. This is a new phenomenon,\ncompared to the critical case, where a diffusion-type critical CIR process\nplays a role.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1609.05865v4", "link": "http://arxiv.org/abs/1609.05865v4"}, {"title": "On the role of backauditing for tax evasion in an agent-based\n  Econophysics model", "summary": "We investigate an inhomogeneous Ising model in the context of tax evasion\ndynamics where different types of agents are parametrized via local\ntemperatures and magnetic fields. In particular, we analyse the impact of\nbackauditing and endogenously determined penalty rates on tax compliance. Both\nfeatures contribute to a microfoundation of agent-based econophysics models of\ntax evasion.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1206.7000v1", "link": "http://dx.doi.org/10.1016/j.physa.2013.01.016"}, {"title": "Information-based models for finance and insurance", "summary": "In financial markets, the information that traders have about an asset is\nreflected in its price. The arrival of new information then leads to price\nchanges. The `information-based framework' of Brody, Hughston and Macrina (BHM)\nisolates the emergence of information, and examines its role as a driver of\nprice dynamics. This approach has led to the development of new models that\ncapture a broad range of price behaviour. This thesis extends the work of BHM\nby introducing a wider class of processes for the generation of the market\nfiltration. In the BHM framework, each asset is associated with a collection of\nrandom cash flows. The asset price is the sum of the discounted expectations of\nthe cash flows. Expectations are taken with respect (i) an appropriate measure,\nand (ii) the filtration generated by a set of so-called information processes\nthat carry noisy or imperfect market information about the cash flows. To model\nthe flow of information, we introduce a class of processes termed L\\'evy random\nbridges (LRBs), generalising the Brownian and gamma information processes of\nBHM. Conditioned on its terminal value, an LRB is identical in law to a L\\'evy\nbridge. We consider in detail the case where the asset generates a single cash\nflow $X_T$ at a fixed date $T$. The flow of information about $X_T$ is modelled\nby an LRB with random terminal value $X_T$. An explicit expression for the\nprice process is found by working out the discounted conditional expectation of\n$X_T$ with respect to the natural filtration of the LRB. New models are\nconstructed using information processes related to the Poisson process, the\nCauchy process, the stable-1/2 subordinator, the variance-gamma process, and\nthe normal inverse-Gaussian process. These are applied to the valuation of\ncredit-risky bonds, vanilla and exotic options, and non-life insurance\nliabilities.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1010.0829v1", "link": "http://arxiv.org/abs/1010.0829v1"}, {"title": "Hedging: Scaling and the Investor Horizon", "summary": "This paper examines the volatility and covariance dynamics of cash and\nfutures contracts that underlie the Optimal Hedge Ratio (OHR) across different\nhedging time horizons. We examine whether hedge ratios calculated over a short\nterm hedging horizon can be scaled and successfully applied to longer term\nhorizons. We also test the equivalence of scaled hedge ratios with those\ncalculated directly from lower frequency data and compare them in terms of\nhedging effectiveness. Our findings show that the volatility and covariance\ndynamics may differ considerably depending on the hedging horizon and this\ngives rise to significant differences between short term and longer term\nhedges. Despite this, scaling provides good hedging outcomes in terms of risk\nreduction which are comparable to those based on direct estimation.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1103.5966v1", "link": "http://arxiv.org/abs/1103.5966v1"}, {"title": "A Hierarchy of Limitations in Machine Learning", "summary": "\"All models are wrong, but some are useful\", wrote George E. P. Box (1979).\nMachine learning has focused on the usefulness of probability models for\nprediction in social systems, but is only now coming to grips with the ways in\nwhich these models are wrong---and the consequences of those shortcomings. This\npaper attempts a comprehensive, structured overview of the specific conceptual,\nprocedural, and statistical limitations of models in machine learning when\napplied to society. Machine learning modelers themselves can use the described\nhierarchy to identify possible failure points and think through how to address\nthem, and consumers of machine learning models can know what to question when\nconfronted with the decision about if, where, and how to apply machine\nlearning. The limitations go from commitments inherent in quantification\nitself, through to showing how unmodeled dependencies can lead to\ncross-validation being overly optimistic as a way of assessing model\nperformance.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2002.05193v2", "link": "http://arxiv.org/abs/2002.05193v2"}, {"title": "Comments are welcome", "summary": "Scholars present their new research at seminars and conferences, and send\ndrafts to peers, hoping to receive comments and suggestions that will improve\nthe quality of their work. Using a dataset of papers published in economics\njournals, this article measures how much peers' individual and collective\ncomments improve the quality of research. Controlling for the quality of the\nresearch idea and author, I find that a one standard deviation increase in the\nnumber of peers' individual and collective comments increases the quality of\nthe journal in which the research is published by 47%.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.08376v2", "link": "http://arxiv.org/abs/2001.08376v2"}, {"title": "Long-Time Fluctuations in a Dynamical Model of Stock Market Indices", "summary": "Financial time series typically exhibit strong fluctuations that cannot be\ndescribed by a Gaussian distribution. In recent empirical studies of stock\nmarket indices it was examined whether the distribution P(r) of returns r(tau)\nafter some time tau can be described by a (truncated) Levy-stable distribution\nL_{alpha}(r) with some index 0 < alpha <= 2. While the Levy distribution cannot\nbe expressed in a closed form, one can identify its parameters by testing the\ndependence of the central peak height on tau as well as the power-law decay of\nthe tails. In an earlier study [Mantegna and Stanley, Nature 376, 46 (1995)] it\nwas found that the behavior of the central peak of P(r) for the Standard & Poor\n500 index is consistent with the Levy distribution with alpha=1.4. In a more\nrecent study [Gopikrishnan et al., Phys. Rev. E 60, 5305 (1999)] it was found\nthat the tails of P(r) exhibit a power-law decay with an exponent alpha ~= 3,\nthus deviating from the Levy distribution. In this paper we study the\ndistribution of returns in a generic model that describes the dynamics of stock\nmarket indices. For the distributions P(r) generated by this model, we observe\nthat the scaling of the central peak is consistent with a Levy distribution\nwhile the tails exhibit a power-law distribution with an exponent alpha > 2,\nnamely beyond the range of Levy-stable distributions. Our results are in\nagreement with both empirical studies and reconcile the apparent disagreement\nbetween their results.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0208464v1", "link": "http://dx.doi.org/10.1103/PhysRevE.64.026101"}, {"title": "Functional Sequential Treatment Allocation with Covariates", "summary": "We consider a multi-armed bandit problem with covariates. Given a realization\nof the covariate vector, instead of targeting the treatment with highest\nconditional expectation, the decision maker targets the treatment which\nmaximizes a general functional of the conditional potential outcome\ndistribution, e.g., a conditional quantile, trimmed mean, or a socio-economic\nfunctional such as an inequality, welfare or poverty measure. We develop\nexpected regret lower bounds for this problem, and construct a near minimax\noptimal assignment policy.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2001.10996v1", "link": "http://arxiv.org/abs/2001.10996v1"}, {"title": "Mathematics underlying the 2008 financial crisis, and a possible remedy", "summary": "This paper has been withdrawn by the authors, because it has been made\nobsolete by the detailed expositions in our papers in arXiv:0812.4885 (the\nmathematics part) and arXiv:0812.4737 (the economics part).", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0811.4678v2", "link": "http://arxiv.org/abs/0811.4678v2"}, {"title": "The interconnectedness of the economic content in the speeches of the US\n  Presidents", "summary": "The speeches stated by influential politicians can have a decisive impact on\nthe future of a country. In particular, the economic content of such speeches\naffects the economy of countries and their financial markets. For this reason,\nwe examine a novel dataset containing the economic content of 951 speeches\nstated by 45 US Presidents from George Washington (April 1789) to Donald Trump\n(February 2017). In doing so, we use an economic glossary carried out by means\nof text mining techniques. The goal of our study is to examine the structure of\nsignificant interconnections within a network obtained from the economic\ncontent of presidential speeches. In such a network, nodes are represented by\ntalks and links by values of cosine similarity, the latter computed using the\noccurrences of the economic terms in the speeches. The resulting network\ndisplays a peculiar structure made up of a core (i.e. a set of highly central\nand densely connected nodes) and a periphery (i.e. a set of non-central and\nsparsely connected nodes). The presence of different economic dictionaries\nemployed by the Presidents characterize the core-periphery structure. The\nPresidents' talks belonging to the network's core share the usage of generic\n(non-technical) economic locutions like \"interest\" or \"trade\". While the use of\nmore technical and less frequent terms characterizes the periphery (e.g.\n\"yield\" ). Furthermore, the speeches close in time share a common economic\ndictionary. These results together with the economics glossary usages during\nthe US periods of boom and crisis provide unique insights on the economic\ncontent relationships among Presidents' speeches.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.07880v1", "link": "http://dx.doi.org/10.1007/s10479-019-03372-2"}, {"title": "Dynamic portfolio selection without risk-free assets", "summary": "We consider the mean--variance portfolio optimization problem under the game\ntheoretic framework and without risk-free assets. The problem is solved\nsemi-explicitly by applying the extended Hamilton--Jacobi--Bellman equation.\nAlthough the coefficient of risk aversion in our model is a constant, the\noptimal amounts of money invested in each stock still depend on the current\nwealth in general. The optimal solution is obtained by solving a system of\nordinary differential equations whose existence and uniqueness are proved and a\nnumerical algorithm as well as its convergence speed are provided. Different\nfrom portfolio selection with risk-free assets, our value function is quadratic\nin the current wealth, and the equilibrium allocation is linearly sensitive to\nthe initial wealth. Numerical results show that this model performs better than\nboth the classical one and the variance model in a bull market.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1602.04975v1", "link": "http://arxiv.org/abs/1602.04975v1"}, {"title": "A proposal of a methodological framework with experimental guidelines to\n  investigate clustering stability on financial time series", "summary": "We present in this paper an empirical framework motivated by the practitioner\npoint of view on stability. The goal is to both assess clustering validity and\nyield market insights by providing through the data perturbations we propose a\nmulti-view of the assets' clustering behaviour. The perturbation framework is\nillustrated on an extensive credit default swap time series database available\nonline at www.datagrapple.com.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1509.05475v1", "link": "http://arxiv.org/abs/1509.05475v1"}, {"title": "Generalized Beta Prime Distribution: Stochastic Model of Economic\n  Exchange and Properties of Inequality Indices", "summary": "We argue that a stochastic model of economic exchange, whose steady-state\ndistribution is a Generalized Beta Prime (also known as GB2), and some unique\nproperties of the latter, are the reason for GB2's success in describing\nwealth/income distributions. We use housing sale prices as a proxy to\nwealth/income distribution to numerically illustrate this point. We also\nexplore parametric limits of the distribution to do so analytically. We discuss\nparametric properties of the inequality indices -- Gini, Hoover, Theil T and\nTheil L -- vis-a-vis those of GB2 and introduce a new inequality index, which\nserves a similar purpose. We argue that Hoover and Theil L are more appropriate\nmeasures for distributions with power-law dependencies, especially fat tails,\nsuch as GB2.", "category": ["econ.EM", "q-fin.MF", "q-fin.ST"], "id": "http://arxiv.org/abs/1906.04822v1", "link": "http://arxiv.org/abs/1906.04822v1"}, {"title": "Kinetic models for optimal control of wealth inequalities", "summary": "We introduce and discuss optimal control strategies for kinetic models for\nwealth distribution in a simple market economy, acting to minimize the variance\nof the wealth density among the population. Our analysis is based on a finite\ntime horizon approximation, or model predictive control, of the corresponding\ncontrol problem for the microscopic agents' dynamic and results in an\nalternative theoretical approach to the taxation and redistribution policy at a\nglobal level. It is shown that in general the control is able to modify the\nPareto index of the stationary solution of the corresponding Boltzmann kinetic\nequation, and that this modification can be exactly quantified. Connections\nbetween previous Fokker-Planck based models and taxation-redistribution\npolicies and the present approach are also discussed.", "category": ["econ.GN", "q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.02171v2", "link": "http://dx.doi.org/10.1140/epjb/e2018-90138-1"}, {"title": "An Explicit Default Contagion Model and Its Application to Credit\n  Derivatives Pricing", "summary": "We propose a novel credit default model that takes into account the impact of\nmacroeconomic information and contagion effect on the defaults of obligors. We\nuse a set-valued Markov chain to model the default process, which is the set of\nall defaulted obligors in the group. We obtain analytic characterizations for\nthe default process, and use them to derive pricing formulas in explicit forms\nfor synthetic collateralized debt obligations (CDOs). Furthermore, we use\nmarket data to calibrate the model and conduct numerical studies on the tranche\nspreads of CDOs. We find evidence to support that systematic default risk\ncoupled with default contagion could have the leading component of the total\ndefault risk.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1706.06285v3", "link": "http://arxiv.org/abs/1706.06285v3"}, {"title": "Alpha Discovery Neural Network based on Prior Knowledge", "summary": "In financial automatic feature construction task, genetic programming (GP) is\nthe state-of-the-art technique. It employs reverse polish expression to\nrepresent features and then simulate the evolution process. However, with the\ndevelopment of deep learning, more choices to design this algorithm are\navailable. This paper proposes Alpha Discovery Neural Network (ADNN), equipped\nwith different kinds of feature extractors to construct diversified financial\ntechnical factors based on prior knowledge. The experiment result shows that\nboth fully-connected network and recurrent network are good at extracting\ninformation from financial time series, but convolution network structure can\nnot effectively extract this information. ADNN effectively enrich the current\nfactor pool because in all cases, ADNN can construct more informative and\ndiversified features than GP. Moreover, features constructed by ADNN can always\nimprove original strategy return, Sharpe ratio and max draw-down.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1912.11761v4", "link": "http://arxiv.org/abs/1912.11761v4"}, {"title": "Asymptotic behavior of prices of path dependent options", "summary": "In this paper, we give a numerical method for pricing long maturity, path\ndependent options by using the Markov property for each underlying asset. This\nenables us to approximate a path dependent option by using some kinds of plain\nvanillas. We give some examples whose underlying assets behave as some popular\nLevy processes. Moreover, we give some payoffs and functions used to\napproximate them.", "category": ["q-fin.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/0911.5579v1", "link": "http://arxiv.org/abs/0911.5579v1"}, {"title": "Behavioural Macroeconomic Policy: New perspectives on time inconsistency", "summary": "This paper brings together divergent approaches to time inconsistency from\nmacroeconomic policy and behavioural economics. Behavioural discount functions\nfrom behavioural microeconomics are embedded into a game-theoretic analysis of\ntemptation versus enforcement to construct an encompassing model, nesting\ncombinations of time consistent and time inconsistent preferences. The analysis\npresented in this paper shows that, with hyperbolic/quasihyperbolic\ndiscounting, the enforceable range of inflation targets is narrowed. This\nsuggests limits to the effectiveness of monetary targets, under certain\nconditions. The paper concludes with a discussion of monetary policy\nimplications, explored specifically in the light of current macroeconomic\npolicy debates.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.07858v1", "link": "http://arxiv.org/abs/1907.07858v1"}, {"title": "Trading VIX Futures under Mean Reversion with Regime Switching", "summary": "This paper studies the optimal VIX futures trading problems under a\nregime-switching model. We consider the VIX as mean reversion dynamics with\ndependence on the regime that switches among a finite number of states. For the\ntrading strategies, we analyze the timings and sequences of the investor's\nmarket participation, which leads to several corresponding coupled system of\nvariational inequalities. The numerical approach is developed to solve these\noptimal double stopping problems by using projected-successive-over-relaxation\n(PSOR) method with Crank-Nicolson scheme. We illustrate the optimal boundaries\nvia numerical examples of two-state Markov chain model. In particular, we\nexamine the impacts of transaction costs and regime-switching timings on the\nVIX futures trading strategies.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1605.07945v2", "link": "http://arxiv.org/abs/1605.07945v2"}, {"title": "SABCEMM-A Simulator for Agent-Based Computational Economic Market Models", "summary": "We introduce the simulation tool SABCEMM (Simulator for Agent-Based\nComputational Economic Market Models) for agent-based computational economic\nmarket (ABCEM) models. Our simulation tool is implemented in C++ and we can\neasily run ABCEM models with several million agents. The object-oriented\nsoftware design enables the isolated implementation of building blocks for\nABCEM models, such as agent types and market mechanisms. The user can design\nand compare ABCEM models in a unified environment by recombining existing\nbuilding blocks using the XML-based SABCEMM configuration file. We introduce an\nabstract ABCEM model class which our simulation tool is built upon.\nFurthermore, we present the software architecture as well as computational\naspects of SABCEMM. Here, we focus on the efficiency of SABCEMM with respect to\nthe run time of our simulations. We show the great impact of different random\nnumber generators on the run time of ABCEM models. The code and documentation\nis published on GitHub at https://github.com/SABCEMM/SABCEMM, such that all\nresults can be reproduced by the reader.", "category": ["q-fin.CP", "econ.EM", "q-fin.GN", "q-fin.TR"], "id": "http://arxiv.org/abs/1801.01811v2", "link": "http://arxiv.org/abs/1801.01811v2"}, {"title": "Is the public sector of your country a diffusion borrower? Empirical\n  evidence from Brazil", "summary": "We propose a diffusion process to describe the global dynamic evolution of\ncredit operations at a national level given observed operations at a\nsubnational level in a sovereign country. Empirical analysis with a unique\ndataset from Brazilian federate constituents supports the conclusions. Despite\nthe heterogeneity observed in credit operations at a subnational level, the\naggregated dynamics at a national level were accurately described with the\nproposed model. Results may guide management of public finances, particularly\ndebt manager authorities in charge of reaching surplus targets.", "category": ["q-fin.GN", "q-fin.ST"], "id": "http://arxiv.org/abs/1604.07782v1", "link": "http://dx.doi.org/10.1371/journal.pone.0185257"}, {"title": "Minimal model of financial stylized facts", "summary": "In this work we afford the statistical characterization of a linear\nStochastic Volatility Model featuring Inverse Gamma stationary distribution for\nthe instantaneous volatility. We detail the derivation of the moments of the\nreturn distribution, revealing the role of the Inverse Gamma law in the\nemergence of fat tails, and of the relevant correlation functions. We also\npropose a systematic methodology for estimating the parameters, and we describe\nthe empirical analysis of the Standard & Poor 500 index daily returns,\nconfirming the ability of the model to capture many of the established stylized\nfact as well as the scaling properties of empirical distributions over\ndifferent time horizons.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1011.5983v2", "link": "http://dx.doi.org/10.1103/PhysRevE.83.041111"}, {"title": "Modeling of Financial Data: Comparison of the Truncated L\u00e9vy Flight\n  and the ARCH(1) and GARCH(1,1) processes", "summary": "We compare our results on empirical analysis of financial data with\nsimulations of two stochastic models of the dynamics of stock market prices.\nThe two models are (i) the truncated L\\'evy flight recently introduced by us\nand (ii) the ARCH(1) and GARCH(1,1) processes. We find that the TLF well\ndescribes the scaling and its breakdown observed in empirical data, while it is\nnot able to properly describe the fluctuations of volatility empirically\ndetected. The ARCH(1) and GARCH(1,1) models are able to describe the\nprobability density function of price changes at a given time horizon, but both\nfail to describe the scaling properties of the PDFs for short time horizons.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/9804126v1", "link": "http://dx.doi.org/10.1016/S0378-4371(98)00020-X"}, {"title": "Expected exponential utility maximization of insurers with a general\n  diffusion factor model : The complete market case", "summary": "In this paper, we consider the problem of optimal investment by an insurer.\nThe insurer invests in a market consisting of a bank account and $m$ risky\nassets. The mean returns and volatilities of the risky assets depend\nnonlinearly on economic factors that are formulated as the solutions of general\nstochastic differential equations. The wealth of the insurer is described by a\nCram\\'er--Lundberg process, and the insurer preferences are exponential.\nAdapting a dynamic programming approach, we derive Hamilton--Jacobi--Bellman\n(HJB) equation. And, we prove the unique solvability of HJB equation. In\naddition, the optimal strategy is also obtained using the coupled forward and\nbackward stochastic differential equations (FBSDEs). Finally, proving the\nverification theorem, we construct the optimal strategy.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1903.08957v1", "link": "http://arxiv.org/abs/1903.08957v1"}, {"title": "Rethinking value creation from the resource based view: the case of\n  human capital in moroccan hotels", "summary": "The growth of the modern knowledge-based economy is becoming less and less\ndependent on tangible assets and more on intangible ones. In this context, the\nrole of human capital in the value creation process has become central. Despite\nthe large amount of scientific work on human capital phenomena, little research\nhas revealed the role of human capital in the process of creating value. The\npurpose of this article is to evaluate the impact of human capital on value\ncreation within 31 classified hotels in Morocco for the period 2013-2015. This\npaper is organized into four sections. First, we return to the main\nconceptualization of value creation. The goal of this first section is to\nsynthesize prior work on this construct and highlight the main role of the\nresource based view (RBV) in explaining it. This view presents the point that\nlinks value creation to human capital given that this latter concept is one of\nthe most resources of the firm. Next, we present the main definition of human\ncapital. To do so, we make use of concepts from psychology, economy and\nstrategic human resource management. Then, we shed light on the existing\nrelationship between the two concepts of our research. Finally, we present the\nmethodology of this research as well as the results. The required data to\ncalculate value creation is obtained mainly from the annual reports of Moroccan\nhotels. Whereas, human capital is assessed by a questionnaire using the scale\nof Subramaniam and Youndt (2005). Data is examined using linear regression by\nPASW statistics software. The results of this study give a more concrete\npicture on the creation of value in this context and refute any link between\nthese two concepts.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1805.05465v1", "link": "http://arxiv.org/abs/1805.05465v1"}, {"title": "Adaptive Financial Fraud Detection in Imbalanced Data with Time-Varying\n  Poisson Processes", "summary": "This paper discusses financial fraud detection in imbalanced dataset using\nhomogeneous and non-homogeneous Poisson processes. The probability of\npredicting fraud on the financial transaction is derived. Applying our\nmethodology to the financial dataset shows a better predicting power than a\nbaseline approach, especially in the case of higher imbalanced data.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1912.04308v1", "link": "http://arxiv.org/abs/1912.04308v1"}, {"title": "Deterministic Factors of Stock Networks based on Cross-correlation in\n  Financial Market", "summary": "The stock market has been known to form homogeneous stock groups with a\nhigher correlation among different stocks according to common economic factors\nthat influence individual stocks. We investigate the role of common economic\nfactors in the market in the formation of stock networks, using the arbitrage\npricing model reflecting essential properties of common economic factors. We\nfind that the degree of consistency between real and model stock networks\nincreases as additional common economic factors are incorporated into our\nmodel. Furthermore, we find that individual stocks with a large number of links\nto other stocks in a network are more highly correlated with common economic\nfactors than those with a small number of links. This suggests that common\neconomic factors in the stock market can be understood in terms of\ndeterministic factors.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0705.0076v1", "link": "http://dx.doi.org/10.1016/j.physa.2007.04.102"}, {"title": "Market Making via Reinforcement Learning", "summary": "Market making is a fundamental trading problem in which an agent provides\nliquidity by continually offering to buy and sell a security. The problem is\nchallenging due to inventory risk, the risk of accumulating an unfavourable\nposition and ultimately losing money. In this paper, we develop a high-fidelity\nsimulation of limit order book markets, and use it to design a market making\nagent using temporal-difference reinforcement learning. We use a linear\ncombination of tile codings as a value function approximator, and design a\ncustom reward function that controls inventory risk. We demonstrate the\neffectiveness of our approach by showing that our agent outperforms both simple\nbenchmark strategies and a recent online learning approach from the literature.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1804.04216v1", "link": "http://arxiv.org/abs/1804.04216v1"}, {"title": "Applications of time-delayed backward stochastic differential equations\n  to pricing, hedging and portfolio management", "summary": "In this paper we investigate novel applications of a new class of equations\nwhich we call time-delayed backward stochastic differential equations.\nTime-delayed BSDEs may arise in finance when we want to find an investment\nstrategy and an investment portfolio which should replicate a liability or meet\na target depending on the applied strategy or the past values of the portfolio.\nIn this setting, a managed investment portfolio serves simultaneously as the\nunderlying security on which the liability/target is contingent and as a\nreplicating portfolio for that liability/target. This is usually the case for\ncapital-protected investments and performance-linked pay-offs. We give examples\nof pricing, hedging and portfolio management problems (asset-liability\nmanagement problems) which could be investigated in the framework of\ntime-delayed BSDEs. Our motivation comes from life insurance and we focus on\nparticipating contracts and variable annuities. We derive the corresponding\ntime-delayed BSDEs and solve them explicitly or at least provide hints how to\nsolve them numerically. We give a financial interpretation of the theoretical\nfact that a time-delayed BSDE may not have a solution or may have multiple\nsolutions.", "category": ["q-fin.PR", "q-fin.RM"], "id": "http://arxiv.org/abs/1005.4417v3", "link": "http://arxiv.org/abs/1005.4417v3"}, {"title": "A Climate Insidium with a Price on Warming", "summary": "In this paper, I introduce a new emissions trading system (ETS) design to\naddress the problems with existing ETSs and carbon taxes. First, existing ETS\ndesigns inhibit emissions but do not constrain warming to any set level.\nExisting ETSs have the indirect objective of reducing emissions instead of\ndirectly reducing warming. Even a global mechanism using an existing ETS cannot\nguarantee a particular warming path. Part 1: A Price on Warming addresses this.\nMy proposed market trades contracts tied to temperature in a double-sided\nauction of emissions permits and sequestration contracts. Unlike existing ETSs,\nthe mechanism has a consistent timescale and metric tied to warming, with\nexplicit limits on global temperature in every period into the far future.\nEvery auction finds prices for emissions into the far future. Second, if a\njurisdiction does not require firms to manage their emissions, the firms have\nlittle incentive to do so. Part 2: A Climate Insidium addresses this. My design\nincentivizes firms to participate even if their jurisdictions do not join. With\nsanctions from member jurisdictions and participating firms, the design has\nbottom-up incentives for joining, and the incentives rise over time under\nrealistic conditions, potentially resulting in a rush to join. Third, existing\ndesigns have high transaction costs for implementation, requiring international\ntreaties to begin. Part 3: A Faster Path Forward addresses this. I propose a\npath without national or international action to begin. A coalition can\nimplement these rules, creating political force to accelerate participation.\nFull implementation still requires national agreements. This design appears to\nbe closer to \"first best\", with a lower cost of climate mitigation, than any in\nthe literature, while increasing the certainty of avoiding catastrophic global\nwarming. It might also provide a faster pathway to implementation.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2003.05114v1", "link": "http://arxiv.org/abs/2003.05114v1"}, {"title": "Specification Testing in Nonparametric Instrumental Quantile Regression", "summary": "There are many environments in econometrics which require nonseparable\nmodeling of a structural disturbance. In a nonseparable model with endogenous\nregressors, key conditions are validity of instrumental variables and\nmonotonicity of the model in a scalar unobservable variable. Under these\nconditions the nonseparable model is equivalent to an instrumental quantile\nregression model. A failure of the key conditions, however, makes instrumental\nquantile regression potentially inconsistent. This paper develops a methodology\nfor testing the hypothesis whether the instrumental quantile regression model\nis correctly specified. Our test statistic is asymptotically normally\ndistributed under correct specification and consistent against any alternative\nmodel. In addition, test statistics to justify the model simplification are\nestablished. Finite sample properties are examined in a Monte Carlo study and\nan empirical illustration is provided.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.10129v1", "link": "http://dx.doi.org/10.1017/S0266466619000288"}, {"title": "How Short Sales Circumvent the Capital Gains Tax System", "summary": "Through a short sale, a person borrows a share of stock from a lender, sells\nthe borrowed share to a third person at the current price, and purchases an\nidentical share in the market at a future date and at a future price to replace\nthe borrowed share of stock. This only makes sense if the short seller\nanticipates a downward trend in share price. The short seller incurs a gain if\nshare price decreases because the cost of replacing the borrowed share falls\nbelow the selling price. The reverse is true in an ordinary sale, where a\nperson owning a share of stock incurs a loss if price decreases because the\nselling price falls below the basis or acquisition cost. Therefore, when a\ntaxpayer simultaneously owns a share of stock and short sells an identical\nstock, any gain in an ordinary sale of the owned stock is offset by a\ncorresponding loss in the short sale of the borrowed identical stock, vice\nversa. This offsetting effect, in turn, creates an unexpected tax deferral\nopportunity abused in other jurisdictions and which remains unregulated in the\nPhilippine tax system.", "category": ["q-fin.GN", "q-fin.TR"], "id": "http://arxiv.org/abs/1712.09987v1", "link": "http://arxiv.org/abs/1712.09987v1"}, {"title": "Systemic Risk in a Unifying Framework for Cascading Processes on\n  Networks", "summary": "We introduce a general framework for models of cascade and contagion\nprocesses on networks, to identify their commonalities and differences. In\nparticular, models of social and financial cascades, as well as the fiber\nbundle model, the voter model, and models of epidemic spreading are recovered\nas special cases. To unify their description, we define the net fragility of a\nnode, which is the difference between its fragility and the threshold that\ndetermines its failure. Nodes fail if their net fragility grows above zero and\ntheir failure increases the fragility of neighbouring nodes, thus possibly\ntriggering a cascade. In this framework, we identify three classes depending on\nthe way the fragility of a node is increased by the failure of a neighbour. At\nthe microscopic level, we illustrate with specific examples how the failure\nspreading pattern varies with the node triggering the cascade, depending on its\nposition in the network and its degree. At the macroscopic level, systemic risk\nis measured as the final fraction of failed nodes, $X^\\ast$, and for each of\nthe three classes we derive a recursive equation to compute its value. The\nphase diagram of $X^\\ast$ as a function of the initial conditions, thus allows\nfor a prediction of the systemic risk as well as a comparison of the three\ndifferent model classes. We could identify which model class lead to a\nfirst-order phase transition in systemic risk, i.e. situations where small\nchanges in the initial conditions may lead to a global failure. Eventually, we\ngeneralize our framework to encompass stochastic contagion models. This\nindicates the potential for further generalizations.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/0907.5325v2", "link": "http://dx.doi.org/10.1140/epjb/e2009-00347-4"}, {"title": "Multivariate Geometric Expectiles", "summary": "A generalization of expectiles for d-dimensional multivariate distribution\nfunctions is introduced. The resulting geometric expectiles are unique\nsolutions to a convex risk minimization problem and are given by d-dimensional\nvectors. They are well behaved under common data transformations and the\ncorresponding sample version is shown to be a consistent estimator. We\nexemplify their usage as risk measures in a number of multivariate settings,\nhighlighting the influence of varying margins and dependence structures.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1704.01503v2", "link": "http://arxiv.org/abs/1704.01503v2"}, {"title": "A maximum entropy network reconstruction of macroeconomic models", "summary": "In this article the problem of reconstructing the pattern of connection\nbetween agents from partial empirical data in a macro-economic model is\naddressed, given a set of behavioral equations. This systemic point of view\nputs the focus on distributional and network effects, rather than\ntime-dependence. Using the theory of complex networks we compare several models\nto reconstruct both the topology and the flows of money of the different types\nof monetary transactions, while imposing a series of constraints related to\nnational accounts, and to empirical network sparsity. Some properties of\nreconstructed networks are compared with their empirical counterpart.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1807.10464v2", "link": "http://dx.doi.org/10.1016/j.physa.2018.12.020"}, {"title": "Systemic risk measures with markets volatility", "summary": "As systemic risk has become a hot topic in the financial markets, how to\nmeasure, allocate and regulate the systemic risk are becoming especially\nimportant. However, the financial markets are becoming more and more\ncomplicate, which makes the usual study of systemic risk to be restricted. In\nthis paper, we will study the systemic risk measures on a special space\n$L^{p(\\cdot)}$ where the variable exponent $p(\\cdot)$ is no longer a given real\nnumber like the space $L^{p}$, but a random variable, which reflects the\npossible volatility of the financial markets. Finally, the dual representation\nfor this new systemic risk measures will be studied. Our results show that\nevery this new systemic risk measure can be decomposed into a convex certain\nfunction and a simple-systemic risk measure, which provides a new ideas for\ndealing with the systemic risk.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1812.06185v3", "link": "http://arxiv.org/abs/1812.06185v3"}, {"title": "Closed Quantum Black-Scholes: Quantum Drift and the Heisenberg Equation\n  of Motion", "summary": "In this article we model a financial derivative price as an observable on the\nmarket state function. We apply geometric techniques to integrating the\nHeisenberg Equation of Motion. We illustrate how the non-commutative nature of\nthe model introduces quantum interference effects that can act as either a drag\nor a boost on the resulting return. The ultimate objective is to investigate\nthe nature of quantum drift in the Accardi-Boukas quantum Black-Scholes\nframework which involves modelling the financial market as a quantum\nobservable, and introduces randomness through the Hudson-Parthasarathy quantum\nstochastic calculus. In particular we aim to differentiate randomness that is\nintroduced through external noise (quantum stochastic calculus) and randomness\nthat is fundamental to a quantum system (Heisenberg Equation of Motion).", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1911.11475v3", "link": "http://arxiv.org/abs/1911.11475v3"}, {"title": "The effect of round-off error on long memory processes", "summary": "We study how the round-off (or discretization) error changes the statistical\nproperties of a Gaussian long memory process. We show that the autocovariance\nand the spectral density of the discretized process are asymptotically rescaled\nby a factor smaller than one, and we compute exactly this scaling factor.\nConsequently, we find that the discretized process is also long memory with the\nsame Hurst exponent as the original process. We consider the properties of two\nestimators of the Hurst exponent, namely the local Whittle (LW) estimator and\nthe Detrended Fluctuation Analysis (DFA). By using analytical considerations\nand numerical simulations we show that, in presence of round-off error, both\nestimators are severely negatively biased in finite samples. Under regularity\nconditions we prove that the LW estimator applied to discretized processes is\nconsistent and asymptotically normal. Moreover, we compute the asymptotic\nproperties of the DFA for a generic (i.e. non Gaussian) long memory process and\nwe apply the result to discretized processes.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1107.4476v3", "link": "http://arxiv.org/abs/1107.4476v3"}, {"title": "Wealth redistribution with finite resources", "summary": "We present a simplified model for the exploitation of finite resources by\ninteracting agents, where each agent receives a random fraction of the\navailable resources. An extremal dynamics ensures that the poorest agent has a\nchance to change its economic welfare. After a long transient, the system\nself-organizes into a critical state that maximizes the average performance of\neach participant. Our model exhibits a new kind of wealth condensation, where\nvery few extremely rich agents are stable in time and the rest stays in the\nmiddle class.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/nlin/0109015v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01737-5"}, {"title": "An FBSDE approach to market impact games with stochastic parameters", "summary": "We analyze a market impact game between $n$ risk averse agents who compete\nfor liquidity in a market impact model with permanent price impact and\nadditional slippage. Most market parameters, including volatility and drift,\nare allowed to vary stochastically. Our first main result characterizes the\nNash equilibrium in terms of a fully coupled system of forward-backward\nstochastic differential equations (FBSDEs). Our second main result provides\nconditions under which this system of FBSDEs has indeed a unique solution,\nwhich in turn yields the unique Nash equilibrium. We furthermore obtain\nclosed-form solutions in special situations and analyze them numerically", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/2001.00622v1", "link": "http://arxiv.org/abs/2001.00622v1"}, {"title": "Large Vector Auto Regressions", "summary": "One popular approach for nonstructural economic and financial forecasting is\nto include a large number of economic and financial variables, which has been\nshown to lead to significant improvements for forecasting, for example, by the\ndynamic factor models. A challenging issue is to determine which variables and\n(their) lags are relevant, especially when there is a mixture of serial\ncorrelation (temporal dynamics), high dimensional (spatial) dependence\nstructure and moderate sample size (relative to dimensionality and lags). To\nthis end, an \\textit{integrated} solution that addresses these three challenges\nsimultaneously is appealing. We study the large vector auto regressions here\nwith three types of estimates. We treat each variable's own lags different from\nother variables' lags, distinguish various lags over time, and is able to\nselect the variables and lags simultaneously. We first show the consequences of\nusing Lasso type estimate directly for time series without considering the\ntemporal dependence. In contrast, our proposed method can still produce an\nestimate as efficient as an \\textit{oracle} under such scenarios. The tuning\nparameters are chosen via a data driven \"rolling scheme\" method to optimize the\nforecasting performance. A macroeconomic and financial forecasting problem is\nconsidered to illustrate its superiority over existing estimators.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1106.3915v1", "link": "http://arxiv.org/abs/1106.3915v1"}, {"title": "Trading leads to scale-free self-organization", "summary": "Financial markets display scale-free behavior in many different aspects. The\npower-law behavior of part of the distribution of individual wealth has been\nrecognized by Pareto as early as the nineteenth century. Heavy-tailed and\nscale-free behavior of the distribution of returns of different financial\nassets have been confirmed in a series of works. The existence of a Pareto-like\ndistribution of the wealth of market participants has been connected with the\nscale-free distribution of trading volumes and price-returns. The origin of the\nPareto-like wealth distribution, however, remained obscure. Here we show that\nit is the process of trading itself that under two mild assumptions\nspontaneously leads to a self-organization of the market with a Pareto-like\nwealth distribution for the market participants and at the same time to a\nscale-free behavior of return fluctuations. These assumptions are (i) everybody\ntrades proportional to his current capacity and (ii) supply and demand\ndetermine the relative value of the goods.", "category": ["q-fin.TR", "q-fin.ST"], "id": "http://arxiv.org/abs/0905.4815v1", "link": "http://arxiv.org/abs/0905.4815v1"}, {"title": "Decision-facilitating information in hidden-action setups: An\n  agent-based approach", "summary": "The hidden-action model captures a fundamental problem of principal-agent\ntheory and provides an optimal sharing rule when only the outcome but not the\neffort can be observed. However, the hidden-action model builds on various\nexplicit and also implicit assumptions about the information of the contracting\nparties. This paper relaxes key assumptions regarding the availability of\ninformation included in the hidden-action model in order to study whether and,\nif so, how fast the optimal sharing rule is achieved and how this is affected\nby the various types of information employed in the principal-agent relation.\nOur analysis particularly focuses on information about the environment and\nabout feasible actions for the agent. We follow an approach to transfer\nclosed-form mathematical models into agent-based computational models and show\nthat the extent of information about feasible options to carry out a task only\nhas an impact on performance if decision makers are well informed about the\nenvironment, and that the decision whether to perform exploration or\nexploitation when searching for new feasible options only affects performance\nin specific situations. Having good information about the environment, on the\ncontrary, appears to be crucial in almost all situations.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1908.07998v2", "link": "http://arxiv.org/abs/1908.07998v2"}, {"title": "Fuzzy Profit Shifting: A Model for Optimal Tax-induced Transfer Pricing\n  with Fuzzy Arm's Length Parameter", "summary": "This paper proposes a model of optimal tax-induced transfer pricing with a\nfuzzy arm's length parameter. Fuzzy numbers provide a suitable structure for\nmodelling the ambiguity that is intrinsic to the arm's length parameter. For\nthe usual conditions regarding the anti-shifting mechanisms, the optimal\ntransfer price becomes a maximising $\\alpha$-cut of the fuzzy arm's length\nparameter. Nonetheless, we show that it is profitable for firms to choose any\nmaximising transfer price if the probability of tax audit is sufficiently low,\neven if the chosen price is considered a completely non-arm's length price by\ntax authorities. In this case, we derive the necessary and sufficient\nconditions to prevent this extreme shifting strategy", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1901.03843v1", "link": "http://arxiv.org/abs/1901.03843v1"}, {"title": "Determining Optimal Trading Rules without Backtesting", "summary": "Calibrating a trading rule using a historical simulation (also called\nbacktest) contributes to backtest overfitting, which in turn leads to\nunderperformance. In this paper we propose a procedure for determining the\noptimal trading rule (OTR) without running alternative model configurations\nthrough a backtest engine. We present empirical evidence of the existence of\nsuch optimal solutions for the case of prices following a discrete\nOrnstein-Uhlenbeck process, and show how they can be computed numerically.\nAlthough we do not derive a closed-form solution for the calculation of OTRs,\nwe conjecture its existence on the basis of the empirical evidence presented.", "category": ["q-fin.PM", "q-fin.MF"], "id": "http://arxiv.org/abs/1408.1159v2", "link": "http://arxiv.org/abs/1408.1159v2"}, {"title": "A Binomial Asset Pricing Model in a Categorical Setting", "summary": "Adachi and Ryu introduced a category Prob of probability spaces whose objects\nare all probability spaces and whose arrows correspond to measurable functions\nsatisfying an absolutely continuous requirement in [Adachi and Ryu, 2019]. In\nthis paper, we develop a binomial asset pricing model based on Prob. We\nintroduce generalized filtrations with which we can represent situations such\nas some agents forget information at some specific time. We investigate the\nvaluations of financial claims along this type of non-standard filtrations.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1905.01894v3", "link": "http://arxiv.org/abs/1905.01894v3"}, {"title": "The super-replication theorem under proportional transaction costs\n  revisited", "summary": "We consider a financial market with one riskless and one risky asset. The\nsuper-replication theorem states that there is no duality gap in the problem of\nsuper-replicating a contingent claim under transaction costs and the associated\ndual problem. We give two versions of this theorem. The first theorem relates a\nnum\\'eraire-based admissibility condition in the primal problem to the notion\nof a local martingale in the dual problem. The second theorem relates a\nnum\\'eraire -free admissibility condition in the primal problem to the notion\nof a uniformly integrable martingale in the dual problem.", "category": ["math.PR", "q-fin.PM"], "id": "http://arxiv.org/abs/1405.1266v1", "link": "http://arxiv.org/abs/1405.1266v1"}, {"title": "How do scientific disciplines evolve in applied sciences? The properties\n  of scientific fission and ambidextrous scientific drivers", "summary": "One of the fundamental questions in science is how scientific disciplines\nevolve and sustain progress in society. No studies to date allows us to explain\nthe endogenous processes that support the evolution of scientific disciplines\nand emergence of new scientific fields in applied sciences of physics. This\nstudy confronts this problem here by investigating the evolution of\nexperimental physics to explain and generalize some characteristics of the\ndynamics of applied sciences. Empirical analysis suggests properties about the\nevolution of experimental physics and in general of applied sciences, such as:\na) scientific fission, the evolution of scientific disciplines generates a\nprocess of division into two or more research fields that evolve as autonomous\nentities over time; b) ambidextrous drivers of science, the evolution of\nscience via scientific fission is due to scientific discoveries or new\ntechnologies; c) new driving research fields, the drivers of scientific\ndisciplines are new research fields rather than old ones; d) science driven by\ndevelopment of general purpose technologies, the evolution of experimental\nphysics and applied sciences is due to the convergence of experimental and\ntheoretical branches of physics associated with the development of computer,\ninformation systems and applied computational science. Results also reveal that\naverage duration of the upwave of scientific production in scientific fields\nsupporting experimental physics is about 80 years. Overall, then, this study\nbegins the process of clarifying and generalizing, as far as possible, some\ncharacteristics of the evolutionary dynamics of scientific disciplines that can\nlay a foundation for the development of comprehensive properties explaining the\nevolution of science as a whole for supporting fruitful research policy\nimplications directed to advancement of science and technological progress in\nsociety.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.05363v1", "link": "http://arxiv.org/abs/1911.05363v1"}, {"title": "Factorial Moments in Complex Systems", "summary": "Factorial moments are convenient tools in particle physics to characterize\nthe multiplicity distributions when phase-space resolution ($\\Delta$) becomes\nsmall. They include all correlations within the system of particles and\nrepresent integral characteristics of any correlation between these particles.\nIn this letter, we show a direct comparison between high energy physics and\nquantitative finance results. Both for physics and finance, we illustrate that\ncorrelations between particles lead to a broadening of the multiplicity\ndistribution and to dynamical fluctuations when the resolution becomes small\nenough. From the generating function of factorial moments, we make a prediction\non the gap probability for sequences of returns of positive or negative signs.\nThe gap is defined as the number of consecutive positive returns after a\nnegative return, thus this is a gap in negative return. Inversely for a gap in\npositive return. Then, the gap probability is shown to be exponentially\nsuppressed within the gap size. We confirm this prediction with data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1108.5946v1", "link": "http://arxiv.org/abs/1108.5946v1"}, {"title": "Scalar multivariate risk measures with a single eligible asset", "summary": "In this paper we present results on scalar risk measures in markets with\ntransaction costs. Such risk measures are defined as the minimal capital\nrequirements in the cash asset. First, some results are provided on the dual\nrepresentation of such risk measures, with particular emphasis given on the\nspace of dual variables as (equivalent) martingale measures and prices\nconsistent with the market model. Then, these dual representations are used to\nobtain the main results of this paper on time consistency for scalar risk\nmeasures in markets with frictions. It is well known from the superhedging risk\nmeasure in markets with transaction costs, as in Jouini and Kallal (1995), Roux\nand Zastawniak (2016), and Loehne and Rudloff (2014), that the usual scalar\nconcept of time consistency is too strong and not satisfied. We will show that\na weaker notion of time consistency can be defined, which corresponds to the\nusual scalar time consistency but under any fixed consistent pricing process.\nWe will prove the equivalence of this weaker notion of time consistency and a\ncertain type of backward recursion with respect to the underlying risk measure\nwith a fixed consistent pricing process. Several examples are given, with\nspecial emphasis on the superhedging risk measure.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1807.10694v2", "link": "http://arxiv.org/abs/1807.10694v2"}, {"title": "Correlation structure and principal components in global crude oil\n  market", "summary": "This article investigates the correlation structure of the global crude oil\nmarket using the daily returns of 71 oil price time series across the world\nfrom 1992 to 2012. We identify from the correlation matrix six clusters of time\nseries exhibiting evident geographical traits, which supports Weiner's (1991)\nregionalization hypothesis of the global oil market. We find that intra-cluster\npairs of time series are highly correlated while inter-cluster pairs have\nrelatively low correlations. Principal component analysis shows that most\neigenvalues of the correlation matrix locate outside the prediction of the\nrandom matrix theory and these deviating eigenvalues and their corresponding\neigenvectors contain rich economic information. Specifically, the largest\neigenvalue reflects a collective effect of the global market, other four\nlargest eigenvalues possess a partitioning function to distinguish the six\nclusters, and the smallest eigenvalues highlight the pairs of time series with\nthe largest correlation coefficients. We construct an index of the global oil\nmarket based on the eigenfortfolio of the largest eigenvalue, which evolves\nsimilarly as the average price time series and has better performance than the\nbenchmark $1/N$ portfolio under the buy-and-hold strategy.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1405.5000v1", "link": "http://dx.doi.org/10.1007/s00181-015-1057-1"}, {"title": "About the non-random Content of Financial Markets", "summary": "For the pedestrian observer, financial markets look completely random with\nerratic and uncontrollable behavior. To a large extend, this is correct. At\nfirst approximation the difference between real price changes and the random\nwalk model is too small to be detected using traditional time series analysis.\nHowever, we show in the following that this difference between real financial\ntime series and random walks, as small as it is, is detectable using modern\nstatistical multivariate analysis, with several triggers encoded in trading\nsystems. This kind of analysis are based on methods widely used in nuclear\nphysics, with large samples of data and advanced statistical inference.\nConsidering the movements of the Euro future contract at high frequency, we\nshow that a part of the non-random content of this series can be inferred,\nnamely the trend-following content depending on volatility ranges. Of course,\nthis is not a general proof of statistical inference, as we focus on one\nparticular example and the generality of the process can not be claimed.\nTherefore, we produce other examples on a completely different markets, largely\nuncorrelated to the Euro future, namely the DAX and Cacao future contracts. The\nsame procedure is followed using a trading system, based on the same\ningredients. We show that similar results can be obtained and we conclude that\nthis is an evidence that some invariants, as encoded in our system, have been\nidentified. They provide a kind of quantification of the non-random content of\nthe financial markets explored over a 10 years period of time.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1108.3155v2", "link": "http://arxiv.org/abs/1108.3155v2"}, {"title": "Following a Trend with an Exponential Moving Average: Analytical Results\n  for a Gaussian Model", "summary": "We investigate how price variations of a stock are transformed into profits\nand losses (P&Ls) of a trend following strategy. In the frame of a Gaussian\nmodel, we derive the probability distribution of P&Ls and analyze its moments\n(mean, variance, skewness and kurtosis) and asymptotic behavior (quantiles). We\nshow that the asymmetry of the distribution (with often small losses and less\nfrequent but significant profits) is reminiscent to trend following strategies\nand less dependent on peculiarities of price variations. At short times, trend\nfollowing strategies admit larger losses than one may anticipate from standard\nGaussian estimates, while smaller losses are ensured at longer times. Simple\nexplicit formulas characterizing the distribution of P&Ls illustrate the basic\nmechanisms of momentum trading, while general matrix representations can be\napplied to arbitrary Gaussian models. We also compute explicitly annualized\nrisk adjusted P&L and strategy turnover to account for transaction costs. We\ndeduce the trend following optimal timescale and its dependence on both\nauto-correlation level and transaction costs. Theoretical results are\nillustrated on the Dow Jones index.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1308.5658v1", "link": "http://dx.doi.org/10.1016/j.physa.2013.10.007"}, {"title": "USDA Forecasts: A meta-analysis study", "summary": "The primary goal of this study is doing a meta-analysis research on two\ngroups of published studies. First, the ones that focus on the evaluation of\nthe United States Department of Agriculture (USDA) forecasts and second, the\nones that evaluate the market reactions to the USDA forecasts. We investigate\nfour questions. 1) How the studies evaluate the accuracy of the USDA forecasts?\n2) How they evaluate the market reactions to the USDA forecasts? 3) Is there\nany heterogeneity in the results of the mentioned studies? 4) Is there any\npublication bias? About the first question, while some researchers argue that\nthe forecasts are unbiased, most of them maintain that they are biased,\ninefficient, not optimal, or not rational. About the second question, while a\nfew studies claim that the forecasts are not newsworthy, most of them maintain\nthat they are newsworthy, provide useful information, and cause market\nreactions. About the third and the fourth questions, based on our findings,\nthere are some clues that the results of the studies are heterogeneous, but we\ndidn't find enough evidences of publication bias.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1801.06575v1", "link": "http://arxiv.org/abs/1801.06575v1"}, {"title": "Forecasting dynamic return distributions based on ordered binary choice", "summary": "We present a simple approach to forecasting conditional probability\ndistributions of asset returns. We work with a parsimonious specification of\nordered binary choice regression that imposes a connection on sign\npredictability across different quantiles. The model forecasts the future\nconditional probability distributions of returns quite precisely when using a\npast indicator and past volatility proxy as predictors. Direct benefits of the\nmodel are revealed in an empirical application to the 29 most liquid U.S.\nstocks. The forecast probability distribution is translated to significant\neconomic gains in a simple trading strategy. Our approach can also be useful in\nmany other applications where conditional distribution forecasts are desired.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1711.05681v3", "link": "http://arxiv.org/abs/1711.05681v3"}, {"title": "Optimal simulation schemes for L\u00e9vy driven stochastic differential\n  equations", "summary": "We consider a general class of high order weak approximation schemes for\nstochastic differential equations driven by L\\'evy processes with infinite\nactivity. These schemes combine a compound Poisson approximation for the jump\npart of the L\\'evy process with a high order scheme for the Brownian driven\ncomponent, applied between the jump times. The overall approximation is\nanalyzed using a stochastic splitting argument. The resulting error bound\ninvolves separate contributions of the compound Poisson approximation and of\nthe discretization scheme for the Brownian part, and allows, on one hand, to\nbalance the two contributions in order to minimize the computational time, and\non the other hand, to study the optimal design of the approximating compound\nPoisson process. For driving processes whose L\\'evy measure explodes near zero\nin a regularly varying way, this procedure allows to construct discretization\nschemes with arbitrary order of convergence.", "category": ["math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1204.4877v1", "link": "http://arxiv.org/abs/1204.4877v1"}, {"title": "Inference for First-Price Auctions with Guerre, Perrigne, and Vuong's\n  Estimator", "summary": "We consider inference on the probability density of valuations in the\nfirst-price sealed-bid auctions model within the independent private value\nparadigm. We show the asymptotic normality of the two-step nonparametric\nestimator of Guerre, Perrigne, and Vuong (2000) (GPV), and propose an easily\nimplementable and consistent estimator of the asymptotic variance. We prove the\nvalidity of the pointwise percentile bootstrap confidence intervals based on\nthe GPV estimator. Lastly, we use the intermediate Gaussian approximation\napproach to construct bootstrap-based asymptotically valid uniform confidence\nbands for the density of the valuations.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.06401v1", "link": "http://dx.doi.org/10.1016/j.jeconom.2019.02.006"}, {"title": "Characteristics of Cost Overruns for Dutch Transport Infrastructure\n  Projects and the Importance of the Decision to Build and Project Phases", "summary": "Using a methodology similar to that used the in the worldwide research, the\ncost performance of Dutch large-scale transport infrastructure projects is\ndetermined. In the Netherlands, cost overruns are as common as cost underruns\nbut because cost overruns are larger than cost underruns projects on average\nhave a cost overrun of 16.5%. The focus on one country further enabled to\nconsider cost overruns during different project development phases. It turned\nout that in the Netherlands the majority of the cost overrun occurs in the\npre-construction phase (the period between the formal decision to build and the\nstart of construction). The frequency as well as the magnitude of\npre-construction cost overrun is significantly higher than in the construction\nphase. The used methodology of calculating cost overruns does however not take\nlock-in into account. This phenomenon shows that the real decision to build was\ntaken much earlier in the decision-making process. Since estimated costs are\nusually lower during these earlier stages, the cost overruns based on this real\ndecision to build are likely to be much higher. Cost overruns presented in\nstudies are therefore often underestimated and the problem of cost overruns is\nmuch larger than we think.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1307.2178v2", "link": "http://dx.doi.org/10.1016/j.tranpol.2012.04.001"}, {"title": "In (Stochastic) Search of a Fairer Alife", "summary": "Economies and societal structures in general are complex stochastic systems\nwhich may not lend themselves well to algebraic analysis. An addition of\nsubjective value criteria to the mechanics of interacting agents will further\ncomplicate analysis. The purpose of this short study is to demonstrate\ncapabilities of agent-based computational economics to be a platform for\nfairness or equity analysis in both a broad and practical sense.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1812.02311v1", "link": "http://arxiv.org/abs/1812.02311v1"}, {"title": "Hierarchical information clustering by means of topologically embedded\n  graphs", "summary": "We introduce a graph-theoretic approach to extract clusters and hierarchies\nin complex data-sets in an unsupervised and deterministic manner, without the\nuse of any prior information. This is achieved by building topologically\nembedded networks containing the subset of most significant links and analyzing\nthe network structure. For a planar embedding, this method provides both the\nintra-cluster hierarchy, which describes the way clusters are composed, and the\ninter-cluster hierarchy which describes how clusters gather together. We\ndiscuss performance, robustness and reliability of this method by first\ninvestigating several artificial data-sets, finding that it can outperform\nsignificantly other established approaches. Then we show that our method can\nsuccessfully differentiate meaningful clusters and hierarchies in a variety of\nreal data-sets. In particular, we find that the application to gene expression\npatterns of lymphoma samples uncovers biologically significant groups of genes\nwhich play key-roles in diagnosis, prognosis and treatment of some of the most\nrelevant human lymphoid malignancies.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1110.4477v1", "link": "http://dx.doi.org/10.1371/journal.pone.0031929"}, {"title": "Clustering of discretely observed diffusion processes", "summary": "In this paper a new dissimilarity measure to identify groups of assets\ndynamics is proposed. The underlying generating process is assumed to be a\ndiffusion process solution of stochastic differential equations and observed at\ndiscrete time. The mesh of observations is not required to shrink to zero. As\ndistance between two observed paths, the quadratic distance of the\ncorresponding estimated Markov operators is considered. Analysis of both\nsynthetic data and real financial data from NYSE/NASDAQ stocks, give evidence\nthat this distance seems capable to catch differences in both the drift and\ndiffusion coefficients contrary to other commonly used metrics.", "category": ["q-fin.ST", "math.PR"], "id": "http://arxiv.org/abs/0809.3902v1", "link": "http://arxiv.org/abs/0809.3902v1"}, {"title": "Option Pricing without Price Dynamics: A Probabilistic Approach", "summary": "Employing probabilistic techniques we compute best possible upper and lower\nbounds on the price of an option on one or two assets with continuous piecewise\nlinear payoff function based on prices of simple call options of possibly\ndistinct maturities and the no-arbitrage condition, but without any assumption\non the price dynamics of underlying assets. We show that the problem reduces to\nsolving linear optimization problems that we explicitly characterize. We report\nnumerical results that illustrate the effectiveness of the algorithms we\ndevelop.", "category": ["math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/math/0612075v1", "link": "http://arxiv.org/abs/math/0612075v1"}, {"title": "Co-existence of Trend and Value in Financial Markets: Estimating an\n  Extended Chiarella Model", "summary": "Trend and Value are pervasive anomalies, common to all financial markets. We\naddress the problem of their co-existence and interaction within the framework\nof Heterogeneous Agent Based Models (HABM). More specifically, we extend the\nChiarella (1992) model by adding noise traders and a non-linear demand of\nfundamentalists. We use Bayesian filtering techniques to calibrate the model on\ntime series of prices across a variety of asset classes since 1800. The\nfundamental value is an output of the calibration, and does not require the use\nof an external pricing model. Our extended model reproduces many empirical\nobservations, including the non-monotonic relation between past trends and\nfuture returns. The destabilizing activity of trend-followers leads to a\nqualitative change of mispricing distribution, from unimodal to bimodal,\nmeaning that some markets tend to be over- (or under-) valued for long periods\nof time.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1807.11751v1", "link": "http://arxiv.org/abs/1807.11751v1"}, {"title": "How fair is an equitable distribution?", "summary": "Envy is a rather complex and irrational emotion. In general, it is very\ndifficult to obtain a measure of this feeling, but in an economical context\nenvy becomes an observable which can be measured. When various individuals\ncompare their possessions, envy arises due to the inequality of their different\nallocations of commodities and different preferences. In this paper we show\nthat an equitable distribution of goods does not guarantee a state of fairness\nbetween agents and in general that envy cannot be controlled by tuning the\ndistribution of goods.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/physics/0603166v3", "link": "http://dx.doi.org/10.1016/j.physa.2006.07.019"}, {"title": "Are news important to predict large losses?", "summary": "In this paper we investigate the impact of news to predict extreme financial\nreturns using high frequency data. We consider several model specifications\ndiffering for the dynamic property of the underlying stochastic process as well\nas for the innovation process. Since news are essentially qualitative measures,\nthey are firstly transformed into quantitative measures which are subsequently\nintroduced as exogenous regressors into the conditional volatility dynamics.\nThree basic sentiment indexes are constructed starting from three list of words\ndefined by historical market news response and by a discriminant analysis.\nModels are evaluated in terms of their predictive accuracy to forecast\nout-of-sample Value-at-Risk of the STOXX Europe 600 sectors at different\nconfidence levels using several statistic tests and the Model Confidence Set\nprocedure of Hansen et al. (2011). Since the Hansen's procedure usually\ndelivers a set of models having the same VaR predictive ability, we propose a\nnew forecasting combination technique that dynamically weights the VaR\npredictions obtained by the models belonging to the optimal final set. Our\nresults confirms that the inclusion of exogenous information as well as the\nright specification of the returns' conditional distribution significantly\ndecrease the number of actual versus expected VaR violations towards one, as\nthis is especially true for higher confidence levels.", "category": ["q-fin.ST", "q-fin.RM"], "id": "http://arxiv.org/abs/1410.6898v2", "link": "http://arxiv.org/abs/1410.6898v2"}, {"title": "A Convergent Linear Regression Method for Forward-Backward Stochastic\n  Differential Equations with Jumps", "summary": "In this paper, we introduce a large class of convergent numerical methods,\nbased on (linear) basis function regression technique, to approximate the\nsolution to a forward-backward stochastic differential equation with jumps\n(FBSDEJ hereafter). Numerical experiment shows good applicability of the\nproposed method.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1805.12105v3", "link": "http://arxiv.org/abs/1805.12105v3"}, {"title": "Log-optimal portfolio without NFLVR: existence, complete\n  characterization, and duality", "summary": "This paper addresses the log-optimal portfolio for a general semimartingale\nmodel. The most advanced literature on the topic elaborates existence and\ncharacterization of this portfolio under no-free-lunch-with-vanishing-risk\nassumption (NFLVR). There are many financial models violating NFLVR, while\nadmitting the log-optimal portfolio on the one hand. On the other hand, for\nfinancial markets under progressively enlargement of filtration, NFLVR remains\ncompletely an open issue, and hence the literature can be applied to these\nmodels. Herein, we provide a complete characterization of log-optimal portfolio\nand its associated optimal deflator, necessary and sufficient conditions for\ntheir existence, and we elaborate their duality as well without NFLVR.", "category": ["q-fin.MF", "q-fin.PM"], "id": "http://arxiv.org/abs/1807.06449v1", "link": "http://arxiv.org/abs/1807.06449v1"}, {"title": "Interconnected risk contributions: an heavy-tail approach to analyse US\n  financial sectors", "summary": "In this paper we consider a multivariate model-based approach to measure the\ndynamic evolution of tail risk interdependence among US banks, financial\nservices and insurance sectors. To deeply investigate the risk contribution of\ninsurers we consider separately life and non-life companies. To achieve this\ngoal we apply the multivariate student-t Markov Switching model and the\nMultiple-CoVaR (CoES) risk measures introduced in Bernardi et. al. (2013b) to\naccount for both the known stylised characteristics of the data and the\ncontemporaneous joint distress events affecting financial sectors. Our\nempirical investigation finds that banks appear to be the major source of risk\nfor all the remaining sectors, followed by the financial services and the\ninsurance sectors, showing that insurance sector significantly contributes as\nwell to the overall risk. Moreover, we find that the role of each sector in\ncontributing to other sectors distress evolves over time accordingly to the\ncurrent predominant financial condition, implying different interconnection\nstrength.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1401.6408v2", "link": "http://arxiv.org/abs/1401.6408v2"}, {"title": "Social capital at venture capital firms and their financial performance:\n  Evidence from China", "summary": "This paper studies the extent to which social capital drives performance in\nthe Chinese venture capital market and explores the trend toward VC syndication\nin China. First, we propose a hybrid model based on syndicated social networks\nand the latent-variable model, which describes the social capital at venture\ncapital firms and builds relationships between social capital and performance\nat VC firms. Then, we build three hypotheses about the relationships and test\nthe hypotheses using our proposed model. Some numerical simulations are given\nto support the test results. Finally, we show that the correlations between\nsocial capital and financial performance at venture capital firms are weak in\nChina and find that China's venture capital firms lack mature social capital\nlinks.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.02952v1", "link": "http://arxiv.org/abs/1810.02952v1"}, {"title": "The space of outcomes of semi-static trading strategies need not be\n  closed", "summary": "Semi-static trading strategies make frequent appearances in mathematical\nfinance, where dynamic trading in a liquid asset is combined with static\nbuy-and-hold positions in options on that asset. We show that the space of\noutcomes of such strategies can have very poor closure properties when all\nEuropean options for a fixed date $T$ are available for static trading. This\ncauses problems for optimal investment, and stands in sharp contrast to the\npurely dynamic case classically considered in mathematical finance.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1606.00631v1", "link": "http://arxiv.org/abs/1606.00631v1"}, {"title": "Practical volume computation of structured convex bodies, and an\n  application to modeling portfolio dependencies and financial crises", "summary": "We examine volume computation of general-dimensional polytopes and more\ngeneral convex bodies, defined as the intersection of a simplex by a family of\nparallel hyperplanes, and another family of parallel hyperplanes or a family of\nconcentric ellipsoids. Such convex bodies appear in modeling and predicting\nfinancial crises. The impact of crises on the economy (labor, income, etc.)\nmakes its detection of prime interest. Certain features of dependencies in the\nmarkets clearly identify times of turmoil. We describe the relationship between\nasset characteristics by means of a copula; each characteristic is either a\nlinear or quadratic form of the portfolio components, hence the copula can be\nconstructed by computing volumes of convex bodies. We design and implement\npractical algorithms in the exact and approximate setting, we experimentally\njuxtapose them and study the tradeoff of exactness and accuracy for speed. We\nanalyze the following methods in order of increasing generality: rejection\nsampling relying on uniformly sampling the simplex, which is the fastest\napproach, but inaccurate for small volumes; exact formulae based on the\ncomputation of integrals of probability distribution functions; an optimized\nLawrence sign decomposition method, since the polytopes at hand are shown to be\nsimple; Markov chain Monte Carlo algorithms using random walks based on the\nhit-and-run paradigm generalized to nonlinear convex bodies and relying on new\nmethods for computing a ball enclosed; the latter is experimentally extended to\nnon-convex bodies with very encouraging results. Our C++ software, based on\nCGAL and Eigen and available on github, is shown to be very effective in up to\n100 dimensions. Our results offer novel, effective means of computing portfolio\ndependencies and an indicator of financial crises, which is shown to correctly\nidentify past crises.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1803.05861v1", "link": "http://arxiv.org/abs/1803.05861v1"}, {"title": "Simulation and Use of Heuristics for Peripheral Economic Policy", "summary": "Recent trends in Agent Computational Economics research, envelop a government\nagent in the model of the economy, whose decisions are based on learning\nalgorithms. In this paper we try to evaluate the performance of simulated\nannealing in this context, by considering a model proposed earlier in the\nliterature, which has modeled an artificial economy consisting of\ngeographically dispersed companies modeled as agents, that try to maximize\ntheir profit, which is yielded by selling an homogeneous product in different\ncities, with different travel costs. The authors have used an evolutionary\nalgorithm there, for modeling the agents' decision process. Our extension\nintroduces a government agent that tries to affect supply and demand by\ndifferent taxation coefficients in the different markets, in order to equate\nthe quantities sold in each city. We have studied the situation that occurs\nwhen a simulated annealing algorithm and a simple search algorithm is used as\nthe government's learning algorithm, and we have evaluated the comparative\nperformance of the two.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0905.3808v1", "link": "http://arxiv.org/abs/0905.3808v1"}, {"title": "Foreign exchange market modelling and an on-line portfolio selection\n  algorithm", "summary": "In this paper, we introduce a matrix-valued time series model for foreign\nexchange market. We then formulate trading matrices, foreign exchange options\nand return options (matrices), as well as on-line portfolio strategies.\nMoreover, we attempt to predict returns of portfolios by developing a cross\nrate method. This leads us to construct an on-line portfolio selection\nalgorithm for this model. At the end, we prove the profitability and the\nuniversality of our algorithm.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1707.00203v1", "link": "http://arxiv.org/abs/1707.00203v1"}, {"title": "Large Dimensional Latent Factor Modeling with Missing Observations and\n  Applications to Causal Inference", "summary": "This paper develops the inferential theory for latent factor models estimated\nfrom large dimensional panel data with missing observations. We estimate a\nlatent factor model by applying principal component analysis to an adjusted\ncovariance matrix estimated from partially observed panel data. We derive the\nasymptotic distribution for the estimated factors, loadings and the imputed\nvalues under a general approximate factor model. The key application is to\nestimate counterfactual outcomes in causal inference from panel data. The\nunobserved control group is modeled as missing values, which are inferred from\nthe latent factor model. The inferential theory for the imputed values allows\nus to test for individual treatment effects at any time. We apply our method to\nportfolio investment strategies and find that around 14% of their average\nreturns are significantly reduced by the academic publication of these\nstrategies.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.08273v3", "link": "http://arxiv.org/abs/1910.08273v3"}, {"title": "Dynamically Stable Matching", "summary": "I introduce a stability notion, dynamic stability, for two-sided dynamic\nmatching markets where (i) matching opportunities arrive over time, (ii)\nmatching is one-to-one, and (iii) matching is irreversible. The definition\naddresses two conceptual issues. First, since not all agents are available to\nmatch at the same time, one must establish which agents are allowed to form\nblocking pairs. Second, dynamic matching markets exhibit a form of externality\nthat is not present in static markets: an agent's payoff from remaining\nunmatched cannot be defined independently of what other contemporaneous agents'\noutcomes are. Dynamically stable matchings always exist. Dynamic stability is a\nnecessary condition to ensure timely participation in the economy by ensuring\nthat agents do not strategically delay the time at which they are available to\nmatch.", "category": [], "id": "http://arxiv.org/abs/1906.11391v4", "link": "http://arxiv.org/abs/1906.11391v4"}, {"title": "On multifractality and fractional derivatives", "summary": "It is shown phenomenologically that the fractional derivative $\\xi=D^\\alpha\nu$ of order $\\alpha$ of a multifractal function has a power-law tail $\\propto\n|\\xi| ^{-p_\\star}$ in its cumulative probability, for a suitable range of\n$\\alpha$'s. The exponent is determined by the condition $\\zeta_{p_\\star} =\n\\alpha p_\\star$, where $\\zeta_p$ is the exponent of the structure function of\norder $p$. A detailed study is made for the case of random multiplicative\nprocesses (Benzi {\\it et al.} 1993 Physica D {\\bf 65}: 352) which are amenable\nto both theory and numerical simulations. Large deviations theory provides a\nconcrete criterion, which involves the departure from straightness of the\n$\\zeta_p$ graph, for the presence of power-law tails when there is only a\nlimited range over which the data possess scaling properties (e.g. because of\nthe presence of a viscous cutoff). The method is also applied to wind tunnel\ndata and financial data.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/nlin/0107057v3", "link": "http://dx.doi.org/10.1023/A:1019843616965"}, {"title": "Numerical study of splitting methods for American option valuation", "summary": "This paper deals with the numerical approximation of American-style option\nvalues governed by partial differential complementarity problems. For a variety\nof one- and two-asset American options we investigate by ample numerical\nexperiments the temporal convergence behaviour of three modern splitting\nmethods: the explicit payoff approach, the Ikonen-Toivanen approach and the\nPeaceman-Rachford method. In addition, the temporal accuracy of these splitting\nmethods is compared to that of the penalty approach.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1610.09622v1", "link": "http://arxiv.org/abs/1610.09622v1"}, {"title": "Clustering Financial Time Series: How Long is Enough?", "summary": "Researchers have used from 30 days to several years of daily returns as\nsource data for clustering financial time series based on their correlations.\nThis paper sets up a statistical framework to study the validity of such\npractices. We first show that clustering correlated random variables from their\nobserved values is statistically consistent. Then, we also give a first\nempirical answer to the much debated question: How long should the time series\nbe? If too short, the clusters found can be spurious; if too long, dynamics can\nbe smoothed out.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1603.04017v2", "link": "http://arxiv.org/abs/1603.04017v2"}, {"title": "Simple Explicit Formula for Near-Optimal Stochastic Lifestyling", "summary": "In life-cycle economics the Samuelson paradigm (Samuelson, 1969) states that\nthe optimal investment is in constant proportions out of lifetime wealth\ncomposed of current savings and the present value of future income. It is well\nknown that in the presence of credit constraints this paradigm no longer\napplies. Instead, optimal lifecycle investment gives rise to so-called\nstochastic lifestyling (Cairns et al., 2006), whereby for low levels of\naccumulated capital it is optimal to invest fully in stocks and then gradually\nswitch to safer assets as the level of savings increases. In stochastic\nlifestyling not only does the ratio between risky and safe assets change but\nalso the mix of risky assets varies over time. While the existing literature\nrelies on complex numerical algorithms to quantify optimal lifestyling the\npresent paper provides a simple formula that captures the main essence of the\nlifestyling effect with remarkable accuracy.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1801.00980v3", "link": "http://dx.doi.org/10.1016/j.ejor.2019.12.032"}, {"title": "Optimal hedging in discrete time", "summary": "Building on the work of Schweizer (1995) and Cern and Kallseny (2007), we\npresent discrete time formulas minimizing the mean square hedging error for\nmultidimensional assets. In particular, we give explicit formulas when a\nregime-switching random walk or a GARCH-type process is utilized to model the\nreturns. Monte Carlo simulations are used to compare the optimal and delta\nhedging methods.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1211.5035v1", "link": "http://arxiv.org/abs/1211.5035v1"}, {"title": "Technological improvement rate estimates for all technologies: Use of\n  patent data and an extended domain description", "summary": "In this work, we attempt to provide a comprehensive granular account of the\npace of technological change. More specifically, we survey estimated yearly\nperformance improvement rates for nearly all definable technologies for the\nfirst time. We do this by creating a correspondence of all patents within the\nUS patent system to a set of technology domains. A technology domain is a body\nof patented inventions achieving the same technological function using the same\nknowledge and scientific principles. We obtain a set of 1757 domains using an\nextension of the previously defined classification overlap method (COM). These\ndomains contain 97.14% of all patents within the entire US patent system. From\nthe identified patent sets, we calculated the average centrality of the patents\nin each domain to estimate their improvement rates, following a methodology\ntested in prior work. The estimated improvement rates vary from a low of 1.9%\nper year for the Mechanical Skin treatment - Hair Removal and wrinkles domain\nto a high of 228.8% per year for the Network management - client-server\napplications domain. We developed a one-line descriptor identifying the\ntechnological function achieved and the underlying knowledge base for the\nlargest 50, fastest 20 as well as slowest 20 of these domains, which cover more\nthan forty percent of the patent system. In general, the rates of improvement\nwere not a strong function of the patent set size and the fastest improving\ndomains are predominantly software-based. We make available an online system\nthat allows for automated searching for domains and improvement rates\ncorresponding to any technology of interest to researchers, strategists and\npolicy formulators.", "category": ["econ.GN", "q-fin.EC", "q-fin.PM"], "id": "http://arxiv.org/abs/2004.13919v1", "link": "http://arxiv.org/abs/2004.13919v1"}, {"title": "Analysis of the optimal exercise boundary of American put option with\n  delivery lags", "summary": "We show that an American put option with delivery lags can be decomposed as a\nEuropean put option and another American-style derivative. The latter is an\noption for which the investor receives the Greek Theta of the corresponding\nEuropean option as the running payoff, and decides an optimal stopping time to\nterminate the contract. Based on the this decomposition, we further show that\nthe associated optimal exercise boundary exists, and is a strictly increasing\nand smooth curve. We also analyze its asymptotic behavior for both large\nmaturity and small time lag using the free-boundary method.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1805.02909v1", "link": "http://arxiv.org/abs/1805.02909v1"}, {"title": "Directed Continuous-Time Random Walk with memory", "summary": "We propose a new Directed Continuous-Time Random Walk (CTRW) model with\nmemory. As CTRW trajectory consists of spatial jumps preceded by waiting times,\nin Directed CTRW, we consider the case with only positive spatial jumps.\nMoreover, we consider the memory in the model as each spatial jump depends on\nthe previous one. Our model is motivated by the financial application of the\nCTRW presented in [Phys. Rev. E 82:046119][Eur. Phys. J. B 90:50]. As CTRW can\nsuccessfully describe the short term negative autocorrelation of returns in\nhigh-frequency financial data (caused by the bid-ask bounce phenomena), we\nasked ourselves to what extent the observed long-term autocorrelation of\nabsolute values of returns can be explained by the same phenomena. It turned\nout that the bid-ask bounce can be responsible only for the small fraction of\nthe memory observed in the high-frequency financial data.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1807.01934v1", "link": "http://dx.doi.org/10.1140/epjb/e2019-90453-y"}, {"title": "Maximization of Non-Concave Utility Functions in Discrete-Time Financial\n  Market Models", "summary": "This paper investigates the problem of maximizing expected terminal utility\nin a (generically incomplete) discrete-time financial market model with finite\ntime horizon. In contrast to the standard setting, a possibly non-concave\nutility function $U$ is considered, with domain of definition $\\mathbb{R}$.\nSimple conditions are presented which guarantee the existence of an optimal\nstrategy for the problem. In particular, the asymptotic elasticity of $U$ plays\na decisive role: existence can be shown when it is strictly greater at\n$-\\infty$ than at $+\\infty$.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1302.0134v3", "link": "http://arxiv.org/abs/1302.0134v3"}, {"title": "Recovery Rates in investment-grade pools of credit assets: A large\n  deviations analysis", "summary": "We consider the effect of recovery rates on a pool of credit assets. We allow\nthe recovery rate to depend on the defaults in a general way. Using the theory\nof large deviations, we study the structure of losses in a pool consisting of a\ncontinuum of types. We derive the corresponding rate function and show that it\nhas a natural interpretation as the favored way to rearrange recoveries and\nlosses among the different types. Numerical examples are also provided.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1006.2711v2", "link": "http://dx.doi.org/10.1016/j.spa.2011.08.005"}, {"title": "Influence of introducing high speed railways on intercity travel\n  behavior in Vietnam", "summary": "It is one of hottest topics in Vietnam whether to construct a High Speed Rail\n(HSR) system or not in near future. To analyze the impacts of introducing the\nHSR on the intercity travel behavior, this research develops an integrated\nintercity demand forecasting model to represent trip generation and frequency,\ndestination choice and travel mode choice behavior. For this purpose, a\ncomprehensive questionnaire survey with both Revealed Preference (RP)\ninformation (an inter-city trip diary) and Stated Preference (SP) information\nwas conducted in Hanoi in 2011. In the SP part, not only HSR, but also Low Cost\nCarrier is included in the choice set, together with other existing inter-city\ntravel modes. To make full use of the advantages of each type of data and to\novercome their disadvantages, RP and SP data are combined to describe the\ndestination choice and mode choice behavior, while trip generation and\nfrequency are represented by using the RP data. The model estimation results\nshow the inter-relationship between trip generation and frequency, destination\nchoice and travel mode choice, and confirm that those components should not\ndealt with separately.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.00155v1", "link": "http://arxiv.org/abs/1810.00155v1"}, {"title": "Combinatorial Models of Cross-Country Dual Meets: What is a Big Victory?", "summary": "Combinatorial/probabilistic models for cross-country dual-meets are proposed.\nThe first model assumes that all runners are equally likely to finish in any\npossible order. The second model assumes that each team is selected from a\nlarge identically distributed population of potential runners and with each\npotential runner's ranking determined by the initial draw from the combined\npopulation.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1911.05044v1", "link": "http://arxiv.org/abs/1911.05044v1"}, {"title": "Synthetic Control Group Methods in the Presence of Interference: The\n  Direct and Spillover Effects of Light Rail on Neighborhood Retail Activity", "summary": "In recent years, Synthetic Control Group (SCG) methods have received great\nattention from scholars and have been subject to extensions and comparisons\nwith alternative approaches for program evaluation. However, the existing\nmethodological literature mainly relies on the assumption of non-interference.\nWe propose to generalize the SCG method to studies where interference between\nthe treated and the untreated units is plausible. We frame our discussion in\nthe potential outcomes approach. Under a partial interference assumption, we\nformally define relevant direct and spillover effects. We also consider the\n\"unrealized\" spillover effect on the treated unit in the hypothetical scenario\nthat another unit in the treated unit's neighborhood had been assigned to the\nintervention. Then we investigate the assumptions under which we can identify\nand estimate the causal effects of interest, and show how they can be estimated\nusing the SCG method. We apply our approach to the analysis of an observational\nstudy, where the focus is on assessing direct and spillover causal effects of a\nnew light rail line recently built in Florence (Italy) on the commercial\nvitality of the street where it was built and of the streets in the treated\nstreet's neighborhood.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2004.05027v1", "link": "http://arxiv.org/abs/2004.05027v1"}, {"title": "The Black-Scholes-Merton dual equation", "summary": "We derive the Black-Scholes-Merton dual equation, which has exactly the same\nform as the Black-Scholes-Merton equation. The new equation is general and\nworks for European, American, Bermudan, Asian, barrier, lookback, etc. options\nand leads to new insights into pricing and hedging. Trivially, a put-call\nequality emerges - all the above-mentioned put (call) options can be priced as\ntheir corresponding calls (puts) by simply swapping stock price (dividend\nyield) for strike price (risk-free rate) simultaneously. More importantly,\ndeltas (gammas) of such puts and calls are linked via analytic formulas. As one\napplication in hedging, the dual equation is utilized to improve the accuracy\nof the recently proposed approach of hedging options statically with\nshort-maturity contracts.", "category": ["q-fin.PR", "q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1912.10380v1", "link": "http://arxiv.org/abs/1912.10380v1"}, {"title": "Corrupted Multidimensional Binary Search: Learning in the Presence of\n  Irrational Agents", "summary": "Standard game-theoretic formulations for settings like contextual pricing and\nsecurity games assume that agents act in accordance with a specific behavioral\nmodel. In practice however, some agents may not prescribe to the dominant\nbehavioral model or may act in ways that are arbitrarily inconsistent. Existing\nalgorithms heavily depend on the model being (approximately) accurate for all\nagents and have poor performance in the presence of even a few such arbitrarily\nirrational agents. How do we design learning algorithms that are robust to the\npresence of arbitrarily irrational agents?\n  We address this question for a number of canonical game-theoretic\napplications by designing a robust algorithm for the fundamental problem of\nmultidimensional binary search. The performance of our algorithm degrades\ngracefully with the number of corrupted rounds, which correspond to irrational\nagents and need not be known in advance. As binary search is the key primitive\nin algorithms for contextual pricing, Stackelberg Security Games, and other\ngame-theoretic applications, we immediately obtain robust algorithms for these\nsettings.\n  Our techniques draw inspiration from learning theory, game theory,\nhigh-dimensional geometry, and convex analysis, and may be of independent\nalgorithmic interest.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.11650v2", "link": "http://arxiv.org/abs/2002.11650v2"}, {"title": "Empirical properties of the variety of a financial portfolio and the\n  single-index model", "summary": "We investigate the variety of a portfolio of stocks in normal and extreme\ndays of market activity. We show that the variety carries information about the\nmarket activity which is not present in the single-index model and we observe\nthat the variety time evolution is not time reversal around the crash days. We\nobtain the theoretical relation between the square variety and the mean return\nof the ensemble return distribution predicted by the single-index model. The\nsingle-index model is able to mimic the average behavior of the square variety\nbut fails in describing quantitatively the relation between the square variety\nand the mean return of the ensemble distribution. The difference between\nempirical data and theoretical description is more pronounced for large\npositive values of the mean return of the ensemble distribution. Other\nsignificant deviations are also observed for extreme negative values of the\nmean return.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0009401v1", "link": "http://dx.doi.org/10.1007/s100510170229"}, {"title": "Dealing with the Dimensionality Curse in Dynamic Pricing Competition:\n  Using Frequent Repricing to Compensate Imperfect Market Anticipations", "summary": "Most sales applications are characterized by competition and limited demand\ninformation. For successful pricing strategies, frequent price adjustments as\nwell as anticipation of market dynamics are crucial. Both effects are\nchallenging as competitive markets are complex and computations of optimized\npricing adjustments can be time-consuming. We analyze stochastic dynamic\npricing models under oligopoly competition for the sale of perishable goods. To\ncircumvent the curse of dimensionality, we propose a heuristic approach to\nefficiently compute price adjustments. To demonstrate our strategy's\napplicability even if the number of competitors is large and their strategies\nare unknown, we consider different competitive settings in which competitors\nfrequently and strategically adjust their prices. For all settings, we verify\nthat our heuristic strategy yields promising results. We compare the\nperformance of our heuristic against upper bounds, which are obtained by\noptimal strategies that take advantage of perfect price anticipations. We find\nthat price adjustment frequencies can have a larger impact on expected profits\nthan price anticipations. Finally, our approach has been applied on Amazon for\nthe sale of used books. We have used a seller's historical market data to\ncalibrate our model. Sales results show that our data-driven strategy\noutperforms the rule-based strategy of an experienced seller by a profit\nincrease of more than 20%.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1809.02433v1", "link": "http://dx.doi.org/10.1016/j.cor.2018.07.011"}, {"title": "Pollution permits, Strategic Trading and Dynamic Technology Adoption", "summary": "This paper analyzes the dynamic incentives for technology adoption under a\ntransferable permits system, which allows for strategic trading on the permit\nmarket. Initially, firms can invest both in low-emitting production\ntechnologies and trade permits. In the model, technology adoption and allowance\nprice are generated endogenously and are inter-dependent. It is shown that the\nnon-cooperative permit trading game possesses a pure-strategy Nash equilibrium,\nwhere the allowance value reflects the level of uncovered pollution (demand),\nthe level of unused allowances (supply), and the technological status. These\nconditions are also satisfied when a price support instrument, which is\ncontingent on the adoption of the new technology, is introduced. Numerical\ninvestigation confirms that this policy generates a floating price floor for\nthe allowances, and it restores the dynamic incentives to invest. Given that\nthis policy comes at a cost, a criterion for the selection of a self-financing\npolicy (based on convex risk measures) is proposed and implemented.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1103.2914v1", "link": "http://arxiv.org/abs/1103.2914v1"}, {"title": "A random forest based approach for predicting spreads in the primary\n  catastrophe bond market", "summary": "We introduce a random forest approach to enable spreads' prediction in the\nprimary catastrophe bond market. We investigate whether all information\nprovided to investors in the offering circular prior to a new issuance is\nequally important in predicting its spread. The whole population of non-life\ncatastrophe bonds issued from December 2009 to May 2018 is used. The random\nforest shows an impressive predictive power on unseen primary catastrophe bond\ndata explaining 93% of the total variability. For comparison, linear\nregression, our benchmark model, has inferior predictive performance explaining\nonly 47% of the total variability. All details provided in the offering\ncircular are predictive of spread but in a varying degree. The stability of the\nresults is studied. The usage of random forest can speed up investment\ndecisions in the catastrophe bond industry.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/2001.10393v1", "link": "http://arxiv.org/abs/2001.10393v1"}, {"title": "On the iterated estimation of dynamic discrete choice games", "summary": "We study the asymptotic properties of a class of estimators of the structural\nparameters in dynamic discrete choice games. We consider K-stage policy\niteration (PI) estimators, where K denotes the number of policy iterations\nemployed in the estimation. This class nests several estimators proposed in the\nliterature such as those in Aguirregabiria and Mira (2002, 2007), Pesendorfer\nand Schmidt-Dengler (2008), and Pakes et al. (2007). First, we establish that\nthe K-PML estimator is consistent and asymptotically normal for all K. This\ncomplements findings in Aguirregabiria and Mira (2007), who focus on K=1 and K\nlarge enough to induce convergence of the estimator. Furthermore, we show under\ncertain conditions that the asymptotic variance of the K-PML estimator can\nexhibit arbitrary patterns as a function of K. Second, we establish that the\nK-MD estimator is consistent and asymptotically normal for all K. For a\nspecific weight matrix, the K-MD estimator has the same asymptotic distribution\nas the K-PML estimator. Our main result provides an optimal sequence of weight\nmatrices for the K-MD estimator and shows that the optimally weighted K-MD\nestimator has an asymptotic distribution that is invariant to K. The invariance\nresult is especially unexpected given the findings in Aguirregabiria and Mira\n(2007) for K-PML estimators. Our main result implies two new corollaries about\nthe optimal 1-MD estimator (derived by Pesendorfer and Schmidt-Dengler (2008)).\nFirst, the optimal 1-MD estimator is optimal in the class of K-MD estimators.\nIn other words, additional policy iterations do not provide asymptotic\nefficiency gains relative to the optimal 1-MD estimator. Second, the optimal\n1-MD estimator is more or equally asymptotically efficient than any K-PML\nestimator for all K. Finally, the appendix provides appropriate conditions\nunder which the optimal 1-MD estimator is asymptotically efficient.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1802.06665v3", "link": "http://arxiv.org/abs/1802.06665v3"}, {"title": "Wealth Dynamics on Complex Networks", "summary": "We study a model of wealth dynamics [Bouchaud and M\\'ezard 2000,\n\\emph{Physica A} \\textbf{282}, 536] which mimics transactions among economic\nagents. The outcomes of the model are shown to depend strongly on the\ntopological properties of the underlying transaction network. The extreme cases\nof a fully connected and a fully disconnected network yield power-law and\nlog-normal forms of the wealth distribution respectively. We perform numerical\nsimulations in order to test the model on more complex network topologies. We\nshow that the mixed form of most empirical distributions (displaying a\nnon-smooth transition from a log-normal to a power-law form) can be traced back\nto a heterogeneous topology with varying link density, which on the other hand\nis a recently observed property of real networks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0402466v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.02.032"}, {"title": "Multi-factor approximation of rough volatility models", "summary": "Rough volatility models are very appealing because of their remarkable fit of\nboth historical and implied volatilities. However, due to the non-Markovian and\nnon-semimartingale nature of the volatility process, there is no simple way to\nsimulate efficiently such models, which makes risk management of derivatives an\nintricate task. In this paper, we design tractable multi-factor stochastic\nvolatility models approximating rough volatility models and enjoying a\nMarkovian structure. Furthermore, we apply our procedure to the specific case\nof the rough Heston model. This in turn enables us to derive a numerical method\nfor solving fractional Riccati equations appearing in the characteristic\nfunction of the log-price in this setting.", "category": ["math.PR", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1801.10359v3", "link": "http://arxiv.org/abs/1801.10359v3"}, {"title": "Global economic dynamics of the forthcoming years. A forecast", "summary": "The paper analyzes the current state of the world economy and offers a\nshort-term forecast of its development. Our analysis of log-periodic\noscillations in the DJIA dynamics suggests that in the second half of 2017 the\nUnited States and other more developed countries could experience a new\nrecession, due to the third phase of the global financial crisis. The economies\nof developing countries will continue their slowdown due to lower prices of raw\ncommodities and the increased pressure of dollar debt load. The bottom of the\nslowdown in global economic growth is likely to be achieved in 2017-2018. Then\nwe expect the start of a new acceleration of global economic growth at the\nupswing phase of the 6th Kondratieff cycle (2018-2050). A speedy and steady\nwithdrawal from the third phase of the global financial crisis requires\ncooperative action between developed and developing countries within G20 to\nstimulate global demand, world trade and a fair solution of the debt problem of\ndeveloping countries.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1612.09189v1", "link": "http://arxiv.org/abs/1612.09189v1"}, {"title": "Portfolio Choice in Markets with Contagion", "summary": "We consider the problem of optimal investment and consumption in a class of\nmultidimensional jump-diffusion models in which asset prices are subject to\nmutually exciting jump processes. This captures a type of contagion where each\ndownward jump in an asset's price results in increased likelihood of further\njumps, both in that asset and in the other assets. We solve in closed-form the\ndynamic consumption-investment problem of a log-utility investor in such a\ncontagion model, prove a theorem verifying its optimality and discuss features\nof the solution, including flight-to-quality. The exponential and power utility\ninvestors are also considered: in these cases, the optimal strategy can be\ncharacterized as a distortion of the strategy of a corresponding non-contagion\ninvestor.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1210.1598v1", "link": "http://arxiv.org/abs/1210.1598v1"}, {"title": "Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler", "summary": "In this work, we propose a model for estimating volatility from financial\ntime series, extending the non-Gaussian family of space-state models with exact\nmarginal likelihood proposed by Gamerman, Santos and Franco (2013). On the\nliterature there are models focused on estimating financial assets risk,\nhowever, most of them rely on MCMC methods based on Metropolis algorithms,\nsince full conditional posterior distributions are not known. We present an\nalternative model capable of estimating the volatility, in an automatic way,\nsince all full conditional posterior distributions are known, and it is\npossible to obtain an exact sample of parameters via Gibbs Sampler. The\nincorporation of jumps in returns allows the model to capture speculative\nmovements of the data, so that their influence does not propagate to\nvolatility. We evaluate the performance of the algorithm using synthetic and\nreal data time series.\n  Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,\nDynamic linear models.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1809.01501v2", "link": "http://arxiv.org/abs/1809.01501v2"}, {"title": "Bootstrapping topology and systemic risk of complex network using the\n  fitness model", "summary": "We present a novel method to reconstruct complex network from partial\ninformation. We assume to know the links only for a subset of the nodes and to\nknow some non-topological quantity (fitness) characterising every node. The\nmissing links are generated on the basis of the latter quan- tity according to\na fitness model calibrated on the subset of nodes for which links are known. We\nmeasure the quality of the reconstruction of several topological properties,\nsuch as the network density and the degree distri- bution as a function of the\nsize of the initial subset of nodes. Moreover, we also study the resilience of\nthe network to distress propagation. We first test the method on ensembles of\nsynthetic networks generated with the Exponential Random Graph model which\nallows to apply common tools from statistical mechanics. We then test it on the\nempirical case of the World Trade Web. In both cases, we find that a subset of\n10 % of nodes is enough to reconstruct the main features of the network along\nwith its resilience with an error of 5%.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1209.6459v1", "link": "http://dx.doi.org/10.1007/s10955-013-0720-1"}, {"title": "Recovering Latent Variables by Matching", "summary": "We propose an optimal-transport-based matching method to nonparametrically\nestimate linear models with independent latent variables. The method consists\nin generating pseudo-observations from the latent variables, so that the\nEuclidean distance between the model's predictions and their matched\ncounterparts in the data is minimized. We show that our nonparametric estimator\nis consistent, and we document that it performs well in simulated data. We\napply this method to study the cyclicality of permanent and transitory income\nshocks in the Panel Study of Income Dynamics. We find that the dispersion of\nincome shocks is approximately acyclical, whereas the skewness of permanent\nshocks is procyclical. By comparison, we find that the dispersion and skewness\nof shocks to hourly wages vary little with the business cycle.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1912.13081v1", "link": "http://arxiv.org/abs/1912.13081v1"}, {"title": "Quantifying the Impact of Leveraging and Diversification on Systemic\n  Risk", "summary": "Excessive leverage, i.e. the abuse of debt financing, is considered one of\nthe primary factors in the default of financial institutions. Systemic risk\nresults from correlations between individual default probabilities that cannot\nbe considered independent. Based on the structural framework by Merton (1974),\nwe discuss a model in which these correlations arise from overlaps in banks'\nportfolios. Portfolio diversification is used as a strategy to mitigate losses\nfrom investments in risky projects. We calculate an optimal level of\ndiversification that has to be reached for a given level of excessive leverage\nto still mitigate an increase in systemic risk. In our model, this optimal\ndiversification further depends on the market size and the market conditions\n(e.g. volatility). It allows to distinguish between a safe regime, in which\nexcessive leverage does not result in an increase of systemic risk, and a risky\nregime, in which excessive leverage cannot be mitigated leading to an increased\nsystemic risk. Our results are of relevance for financial regulators.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1303.5552v1", "link": "http://arxiv.org/abs/1303.5552v1"}, {"title": "Repeated Coordination with Private Learning", "summary": "We study a repeated game with payoff externalities and observable actions\nwhere two players receive information over time about an underlying\npayoff-relevant state, and strategically coordinate their actions. Players\nlearn about the true state from private signals, as well as the actions of\nothers. They commonly learn the true state (Cripps et al., 2008), but do not\ncoordinate in every equilibrium. We show that there exist stable equilibria in\nwhich players can overcome unfavorable signal realizations and eventually\ncoordinate on the correct action, for any discount factor. For high discount\nfactors, we show that in addition players can also achieve efficient payoffs.", "category": [], "id": "http://arxiv.org/abs/1809.00051v1", "link": "http://arxiv.org/abs/1809.00051v1"}, {"title": "Empirical Limitations on High Frequency Trading Profitability", "summary": "Addressing the ongoing examination of high-frequency trading practices in\nfinancial markets, we report the results of an extensive empirical study\nestimating the maximum possible profitability of the most aggressive such\npractices, and arrive at figures that are surprisingly modest. By \"aggressive\"\nwe mean any trading strategy exclusively employing market orders and relatively\nshort holding periods. Our findings highlight the tension between execution\ncosts and trading horizon confronted by high-frequency traders, and provide a\ncontrolled and large-scale empirical perspective on the high-frequency debate\nthat has heretofore been absent. Our study employs a number of novel empirical\nmethods, including the simulation of an \"omniscient\" high-frequency trader who\ncan see the future and act accordingly.", "category": ["q-fin.TR", "q-fin.CP"], "id": "http://arxiv.org/abs/1007.2593v2", "link": "http://arxiv.org/abs/1007.2593v2"}, {"title": "An Optimal Multi-layer Reinsurance Policy under Conditional Tail\n  Expectation", "summary": "A usual reinsurance policy for insurance companies admits one or two layers\nof the payment deductions. Under optimal criterion of minimizing the\nconditional tail expectation (CTE) risk measure of the insurer's total risk,\nthis article generalized an optimal stop-loss reinsurance policy to an optimal\nmulti-layer reinsurance policy. To achieve such optimal multi-layer reinsurance\npolicy, this article starts from a given optimal stop-loss reinsurance policy\n$f(\\cdot).$ In the first step, it cuts down an interval $[0,\\infty)$ into two\nintervals $[0,M_1)$ and $[M_1,\\infty).$ By shifting the origin of Cartesian\ncoordinate system to $(M_{1},f(M_{1})),$ and showing that under the $CTE$\ncriteria $f(x)I_{[0, M_1)}(x)+(f(M_1)+f(x-M_1))I_{[M_1,\\infty)}(x)$ is, again,\nan optimal policy. This extension procedure can be repeated to obtain an\noptimal k-layer reinsurance policy. Finally, unknown parameters of the optimal\nmulti-layer reinsurance policy are estimated using some additional appropriate\ncriteria. Three simulation-based studies have been conducted to demonstrate:\n({\\bf 1}) The practical applications of our findings and ({\\bf 2}) How one may\nemploy other appropriate criteria to estimate unknown parameters of an optimal\nmulti-layer contract. The multi-layer reinsurance policy, similar to the\noriginal stop-loss reinsurance policy is optimal, in a same sense. Moreover it\nhas some other optimal criteria which the original policy does not have. Under\noptimal criterion of minimizing general translative and monotone risk measure\n$\\rho(\\cdot)$ of {\\it either} the insurer's total risk {\\it or} both the\ninsurer's and the reinsurer's total risks, this article (in its discussion)\nalso extends a given optimal reinsurance contract $f(\\cdot)$ to a multi-layer\nand continuous reinsurance policy.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1701.05447v1", "link": "http://arxiv.org/abs/1701.05447v1"}, {"title": "The Quantitative Relations between Stock Prices and Quantities of\n  Tradable Stock Shares and Its Applications", "summary": "This paper analyzes the quantitative relations between stock prices and\nquantities of tradable stock shares in Chinese stock markets at six time points\nby means of Exploratory Data Analysis (EDA) method. It is found the resulting\nformulae have the same structure but different parameters. This paper also uses\nthese relationships in order to analyse the feasibility of policies for Chinese\nGovernment to sell the state-owned shares in Chinese stock markets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0504002v1", "link": "http://arxiv.org/abs/physics/0504002v1"}, {"title": "The exponentially truncated q-distribution: A generalized distribution\n  for real complex systems", "summary": "To know the statistical distribution of a variable is an important problem in\nmanagement of resources. Distributions of the power law type are observed in\nmany real systems. However power law distributions have an infinite variance\nand thus can not be used as a standard distribution. Normally professionals in\nthe area use normal distribution with variable parameters or some other\napproximate distribution like Gumbel, Wakeby, or Pareto, which has limited\nvalidity.\n  Tsallis presented a microscopic theory of power law in the framework of\nnon-extensive thermodynamics considering long-range interactions or long\nmemory. In the present work, we consider softing of long-range interactions or\nmemory and presented a generalized distribution which have finite variance and\ncan be used as a standard distribution for all real complex systems with power\nlaw behaviour. We applied this distribution for a financial system, rain\nprecipitation and some geophysical and social systems. We found a good\nagreement for entire range in all cases for the probability density function\n(pdf) as well as the accumulated probability. This distribution shows universal\nnature of the size limiting in real systems.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/0807.0563v1", "link": "http://arxiv.org/abs/0807.0563v1"}, {"title": "A Simple Estimator for Quantile Panel Data Models Using Smoothed\n  Quantile Regressions", "summary": "Canay (2011)'s two-step estimator of quantile panel data models, due to its\nsimple intuition and low computational cost, has been widely used in empirical\nstudies in recent years. In this paper, we revisit the estimator of Canay\n(2011) and point out that in his asymptotic analysis the bias of his estimator\ndue to the estimation of the fixed effects is mistakenly omitted, and that such\nomission will lead to invalid inference on the coefficients. To solve this\nproblem, we propose a similar easy-to-implement estimator based on smoothed\nquantile regressions. The asymptotic distribution of the new estimator is\nestablished and the analytical expression of its asymptotic bias is derived.\nBased on these results, we show how to make asymptotically valid inference\nbased on both analytical and split-panel jackknife bias corrections. Finally,\nfinite sample simulations are used to support our theoretical analysis and to\nillustrate the importance of bias correction in quantile regressions for panel\ndata.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1911.04729v1", "link": "http://arxiv.org/abs/1911.04729v1"}, {"title": "Muckenhoupt's $(A_p)$ condition and the existence of the optimal\n  martingale measure", "summary": "In the problem of optimal investment with utility function defined on\n$(0,\\infty)$, we formulate sufficient conditions for the dual optimizer to be a\nuniformly integrable martingale. Our key requirement consists of the existence\nof a martingale measure whose density process satisfies the probabilistic\nMuckenhoupt $(A_p)$ condition for the power $p=1/(1-a)$, where $a\\in (0,1)$ is\na lower bound on the relative risk-aversion of the utility function. We\nconstruct a counterexample showing that this $(A_p)$ condition is sharp.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1507.05865v1", "link": "http://dx.doi.org/10.1016/j.spa.2016.02.012"}, {"title": "Stability for gains from large investors' strategies in M1/J1 topologies", "summary": "We prove continuity of a controlled SDE solution in Skorokhod's $M_1$ and\n$J_1$ topologies and also uniformly, in probability, as a non-linear functional\nof the control strategy. The functional comes from a finance problem to model\nprice impact of a large investor in an illiquid market. We show that\n$M_1$-continuity is the key to ensure that proceeds and wealth processes from\n(self-financing) c\\`{a}dl\\`{a}g trading strategies are determined as the\ncontinuous extensions for those from continuous strategies. We demonstrate by\nexamples how continuity properties are useful to solve different stochastic\ncontrol problems on optimal liquidation and to identify asymptotically\nrealizable proceeds.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1701.02167v3", "link": "http://dx.doi.org/10.3150/17-BEJ1014"}, {"title": "On the Strong Convergence of the Optimal Linear Shrinkage Estimator for\n  Large Dimensional Covariance Matrix", "summary": "In this work we construct an optimal linear shrinkage estimator for the\ncovariance matrix in high dimensions. The recent results from the random matrix\ntheory allow us to find the asymptotic deterministic equivalents of the optimal\nshrinkage intensities and estimate them consistently. The developed\ndistribution-free estimators obey almost surely the smallest Frobenius loss\nover all linear shrinkage estimators for the covariance matrix. The case we\nconsider includes the number of variables $p\\rightarrow\\infty$ and the sample\nsize $n\\rightarrow\\infty$ so that $p/n\\rightarrow c\\in (0, +\\infty)$.\nAdditionally, we prove that the Frobenius norm of the sample covariance matrix\ntends almost surely to a deterministic quantity which can be consistently\nestimated.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/1308.2608v2", "link": "http://dx.doi.org/10.1016/j.jmva.2014.08.006"}, {"title": "Realized wavelet-based estimation of integrated variance and jumps in\n  the presence of noise", "summary": "We introduce wavelet-based methodology for estimation of realized variance\nallowing its measurement in the time-frequency domain. Using smooth wavelets\nand Maximum Overlap Discrete Wavelet Transform, we allow for the decomposition\nof the realized variance into several investment horizons and jumps. Basing our\nestimator in the two-scale realized variance framework, we are able to utilize\nall available data and get feasible estimator in the presence of microstructure\nnoise as well. The estimator is tested in a large numerical study of the finite\nsample performance and is compared to other popular realized variation\nestimators. We use different simulation settings with changing noise as well as\njump level in different price processes including long memory fractional\nstochastic volatility model. The results reveal that our wavelet-based\nestimator is able to estimate and forecast the realized measures with the\ngreatest precision. Our time-frequency estimators not only produce feasible\nestimates, but also decompose the realized variation into arbitrarily chosen\ninvestment horizons. We apply it to study the volatility of forex futures\nduring the recent crisis at several investment horizons and obtain the results\nwhich provide us with better understanding of the volatility dynamics.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1202.1854v2", "link": "http://arxiv.org/abs/1202.1854v2"}, {"title": "A Data-Driven Approach for Modeling Stochasticity in Oil Market", "summary": "Global oil price is an important factor in determining many economic\nvariables in the world's economy. It is generally modeled as a stochastic\nprocess and have been studied through different techniques by comparing the\nhistoric time series of demand, supply and the price itself. However, there are\nmany historic events where the demand or supply changes are not sufficient in\nexplaining the price changes. In such cases, it is the expectations on the\nfuture changes of demand or supply that causes heavy and quick influences on\nthe price. There are many parameters and variables that shape these\nexpectations, and are usually neglected in traditional models. In this paper,\nwe have proposed a model based on System Dynamics approach that takes into\naccount these non-traditional factors. The validity of the proposed model is\nthen evaluated using real and potential scenarios in which the proposed model\nfollows the trend of the real data.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1805.12110v1", "link": "http://arxiv.org/abs/1805.12110v1"}, {"title": "Pseudo Linear Pricing Rule for Utility Indifference Valuation", "summary": "This paper considers exponential utility indifference pricing for a\nmultidimensional non-traded assets model, and provides two linear\napproximations for the utility indifference price. The key tool is a\nprobabilistic representation for the utility indifference price by the solution\nof a functional differential equation, which is termed \\emph{pseudo linear\npricing rule}. We also provide an alternative derivation of the quadratic BSDE\nrepresentation for the utility indifference price.", "category": ["q-fin.PM", "math.PR"], "id": "http://arxiv.org/abs/1403.7830v1", "link": "http://arxiv.org/abs/1403.7830v1"}, {"title": "Value-at-Risk Prediction in R with the GAS Package", "summary": "GAS models have been recently proposed in time-series econometrics as\nvaluable tools for signal extraction and prediction. This paper details how\nfinancial risk managers can use GAS models for Value-at-Risk (VaR) prediction\nusing the novel GAS package for R. Details and code snippets for prediction,\ncomparison and backtesting with GAS models are presented. An empirical\napplication considering Dow Jones Index constituents investigates the VaR\nforecasting performance of GAS models.", "category": ["q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/1611.06010v1", "link": "http://arxiv.org/abs/1611.06010v1"}, {"title": "A new decomposition of portfolio return", "summary": "For a functionally generated portfolio, there is a natural decomposition of\nthe relative log-return into the log-change in the generating function and a\ndrift process. In this note, this decomposition is extended to arbitrary stock\nportfolios by an application of Fisk-Stratonovich integration. With the\nextended methodology, the generating function is represented by a structural\nprocess, and the drift process is subsumed into a trading process that measures\nthe profit and loss to the portfolio from trading.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1606.05877v1", "link": "http://arxiv.org/abs/1606.05877v1"}, {"title": "Uncertainty relations in models of market microstructure", "summary": "This paper presents a new interacting particle system and uses it as a spin\nmodel for financial market microstructure. The asymptotic analysis of this\nstochastic process exhibits a lower bound to the contemporaneous measurement of\nprice and trading volume under the invariant measure in the `frozen' phase of\nthe supercritical regime.", "category": ["math.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/math/0409076v2", "link": "http://dx.doi.org/10.1016/j.physa.2005.02.085"}, {"title": "A recursive algorithm for multivariate risk measures and a set-valued\n  Bellman's principle", "summary": "A method for calculating multi-portfolio time consistent multivariate risk\nmeasures in discrete time is presented. Market models for $d$ assets with\ntransaction costs or illiquidity and possible trading constraints are\nconsidered on a finite probability space. The set of capital requirements at\neach time and state is calculated recursively backwards in time along the event\ntree. We motivate why the proposed procedure can be seen as a set-valued\nBellman's principle, that might be of independent interest within the growing\nfield of set optimization. We give conditions under which the backwards\ncalculation of the sets reduces to solving a sequence of linear, respectively\nconvex vector optimization problems. Numerical examples are given and include\nsuperhedging under illiquidity, the set-valued entropic risk measure, and the\nmulti-portfolio time consistent version of the relaxed worst case risk measure\nand of the set-valued average value at risk.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1508.02367v2", "link": "http://dx.doi.org/10.1007/s10898-016-0459-8"}, {"title": "The small-maturity smile for exponential Levy models", "summary": "We derive a small-time expansion for out-of-the-money call options under an\nexponential Levy model, using the small-time expansion for the distribution\nfunction given in Figueroa-Lopez & Houdre (2009), combined with a change of\nnum\\'eraire via the Esscher transform. In particular, we quantify find that the\neffect of a non-zero volatility $\\sigma$ of the Gaussian component of the\ndriving L\\'{e}vy process is to increase the call price by $1/2\\sigma^2 t^2\ne^{k}\\nu(k)(1+o(1))$ as $t \\to 0$, where $\\nu$ is the L\\'evy density. Using the\nsmall-time expansion for call options, we then derive a small-time expansion\nfor the implied volatility, which sharpens the first order estimate given in\nTankov (2010). Our numerical results show that the second order approximation\ncan significantly outperform the first order approximation. Our results are\nalso extended to a class of time-changed L\\'evy models. We also consider a\nsmall-time, small log-moneyness regime for the CGMY model, and apply this\napproach to the small-time pricing of at-the-money call options.", "category": ["q-fin.PR", "math.PR", "q-fin.CP"], "id": "http://arxiv.org/abs/1105.3180v2", "link": "http://arxiv.org/abs/1105.3180v2"}, {"title": "A Time Series Analysis-Based Forecasting Framework for the Indian\n  Healthcare Sector", "summary": "Designing efficient and robust algorithms for accurate prediction of stock\nmarket prices is one of the most exciting challenges in the field of time\nseries analysis and forecasting. With the exponential rate of development and\nevolution of sophisticated algorithms and with the availability of fast\ncomputing platforms, it has now become possible to effectively and efficiently\nextract, store, process and analyze high volume of stock market data with\ndiversity in its contents. Availability of complex algorithms which can execute\nvery fast on parallel architecture over the cloud has made it possible to\nachieve higher accuracy in forecasting results while reducing the time required\nfor computation. In this paper, we use the time series data of the healthcare\nsector of India for the period January 2010 till December 2016. We first\ndemonstrate a decomposition approach of the time series and then illustrate how\nthe decomposition results provide us with useful insights into the behavior and\nproperties exhibited by the time series. Further, based on the structural\nanalysis of the time series, we propose six different methods of forecasting\nfor predicting the time series index of the healthcare sector. Extensive\nresults are provided on the performance of the forecasting methods to\ndemonstrate their effectiveness.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1705.01144v1", "link": "http://arxiv.org/abs/1705.01144v1"}, {"title": "Model-free bounds on Value-at-Risk using extreme value information and\n  statistical distances", "summary": "We derive bounds on the distribution function, therefore also on the\nValue-at-Risk, of $\\varphi(\\mathbf X)$ where $\\varphi$ is an aggregation\nfunction and $\\mathbf X = (X_1,\\dots,X_d)$ is a random vector with known\nmarginal distributions and partially known dependence structure. More\nspecifically, we analyze three types of available information on the dependence\nstructure: First, we consider the case where extreme value information, such as\nthe distributions of partial minima and maxima of $\\mathbf X$, is available. In\norder to include this information in the computation of Value-at-Risk bounds,\nwe utilize a reduction principle that relates this problem to an optimization\nproblem over a standard Fr\\'echet class, which can then be solved by means of\nthe rearrangement algorithm or using analytical results. Second, we assume that\nthe copula of $\\mathbf X$ is known on a subset of its domain, and finally we\nconsider the case where the copula of $\\mathbf X$ lies in the vicinity of a\nreference copula as measured by a statistical distance. In order to derive\nValue-at-Risk bounds in the latter situations, we first improve the\nFr\\'echet--Hoeffding bounds on copulas so as to include this additional\ninformation on the dependence structure. Then, we translate the improved\nFr\\'echet--Hoeffding bounds to bounds on the Value-at-Risk using the so-called\nimproved standard bounds. In numerical examples we illustrate that the\nadditional information typically leads to a significant improvement of the\nbounds compared to the marginals-only case.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/1610.09734v4", "link": "http://arxiv.org/abs/1610.09734v4"}, {"title": "Perturbation analysis of sub/super hedging problems", "summary": "We investigate the links between various no-arbitrage conditions and the\nexistence of pricing functionals in general markets, and prove the Fundamental\nTheorem of Asset Pricing therein. No-arbitrage conditions, either in this\nabstract setting or in the case of a market consisting of European Call\noptions, give rise to duality properties of infinite-dimensional sub- and\nsuper-hedging problems. With a view towards applications, we show how duality\nis preserved when reducing these problems over finite-dimensional bases. We\nfinally perform a rigorous perturbation analysis of those linear programming\nproblems, and highlight numerically the influence of smile extrapolation on the\nbounds of exotic options.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/1806.03543v2", "link": "http://arxiv.org/abs/1806.03543v2"}, {"title": "Multi-Stage Compound Real Options Valuation in Residential PV-Battery\n  Investment", "summary": "Strategic valuation of efficient and well-timed network investments under\nuncertain electricity market environment has become increasingly challenging,\nbecause there generally exist multiple interacting options in these\ninvestments, and failing to systematically consider these options can lead to\ndecisions that undervalue the investment. In our work, a real options valuation\n(ROV) framework is proposed to determine the optimal strategy for executing\nmultiple interacting options within a distribution network investment, to\nmitigate the risk of financial losses in the presence of future uncertainties.\nTo demonstrate the characteristics of the proposed framework, we determine the\noptimal strategy to economically justify the investment in residential\nPV-battery systems for additional grid supply during peak demand periods. The\noptions to defer, and then expand, are considered as multi-stage compound\noptions, since the option to expand is a subsequent option of the former. These\noptions are valued via the least squares Monte Carlo method, incorporating\nuncertainty over growing power demand, varying diesel fuel price, and the\ndeclining cost of PV-battery technology as random variables. Finally, a\nsensitivity analysis is performed to demonstrate how the proposed framework\nresponds to uncertain events. The proposed framework shows that executing the\ninteracting options at the optimal timing increases the investment value.", "category": ["econ.EM", "q-fin.GN"], "id": "http://arxiv.org/abs/1910.09132v1", "link": "http://arxiv.org/abs/1910.09132v1"}, {"title": "Analysis of Networks via the Sparse $\u03b2$-Model", "summary": "Data in the form of networks are increasingly available in a variety of\nareas, yet statistical models allowing for parameter estimates with desirable\nstatistical properties for sparse networks remain scarce. To address this, we\npropose the Sparse $\\beta$-Model (S$\\beta$M), a new network model that\ninterpolates the celebrated Erd\\H{o}s-R\\'enyi model and the $\\beta$-model that\nassigns one different parameter to each node. By a novel reparameterization of\nthe $\\beta$-model to distinguish global and local parameters, our S$\\beta$M can\ndrastically reduce the dimensionality of the $\\beta$-model by requiring some of\nthe local parameters to be zero. We derive the asymptotic distribution of the\nmaximum likelihood estimator of the S$\\beta$M when the support of the parameter\nvector is known. When the support is unknown, we formulate a penalized\nlikelihood approach with the $\\ell_0$-penalty. Remarkably, we show via a\nmonotonicity lemma that the seemingly combinatorial computational problem due\nto the $\\ell_0$-penalty can be overcome by assigning nonzero parameters to\nthose nodes with the largest degrees. We further show that a $\\beta$-min\ncondition guarantees our method to identify the true model and provide excess\nrisk bounds for the estimated parameters. The estimation procedure enjoys good\nfinite sample properties as shown by simulation studies. The usefulness of the\nS$\\beta$M is further illustrated via the analysis of a microfinance take up\nexample.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1908.03152v2", "link": "http://arxiv.org/abs/1908.03152v2"}, {"title": "Weighted Monte Carlo: Calibrating the Smile and Preserving Martingale\n  Condition", "summary": "Weighted Monte Carlo prices exotic options calibrating the probabilities of\npreviously generated paths by a regular Monte Carlo to fit a set of option\npremiums. When only vanilla call and put options and forward prices are\nconsidered, the Martingale condition might not be preserved. This paper shows\nthat this is indeed the case and overcomes the problem by adding additional\nsynthetic options. A robust, fast and easy-to-implement calibration algorithm\nis presented. The results are illustrated with a geometric cliquet option which\nshows how the price impact can be significant.", "category": ["q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1102.3541v1", "link": "http://arxiv.org/abs/1102.3541v1"}, {"title": "ADOL - Markovian approximation of rough lognormal model", "summary": "In this paper we apply Markovian approximation of the fractional Brownian\nmotion (BM), known as the Dobric-Ojeda (DO) process, to the fractional\nstochastic volatility model where the instantaneous variance is modelled by a\nlognormal process with drift and fractional diffusion. Since the DO process is\na semi-martingale, it can be represented as an \\Ito diffusion. It turns out\nthat in this framework the process for the spot price $S_t$ is a geometric BM\nwith stochastic instantaneous volatility $\\sigma_t$, the process for $\\sigma_t$\nis also a geometric BM with stochastic speed of mean reversion and\ntime-dependent colatility of volatility, and the supplementary process\n$\\calV_t$ is the Ornstein-Uhlenbeck process with time-dependent coefficients,\nand is also a function of the Hurst exponent. We also introduce an adjusted DO\nprocess which provides a uniformly good approximation of the fractional BM for\nall Hurst exponents $H \\in [0,1]$ but requires a complex measure. Finally, the\ncharacteristic function (CF) of $\\log S_t$ in our model can be found in closed\nform by using asymptotic expansion. Therefore, pricing options and variance\nswaps (by using a forward CF) can be done via FFT, which is much easier than in\nrough volatility models.", "category": ["q-fin.MF", "q-fin.CP", "q-fin.PR"], "id": "http://arxiv.org/abs/1904.09240v1", "link": "http://arxiv.org/abs/1904.09240v1"}, {"title": "Invoice Financing of Supply Chains with Blockchain technology and\n  Artificial Intelligence", "summary": "Supply chains lend themselves to blockchain technology, but certain\nchallenges remain, especially around invoice financing. For example, the\nfurther a supplier is removed from the final consumer product, the more\ndifficult it is to get their invoices financed. Moreover, for competitive\nreasons, retailers and manufacturers do not want to disclose their supply\nchains. However, upstream suppliers need to prove that they are part of a\n`stable' supply chain to get their invoices financed, which presents the\nupstream suppliers with huge, and often unsurmountable, obstacles to get the\nnecessary finance to fulfil the next order, or to expand their business. Using\na fictitious supply chain use case, which is based on a real world use case, we\ndemonstrate how these challenges have the potential to be solved by combining\nmore advanced and specialised blockchain technologies with other technologies\nsuch as Artificial Intelligence. We describe how atomic crosschain\nfunctionality can be utilised across private blockchains to retrieve the\ninformation required for an invoice financier to make informed decisions under\nuncertainty, and consider the effect this decision has on the overall stability\nof the supply chain.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1906.03306v1", "link": "http://arxiv.org/abs/1906.03306v1"}, {"title": "Portfolio optimization in the case of an exponential utility function\n  and in the presence of an illiquid asset", "summary": "We study an optimization problem for a portfolio with a risk-free, a liquid,\nand an illiquid risky asset. The illiquid risky asset is sold in an exogenous\nrandom moment with a prescribed liquidation time distribution. The investor\nprefers a negative or a positive exponential utility function. We prove that\nboth cases are connected by a one-to-one analytical substitution and are\nidentical from the economic, analytical, or Lie algebraic points of view.\n  It is well known that the exponential utility function is connected with the\nHARA utility function through a limiting procedure if the parameter of the HARA\nutility function is going to infinity. We show that the optimization problem\nwith the exponential utility function is not connected to the HARA case by the\nlimiting procedure and we obtain essentially different results.\n  For the main three dimensional PDE with the exponential utility function we\nobtain the complete set of the nonequivalent Lie group invariant reductions to\ntwo dimensional PDEs according to an optimal system of subalgebras of the\nadmitted Lie algebra. We prove that in just one case the invariant reduction is\nconsistent with the boundary condition. This reduction represents a significant\nsimplification of the original problem.", "category": ["q-fin.PM", "q-fin.MF"], "id": "http://arxiv.org/abs/1910.07417v2", "link": "http://arxiv.org/abs/1910.07417v2"}, {"title": "On anomalous distributions in intra-day financial time series and\n  Non-extensive Statistical Mechanics", "summary": "In this paper one studies the distribution of log-returns (tick-by-tick) in\nthe Lisbon stock market and shows that it is well adjusted by the solution of\nthe equation, {$\\frac{dp_{x}}{d| x|}=-\\beta_{q^{\\prime\n}}p_{x}^{q^{\\prime}}-(\\beta_{q}-\\beta_{q^{\\prime}}) p_{x}^{q}$}, which\ncorresponds to a generalization of the differential equation which has as\nsolution the power-laws that optimise the entropic form $S_{q}=-k \\frac{1-\\int\np_{x}^{q} dx}{1-q}$, base of present non-extensive statistical mechanics.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0403624v1", "link": "http://arxiv.org/abs/cond-mat/0403624v1"}, {"title": "Annuitization and asset allocation", "summary": "This paper examines the optimal annuitization, investment and consumption\nstrategies of a utility-maximizing retiree facing a stochastic time of death\nunder a variety of institutional restrictions. We focus on the impact of aging\non the optimal purchase of life annuities which form the basis of most Defined\nBenefit pension plans. Due to adverse selection, acquiring a lifetime payout\nannuity is an irreversible transaction that creates an incentive to delay.\nUnder the institutional all-or-nothing arrangement where annuitization must\ntake place at one distinct point in time (i.e. retirement), we derive the\noptimal age at which to annuitize and develop a metric to capture the loss from\nannuitizing prematurely. In contrast, under an open-market structure where\nindividuals can annuitize any fraction of their wealth at anytime, we locate a\ngeneral optimal annuity purchasing policy. In this case, we find that an\nindividual will initially annuitize a lump sum and then buy annuities to keep\nwealth to one side of a separating ray in wealth-annuity space. We believe our\npaper is the first to integrate life annuity products into the portfolio choice\nliterature while taking into account realistic institutional restrictions which\nare unique to the market for mortality-contingent claims.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1506.05990v1", "link": "http://arxiv.org/abs/1506.05990v1"}, {"title": "Stylized Facts and Agent-Based Modeling", "summary": "The existence of stylized facts in financial data has been documented in many\nstudies. In the past decade the modeling of financial markets by agent-based\ncomputational economic market models has become a frequently used modeling\napproach. The main purpose of these models is to replicate stylized facts and\nto identify sufficient conditions for their creations. In this paper we\nintroduce the most prominent examples of stylized facts and especially present\nstylized facts of financial data. Furthermore, we given an introduction to\nagent-based modeling. Here, we not only provide an overview of this topic but\nintroduce the idea of universal building blocks for agent-based economic market\nmodels.", "category": ["q-fin.GN", "econ.EM"], "id": "http://arxiv.org/abs/1912.02684v1", "link": "http://arxiv.org/abs/1912.02684v1"}, {"title": "Optimal portfolio selection under vanishing fixed transaction costs", "summary": "In this paper, asymptotic results in a long-term growth rate portfolio\noptimization model under both fixed and proportional transaction costs are\nobtained. More precisely, the convergence of the model when the fixed costs\ntend to zero is investigated. A suitable limit model with purely proportional\ncosts is introduced and an optimal strategy is shown to consist of keeping the\nrisky fraction process in a unique interval $[A,B]\\subseteq\\,]0,1[$ with\nminimal effort. Furthermore, the convergence of optimal boundaries, asymptotic\ngrowth rates, and optimal risky fraction processes is rigorously proved. The\nresults are based on an in-depth analysis of the convergence of the solutions\nto the corresponding HJB-equations.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1611.01280v2", "link": "http://arxiv.org/abs/1611.01280v2"}, {"title": "Convergence rates of large-time sensitivities with the\n  Hansen--Scheinkman decomposition", "summary": "This paper investigates the large-time asymptotic behavior of the\nsensitivities of cash flows. In quantitative finance, the price of a cash flow\nis expressed in terms of a pricing operator of a Markov diffusion process. We\nstudy the extent to which the pricing operator is affected by small changes of\nthe underlying Markov diffusion. The main idea is a partial differential\nequation (PDE) representation of the pricing operator by incorporating the\nHansen--Scheinkman decomposition method. The sensitivities of the cash flows\nand their large-time convergence rates can be represented via simple\nexpressions in terms of eigenvalues and eigenfunctions of the pricing operator.\nFurthermore, compared to the work of Park (Finance Stoch. 4:773-825, 2018),\nmore detailed convergence rates are provided. In addition, we discuss the\napplication of our results to three practical problems: utility maximization,\nentropic risk measures, and bond prices. Finally, as examples, explicit results\nfor several market models such as the Cox--Ingersoll--Ross (CIR) model, 3/2\nmodel and constant elasticity of variance (CEV) model are presented.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1912.03404v1", "link": "http://arxiv.org/abs/1912.03404v1"}, {"title": "Emergence of frustration signals systemic risk", "summary": "We show that the emergence of systemic risk in complex systems can be\nunderstood from the evolution of functional networks representing interactions\ninferred from fluctuation correlations between macroscopic observables.\nSpecifically, we analyze the long-term collective dynamics of the New York\nStock Exchange between 1926-2016, showing that periods marked by systemic\ncrisis, viz., around the Great Depression of 1929-33 and the Great Recession of\n2007-09, are associated with emergence of frustration indicated by the loss of\nstructural balance in the interaction networks. During these periods the\ndominant eigenmodes characterizing the collective behavior exhibit\ndelocalization leading to increased coherence in the dynamics. The topological\nstructure of the networks exhibits a slowly evolving trend marked by the\nemergence of a prominent core-periphery organization around both of the crisis\nperiods.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1807.02923v1", "link": "http://dx.doi.org/10.1103/PhysRevE.99.052306"}, {"title": "Minimal Cost of a Brownian Risk without Ruin", "summary": "In this paper, we study a risk process modeled by a Brownian motion with\ndrift (the diffusion approximation model). The insurance entity can purchase\nreinsurance to lower its risk and receive cash injections at discrete times to\navoid ruin. Proportional reinsurance and excess-of-loss reinsurance are\nconsidered. The objective is to find the optimal reinsurance and cash injection\nstrategy that minimizes the total cost to keep the company's surplus process\nnon-negative, i.e. without ruin, where the cost function is defined as the\ntotal discounted value of the injections. The optimal solution is found\nexplicitly by solving the according quasi-variational inequalities (QVIs).", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1112.4005v1", "link": "http://arxiv.org/abs/1112.4005v1"}, {"title": "The Price of BitCoin: GARCH Evidence from High Frequency Data", "summary": "This is the first paper that estimates the price determinants of BitCoin in a\nGeneralised Autoregressive Conditional Heteroscedasticity framework using high\nfrequency data. Derived from a theoretical model, we estimate BitCoin\ntransaction demand and speculative demand equations in a GARCH framework using\nhourly data for the period 2013-2018. In line with the theoretical model, our\nempirical results confirm that both the BitCoin transaction demand and\nspeculative demand have a statistically significant impact on the BitCoin price\nformation. The BitCoin price responds negatively to the BitCoin velocity,\nwhereas positive shocks to the BitCoin stock, interest rate and the size of the\nBitCoin economy exercise an upward pressure on the BitCoin price.", "category": ["q-fin.ST", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1812.09452v1", "link": "http://arxiv.org/abs/1812.09452v1"}, {"title": "Truncated Variation, Upward Truncated Variation and Downward Truncated\n  Variation of Brownian Motion with Drift - their Characteristics and\n  Applications", "summary": "In the paper \"On Truncated Variation of Brownian Motion with Drift\" (Bull.\nPol. Acad. Sci. Math. 56 (2008), no.4, 267 - 281) we defined truncated\nvariation of Brownian motion with drift, $W_t = B_t + \\mu t, t\\geq 0,$ where\n$(B_t)$ is a standard Brownian motion. Truncated variation differs from regular\nvariation by neglecting jumps smaller than some fixed $c > 0$. We prove that\ntruncated variation is a random variable with finite moment-generating function\nfor any complex argument. We also define two closely related quantities -\nupward truncated variation and downward truncated variation. The defined\nquantities may have some interpretation in financial mathematics. Exponential\nmoment of upward truncated variation may be interpreted as the maximal possible\nreturn from trading a financial asset in the presence of flat commission when\nthe dynamics of the prices of the asset follows a geometric Brownian motion\nprocess. We calculate the Laplace transform with respect to time parameter of\nthe moment-generating functions of the upward and downward truncated\nvariations. As an application of the obtained formula we give an exact formula\nfor expected value of upward and downward truncated variations. We give also\nexact (up to universal constants) estimates of the expected values of the\nmentioned quantities.", "category": ["math.PR", "q-fin.ST"], "id": "http://arxiv.org/abs/0912.4533v3", "link": "http://dx.doi.org/10.1016/j.spa.2010.10.005"}, {"title": "Hybrid symbiotic organisms search feedforward neural network model for\n  stock price prediction", "summary": "The prediction of stock prices is an important task in economics, investment\nand financial decision-making. It has for several decades, spurred the interest\nof many researchers to design stock price predictive models. In this paper, the\nsymbiotic organisms search algorithm, a new metaheuristic algorithm is employed\nas an efficient method for training feedforward neural networks (FFNN). The\ntraining process is used to build a better stock price predictive model. The\nStraits Times Index, Nikkei 225, NASDAQ Composite, S&P 500, and Dow Jones\nIndustrial Average indices were utilized as time series data sets for training\nand testing proposed predic-tive model. Three evaluation methods namely, Root\nMean Squared Error, Mean Absolute Percentage Error and Mean Absolution\nDeviation are used to compare the results of the implemented model. The\ncomputational results obtained revealed that the hybrid Symbiotic Organisms\nSearch Algorithm exhibited outstanding predictive performance when compared to\nthe hybrid Particle Swarm Optimization, Genetic Algorithm, and ARIMA based\nmodels. The new model is a promising predictive technique for solving high\ndimensional nonlinear time series data that are difficult to capture by\ntraditional models.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1906.10121v2", "link": "http://arxiv.org/abs/1906.10121v2"}, {"title": "Are European equity markets efficient? New evidence from fractal\n  analysis", "summary": "Fractal analysis is carried out on the stock market indices of seven European\ncountries and the US. We find evidence of long range dependence in the log\nreturn series of the Mibtel (Italy) and the PX Glob (Czech Republic). Long\nrange dependence implies that predictable patterns in the log returns do not\ndissipate quickly, and may therefore produce potential arbitrage opportunities.\nTherefore, these results are in contravention of the Efficient Market\nHypothesis. We show that correcting for short range dependence, or\nprefiltering, may dispose of genuine long range dependence, suggesting that the\nmarket is efficient in cases when it is not. Prefiltering does not reduce\nsignificantly the power of the tests only for cases for which the Hurst\nexponent (a measure of the long range dependence) lies well outside the\nboundaries of no long range dependence. For borderline cases, the prefiltering\nprocedure reduces the power of the test. On the other hand, the absence of\nprefiltering does not result in a test that is significantly oversized.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1402.1440v1", "link": "http://dx.doi.org/10.1016/j.irfa.2011.02.004"}, {"title": "How much diversification potential is there in a single market? Evidence\n  from the Australian Stock Exchange", "summary": "We present four methods of assessing the diversification potential within a\nstock market, two of these are based on principal component analysis. They were\napplied to the Australian stock exchange for the years 2000 to 2014 and all\nshow a consistent picture. The potential for diversification declined almost\nmonotonically in the three years prior to the 2008 financial crisis. On one of\nthe measures the diversification potential declined even further in the 2011\nEuropean debt crisis and the American credit downgrade.", "category": ["q-fin.PM", "q-fin.ST"], "id": "http://arxiv.org/abs/1512.06486v1", "link": "http://arxiv.org/abs/1512.06486v1"}, {"title": "Adapted Downhill Simplex Method for Pricing Convertible Bonds", "summary": "The paper is devoted to modeling optimal exercise strategies of the behavior\nof investors and issuers working with convertible bonds. This implies solution\nof the problems of stock price modeling, payoff computation and min-max\noptimization.\n  Stock prices (underlying asset) were modeled under the assumption of the\ngeometric Brownian motion of their values. The Monte Carlo method was used for\ncalculating the real payoff which is the objective function. The min-max\noptimization problem was solved using the derivative-free Downhill Simplex\nmethod.\n  The performed numerical experiments allowed to formulate recommendations for\nthe choice of appropriate size of the initial simplex in the Downhill Simplex\nMethod, the number of generated trajectories of underlying asset, the size of\nthe problem and initial trajectories of the behavior of investors and issuers.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/0710.0241v1", "link": "http://arxiv.org/abs/0710.0241v1"}, {"title": "A Statistical Test of Walrasian Equilibrium by Means of Complex Networks\n  Theory", "summary": "We represent an exchange economy in terms of statistical ensembles for\ncomplex networks by introducing the concept of market configuration. This is\ndefined as a sequence of nonnegative discrete random variables $\\{w_{ij}\\}$\ndescribing the flow of a given commodity from agent $i$ to agent $j$. This\nsequence can be arranged in a nonnegative matrix $W$ which we can regard as the\nrepresentation of a weighted and directed network or digraph $G$. Our main\nresult consists in showing that general equilibrium theory imposes highly\nrestrictive conditions upon market configurations, which are in most cases not\nfulfilled by real markets. An explicit example with reference to the e-MID\ninterbank credit market is provided.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1307.0817v2", "link": "http://dx.doi.org/10.1007/s10955-016-1599-4"}, {"title": "Semi-Parametric Hierarchical Bayes Estimates of New Yorkers' Willingness\n  to Pay for Features of Shared Automated Vehicle Services", "summary": "In this paper, we contrast parametric and semi-parametric representations of\nunobserved heterogeneity in hierarchical Bayesian multinomial logit models and\nleverage these methods to infer distributions of willingness to pay for\nfeatures of shared automated vehicle (SAV) services. Specifically, we compare\nthe multivariate normal (MVN), finite mixture of normals (F-MON) and Dirichlet\nprocess mixture of normals (DP-MON) mixing distributions. The latter promises\nto be particularly flexible in respect to the shapes it can assume and unlike\nother semi-parametric approaches does not require that its complexity is fixed\nprior to estimation. However, its properties relative to simpler mixing\ndistributions are not well understood. In this paper, we evaluate the\nperformance of the MVN, F-MON and DP-MON mixing distributions using simulated\ndata and real data sourced from a stated choice study on preferences for SAV\nservices in New York City. Our analysis shows that the DP-MON mixing\ndistribution provides superior fit to the data and performs at least as well as\nthe competing methods at out-of-sample prediction. The DP-MON mixing\ndistribution also offers substantive behavioural insights into the adoption of\nSAVs. We find that preferences for in-vehicle travel time by SAV with\nride-splitting are strongly polarised. Whereas one third of the sample is\nwilling to pay between 10 and 80 USD/h to avoid sharing a vehicle with\nstrangers, the remainder of the sample is either indifferent to ride-splitting\nor even desires it. Moreover, we estimate that new technologies such as vehicle\nautomation and electrification are relatively unimportant to travellers. This\nsuggests that travellers may primarily derive indirect, rather than immediate\nbenefits from these new technologies through increases in operational\nefficiency and lower operating costs.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1907.09639v1", "link": "http://arxiv.org/abs/1907.09639v1"}, {"title": "Geographic Concentration in Portugal and Regional Specific Factors", "summary": "This paper pretends to analyze the importance which the natural advantages\nand local resources are in the manufacturing industry location, in relation\nwith the \"spillovers\" effects and industrial policies. To this, we estimate the\nRybczynski equation matrix for the various manufacturing industries in\nPortugal, at regional level (NUTS II) and for the period 1980 to 1999.\nEstimations are displayed with the model mentioned and for four different\nperiods, namely 1980 to 1985, from 1986 to 1994, from 1980 to 1994 and from\n1995 to 1999. The consideration of the various periods until 1994, aims to\ncapture the effects of our entrance at the, in that time, EEC (European\nEconomic Community) and the consideration of a period from 1995 is because the\nchange in methodology for compiling statistical data taken from this time in\nPortugal. As a summary conclusion, noted that the location of manufacturing in\nPortugal is still mostly explained by specific factors, with a tendency to\nincrease in some cases the explanation by these factors, having the effect\n\"spillovers\" and industrial policies little importance in this context.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1110.5558v1", "link": "http://arxiv.org/abs/1110.5558v1"}, {"title": "Quantitative evaluation of consecutive resilience cycles in stock market\n  performance: A systems-oriented approach", "summary": "Financial markets can be seen as complex systems that are constantly evolving\nand sensitive to external disturbance, such as systemic risks and economic\ninstabilities. Analysis of resilient market performance, therefore, becomes\nuseful for investors. From a systems perspective, this paper proposes a novel\nfunction-based resilience metric that considers the effect of two\nfault-tolerance thresholds: the Robustness Range (RR) and the Elasticity\nThreshold (ET). We examined the consecutive resilience cycles and their\ndynamics in the performance of two stock markets, NASDAQ and SSE. The proposed\nmetric was also compared with three well-documented resilience models. The\nresults showed that this new metric could satisfactorily quantify the\ntime-varying resilience cycles in the multi-cycle volatile performance of stock\nmarkets while also being more feasible in comparative analysis. Furthermore,\nanalysis of dynamics revealed that those consecutive resilience cycles in\nmarket performance were distributed non-linearly, following a power-law\nbehavior in the upper tail. Finally, sensitivity tests demonstrated the\nlarge-value resilience cycles were relatively sensitive to changes in RR. In\npractice, RR could indicate investors' psychological capability to withstand\ndownturns. It supports the observation that perception on the market's\nresilient responses may vary among investors. This study provides a new tool\nand valuable insight for researchers, practitioners, and investors when\nevaluating market performance.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1903.03201v1", "link": "http://dx.doi.org/10.1016/j.physa.2019.121794"}, {"title": "Goodness-of-Fit Tests based on Series Estimators in Nonparametric\n  Instrumental Regression", "summary": "This paper proposes several tests of restricted specification in\nnonparametric instrumental regression. Based on series estimators, test\nstatistics are established that allow for tests of the general model against a\nparametric or nonparametric specification as well as a test of exogeneity of\nthe vector of regressors. The tests' asymptotic distributions under correct\nspecification are derived and their consistency against any alternative model\nis shown. Under a sequence of local alternative hypotheses, the asymptotic\ndistributions of the tests is derived. Moreover, uniform consistency is\nestablished over a class of alternatives whose distance to the null hypothesis\nshrinks appropriately as the sample size increases. A Monte Carlo study\nexamines finite sample performance of the test statistics.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.10133v1", "link": "http://dx.doi.org/10.1016/j.jeconom.2014.09.006"}, {"title": "Identifying Different Definitions of Future in the Assessment of Future\n  Economic Conditions: Application of PU Learning and Text Mining", "summary": "The Economy Watcher Survey, which is a market survey published by the\nJapanese government, contains \\emph{assessments of current and future economic\nconditions} by people from various fields. Although this survey provides\ninsights regarding economic policy for policymakers, a clear definition of the\nword \"future\" in future economic conditions is not provided. Hence, the\nassessments respondents provide in the survey are simply based on their\ninterpretations of the meaning of \"future.\" This motivated us to reveal the\ndifferent interpretations of the future in their judgments of future economic\nconditions by applying weakly supervised learning and text mining. In our\nresearch, we separate the assessments of future economic conditions into\neconomic conditions of the near and distant future using learning from positive\nand unlabeled data (PU learning). Because the dataset includes data from\nseveral periods, we devised new architecture to enable neural networks to\nconduct PU learning based on the idea of multi-task learning to efficiently\nlearn a classifier. Our empirical analysis confirmed that the proposed method\ncould separate the future economic conditions, and we interpreted the\nclassification results to obtain intuitions for policymaking.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.03348v3", "link": "http://arxiv.org/abs/1909.03348v3"}, {"title": "Bitcoin price and its marginal cost of production: support for a\n  fundamental value", "summary": "This study back-tests a marginal cost of production model proposed to value\nthe digital currency bitcoin. Results from both conventional regression and\nvector autoregression (VAR) models show that the marginal cost of production\nplays an important role in explaining bitcoin prices, challenging recent\nallegations that bitcoins are essentially worthless. Even with markets pricing\nbitcoin in the thousands of dollars each, the valuation model seems robust. The\ndata show that a price bubble that began in the Fall of 2017 resolved itself in\nearly 2018, converging with the marginal cost model. This suggests that while\nbubbles may appear in the bitcoin market, prices will tend to this bound and\nnot collapse to zero.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1805.07610v1", "link": "http://arxiv.org/abs/1805.07610v1"}, {"title": "Simulation of Stylized Facts in Agent-Based Computational Economic\n  Market Models", "summary": "We study the qualitative and quantitative appearance of stylized facts in\nseveral agent-based computational economic market (ABCEM) models. We perform\nour simulations with the SABCEMM (Simulator for Agent-Based Computational\nEconomic Market Models) tool recently introduced by the authors (Trimborn et\nal. 2019). Furthermore, we present novel ABCEM models created by recombining\nexisting models and study them with respect to stylized facts as well. This can\nbe efficiently performed by the SABCEMM tool thanks to its object-oriented\nsoftware design. The code is available on GitHub (Trimborn et al. 2018), such\nthat all results can be reproduced by the reader.", "category": ["econ.GN", "econ.EM", "q-fin.EC", "q-fin.TR"], "id": "http://arxiv.org/abs/1812.02726v2", "link": "http://arxiv.org/abs/1812.02726v2"}, {"title": "Double Kernel estimation of sensitivities", "summary": "This paper adresses the general issue of estimating the sensitivity of the\nexpectation of a random variable with respect to a parameter characterizing its\nevolution. In finance for example, the sensitivities of the price of a\ncontingent claim are called the Greeks. A new way of estimating the Greeks has\nbeen recently introduced by Elie, Fermanian and Touzi through a randomization\nof the parameter of interest combined with non parametric estimation\ntechniques. This paper studies another type of those estimators whose interest\nis to be closely related to the score function, which is well known to be the\noptimal Greek weight. This estimator relies on the use of two distinct kernel\nfunctions and the main interest of this paper is to provide its asymptotic\nproperties. Under a little more stringent condition, its rate of convergence\nequals the one of those introduced by Elie, Fermanian and Touzi and outperforms\nthe finite differences estimator. In addition to the technical interest of the\nproofs, this result is very encouraging in the dynamic of creating new type of\nestimators for sensitivities.", "category": ["q-fin.CP", "math.PR"], "id": "http://arxiv.org/abs/0909.2624v1", "link": "http://arxiv.org/abs/0909.2624v1"}, {"title": "Estimation of high-dimensional factor models and its application in\n  power data analysis", "summary": "In dealing with high-dimensional data, factor models are often used for\nreducing dimensions and extracting relevant information. The spectrum of\ncovariance matrices from power data exhibits two aspects: 1) bulk, which arises\nfrom random noise or fluctuations and 2) spikes, which represents factors\ncaused by anomaly events. In this paper, we propose a new approach to the\nestimation of high-dimensional factor models, minimizing the distance between\nthe empirical spectral density (ESD) of covariance matrices of the residuals of\npower data that are obtained by subtracting principal components and the\nlimiting spectral density (LSD) from a multiplicative covariance structure\nmodel. The free probability theory (FPT) is used to derive the spectral density\nof the multiplicative covariance model, which efficiently solves the\ncomputational difficulties. The proposed approach connects the estimation of\nthe number of factors to the LSD of covariance matrices of the residuals, which\nprovides estimators of the number of factors and the correlation structure\ninformation in the residuals. Considering a lot of measurement noise is\ncontained in the power data and the correlation structure is complex for the\nresiduals, the approach prefers approaching the ESD of covariance matrices of\nthe residuals through a multiplicative covariance model, which avoids making\ncrude assumptions or simplifications on the complex structure of the data.\nTheoretical studies show the proposed approach is robust against noise and\nsensitive to the presence of weak factors. The synthetic data from IEEE 118-bus\npower system is used to validate the effectiveness of the approach.\nFurthermore, the application to the analysis of the real-world online\nmonitoring data in a power grid shows that the estimators in the approach can\nbe used to indicate the system behavior.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1905.02061v2", "link": "http://arxiv.org/abs/1905.02061v2"}, {"title": "Testing for Quantile Sample Selection", "summary": "This paper provides a testing approach for detecting sample selection in\nnonparametric conditional quantile functions. Our testing strategy consists of\na two-step procedure: the first test is an omitted predictor test with the\npropensity score as the omitted variable. As with any omnibus test, in the case\nof rejection we cannot distinguish between rejection due to genuine selection\nor to misspecification. Thus, since the differentiation of the two causes has\nimplications for nonparametric (point) identification and estimation of the\nconditional quantile function(s), we suggest a second test to identify whether\nthe cause for rejection at the first stage was solely due to selection or not.\nUsing only individuals with propensity score close to one, this second test\nrelies on an `identification at infinity' argument, but accommodates cases of\nirregular identification. Our testing procedure does not require any parametric\nassumptions on the selection equation, and all our results hold uniformly\nacross quantile ranks in a compact set. We apply our procedure to test for\nselection in log hourly wages using UK Family Expenditure Survey data.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1907.07412v3", "link": "http://arxiv.org/abs/1907.07412v3"}, {"title": "Representing All Stable Matchings by Walking a Maximal Chain", "summary": "The seminal book of Gusfield and Irving [GI89] provides a compact and\nalgorithmically useful way to represent the collection of stable matches\ncorresponding to a given set of preferences. In this paper, we reinterpret the\nmain results of [GI89], giving a new proof of the characterization which is\nable to bypass a lot of the \"theory building\" of the original works. We also\nprovide a streamlined and efficient way to compute this representation. Our\nproofs and algorithms emphasize the connection to well-known properties of the\ndeferred acceptance algorithm.", "category": [], "id": "http://arxiv.org/abs/1910.04401v1", "link": "http://arxiv.org/abs/1910.04401v1"}, {"title": "Group Average Treatment Effects for Observational Studies", "summary": "The paper proposes an estimator to make inference of heterogeneous treatment\neffects sorted by impact groups (GATES) for non-randomised experiments. The\ngroups can be understood as a broader aggregation of the conditional average\ntreatment effect (CATE) where the number of groups is set in advance. In\neconomics, this approach is similar to pre-analysis plans. Observational\nstudies are standard in policy evaluation from labour markets, educational\nsurveys and other empirical studies. To control for a potential selection-bias,\nwe implement a doubly-robust estimator in the first stage. We use machine\nlearning methods to learn the conditional mean functions as well as the\npropensity score. The group average treatment effect is then estimated via a\nlinear projection model. The linear model is easy to interpret, provides\np-values and confidence intervals, and limits the danger of finding spurious\nheterogeneity due to small subgroups in the CATE. To control for confounding in\nthe linear model, we use Neyman-orthogonal moments to partial out the effect\nthat covariates have on both, the treatment assignment and the outcome. The\nresult is a best linear predictor for effect heterogeneity based on impact\ngroups. We find that our proposed method has lower absolute errors as well as\nsmaller bias than the benchmark doubly-robust estimator. We further introduce a\nbagging type averaging for the CATE function for each observation to avoid\nbiases through sample splitting. The advantage of the proposed method is a\nrobust linear estimation of heterogeneous group treatment effects in\nobservational studies.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1911.02688v5", "link": "http://arxiv.org/abs/1911.02688v5"}, {"title": "Time-Changed Ornstein-Uhlenbeck Processes And Their Applications In\n  Commodity Derivative Models", "summary": "This paper studies subordinate Ornstein-Uhlenbeck (OU) processes, i.e., OU\ndiffusions time changed by L\\'{e}vy subordinators. We construct their sample\npath decomposition, show that they possess mean-reverting jumps, study their\nequivalent measure transformations, and the spectral representation of their\ntransition semigroups in terms of Hermite expansions. As an application, we\npropose a new class of commodity models with mean-reverting jumps based on\nsubordinate OU process. Further time changing by the integral of a CIR process\nplus a deterministic function of time, we induce stochastic volatility and time\ninhomogeneity, such as seasonality, in the models. We obtain analytical\nsolutions for commodity futures options in terms of Hermite expansions. The\nmodels are consistent with the initial futures curve, exhibit Samuelson's\nmaturity effect, and are flexible enough to capture a variety of implied\nvolatility smile patterns observed in commodities futures options.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1204.3679v1", "link": "http://arxiv.org/abs/1204.3679v1"}, {"title": "Identifying the Effect of Persuasion", "summary": "We set up an econometric model of persuasion and study identification of key\nparameters under various scenarios of data availability. We find that a\ncommonly used measure of persuasion does not estimate the persuasion rate of\nany population in general. We provide formal identification results, recommend\nseveral new parameters to estimate, and discuss their interpretation. Further,\nwe propose methods for carrying out inference. We revisit the empirical\nliterature on persuasion to show that the persuasive effect is highly\nheterogeneous. We also show that the existence of a continuous instrument opens\nup the possibility of point identification for the policy-relevant population.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1812.02276v2", "link": "http://arxiv.org/abs/1812.02276v2"}, {"title": "Behavioral Finance -- Asset Prices Predictability, Equity Premium\n  Puzzle, Volatility Puzzle: The Rational Finance Approach", "summary": "In this paper we address three main objections of behavioral finance to the\ntheory of rational finance, considered as anomalies the theory of rational\nfinance cannot explain: Predictability of asset returns, The Equity Premium,\n(The Volatility Puzzle. We offer resolutions of those objections within the\nrational finance. We do not claim that those are the only possible explanations\nof the anomalies, but offer statistical models within the rational theory of\nfinance which can be used without relying on behavioral finance assumptions\nwhen searching for explanations of those anomalies.", "category": ["q-fin.MF", "q-fin.GN"], "id": "http://arxiv.org/abs/1710.03211v2", "link": "http://dx.doi.org/10.13140/RG.2.2.29789.77287"}, {"title": "The Jacobi Stochastic Volatility Model", "summary": "We introduce a novel stochastic volatility model where the squared volatility\nof the asset return follows a Jacobi process. It contains the Heston model as a\nlimit case. We show that the joint density of any finite sequence of log\nreturns admits a Gram-Charlier A expansion with closed-form coefficients. We\nderive closed-form series representations for option prices whose discounted\npayoffs are functions of the asset price trajectory at finitely many time\npoints. This includes European call, put, and digital options, forward start\noptions, and can be applied to discretely monitored Asian options. In a\nnumerical analysis we show that option prices can be accurately and efficiently\napproximated by truncating their series representations.", "category": ["q-fin.MF", "q-fin.CP"], "id": "http://arxiv.org/abs/1605.07099v4", "link": "http://dx.doi.org/10.1007/s00780-018-0364-8"}, {"title": "Estimating Operational Risk Capital with Greater Accuracy, Precision,\n  and Robustness", "summary": "The largest US banks are required by regulatory mandate to estimate the\noperational risk capital they must hold using an Advanced Measurement Approach\n(AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution\nApproach (LDA) which defines the aggregate loss distribution as the convolution\nof a frequency and a severity distribution representing the number and\nmagnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th\npercentile) estimate of this annual loss distribution. In practice, the\nseverity distribution drives the capital estimate, which is essentially a very\nhigh quantile of the estimated severity distribution. Unfortunately, because\nthe relevant severities are heavy-tailed AND the quantiles being estimated are\nso high, VaR always appears to be a convex function of the severity parameters,\ncausing all widely-used estimators to generate biased capital estimates\n(apparently) due to Jensen's Inequality. The observed capital inflation is\nsometimes enormous, even at the unit-of-measure (UoM) level (even billions\nUSD). Herein I present an estimator of capital that essentially eliminates this\nupward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with\nthe regulatory intent of the LDA framework than implementations that fail to\nmitigate this bias. RCE also notably increases the precision of the capital\nestimate and consistently increases its robustness to violations of the i.i.d.\ndata presumption (which are endemic to operational risk loss event data). So\nwith greater capital accuracy, precision, and robustness, RCE lowers capital\nrequirements at both the UoM and enterprise levels, increases capital stability\nfrom quarter to quarter, ceteris paribus, and does both while more accurately\nand precisely reflecting regulatory intent. RCE is straightforward to implement\nusing any major statistical software package.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1406.0389v6", "link": "http://arxiv.org/abs/1406.0389v6"}, {"title": "Stratified regression-based variance reduction approach for weak\n  approximation schemes", "summary": "In this paper we suggest a modification of the regression-based variance\nreduction approach recently proposed in Belomestny et al. This modification is\nbased on the stratification technique and allows for a further significant\nvariance reduction. The performance of the proposed approach is illustrated by\nseveral numerical examples.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1612.05255v2", "link": "http://dx.doi.org/10.1016/j.matcom.2017.05.003"}, {"title": "Interest rate models and Whittaker functions", "summary": "I present the technique which can analyse some interest rate models:\nConstantinides-Ingersoll, CIR-model, geometric CIR and Geometric Brownian\nMotion. All these models have the unified structure of Whittaker function. The\nmain focus of this text is closed-form solutions of the zero-coupon bond value\nin these models. In text I emphasize the specific details of mathematical\nmethods of their determination such as Laplace transform and hypergeometric\nfunctions.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1405.2459v1", "link": "http://arxiv.org/abs/1405.2459v1"}, {"title": "Hedging with transient price impact for non-covered and covered options", "summary": "We solve the superhedging problem for European options in a market with\nfinite liquidity where trading has transient impact on prices, and possibly a\npermanent one in addition. Impact is multiplicative to ensure positive asset\nprices. Hedges and option prices depend on the physical and cash delivery\nspecifications of the option settlement. For non-covered options, where impact\nat the inception and maturity dates matters, we characterize the superhedging\nprice as a viscosity solution of a degenerate semilinear pde that can have\ngradient constraints. The non-linearity of the pde is governed by the transient\nnature of impact through a resilience function. For covered options, the\npricing pde involves gamma constraints but is not affected by transience of\nimpact. We use stochastic target techniques and geometric dynamic programming\nin reduced coordinates.", "category": ["q-fin.PR", "math.PR", "q-fin.TR"], "id": "http://arxiv.org/abs/1807.05917v1", "link": "http://arxiv.org/abs/1807.05917v1"}, {"title": "Multivariate Fractional Components Analysis", "summary": "We propose a setup for fractionally cointegrated time series which is\nformulated in terms of latent integrated and short-memory components. It\naccommodates nonstationary processes with different fractional orders and\ncointegration of different strengths and is applicable in high-dimensional\nsettings. In an application to realized covariance matrices, we find that\northogonal short- and long-memory components provide a reasonable fit and\ncompetitive out-of-sample performance compared to several competing methods.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1812.09149v2", "link": "http://arxiv.org/abs/1812.09149v2"}, {"title": "A discussion of stock market speculation by Pierre-Joseph Proudhon", "summary": "The object of this contribution is to present the ideas behind the thinking\nof the French economist Pierre-Joseph Proudhon (1809-1865) in relation to the\ncauses and effects of Stock market speculation. It is based upon the works of\nthis author but particularly on his \"Manuel du sp\\'eculateur \\`a la Bourse\"\n(Stock Market Speculator Manual) edited in 1857 in Paris. Compared to the\nmarkets of today, however, the stock market described by Proudhon appears\nembryonic. Nevertheless it represents the location for transactions in\nfinancial assets, commodities, precious metals and even some transactions\ninvolving options. This contribution is organised in the following manner - the\nfirst section is devoted to the development of Proudhon's thought in relation\nto speculation. It is divided into two parts. The first part is dedicated to\nPierre-Joseph Proudhon's definitions of stock market speculation or gambling\nwith shares that for him served no purpose either from a human or economic\nperspective and was therefore condemnable and to be contrasted with\nentrepreneurial speculation that, even though it is a highly-risky activity,\ninvolves the spirit of enterprise and provides the lifeblood of economic\ngrowth. The second part allows us to present Pierre-Joseph Proudhon's\npropositions in relation to restricting the speculation that he considers\nobnoxious. The second section has two objectives: one part places in\nperspective the views of Proudhon and the characteristics of stock market\nactivity under the Second Empire whilst the other part examines current-day\naspects of the characteristics evoked by Proudhon. We are interested especially\nin the question of the regulation and that of the relevance today of certain\naccounting practices.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1005.0221v1", "link": "http://arxiv.org/abs/1005.0221v1"}, {"title": "Second Order BSDEs with Jumps: Existence and probabilistic\n  representation for fully-nonlinear PIDEs", "summary": "In this paper, we pursue the study of second order BSDEs with jumps (2BSDEJs\nfor short) started in our accompanying paper [15]. We prove existence of these\nequations by a direct method, thus providing complete wellposedness for\n2BSDEJs. These equations are a natural candidate for the probabilistic\ninterpretation of some fully non-linear partial integro-differential equations,\nwhich is the point of the second part of this work. We prove a non-linear\nFeynman-Kac formula and show that solutions to 2BSDEJs provide viscosity\nsolutions of the associated PIDEs.", "category": ["math.PR", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1208.0763v2", "link": "http://arxiv.org/abs/1208.0763v2"}, {"title": "Tracking the circulation routes of fresh coins in Bitcoin: A way of\n  identifying coin miners with transaction network structural properties", "summary": "Bitcoin draws the highest degree of attention among cryptocurrencies, while\ncoin mining is one of the most important fashion of profiting in the Bitcoin\necosystem. This paper constructs fresh coin circulation networks by tracking\nthe fresh coin transfer routes with transaction referencing in Bitcoin\nblockchain. This paper proposes a heuristic algorithm to identifying coin\nminers by comparing coin circulation networks from different mining pools and\nthereby inferring the common profit distribution schemes of Bitcoin mining\npools. Furthermore, this paper characterizes the increasing trend of Bitcoin\nminer numbers during recent years.", "category": ["q-fin.GN", "econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1911.06400v1", "link": "http://dx.doi.org/10.13878/j.cnki.jnuist.2018.04.009"}, {"title": "Quantification of systemic risk from overlapping portfolios in the\n  financial system", "summary": "Financial markets are exposed to systemic risk, the risk that a substantial\nfraction of the system ceases to function and collapses. Systemic risk can\npropagate through different mechanisms and channels of contagion. One important\nform of financial contagion arises from indirect interconnections between\nfinancial institutions mediated by financial markets. This indirect\ninterconnection occurs when financial institutions invest in common assets and\nis referred to as overlapping portfolios. In this work we quantify systemic\nrisk from indirect interconnections between financial institutions. Having\ncomplete information of security holdings of major Mexican financial\nintermediaries and the ability to uniquely identify securities in their\nportfolios, allows us to represent the Mexican financial system as a bipartite\nnetwork of securities and financial institutions. This makes it possible to\nquantify systemic risk arising from overlapping portfolios. We show that\nfocusing only on direct exposures underestimates total systemic risk levels by\nup to 50%. By representing the financial system as a multi-layer network of\ndirect exposures (default contagion) and indirect exposures (overlapping\nportfolios) we estimate the mutual influence of different channels of\ncontagion. The method presented here is the first objective data-driven\nquantification of systemic risk on national scales that includes overlapping\nportfolios.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1802.00311v1", "link": "http://arxiv.org/abs/1802.00311v1"}, {"title": "Performance of tail hedged portfolio with third moment variation swap", "summary": "The third moment variation of a financial asset return process is defined by\nthe quadratic covariation between the return and square return processes. The\nskew and fat tail risk of an underlying asset can be hedged using a third\nmoment variation swap under which a predetermined fixed leg and the floating\nleg of the realized third moment variation are exchanged. The probability\ndensity function of the hedged portfolio with the third moment variation swap\nwas examined using a partial differential equation approach. An alternating\ndirection implicit method was used for numerical analysis of the partial\ndifferential equation. Under the stochastic volatility and jump diffusion\nstochastic volatility models, the distributions of the hedged portfolio return\nare symmetric and have more Gaussian-like thin-tails.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.PM", "q-fin.RM"], "id": "http://arxiv.org/abs/1908.05105v1", "link": "http://dx.doi.org/10.1007/s10614-016-9593-0"}, {"title": "High-order compact finite difference scheme for option pricing in\n  stochastic volatility with contemporaneous jump models", "summary": "We extend the scheme developed in B. D\\\"uring, A. Pitkin, \"High-order compact\nfinite difference scheme for option pricing in stochastic volatility jump\nmodels\", 2019, to the so-called stochastic volatility with contemporaneous\njumps (SVCJ) model, derived by Duffie, Pan and Singleton. The performance of\nthe scheme is assessed through a number of numerical experiments, using\ncomparisons against a standard second-order central difference scheme. We\nobserve that the new high-order compact scheme achieves fourth order\nconvergence and discuss the effects on efficiency and computation time.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1810.13248v2", "link": "http://arxiv.org/abs/1810.13248v2"}, {"title": "A First-Order BSPDE for Swing Option Pricing", "summary": "We study an optimal control problem related to swing option pricing in a\ngeneral non-Markovian setting in continuous time. As a main result we show that\nthe value process solves a first-order non-linear backward stochastic partial\ndifferential equation. Based on this result we can characterize the set of\noptimal controls and derive a dual minimization problem.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1305.3988v1", "link": "http://arxiv.org/abs/1305.3988v1"}, {"title": "Cointegration in high frequency data", "summary": "In this paper, we consider a framework adapting the notion of cointegration\nwhen two asset prices are generated by a driftless It\\^{o}-semimartingale\nfeaturing jumps with infinite activity, observed synchronously and regularly at\nhigh frequency. We develop a regression based estimation of the cointegrated\nrelations method and show the related consistency and central limit theory when\nthere is cointegration within that framework. We also provide a Dickey-Fuller\ntype residual based test for the null of no cointegration against the\nalternative of cointegration, along with its limit theory. Under no\ncointegration, the asymptotic limit is the same as that of the original\nDickey-Fuller residual based test, so that critical values can be easily\ntabulated in the same way. Finite sample indicates adequate size and good power\nproperties in a variety of realistic configurations, outperforming original\nDickey-Fuller and Phillips-Perron type residual based tests, whose sizes are\ndistorted by non ergodic time-varying variance and power is altered by price\njumps. Two empirical examples consolidate the Monte-Carlo evidence that the\nadapted tests can be rejected while the original tests are not, and vice versa.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/1905.07081v1", "link": "http://arxiv.org/abs/1905.07081v1"}, {"title": "Verifying the existence of maximum likelihood estimates for generalized\n  linear models", "summary": "A fundamental problem with nonlinear estimation models is that estimates are\nnot guaranteed to exist. However, while non-existence is a well-studied issue\nfor binary choice models, it presents significant challenges for other models\nas well and is not as well understood in more general settings. These\nchallenges are only magnified for models that feature many fixed effects and\nother high-dimensional parameters. We address the current ambiguity surrounding\nthis topic by studying the conditions that govern the existence of estimates\nfor a wide class of generalized linear models (GLMs). We show that some, but\nnot all, GLMs can still deliver consistent estimates of at least some of the\nlinear parameters when these conditions fail to hold. We also demonstrate how\nto verify these conditions in the presence of high-dimensional fixed effects,\nas are often recommended in the international trade literature and in other\ncommon panel settings", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1903.01633v5", "link": "http://arxiv.org/abs/1903.01633v5"}, {"title": "Sufficient Statistics for Unobserved Heterogeneity in Structural Dynamic\n  Logit Models", "summary": "We study the identification and estimation of structural parameters in\ndynamic panel data logit models where decisions are forward-looking and the\njoint distribution of unobserved heterogeneity and observable state variables\nis nonparametric, i.e., fixed-effects model. We consider models with two\nendogenous state variables: the lagged decision variable, and the time duration\nin the last choice. This class of models includes as particular cases important\neconomic applications such as models of market entry-exit, occupational choice,\nmachine replacement, inventory and investment decisions, or dynamic demand of\ndifferentiated products. The identification of structural parameters requires a\nsufficient statistic that controls for unobserved heterogeneity not only in\ncurrent utility but also in the continuation value of the forward-looking\ndecision problem. We obtain the minimal sufficient statistic and prove\nidentification of some structural parameters using a conditional likelihood\napproach. We apply this estimator to a machine replacement model.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1805.04048v1", "link": "http://arxiv.org/abs/1805.04048v1"}, {"title": "Optional projection under equivalent local martingale measures", "summary": "Motivation for this paper is to understand the impact of information on asset\nprice bubbles and perceived arbitrage opportunities. This boils down to study\noptional projections of $\\mathbb{G}$-adapted strict local martingales into a\nsmaller filtration $\\mathbb{F}$ under equivalent martingale measures. We give\nsome general results as well as analyze in details two specific examples given\nby the inverse three dimensional Bessel process and a class of stochastic\nvolatility models.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/2003.09940v1", "link": "http://arxiv.org/abs/2003.09940v1"}, {"title": "Gaussian Process Regression for Derivative Portfolio Modeling and\n  Application to CVA Computations", "summary": "Modeling counterparty risk is computationally challenging because it requires\nthe simultaneous evaluation of all the trades with each counterparty under both\nmarket and credit risk. We present a multi-Gaussian process regression\napproach, which is well suited for OTC derivative portfolio valuation involved\nin CVA computation. Our approach avoids nested simulation or simulation and\nregression of cash flows by learning a Gaussian metamodel for the\nmark-to-market cube of a derivative portfolio. We model the joint posterior of\nthe derivatives as a Gaussian process over function space, with the spatial\ncovariance structure imposed on the risk factors. Monte-Carlo simulation is\nthen used to simulate the dynamics of the risk factors. The uncertainty in\nportfolio valuation arising from the Gaussian process approximation is\nquantified numerically. Numerical experiments demonstrate the accuracy and\nconvergence properties of our approach for CVA computations, including a\ncounterparty portfolio of interest rate swaps.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1901.11081v2", "link": "http://arxiv.org/abs/1901.11081v2"}, {"title": "Trading with Small Price Impact", "summary": "An investor trades a safe and several risky assets with linear price impact\nto maximize expected utility from terminal wealth. In the limit for small\nimpact costs, we explicitly determine the optimal policy and welfare, in a\ngeneral Markovian setting allowing for stochastic market, cost, and preference\nparameters. These results shed light on the general structure of the problem at\nhand, and also unveil close connections to optimal execution problems and to\nother market frictions such as proportional and fixed transaction costs.", "category": ["q-fin.PM", "q-fin.TR"], "id": "http://arxiv.org/abs/1402.5304v4", "link": "http://arxiv.org/abs/1402.5304v4"}, {"title": "Dynamics of Value-Tracking in Financial Markets", "summary": "The efficiency of a modern economy depends on what we call the Value-Tracking\nHypothesis: that market prices of key assets broadly track some underlying\nvalue. This can be expected if a sufficient weight of market participants are\nvaluation-based traders, buying and selling an asset when its price is,\nrespectively, below and above their well-informed private valuations. Such\ntracking will never be perfect, and we propose a natural unit of tracking\nerror, the 'deciblack'. We then use a simple discrete-time model to show how\nlarge tracking errors can arise if enough market participants are not\nvaluation-based traders, regardless of how much information the valuation-based\ntraders have. We find a threshold above which value-tracking breaks down\nwithout any changes in the underlying value of the asset. Because financial\nmarkets are increasingly dominated by non-valuation-based traders, assessing\nhow much valuation-based investing is required for reasonable value tracking is\nof urgent practical interest.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/1903.09898v2", "link": "http://arxiv.org/abs/1903.09898v2"}, {"title": "Comparative Study of Two Extensions of Heston Stochastic Volatility\n  Model", "summary": "In the option valuation literature, the shortcomings of one factor stochastic\nvolatility models have traditionally been addressed by adding jumps to the\nstock price process. An alternate approach in the context of option pricing and\ncalibration of implied volatility is the addition of a few other factors to the\nvolatility process. This paper contemplates two extensions of the Heston\nstochastic volatility model. Out of which, one considers the addition of jumps\nto the stock price process (a stochastic volatility jump diffusion model) and\nanother considers an additional stochastic volatility factor varying at a\ndifferent time scale (a multiscale stochastic volatility model). An empirical\nanalysis is carried out on the market data of options with different strike\nprices and maturities, to compare the pricing performance of these models and\nto capture their implied volatility fit. The unknown parameters of these models\nare calibrated using the non-linear least square optimization. It has been\nfound that the multiscale stochastic volatility model performs better than the\nHeston stochastic volatility model and the stochastic volatility jump diffusion\nmodel for the data set under consideration.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1912.10237v1", "link": "http://arxiv.org/abs/1912.10237v1"}, {"title": "Estimation and prediction of credit risk based on rating transition\n  systems", "summary": "Risk management is an important practice in the banking industry. In this\npaper we develop a new methodology to estimate and predict the probability of\ndefault (PD) based on the rating transition matrices, which relates the rating\ntransition matrices to the macroeconomic variables. Our method can overcome the\nshortcomings of the framework of Belkin et al. (1998), and is especially useful\nin predicting the PD and doing stress testing. Simulation is conducted at the\nend, which shows that our method can provide more accurate estimate than that\nobtained by the method of Belkin et al. (1998).", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1607.00448v2", "link": "http://arxiv.org/abs/1607.00448v2"}, {"title": "Chain effects of clean water: The Mills-Reincke phenomenon in early\n  twentieth-century Japan", "summary": "This study explores the validity of chain effects of clean water, which are\nknown as the \"Mills-Reincke phenomenon,\" in early twentieth-century Japan.\nRecent studies have reported that water purifications systems are responsible\nfor huge contributions to human capital. Although some studies have\ninvestigated the instantaneous effects of water-supply systems in pre-war\nJapan, little is known about the chain effects of these systems. By analyzing\ncity-level cause-specific mortality data from 1922-1940, we find that a decline\nin typhoid deaths by one per 1,000 people decreased the risk of death due to\nnon-waterborne diseases such as tuberculosis and pneumonia by 0.742-2.942 per\n1,000 people. Our finding suggests that the observed Mills-Reincke phenomenon\ncould have resulted in the relatively rapid decline in the mortality rate in\nearly twentieth-century Japan.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1805.00875v3", "link": "http://arxiv.org/abs/1805.00875v3"}, {"title": "Realized Volatility Analysis in A Spin Model of Financial Markets", "summary": "We calculate the realized volatility in the spin model of financial markets\nand examine the returns standardized by the realized volatility. We find that\nmoments of the standardized returns agree with the theoretical values of\nstandard normal variables. This is the first evidence that the return dynamics\nof the spin financial market is consistent with the view of the\nmixture-of-distribution hypothesis that also holds in the real financial\nmarkets.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1511.08997v1", "link": "http://dx.doi.org/10.7566/JPSCP.1.019007"}, {"title": "Multi-scaling of wholesale electricity prices", "summary": "We empirically analyze the most volatile component of the electricity price\ntime series from two North-American wholesale electricity markets. We show that\nthese time series exhibit fluctuations which are not described by a Brownian\nMotion, as they show multi-scaling, high Hurst exponents and sharp price\nmovements. We use the generalized Hurst exponent (GHE, $H(q)$) to show that\nalthough these time-series have strong cyclical components, the fluctuations\nexhibit persistent behaviour, i.e., $H(q)>0.5$. We investigate the\neffectiveness of the GHE as a predictive tool in a simple linear forecasting\nmodel, and study the forecast error as a function of $H(q)$, with $q=1$ and\n$q=2$. Our results suggest that the GHE can be used as prediction tool for\nthese time series when the Hurst exponent is dynamically evaluated on rolling\ntime windows of size $\\approx 50 - 100$ hours. These results are also compared\nto the case in which the cyclical components have been subtracted from the time\nseries, showing the importance of cyclicality in the prediction power of the\nHurst exponent.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1507.06219v1", "link": "http://arxiv.org/abs/1507.06219v1"}, {"title": "A Method for Comparing Hedge Funds", "summary": "The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system to\nidentify behavioral similarities among time-series representing monthly returns\nof 11,312 hedge funds operated during approximately one decade (2000 - 2010).\nThe presented approach of cross-category and cross-location classification\nassists the investor to identify alternative investments.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1303.0073v2", "link": "http://arxiv.org/abs/1303.0073v2"}, {"title": "Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation", "summary": "We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata.", "category": ["q-fin.ST", "q-fin.CP"], "id": "http://arxiv.org/abs/1008.0149v1", "link": "http://arxiv.org/abs/1008.0149v1"}, {"title": "Measurement of the evolution of technology: A new perspective", "summary": "A fundamental problem in technological studies is how to measure the\nevolution of technology. The literature has suggested several approaches to\nmeasuring the level of technology (or state-of-the-art) and changes in\ntechnology. However, the measurement of technological advances and\ntechnological evolution is often a complex and elusive topic in science. The\nstudy here starts by establishing a conceptual framework of technological\nevolution based on the theory of technological parasitism, in broad analogy\nwith biology. Then, the measurement of the evolution of technology is modelled\nin terms of morphological changes within complex systems considering the\ninteraction between a host technology and its subsystems of technology. The\ncoefficient of evolutionary growth of the model here indicates the grade and\ntype of the evolutionary route of a technology. This coefficient is quantified\nin real instances using historical data of farm tractor, freight locomotive and\nelectricity generation technology in steam-powered plants and\ninternal-combustion plants. Overall, then, it seems that the approach here is\nappropriate in grasping the typology of evolution of complex systems of\ntechnology and in predicting which technologies are likeliest to evolve\nrapidly.", "category": ["q-fin.EC"], "id": "http://arxiv.org/abs/1803.08698v1", "link": "http://arxiv.org/abs/1803.08698v1"}, {"title": "Commuting Service Platform: Concept and Analysis", "summary": "We propose and investigate the concept of commuting service platforms (CSP)\nthat leverage emerging mobility services to provide commuting services and\nconnect directly commuters (employees) and their worksites (employers). By\napplying the two-sided market analysis framework, we show under what conditions\na CSP may present the two-sidedness. Both the monopoly and duopoly CSPs are\nthen analyzed. We showhowthe price allocation, i.e., the prices charged to\ncommuters and worksites, can impact the participation and profit of the CSPs.\nWe also add demand constraints to the duopoly model so that the participation\nrates ofworksites and employees are (almost) the same. With demand constraints,\nthe competition between the two CSPs becomes less intense in general.\nDiscussions are presented on how the results and findings in this paper may\nhelp build CSP in practice and how to develop new, CSP-based travel demand\nmanagement strategies.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.03646v1", "link": "http://arxiv.org/abs/2001.03646v1"}, {"title": "Taxation as an instrument of stimulation of innovation-active business\n  entities", "summary": "The analysis of the theoretical material revealed the lack of consensus on\ndefini-tion of the tax stimulation of innovation-active business entities\nwithin the re-gional taxation. The definition tax stimulation of\ninnovation-active business en-tities is specified.", "category": ["q-fin.EC", "q-fin.GN"], "id": "http://arxiv.org/abs/1412.2746v1", "link": "http://arxiv.org/abs/1412.2746v1"}, {"title": "The role of industry, occupation, and location specific knowledge in the\n  survival of new firms", "summary": "How do regions acquire the knowledge they need to diversify their economic\nactivities? How does the migration of workers among firms and industries\ncontribute to the diffusion of that knowledge? Here we measure the industry,\noccupation, and location-specific knowledge carried by workers from one\nestablishment to the next using a dataset summarizing the individual work\nhistory for an entire country. We study pioneer firms--firms operating in an\nindustry that was not present in a region--because the success of pioneers is\nthe basic unit of regional economic diversification. We find that the growth\nand survival of pioneers increase significantly when their first hires are\nworkers with experience in a related industry, and with work experience in the\nsame location, but not with past experience in a related occupation. We compare\nthese results with new firms that are not pioneers and find that\nindustry-specific knowledge is significantly more important for pioneer than\nnon-pioneer firms. To address endogeneity we use Bartik instruments, which\nleverage national fluctuations in the demand for an activity as shocks for\nlocal labor supply. The instrumental variable estimates support the finding\nthat industry-related knowledge is a predictor of the survival and growth of\npioneer firms. These findings expand our understanding of the micro-mechanisms\nunderlying regional economic diversification events.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1808.01237v1", "link": "http://arxiv.org/abs/1808.01237v1"}, {"title": "Waiting-times and returns in high-frequency financial data: an empirical\n  study", "summary": "In financial markets, not only prices and returns can be considered as random\nvariables, but also the waiting time between two transactions varies randomly.\nIn the following, we analyse the statistical properties of General Electric\nstock prices, traded at NYSE, in October 1999. These properties are critically\nrevised in the framework of theoretical predictions based on a continuous-time\nrandom walk model.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0203596v1", "link": "http://dx.doi.org/10.1016/S0378-4371(02)01048-8"}, {"title": "How well can we learn large factor models without assuming strong\n  factors?", "summary": "In this paper, we consider the problem of learning models with a latent\nfactor structure. The focus is to find what is possible and what is impossible\nif the usual strong factor condition is not imposed. We study the minimax rate\nand adaptivity issues in two problems: pure factor models and panel regression\nwith interactive fixed effects. For pure factor models, if the number of\nfactors is known, we develop adaptive estimation and inference procedures that\nattain the minimax rate. However, when the number of factors is not specified a\npriori, we show that there is a tradeoff between validity and efficiency: any\nconfidence interval that has uniform validity for arbitrary factor strength has\nto be conservative; in particular its width is bounded away from zero even when\nthe factors are strong. Conversely, any data-driven confidence interval that\ndoes not require as an input the exact number of factors (including weak ones)\nand has shrinking width under strong factors does not have uniform coverage and\nthe worst-case coverage probability is at most 1/2. For panel regressions with\ninteractive fixed effects, the tradeoff is much better. We find that the\nminimax rate for learning the regression coefficient does not depend on the\nfactor strength and propose a simple estimator that achieves this rate.\nHowever, when weak factors are allowed, uncertainty in the number of factors\ncan cause a great loss of efficiency although the rate is not affected. In most\ncases, we find that the strong factor condition (and/or exact knowledge of\nnumber of factors) improves efficiency, but this condition needs to be imposed\nby faith and cannot be verified in data for inference purposes.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.10382v3", "link": "http://arxiv.org/abs/1910.10382v3"}, {"title": "Improving Point and Interval Estimates of Monotone Functions by\n  Rearrangement", "summary": "Suppose that a target function is monotonic, namely, weakly increasing, and\nan available original estimate of this target function is not weakly\nincreasing. Rearrangements, univariate and multivariate, transform the original\nestimate to a monotonic estimate that always lies closer in common metrics to\nthe target function. Furthermore, suppose an original simultaneous confidence\ninterval, which covers the target function with probability at least\n$1-\\alpha$, is defined by an upper and lower end-point functions that are not\nweakly increasing. Then the rearranged confidence interval, defined by the\nrearranged upper and lower end-point functions, is shorter in length in common\nnorms than the original interval and also covers the target function with\nprobability at least $1-\\alpha$. We demonstrate the utility of the improved\npoint and interval estimates with an age-height growth chart example.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/0806.4730v3", "link": "http://dx.doi.org/10.1093/biomet/asp030"}, {"title": "Fourth order compact scheme for option pricing under Merton and Kou\n  jump-diffusion models", "summary": "In this article, a three-time levels compact scheme is proposed to solve the\npartial integro-differential equation governing the option prices under\njump-diffusion models. In the proposed compact scheme, the second derivative\napproximation of unknowns is approximated by the value of unknowns and their\nfirst derivative approximations which allow us to obtain a tri-diagonal system\nof linear equations for the fully discrete problem. Moreover, consistency and\nstability of the proposed compact scheme are proved. Due to the low regularity\nof typical initial conditions, the smoothing operator is employed to ensure the\nfourth-order convergence rate. Numerical illustrations for pricing European\noptions under Merton and Kou jump-diffusion models are presented to validate\nthe theoretical results.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1804.07534v1", "link": "http://arxiv.org/abs/1804.07534v1"}, {"title": "Relative Maximum Likelihood Updating of Ambiguous Beliefs", "summary": "When ambiguous beliefs are represented by multiple priors, updating such a\nbelief may also include refining of the initial belief by discarding some of\nthe priors. Maximum Likelihood (ML) and Full Bayesian (FB) are the two extremes\nof refining beliefs based on likelihood. To capture behaviors between these two\nextremes, this paper proposes a new updating rule: Relative Maximum Likelihood\n(RML) updating. This rule updates an intermediate set of priors that is a\nlinear contraction of the initial set with respect to the set of priors\nattaining maximum likelihood. The linear contraction parameter captures the\nDM's relative inclination towards ML with respect to FB, hence ML and FB are\nthe two extreme special cases of RML.\n  More importantly, characterization of conditional preferences given by RML\nupdating is provided when the preferences are represented by the Maxmin\nExpected Utility. The axiomatization relies on weakening the axioms that\ncharacterize FB and ML. In particular, the axiom that characterizes ML is\nidentified in this paper, fixing an existing problem in the literature.", "category": [], "id": "http://arxiv.org/abs/1911.02678v3", "link": "http://arxiv.org/abs/1911.02678v3"}, {"title": "Study of a Market Model with Conservative Exchanges on Complex Networks", "summary": "Many models of market dynamics make use of the idea of conservative wealth\nexchanges among economic agents. A few years ago an exchange model using\nextremal dynamics was developed and a very interesting result was obtained: a\nself-generated minimum wealth or poverty line. On the other hand, the wealth\ndistribution exhibited an exponential shape as a function of the square of the\nwealth. These results have been obtained both considering exchanges between\nnearest neighbors or in a mean field scheme. In the present paper we study the\neffect of distributing the agents on a complex network. We have considered\narchetypical complex networks: Erd\\\"{o}s-R\\'enyi random networks and scale-free\nnetworks. The presence of a poverty line with finite wealth is preserved but\nspatial correlations are important, particularly between the degree of the node\nand the wealth. We present a detailed study of the correlations, as well as the\nchanges in the Gini coefficient, that measures the inequality, as a function of\nthe type and average degree of the considered networks.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1212.1061v2", "link": "http://dx.doi.org/10.1016/j.physa.2012.12.030"}, {"title": "Rental Housing Spot Markets: How Online Information Exchanges Can\n  Supplement Transacted-Rents Data", "summary": "Traditional US rental housing data sources such as the American Community\nSurvey and the American Housing Survey report on the transacted market - what\nexisting renters pay each month. They do not explicitly tell us about the spot\nmarket - i.e., the asking rents that current homeseekers must pay to acquire\nhousing - though they are routinely used as a proxy. This study compares\ngovernmental data to millions of contemporaneous rental listings and finds that\nasking rents diverge substantially from these most recent estimates.\nConventional housing data understate current market conditions and\naffordability challenges, especially in cities with tight and expensive rental\nmarkets.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2002.01578v1", "link": "http://dx.doi.org/10.1177/0739456X20904435"}, {"title": "Quantifying Stock Price Response to Demand Fluctuations", "summary": "We address the question of how stock prices respond to changes in demand. We\nquantify the relations between price change $G$ over a time interval $\\Delta t$\nand two different measures of demand fluctuations: (a) $\\Phi$, defined as the\ndifference between the number of buyer-initiated and seller-initiated trades,\nand (b) $\\Omega$, defined as the difference in number of shares traded in buyer\nand seller initiated trades. We find that the conditional expectations $<G\n>_{\\Omega}$ and $<G >_{\\Phi}$ of price change for a given $\\Omega$ or $\\Phi$\nare both concave. We find that large price fluctuations occur when demand is\nvery small --- a fact which is reminiscent of large fluctuations that occur at\ncritical points in spin systems, where the divergent nature of the response\nfunction leads to large fluctuations.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0106657v1", "link": "http://dx.doi.org/10.1103/PhysRevE.66.027104"}, {"title": "Eliminating the effect of rating bias on reputation systems", "summary": "The ongoing rapid development of the e-commercial and interest-base websites\nmake it more pressing to evaluate objects' accurate quality before\nrecommendation by employing an effective reputation system. The objects'\nquality are often calculated based on their historical information, such as\nselected records or rating scores, to help visitors to make decisions before\nwatching, reading or buying. Usually high quality products obtain a higher\naverage ratings than low quality products regardless of rating biases or\nerrors. However many empirical cases demonstrate that consumers may be misled\nby rating scores added by unreliable users or deliberate tampering. In this\ncase, users' reputation, i.e., the ability to rating trustily and precisely,\nmake a big difference during the evaluating process. Thus, one of the main\nchallenges in designing reputation systems is eliminating the effects of users'\nrating bias on the evaluation results. To give an objective evaluation of each\nuser's reputation and uncover an object's intrinsic quality, we propose an\niterative balance (IB) method to correct users' rating biases. Experiments on\ntwo online video-provided Web sites, namely MovieLens and Netflix datasets,\nshow that the IB method is a highly self-consistent and robust algorithm and it\ncan accurately quantify movies' actual quality and users' stability of rating.\nCompared with existing methods, the IB method has higher ability to find the\n\"dark horses\", i.e., not so popular yet good movies, in the Academy Awards.", "category": [], "id": "http://arxiv.org/abs/1801.05734v1", "link": "http://arxiv.org/abs/1801.05734v1"}, {"title": "How connected is too connected? Impact of network topology on systemic\n  risk and collapse of complex economic systems", "summary": "Economic interdependencies have become increasingly present in globalized\nproduction, financial and trade systems. While establishing interdependencies\namong economic agents is crucial for the production of complex products, they\nmay also increase systemic risks due to failure propagation. It is crucial to\nidentify how network connectivity impacts both the emergent production and risk\nof collapse of economic systems. In this paper we propose a model to study the\neffects of network structure on the behavior of economic systems by varying the\ndensity and centralization of connections among agents. The complexity of\nproduction increases with connectivity given the combinatorial explosion of\nparts and products. Emergent systemic risks arise when interconnections\nincrease vulnerabilities. Our results suggest a universal description of\neconomic collapse given in the emergence of tipping points and phase\ntransitions in the relationship between network structure and risk of\nindividual failure. This relationship seems to follow a sigmoidal form in the\ncase of increasingly denser or centralized networks. The model sheds new light\non the relevance of policies for the growth of economic complexity, and\nhighlights the trade-off between increasing the potential production of the\nsystem and its robustness to collapse. We discuss the policy implications of\nintervening in the organization of interconnections and system features, and\nstress how different network structures and node characteristics suggest\ndifferent directions in order to promote complex and robust economic systems.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1912.09814v1", "link": "http://arxiv.org/abs/1912.09814v1"}, {"title": "Multi-time state mean-variance model in continuous time", "summary": "In the continuous time mean-variance model, we want to minimize the variance\n(risk) of the investment portfolio with a given mean at terminal time. However,\nthe investor can stop the investment plan at any time before the terminal time.\nTo solve this kind of problem, we consider to minimize the variances of the\ninvestment portfolio at multi-time state. The advantage of this multi-time\nstate mean-variance model is that we can minimize the risk of the investment\nportfolio along the investment period. To obtain the optimal strategy of the\nmulti-time state mean-variance model, we introduce a sequence of Riccati\nequations which are connected by a jump boundary condition. Based on this\nsequence Riccati equations, we establish the relationship between the means and\nvariances of this multi-time state mean-variance model. Furthermore, we use an\nexample to verify that minimizing the variances of the multi-time state can\naffect the average of Maximum-Drawdown of the investment portfolio.", "category": ["q-fin.MF"], "id": "http://arxiv.org/abs/1912.01793v1", "link": "http://arxiv.org/abs/1912.01793v1"}, {"title": "Simplifying credit scoring rules using LVQ+PSO", "summary": "One of the key elements in the banking industry rely on the appropriate\nselection of customers. In order to manage credit risk, banks dedicate special\nefforts in order to classify customers according to their risk. The usual\ndecision making process consists in gathering personal and financial\ninformation about the borrower. Processing this information can be time\nconsuming, and presents some difficulties due to the heterogeneous structure of\ndata. We offer in this paper an alternative method that is able to classify\ncustomers' profiles from numerical and nominal attributes. The key feature of\nour method, called LVQ+PSO, is the finding of a reduced set of classifying\nrules. This is possible, due to the combination of a competitive neural network\nwith an optimization technique. These rules constitute a predictive model for\ncredit risk approval. The reduced quantity of rules makes this method not only\nuseful for credit officers aiming to make quick decisions about granting a\ncredit, but also could act as borrower's self selection. Our method was applied\nto an actual database of a credit consumer financial institution in Ecuador. We\nobtain very satisfactory results. Future research lines are exposed.", "category": ["q-fin.RM", "q-fin.CP"], "id": "http://arxiv.org/abs/1704.04450v1", "link": "http://dx.doi.org/10.1108/K-06-2016-0158"}, {"title": "Non-Parametric Inference Adaptive to Intrinsic Dimension", "summary": "We consider non-parametric estimation and inference of conditional moment\nmodels in high dimensions. We show that even when the dimension $D$ of the\nconditioning variable is larger than the sample size $n$, estimation and\ninference is feasible as long as the distribution of the conditioning variable\nhas small intrinsic dimension $d$, as measured by locally low doubling\nmeasures. Our estimation is based on a sub-sampled ensemble of the $k$-nearest\nneighbors ($k$-NN) $Z$-estimator. We show that if the intrinsic dimension of\nthe covariate distribution is equal to $d$, then the finite sample estimation\nerror of our estimator is of order $n^{-1/(d+2)}$ and our estimate is\n$n^{1/(d+2)}$-asymptotically normal, irrespective of $D$. The sub-sampling size\nrequired for achieving these results depends on the unknown intrinsic dimension\n$d$. We propose an adaptive data-driven approach for choosing this parameter\nand prove that it achieves the desired rates. We discuss extensions and\napplications to heterogeneous treatment effect estimation.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1901.03719v3", "link": "http://arxiv.org/abs/1901.03719v3"}, {"title": "Inferring Fundamental Value and Crash Nonlinearity from Bubble\n  Calibration", "summary": "Identifying unambiguously the presence of a bubble in an asset price remains\nan unsolved problem in standard econometric and financial economic approaches.\nA large part of the problem is that the fundamental value of an asset is, in\ngeneral, not directly observable and it is poorly constrained to calculate.\nFurther, it is not possible to distinguish between an exponentially growing\nfundamental price and an exponentially growing bubble price. We present a\nseries of new models based on the Johansen-Ledoit-Sornette (JLS) model, which\nis a flexible tool to detect bubbles and predict changes of regime in financial\nmarkets. Our new models identify the fundamental value of an asset price and\ncrash nonlinearity from a bubble calibration. In addition to forecasting the\ntime of the end of a bubble, the new models can also estimate the fundamental\nvalue and the crash nonlinearity. Besides, the crash nonlinearity obtained in\nthe new models presents a new approach to possibly identify the dynamics of a\ncrash after a bubble. We test the models using data from three historical\nbubbles ending in crashes from different markets. They are: the Hong Kong Hang\nSeng index 1997 crash, the S&P 500 index 1987 crash and the Shanghai Composite\nindex 2009 crash. All results suggest that the new models perform very well in\ndescribing bubbles, forecasting their ending times and estimating fundamental\nvalue and the crash nonlinearity. The performance of the new models is tested\nunder both the Gaussian and non-Gaussian residual assumption. Under the\nGaussian residual assumption, nested hypotheses with the Wilks statistics are\nused and the p-values suggest that models with more parameters are necessary.\nUnder non-Gaussian residual assumption, we use a bootstrap method to get type I\nand II errors of the hypotheses. All tests confirm that the generalized JLS\nmodels provide useful improvements over the standard JLS model.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1011.5343v1", "link": "http://arxiv.org/abs/1011.5343v1"}, {"title": "Econometric modelling and forecasting of intraday electricity prices", "summary": "In the following paper, we analyse the ID$_3$-Price in the German Intraday\nContinuous electricity market using an econometric time series model. A\nmultivariate approach is conducted for hourly and quarter-hourly products\nseparately. We estimate the model using lasso and elastic net techniques and\nperform an out-of-sample, very short-term forecasting study. The model's\nperformance is compared with benchmark models and is discussed in detail.\nForecasting results provide new insights to the German Intraday Continuous\nelectricity market regarding its efficiency and to the ID$_3$-Price behaviour.", "category": ["q-fin.ST", "econ.EM"], "id": "http://arxiv.org/abs/1812.09081v2", "link": "http://dx.doi.org/10.1016/j.jcomm.2019.100107"}, {"title": "High-order accurate implicit methods for the pricing of barrier options", "summary": "This paper deals with a high-order accurate implicit finite-difference\napproach to the pricing of barrier options. In this way various types of\nbarrier options are priced, including barrier options paying rebates, and\noptions on dividend-paying-stocks. Moreover, the barriers may be monitored\neither continuously or discretely. In addition to the high-order accuracy of\nthe scheme, and the stretching effect of the coordinate transformation, the\nmain feature of this approach lies on a probability-based optimal determination\nof boundary conditions. This leads to much faster and accurate results when\ncompared with similar pricing approaches. The strength of the present scheme is\nparticularly demonstrated in the valuation of discretely monitored barrier\noptions where it yields values closest to those obtained from the only\nsemi-analytical valuation method available.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/0710.0069v1", "link": "http://arxiv.org/abs/0710.0069v1"}, {"title": "Banking retail consumer finance data generator - credit scoring data\n  repository", "summary": "This paper presents two cases of random banking data generators based on\nmigration matrices and scoring rules. The banking data generator is a new hope\nin researches of finding the proving method of comparisons of various credit\nscoring techniques. There is analyzed the influence of one cyclic\nmacro--economic variable on stability in the time account and client\ncharacteristics. Data are very useful for various analyses to understand in the\nbetter way the complexity of the banking processes and also for students and\ntheir researches. There are presented very interesting conclusions for crisis\nbehavior, namely that if a crisis is impacted by many factors, both customer\ncharacteristics: application and behavioral; then there is very difficult to\nindicate these factors in the typical scoring analysis and the crisis is\neverywhere, in every kind of risk reports.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1105.2968v1", "link": "http://arxiv.org/abs/1105.2968v1"}, {"title": "Modelling volatility with v-transforms", "summary": "An approach to the modelling of financial return series using a class of\nuniformity-preserving transforms for uniform random variables is proposed.\nV-transforms describe the relationship between quantiles of the return\ndistribution and quantiles of the distribution of a predictable volatility\nproxy variable constructed as a function of the return. V-transforms can be\nrepresented as copulas and permit the formulation and estimation of models that\ncombine arbitrary marginal distributions with linear or non-linear time series\nmodels for the dynamics of the volatility proxy. The idea is illustrated using\na transformed Gaussian ARMA process for volatility, yielding the class of\nVT-ARMA copula models. These can replicate many of the stylized facts of\nfinancial return series and facilitate the calculation of marginal and\nconditional characteristics of the model including quantile measures of risk.\nEstimation of models is carried out by adapting the exact maximum likelihood\napproach to the estimation of ARMA processes.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/2002.10135v2", "link": "http://arxiv.org/abs/2002.10135v2"}, {"title": "Universal features of price formation in financial markets: perspectives\n  from Deep Learning", "summary": "Using a large-scale Deep Learning approach applied to a high-frequency\ndatabase containing billions of electronic market quotes and transactions for\nUS equities, we uncover nonparametric evidence for the existence of a universal\nand stationary price formation mechanism relating the dynamics of supply and\ndemand for a stock, as revealed through the order book, to subsequent\nvariations in its market price. We assess the model by testing its\nout-of-sample predictions for the direction of price moves given the history of\nprice and order flow, across a wide range of stocks and time periods. The\nuniversal price formation model is shown to exhibit a remarkably stable\nout-of-sample prediction accuracy across time, for a wide range of stocks from\ndifferent sectors. Interestingly, these results also hold for stocks which are\nnot part of the training sample, showing that the relations captured by the\nmodel are universal and not asset-specific.\n  The universal model --- trained on data from all stocks --- outperforms, in\nterms of out-of-sample prediction accuracy, asset-specific linear and nonlinear\nmodels trained on time series of any given stock, showing that the universal\nnature of price formation weighs in favour of pooling together financial data\nfrom various stocks, rather than designing asset- or sector-specific models as\ncommonly done. Standard data normalizations based on volatility, price level or\naverage spread, or partitioning the training data into sectors or categories\nsuch as large/small tick stocks, do not improve training results. On the other\nhand, inclusion of price and order flow history over many past observations is\nshown to improve forecasting performance, showing evidence of path-dependence\nin price dynamics.", "category": ["q-fin.ST", "q-fin.TR"], "id": "http://arxiv.org/abs/1803.06917v1", "link": "http://arxiv.org/abs/1803.06917v1"}, {"title": "Optimal Dividends Paid in a Foreign Currency for a L\u00e9vy Insurance Risk\n  Model", "summary": "This paper considers an optimal dividend distribution problem for an\ninsurance company where the dividends are paid in a foreign currency. In the\nabsence of dividend payments, our risk process follows a spectrally negative\nL\\'evy process. We assume that the exchange rate is described by a an\nexponentially L\\'evy process, possibly containing the same risk sources like\nthe surplus of the insurance company under consideration. The control mechanism\nchooses the amount of dividend payments. The objective is to maximise the\nexpected dividend payments received until the time of ruin and a penalty\npayment at the time of ruin, which is an increasing function of the size of the\nshortfall at ruin. A complete solution is presented to the corresponding\nstochastic control problem. Via the corresponding Hamilton--Jacobi--Bellman\nequation we find the necessary and sufficient conditions for optimality of a\nsingle dividend barrier strategy. A number of numerical examples illustrate the\ntheoretical analysis.", "category": ["q-fin.MF", "math.PR"], "id": "http://arxiv.org/abs/2001.03733v1", "link": "http://arxiv.org/abs/2001.03733v1"}, {"title": "A statistical physics perspective on criticality in financial markets", "summary": "Stock markets are complex systems exhibiting collective phenomena and\nparticular features such as synchronization, fluctuations distributed as\npower-laws, non-random structures and similarity to neural networks. Such\nspecific properties suggest that markets operate at a very special point.\nFinancial markets are believed to be critical by analogy to physical systems\nbut few statistically founded evidence have been given. Through a data-based\nmethodology and comparison to simulations inspired by statistical physics of\ncomplex systems, we show that the Dow Jones and indices sets are not rigorously\ncritical. However, financial systems are closer to the criticality in the crash\nneighborhood.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1310.2446v3", "link": "http://dx.doi.org/10.1088/1742-5468/2013/11/P11004"}, {"title": "Uniform Inference in High-Dimensional Gaussian Graphical Models", "summary": "Graphical models have become a very popular tool for representing\ndependencies within a large set of variables and are key for representing\ncausal structures. We provide results for uniform inference on high-dimensional\ngraphical models with the number of target parameters $d$ being possible much\nlarger than sample size. This is in particular important when certain features\nor structures of a causal model should be recovered. Our results highlight how\nin high-dimensional settings graphical models can be estimated and recovered\nwith modern machine learning methods in complex data sets. To construct\nsimultaneous confidence regions on many target parameters, sufficiently fast\nestimation rates of the nuisance functions are crucial. In this context, we\nestablish uniform estimation rates and sparsity guarantees of the square-root\nestimator in a random design under approximate sparsity conditions that might\nbe of independent interest for related problems in high-dimensions. We also\ndemonstrate in a comprehensive simulation study that our procedure has good\nsmall sample properties.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1808.10532v2", "link": "http://arxiv.org/abs/1808.10532v2"}, {"title": "Averaging estimation for instrumental variables quantile regression", "summary": "This paper proposes averaging estimation methods to improve the finite-sample\nefficiency of the instrumental variables quantile regression (IVQR) estimation.\nFirst, I apply Cheng, Liao, Shi's (2019) averaging GMM framework to the IVQR\nmodel. I propose using the usual quantile regression moments for averaging to\ntake advantage of cases when endogeneity is not too strong. I also propose\nusing two-stage least squares slope moments to take advantage of cases when\nheterogeneity is not too strong. The empirical optimal weight formula of Cheng\net al. (2019) helps optimize the bias-variance tradeoff, ensuring uniformly\nbetter (asymptotic) risk of the averaging estimator over the standard IVQR\nestimator under certain conditions. My implementation involves many\ncomputational considerations and builds on recent developments in the quantile\nliterature. Second, I propose a bootstrap method that directly averages among\nIVQR, quantile regression, and two-stage least squares estimators. More\nspecifically, I find the optimal weights in the bootstrap world and then apply\nthe bootstrap-optimal weights to the original sample. The bootstrap method is\nsimpler to compute and generally performs better in simulations, but it lacks\nthe formal uniform dominance results of Cheng et al. (2019). Simulation results\ndemonstrate that in the multiple-regressors/instruments case, both the GMM\naveraging and bootstrap estimators have uniformly smaller risk than the IVQR\nestimator across data-generating processes (DGPs) with all kinds of\ncombinations of different endogeneity levels and heterogeneity levels. In DGPs\nwith a single endogenous regressor and instrument, where averaging estimation\nis known to have least opportunity for improvement, the proposed averaging\nestimators outperform the IVQR estimator in some cases but not others.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1910.04245v1", "link": "http://arxiv.org/abs/1910.04245v1"}, {"title": "A Supermartingale Relation for Multivariate Risk Measures", "summary": "The equivalence between multiportfolio time consistency of a dynamic\nmultivariate risk measure and a supermartingale property is proven.\nFurthermore, the dual variables under which this set-valued supermartingale is\na martingale are characterized as the worst-case dual variables in the dual\nrepresentation of the risk measure. Examples of multivariate risk measures\nsatisfying the supermartingale property are given. Crucial for obtaining the\nresults are dual representations of scalarizations of set-valued dynamic risk\nmeasures, which are of independent interest in the fast growing literature on\nmultivariate risks.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/1510.05561v4", "link": "http://arxiv.org/abs/1510.05561v4"}, {"title": "Socio-Economic Impacts of COVID-19 on Household Consumption and Poverty", "summary": "The COVID-19 pandemic has caused a massive economic shock across the world\ndue to business interruptions and shutdowns from social-distancing measures. To\nevaluate the socio-economic impact of COVID-19 on individuals, a micro-economic\nmodel is developed to estimate the direct impact of distancing on household\nincome, savings, consumption, and poverty. The model assumes two periods: a\ncrisis period during which some individuals experience a drop in income and can\nuse their precautionary savings to maintain consumption; and a recovery period,\nwhen households save to replenish their depleted savings to pre-crisis level.\nThe San Francisco Bay Area is used as a case study, and the impacts of a\nlockdown are quantified, accounting for the effects of unemployment insurance\n(UI) and the CARES Act federal stimulus. Assuming a shelter-in-place period of\nthree months, the poverty rate would temporarily increase from 17.1% to 25.9%\nin the Bay Area in the absence of social protection, and the lowest income\nearners would suffer the most in relative terms. If fully implemented, the\ncombination of UI and CARES could keep the increase in poverty close to zero,\nand reduce the average recovery time, for individuals who suffer an income\nloss, from 11.8 to 6.7 months. However, the severity of the economic impact is\nspatially heterogeneous, and certain communities are more affected than the\naverage and could take more than a year to recover. Overall, this model is a\nfirst step in quantifying the household-level impacts of COVID-19 at a regional\nscale. This study can be extended to explore the impact of indirect\nmacroeconomic effects, the role of uncertainty in households' decision-making\nand the potential effect of simultaneous exogenous shocks (e.g., natural\ndisasters).", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2005.05945v1", "link": "http://arxiv.org/abs/2005.05945v1"}, {"title": "Bilateral Credit Valuation Adjustment of an Optional Early Termination\n  Clause", "summary": "Is an option to early terminate a swap at its market value worth zero? At\nfirst sight it is, but in presence of counterparty risk it depends on the\ncriteria used to determine such market value. In case of a single\nuncollateralised swap transaction under ISDA between two defaultable\ncounterparties, the additional unilateral option to early terminate the swap at\npredefined dates requires a Bermudan credit valuation adjustment. We give a\ngeneral pricing formula assuming a default-free close-out amount, and apply it\nin a simplified setting with deterministic intensity and one single date of\noptional early termination, showing that the impact on the fair value of the\ntransaction at inception might be non negligible.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1205.2013v3", "link": "http://arxiv.org/abs/1205.2013v3"}, {"title": "Polarization Versus Agglomeration", "summary": "The aim of this paper is to analyze the processes of polarization and\nagglomeration, to explain the mechanisms and causes of these phenomena in order\nto identify similarities and differences. As the main implication of this study\nshould be noted that both process pretend to explain the concentration of\neconomic activity and population in certain places, through cumulative\nphenomena, but with different perspectives, in other words, the polarization\nwith a view of economic development and agglomeration with a perspective of\nspace.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1110.5557v1", "link": "http://arxiv.org/abs/1110.5557v1"}, {"title": "Some Results on Skorokhod Embedding and Robust Hedging with Local Time", "summary": "In this paper, we provide some results on Skorokhod embedding with local time\nand its applications to the robust hedging problem in finance. First we\ninvestigate the robust hedging of options depending on the local time by using\nthe recently introduced stochastic control approach, in order to identify the\noptimal hedging strategies, as well as the market models that realize the\nextremal no-arbitrage prices. As a by-product, the optimality of Vallois'\nSkorokhod embeddings is recovered. In addition, under appropriate conditions,\nwe derive a new solution to the two-marginal Skorokhod embedding as a\ngeneralization of the Vallois solution. It turns out from our analysis that one\nneeds to relax the monotonicity assumption on the embedding functions in order\nto embed a larger class of marginal distributions. Finally, in a full-marginal\nsetting where the stopping times given by Vallois are well-ordered, we\nconstruct a remarkable Markov martingale which provides a new example of fake\nBrownian motion.", "category": ["math.PR", "q-fin.MF"], "id": "http://arxiv.org/abs/1511.07230v3", "link": "http://arxiv.org/abs/1511.07230v3"}, {"title": "Legal Architecture and Design for Gulf Cooperation Council Economic\n  Integration", "summary": "The Cooperation Council for the Arab States of the Gulf (GCC) is generally\nregarded as a success story for economic integration in Arab countries. The\nidea of regional integration gained ground by signing the GCC Charter. It\nenvisioned a closer economic relationship between member states.Although\neconomic integration among GCC member states is an ambitious step in the right\ndirection, there are gaps and challenges ahead. The best way to address the\ngaps and challenges that exist in formulating integration processes in the GCC\nis to start with a clear set of rules and put the necessary mechanisms in\nplace. Integration attempts must also exhibit a high level of commitment in\norder to deflect dynamics of disintegration that have all too often frustrated\nmeaningful integration in Arab countries. If the GCC can address these issues,\nit could become an economic powerhouse within Arab countries and even Asia.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1909.08798v1", "link": "http://arxiv.org/abs/1909.08798v1"}, {"title": "Stochastic Frontier I & D of fractal dimensions for technological\n  innovation", "summary": "This paper presents an analysis of the study variables such as gdp,\nemployment levels, the level of R & D and technology that will serve as the\nbasis for stochastic modeling of production possibilities frontier in the\ngoodness of fractal dimensions Ex Ante and Ex Post a priori to determine the\nlevels of causality immediately and check its accuracy and power of indexing,\nusing high frequency data and thus address the response this assumption of\nstochastic frontiers with level N of partitions in time.", "category": ["q-fin.EC", "q-fin.ST"], "id": "http://arxiv.org/abs/1509.01212v1", "link": "http://dx.doi.org/10.1016/S0185-0849(13)71335-5"}, {"title": "Generalized asset pricing: Expected Downside Risk-Based Equilibrium\n  Modelling", "summary": "We introduce an equilibrium asset pricing model, which we build on the\nrelationship between a novel risk measure, the Expected Downside Risk (EDR) and\nthe expected return. On the one hand, our proposed risk measure uses a\nnonparametric approach that allows us to get rid of any assumption on the\ndistribution of returns. On the other hand, our asset pricing model is based on\nloss-averse investors of Prospect Theory, through which we implement the\nrisk-seeking behaviour of investors in a dynamic setting. By including EDR in\nour proposed model unrealistic assumptions of commonly used equilibrium models\n- such as the exclusion of risk-seeking or price-maker investors and the\nassumption of unlimited leverage opportunity for a unique interest rate - can\nbe omitted. Therefore, we argue that based on more realistic assumptions our\nmodel is able to describe equilibrium expected returns with higher accuracy,\nwhich we support by empirical evidence as well.", "category": ["q-fin.PR", "q-fin.GN"], "id": "http://arxiv.org/abs/1512.01806v1", "link": "http://dx.doi.org/10.1016/j.econmod.2015.10.036"}, {"title": "Machine Learning Portfolio Allocation", "summary": "We find economically and statistically significant gains from using machine\nlearning to dynamically allocate between the market index and the risk-free\nasset. We model the market price of risk as a function of lagged dividend\nyields and volatilities to determine the optimal weights in the portfolio:\nreward-risk market timing. This involves forecasting the direction of next\nmonth's excess return, which gives the reward, and constructing a dynamic\nvolatility estimator that is optimized with a machine learning model, which\ngives the risk. Reward-risk timing with machine learning provides substantial\nimprovements over the market index in investor utility, alphas, Sharpe ratios,\nand maximum drawdowns, after accounting for transaction costs, leverage\nconstraints, and on a new out-of-sample set of returns. This paper provides a\nunifying framework for machine learning applied to both return- and\nvolatility-timing.for machine learning applied to both return- and\nvolatility-timing.", "category": ["q-fin.PM", "q-fin.GN", "q-fin.PR", "q-fin.RM", "q-fin.ST"], "id": "http://arxiv.org/abs/2003.00656v3", "link": "http://arxiv.org/abs/2003.00656v3"}, {"title": "LM-BIC Model Selection in Semiparametric Models", "summary": "This paper studies model selection in semiparametric econometric models. It\ndevelops a consistent series-based model selection procedure based on a\nBayesian Information Criterion (BIC) type criterion to select between several\nclasses of models. The procedure selects a model by minimizing the\nsemiparametric Lagrange Multiplier (LM) type test statistic from Korolev (2018)\nbut additionally rewards simpler models. The paper also develops consistent\nupward testing (UT) and downward testing (DT) procedures based on the\nsemiparametric LM type specification test. The proposed semiparametric LM-BIC\nand UT procedures demonstrate good performance in simulations. To illustrate\nthe use of these semiparametric model selection procedures, I apply them to the\nparametric and semiparametric gasoline demand specifications from Yatchew and\nNo (2001). The LM-BIC procedure selects the semiparametric specification that\nis nonparametric in age but parametric in all other variables, which is in line\nwith the conclusions in Yatchew and No (2001). The results of the UT and DT\nprocedures heavily depend on the choice of tuning parameters and assumptions\nabout the model errors.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1811.10676v1", "link": "http://arxiv.org/abs/1811.10676v1"}, {"title": "Optimal Iterative Threshold-Kernel Estimation of Jump Diffusion\n  Processes", "summary": "In this paper, we propose a new threshold-kernel jump-detection method for\njump-diffusion processes, which iteratively applies thresholding and kernel\nmethods in an approximately optimal way to achieve improved finite-sample\nperformance. We use the expected number of jump misclassifications as the\nobjective function to optimally select the threshold parameter of the jump\ndetection scheme. We prove that the objective function is quasi-convex and\nobtain a new second-order infill approximation of the optimal threshold in\nclosed form. The approximate optimal threshold depends not only on the spot\nvolatility, but also the jump intensity and the value of the jump density at\nthe origin. Estimation methods for these quantities are then developed, where\nthe spot volatility is estimated by a kernel estimator with thresholding and\nthe value of the jump density at the origin is estimated by a density kernel\nestimator applied to those increments deemed to contain jumps by the chosen\nthresholding criterion. Due to the interdependency between the model parameters\nand the approximate optimal estimators built to estimate them, a type of\niterative fixed-point algorithm is developed to implement them. Simulation\nstudies for a prototypical stochastic volatility model show that it is not only\nfeasible to implement the higher-order local optimal threshold scheme but also\nthat this is superior to those based only on the first order approximation\nand/or on average values of the parameters over the estimation time period.", "category": ["econ.EM", "q-fin.ST"], "id": "http://arxiv.org/abs/1811.07499v4", "link": "http://dx.doi.org/10.1007/s11203-020-09211-7."}, {"title": "Statistical inference of co-movements of stocks during a financial\n  crisis", "summary": "In order to figure out and to forecast the emergence phenomena of social\nsystems, we propose several probabilistic models for the analysis of financial\nmarkets, especially around a crisis. We first attempt to visualize the\ncollective behaviour of markets during a financial crisis through\ncross-correlations between typical Japanese daily stocks by making use of\nmulti- dimensional scaling. We find that all the two-dimensional points\n(stocks) shrink into a single small region when a economic crisis takes place.\nBy using the properties of cross-correlations in financial markets especially\nduring a crisis, we next propose a theoretical framework to predict several\ntime-series simultaneously. Our model system is basically described by a\nvariant of the multi-layered Ising model with random fields as non-stationary\ntime series. Hyper-parameters appearing in the probabilistic model are\nestimated by means of minimizing the 'cumulative error' in the past market\nhistory. The justification and validity of our approaches are numerically\nexamined for several empirical data sets.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/1309.1871v1", "link": "http://dx.doi.org/10.1088/1742-6596/473/1/012008"}, {"title": "Expected Shortfall and Beyond", "summary": "Financial institutions have to allocate so-called \"economic capital\" in order\nto guarantee solvency to their clients and counter parties. Mathematically\nspeaking, any methodology of allocating capital is a \"risk measure\", i.e. a\nfunction mapping random variables to the real numbers. Nowadays\n\"value-at-risk\", which is defined as a fixed level quantile of the random\nvariable under consideration, is the most popular risk measure. Unfortunately,\nit fails to reward diversification, as it is not \"subadditive\". In the search\nfor a suitable alternative to value-at-risk, \"Expected Shortfall\" (or\n\"conditional value-at-risk\" or \"tail value-at-risk\") has been characterized as\nthe smallest \"coherent\" and \"law invariant\" risk measure to dominate\nvalue-at-risk. We discuss these and some other properties of Expected Shortfall\nas well as its generalization to a class of coherent risk measures which can\nincorporate higher moment effects. Moreover, we suggest a general method on how\nto attribute Expected Shortfall \"risk contributions\" to portfolio components.\n  Key words: Expected Shortfall; Value-at-Risk; Spectral Risk Measure;\ncoherence; risk contribution.", "category": ["q-fin.RM"], "id": "http://arxiv.org/abs/cond-mat/0203558v3", "link": "http://arxiv.org/abs/cond-mat/0203558v3"}, {"title": "Utility Indifference Pricing: A Time Consistent Approach", "summary": "This paper considers the optimal portfolio selection problem in a dynamic\nmulti-period stochastic framework with regime switching. The risk preferences\nare of exponential (CARA) type with an absolute coefficient of risk aversion\nwhich changes with the regime. The market model is incomplete and there are two\nrisky assets: one tradable and one non-tradable. In this context, the optimal\ninvestment strategies are time inconsistent. Consequently, the subgame perfect\nequilibrium strategies are considered. The utility indifference prices of a\ncontingent claim written on the risky assets are computed via an indifference\nvaluation algorithm. By running numerical experiments, we examine how these\nprices vary in response to changes in model parameters.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1102.5075v1", "link": "http://arxiv.org/abs/1102.5075v1"}, {"title": "Geert Hofstede et al's set of national cultural dimensions - popularity\n  and criticisms", "summary": "This article outlines different stages in development of the national culture\nmodel, created by Geert Hofstede and his affiliates. This paper reveals and\nsynthesizes the contemporary review of the application spheres of this\nframework. Numerous applications of the dimensions set are used as a source of\nidentifying significant critiques, concerning different aspects in model's\noperation. These critiques are classified and their underlying reasons are also\noutlined by means of a fishbone diagram.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1810.02621v1", "link": "http://dx.doi.org/10.5281/zenodo.1434882"}, {"title": "Housing Market Microstructure", "summary": "In this article, we develop a model for the evolution of real estate prices.\nA wide range of inputs, including stochastic interest rates and changing\ndemands for the asset, are considered. Maximizing their expected utility, home\nowners make optimal sale decisions given these changing market conditions.\nUsing these optimal sale decisions, we simulate the implied evolution of\nhousing prices providing insights into the recent subprime lending crisis.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/0907.1853v1", "link": "http://arxiv.org/abs/0907.1853v1"}, {"title": "State Drug Policy Effectiveness: Comparative Policy Analysis of Drug\n  Overdose Mortality", "summary": "Opioid overdose rates have reached an epidemic level and state-level policy\ninnovations have followed suit in an effort to prevent overdose deaths.\nState-level drug law is a set of policies that may reinforce or undermine each\nother, and analysts have a limited set of tools for handling the policy\ncollinearity using statistical methods. This paper uses a machine learning\nmethod called hierarchical clustering to empirically generate \"policy bundles\"\nby grouping states with similar sets of policies in force at a given time\ntogether for analysis in a 50-state, 10-year interrupted time series regression\nwith drug overdose deaths as the dependent variable. Policy clusters were\ngenerated from 138 binomial variables observed by state and year from the\nPrescription Drug Abuse Policy System. Clustering reduced the policies to a set\nof 10 bundles. The approach allows for ranking of the relative effect of\ndifferent bundles and is a tool to recommend those most likely to succeed. This\nstudy shows that a set of policies balancing Medication Assisted Treatment,\nNaloxone Access, Good Samaritan Laws, Medication Assisted Treatment,\nPrescription Drug Monitoring Programs and legalization of medical marijuana\nleads to a reduced number of overdose deaths, but not until its second year in\nforce.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.01936v2", "link": "http://arxiv.org/abs/1909.01936v2"}, {"title": "Inner Market as a \"Black Box\"", "summary": "Each market has its singular characteristic. Its inner structure is directly\nresponsible for the observed distributions of returns though this fact is\nwidely overlooked. Big orders lead to doubling the tails. The behavior of a\nmarket maker with many or few ``friends'' who can reliably loan money or stock\nto him is quite different from the one without. After representing the inner\nmarket ``case'' we suggest how to analyze its structure.", "category": ["q-fin.TR"], "id": "http://arxiv.org/abs/cond-mat/0106401v1", "link": "http://arxiv.org/abs/cond-mat/0106401v1"}, {"title": "The Impact of Age on Nationality Bias: Evidence from Ski Jumping", "summary": "This empirical research explores the impact of age on nationality bias. World\nCup competition data suggest that judges of professional ski jumping\ncompetitions prefer jumpers of their own nationality and exhibit this\npreference by rewarding them with better marks. Furthermore, the current study\nreveals that this nationality bias is diminished among younger judges, in\naccordance with the reported lower levels of national discrimination among\nyounger generations. Globalisation and its effect in reducing class-based\nthinking may explain this reduced bias in judgment of others.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/1808.03804v1", "link": "http://arxiv.org/abs/1808.03804v1"}, {"title": "Accrual valuation and mark to market adjustment", "summary": "This paper provides intuition on the relationship of accrual and\nmark-to-market valuation for cash and forward interest rate trades. Discounted\ncashflow valuation is compared to spread-based valuation for forward trades,\nwhich explains the trader's view on valuation. This is followed by Taylor\nseries approximation for cash trades, uncovering simple intuition behind\naccrual valuation and mark-to-market adjustment. It is followed by the PNL\nexample modelled in R. Within the Taylor approximation framework, theta and\ndelta are explained. The concept of deferral is explained taking Forward Rate\nAgreement (FRA) as an example.", "category": ["q-fin.PR"], "id": "http://arxiv.org/abs/1602.06189v1", "link": "http://arxiv.org/abs/1602.06189v1"}, {"title": "Optimal excess-of-loss reinsurance and investment problem for an insurer\n  with default risk under a stochastic volatility model", "summary": "In this paper, we study an optimal excess-of-loss reinsurance and investment\nproblem for an insurer in defaultable market. The insurer can buy reinsurance\nand invest in the following securities: a bank account, a risky asset with\nstochastic volatility and a defaultable corporate bond. We discuss the optimal\ninvestment strategy into two subproblems: a pre-default case and a post-default\ncase. We show the existence of a classical solution to a pre-default case via\nsuper-sub solution techniques and give an explicit characterization of the\noptimal reinsurance and investment policies that maximize the expected CARA\nutility of the terminal wealth. We prove a verification theorem establishing\nthe uniqueness of the solution. Numerical results are presented in the case of\nthe Scott model and we discuss economic insights obtained from these results.", "category": ["q-fin.PM"], "id": "http://arxiv.org/abs/1704.08234v1", "link": "http://arxiv.org/abs/1704.08234v1"}, {"title": "Option pricing in the model with stochastic volatility driven by\n  Ornstein--Uhlenbeck process. Simulation", "summary": "We consider a discrete-time approximation of paths of an Ornstein--Uhlenbeck\nprocess as a mean for estimation of a price of European call option in the\nmodel of financial market with stochastic volatility. The Euler--Maruyama\napproximation scheme is implemented. We determine the estimates for the option\nprice for predetermined sets of parameters. The rate of convergence of the\nprice and an average volatility when discretization intervals tighten are\ndetermined. Discretization precision is analyzed for the case where the exact\nvalue of the price can be derived.", "category": ["q-fin.CP", "math.PR", "q-fin.PR"], "id": "http://arxiv.org/abs/1601.01128v1", "link": "http://dx.doi.org/10.15559/15-VMSTA43"}, {"title": "Efficient hedging in Bates model using high-order compact finite\n  differences", "summary": "We evaluate the hedging performance of a high-order compact finite difference\nscheme from [4] for option pricing in Bates model. We compare the scheme's\nhedging performance to standard finite difference methods in different\nexamples. We observe that the new scheme outperforms a standard, second-order\ncentral finite difference approximation in all our experiments.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1710.05542v1", "link": "http://arxiv.org/abs/1710.05542v1"}, {"title": "Pricing options on illiquid assets with liquid proxies using utility\n  indifference and dynamic-static hedging", "summary": "This work addresses the problem of optimal pricing and hedging of a European\noption on an illiquid asset Z using two proxies: a liquid asset S and a liquid\nEuropean option on another liquid asset Y. We assume that the S-hedge is\ndynamic while the Y-hedge is static. Using the indifference pricing approach we\nderive a HJB equation for the value function, and solve it analytically (in\nquadratures) using an asymptotic expansion around the limit of the perfect\ncorrelation between assets Y and Z. While in this paper we apply our framework\nto an incomplete market version of the credit-equity Merton's model, the same\napproach can be used for other asset classes (equity, commodity, FX, etc.),\ne.g. for pricing and hedging options with illiquid strikes or illiquid exotic\noptions.", "category": ["q-fin.PR", "q-fin.CP", "q-fin.GN"], "id": "http://arxiv.org/abs/1205.3507v1", "link": "http://arxiv.org/abs/1205.3507v1"}, {"title": "Pricing TARN Using a Finite Difference Method", "summary": "Typically options with a path dependent payoff, such as Target Accumulation\nRedemption Note (TARN), are evaluated by a Monte Carlo method. This paper\ndescribes a finite difference scheme for pricing a TARN option. Key steps in\nthe proposed scheme involve tracking of multiple one-dimensional finite\ndifference solutions, application of jump conditions at each cash flow exchange\ndate, and a cubic spline interpolation of results after each jump. Since a\nfinite difference scheme for TARN has significantly different features from a\ntypical finite difference scheme for options with a path independent payoff, we\ngive a step by step description on the implementation of the scheme, which is\nnot available in the literature. The advantages of the proposed finite\ndifference scheme over the Monte Carlo method are illustrated by examples with\nthree different knockout types. In the case of constant or time dependent\nvolatility models (where Monte Carlo requires simulation at cash flow dates\nonly), the finite difference method can be faster by an order of magnitude than\nthe Monte Carlo method to achieve the same accuracy in price. Finite difference\nmethod can be even more efficient in comparison with Monte Carlo in the case of\nlocal volatility model where Monte Carlo requires significantly larger number\nof time steps. In terms of robust and accurate estimation of Greeks, the\nadvantage of the finite difference method will be even more pronounced.", "category": ["q-fin.CP"], "id": "http://arxiv.org/abs/1304.7563v2", "link": "http://arxiv.org/abs/1304.7563v2"}, {"title": "Modeling inequality and spread in multiple regression", "summary": "We consider concepts and models for measuring inequality in the distribution\nof resources with a focus on how inequality varies as a function of covariates.\nLorenz introduced a device for measuring inequality in the distribution of\nincome that indicates how much the incomes below the u$^{th}$ quantile fall\nshort of the egalitarian situation where everyone has the same income. Gini\nintroduced a summary measure of inequality that is the average over u of the\ndifference between the Lorenz curve and its values in the egalitarian case.\nMore generally, measures of inequality are useful for other response variables\nin addition to income, e.g. wealth, sales, dividends, taxes, market share and\ntest scores. In this paper we show that a generalized van Zwet type dispersion\nordering for distributions of positive random variables induces an ordering on\nthe Lorenz curve, the Gini coefficient and other measures of inequality. We use\nthis result and distributional orderings based on transformations of\ndistributions to motivate parametric and semiparametric models whose regression\ncoefficients measure effects of covariates on inequality. In particular, we\nextend a parametric Pareto regression model to a flexible semiparametric\nregression model and give partial likelihood estimates of the regression\ncoefficients and a baseline distribution that can be used to construct\nestimates of the various conditional measures of inequality.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/math/0610852v1", "link": "http://dx.doi.org/10.1214/074921706000000428"}, {"title": "Rational Groupthink", "summary": "We study how long-lived rational agents learn from repeatedly observing a\nprivate signal and each others' actions. With normal signals, a group of any\nsize learns more slowly than just four agents who directly observe each others'\nprivate signals in each period. Similar results apply to general signal\nstructures. We identify rational groupthink---in which agents ignore their\nprivate signals and choose the same action for long periods of time---as the\ncause of this failure of information aggregation.", "category": ["math.PR"], "id": "http://arxiv.org/abs/1412.7172v6", "link": "http://arxiv.org/abs/1412.7172v6"}, {"title": "Arctic Amplification of Anthropogenic Forcing: A Vector Autoregressive\n  Analysis", "summary": "Arctic sea ice extent (SIE) in September 2019 ranked second-to-lowest in\nhistory and is trending downward. The understanding of how internal variability\namplifies the effects of external $\\text{CO}_2$ forcing is still limited. We\npropose the VARCTIC, which is a Vector Autoregression (VAR) designed to capture\nand extrapolate Arctic feedback loops. VARs are dynamic simultaneous systems of\nequations, routinely estimated to predict and understand the interactions of\nmultiple macroeconomic time series. Hence, the VARCTIC is a parsimonious\ncompromise between fullblown climate models and purely statistical approaches\nthat usually offer little explanation of the underlying mechanism. Our\n\"business as usual\" completely unconditional forecast has SIE hitting 0 in\nSeptember by the 2060s. Impulse response functions reveal that anthropogenic\n$\\text{CO}_2$ emission shocks have a permanent effect on SIE - a property\nshared by no other shock. Further, we find Albedo- and Thickness-based\nfeedbacks to be the main amplification channels through which $\\text{CO}_2$\nanomalies impact SIE in the short/medium run. Conditional forecast analyses\nreveal that the future path of SIE crucially depends on the evolution of\n$\\text{CO}_2$ emissions, with outcomes ranging from recovering SIE to it\nreaching 0 in the 2050s. Finally, Albedo and Thickness feedbacks are shown to\nplay an important role in accelerating the speed at which predicted SIE is\nheading towards 0.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/2005.02535v1", "link": "http://arxiv.org/abs/2005.02535v1"}, {"title": "Boom and bust in continuous time evolving economic model", "summary": "We show that a simple model of a spatially resolved evolving economic system,\nwhich has a steady state under simultaneous updating, shows stable oscillations\nin price when updated asynchronously. The oscillations arise from a gradual\ndecline of the mean price due to competition among sellers competing for the\nsame resource. This lowers profitability and hence population but is followed\nby a sharp rise as speculative sellers invade the large un-inhabited areas.\nThis cycle then begins again.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/0801.3973v2", "link": "http://dx.doi.org/10.1140/epjb/e2009-00243-y"}, {"title": "Comparing School Choice and College Admission Mechanisms By Their\n  Immunity to Strategic Admissions", "summary": "Recently dozens of school districts and college admissions systems around the\nworld have reformed their admission rules. As a main motivation for these\nreforms the policymakers cited strategic flaws of the rules: students had\nstrong incentives to game the system, which caused dramatic consequences for\nnon-strategic students. However, almost none of the new rules were\nstrategy-proof. We explain this puzzle. We show that after the reforms the\nrules became more immune to strategic admissions: each student received a\nsmaller set of schools that he can get in using a strategy, weakening\nincentives to manipulate. Simultaneously, the admission to each school became\nstrategy-proof to a larger set of students, making the schools more available\nfor non-strategic students. We also show that the existing explanation of the\npuzzle due to Pathak and S\\\"onmez (2013) is incomplete.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.06166v2", "link": "http://arxiv.org/abs/2001.06166v2"}, {"title": "A Consistent LM Type Specification Test for Semiparametric Panel Data\n  Models", "summary": "This paper develops a consistent series-based specification test for\nsemiparametric panel data models with fixed effects. The test statistic\nresembles the Lagrange Multiplier (LM) test statistic in parametric models and\nis based on a quadratic form in the restricted model residuals. The use of\nseries methods facilitates both estimation of the null model and computation of\nthe test statistic. The asymptotic distribution of the test statistic is\nstandard normal, so that appropriate critical values can easily be computed.\nThe projection property of series estimators allows me to develop a degrees of\nfreedom correction. This correction makes it possible to account for the\nestimation variance and obtain refined asymptotic results. It also\nsubstantially improves the finite sample performance of the test.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1909.05649v1", "link": "http://arxiv.org/abs/1909.05649v1"}, {"title": "Dynamical system theory of periodically collapsing bubbles", "summary": "We propose a reduced form set of two coupled continuous time equations\nlinking the price of a representative asset and the price of a bond, the later\nquantifying the cost of borrowing. The feedbacks between asset prices and bonds\nare mediated by the dependence of their \"fundamental values\" on past asset\nprices and bond themselves. The obtained nonlinear self-referencing price\ndynamics can induce, in a completely objective deterministic way, the\nappearance of periodically exploding bubbles ending in crashes. Technically,\nthe periodically explosive bubbles arise due to the proximity of two types of\nbifurcations as a function of the two key control parameters $b$ and $g$, which\nrepresent, respectively, the sensitivity of the fundamental asset price on past\nasset and bond prices and of the fundamental bond price on past asset prices.\nOne is a Hopf bifurcation, when a stable focus transforms into an unstable\nfocus and a limit cycle appears. The other is a rather unusual bifurcation,\nwhen a stable node and a saddle merge together and disappear, while an unstable\nfocus survives and a limit cycle develops. The lines, where the periodic\nbubbles arise, are analogous to the critical lines of phase transitions in\nstatistical physics. The amplitude of bubbles and waiting times between them\nrespectively diverge with the critical exponents $\\gamma = 1$ and $\\nu = 1/2$,\nas the critical lines are approached.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1507.05311v1", "link": "http://arxiv.org/abs/1507.05311v1"}, {"title": "Implementing Flexible Demand: Real-time Price vs. Market Integration", "summary": "This paper proposes an agent-based model that combines both spot and\nbalancing electricity markets. From this model, we develop a multi-agent\nsimulation to study the integration of the consumers' flexibility into the\nsystem. Our study identifies the conditions that real-time prices may lead to\nhigher electricity costs, which in turn contradicts the usual claim that such a\npricing scheme reduces cost. We show that such undesirable behavior is in fact\nsystemic. Due to the existing structure of the wholesale market, the predicted\ndemand that is used in the formation of the price is never realized since the\nflexible users will change their demand according to such established price. As\nthe demand is never correctly predicted, the volume traded through the\nbalancing markets increases, leading to higher overall costs. In this case, the\nsystem can sustain, and even benefit from, a small number of flexible users,\nbut this solution can never upscale without increasing the total costs. To\navoid this problem, we implement the so-called \"exclusive groups.\" Our results\nillustrate the importance of rethinking the current practices so that\nflexibility can be successfully integrated considering scenarios with and\nwithout intermittent renewable sources.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1709.02667v2", "link": "http://arxiv.org/abs/1709.02667v2"}, {"title": "Stock portfolio structure of individual investors infers future trading\n  behavior", "summary": "Although the understanding of and motivation behind individual trading\nbehavior is an important puzzle in finance, little is known about the\nconnection between an investor's portfolio structure and her trading behavior\nin practice. In this paper, we investigate the relation between what stocks\ninvestors hold, and what stocks they buy, and show that investors with similar\nportfolio structures to a great extent trade in a similar way. With data from\nthe central register of shareholdings in Sweden, we model the market in a\nsimilarity network, by considering investors as nodes, connected with links\nrepresenting portfolio similarity. From the network, we find groups of\ninvestors that not only identify different investment strategies, but also\nrepresent groups of individual investors trading in a similar way. These\nfindings suggest that the stock portfolios of investors hold meaningful\ninformation, which could be used to earn a better understanding of stock market\ndynamics.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1402.2494v2", "link": "http://dx.doi.org/10.1371/journal.pone.0103006"}, {"title": "L'utilisation des splines bidimensionnels pour l'estimation de lois de\n  maintien en arr\u00eat de travail", "summary": "The aim of this paper is to propose an operational two-dimensional parametric\nadjustment for laws of maintenance in disability. The method suggested rests on\nsplines in dimension 2; it is applied to a real data set, and the scale of\nreserving which results from it is compared with the scale of reference of the\nBCAC.", "category": ["q-fin.GN", "q-fin.CP"], "id": "http://arxiv.org/abs/1001.1907v1", "link": "http://arxiv.org/abs/1001.1907v1"}, {"title": "Correlation structure of extreme stock returns", "summary": "It is commonly believed that the correlations between stock returns increase\nin high volatility periods. We investigate how much of these correlations can\nbe explained within a simple non-Gaussian one-factor description with time\nindependent correlations. Using surrogate data with the true market return as\nthe dominant factor, we show that most of these correlations, measured by a\nvariety of different indicators, can be accounted for. In particular, this\none-factor model can explain the level and asymmetry of empirical exceedance\ncorrelations. However, more subtle effects require an extension of the one\nfactor model, where the variance and skewness of the residuals also depend on\nthe market return.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/cond-mat/0006034v2", "link": "http://arxiv.org/abs/cond-mat/0006034v2"}, {"title": "Arbitrage-Free Pricing of XVA -- Part I: Framework and Explicit Examples", "summary": "We develop a novel framework for computing the total valuation adjustment\n(XVA) of a European claim accounting for funding costs, counterparty credit\nrisk, and collateralization. Based on no-arbitrage arguments, we derive the\nnonlinear backward stochastic differential equations (BSDEs) associated with\nthe replicating portfolios of long and short positions in the claim. This leads\nto the definition of buyer's and seller's XVA which in turn identify a\nno-arbitrage interval. When borrowing and lending rates coincide we provide a\nfully explicit expression for the uniquely determined price of XVA, expressed\nas a percentage of the price of the traded claim, and for the corresponding\nreplication strategies. This extends the result of Piterbarg by incorporating\nthe effect of premature contract termination due to default risk of the trader\nand of his counterparty.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1501.05893v3", "link": "http://arxiv.org/abs/1501.05893v3"}, {"title": "Special Drawing Rights in a New Decentralized Century", "summary": "Unfulfilled expectations from macro-economic initiatives during the Great\nRecession and the massive shift into globalization echo today with political\nupheaval, anti-establishment propaganda, and looming trade/currency wars that\nthreaten domestic and international value chains. Once stable entities like the\nEU now look fragile and political instability in the US presents unprecedented\nchallenges to an International Monetary System (IMS) that predominantly relies\non the USD and EUR as reserve currencies. In this environment, it is critical\nfor an international organization mandated to ensure stability to plan and act\nahead. This paper argues that Decentralized Ledger-based technology (DLT) is\nkey for the International Monetary Fund (IMF) to mitigate some of those risks,\npromote stability and safeguard world prosperity. Over the last two years, DLT\nhas made headline news globally and created a worldwide excitement not seen\nsince the internet entered the mainstream. The rapid adoption and open-to-all\nphilosophy of DLT has already redefined global socioeconomics, promises to\nshake up the world of commerce/finance and challenges the workings of central\ngovernments/regulators. This paper examines DLT core premises and proposes a\ntwo-step approach for the IMF to expand Special Drawing Rights (SDR) into that\nsphere so as to become the originally envisioned numeraire and reserve currency\nfor cross-border transactions in this new decentralized century.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/1907.11057v1", "link": "http://arxiv.org/abs/1907.11057v1"}, {"title": "Market Efficiency in Foreign Exchange Markets", "summary": "We investigate the relative market efficiency in financial market data, using\nthe approximate entropy(ApEn) method for a quantification of randomness in time\nseries. We used the global foreign exchange market indices for 17 countries\nduring two periods from 1984 to 1998 and from 1999 to 2004 in order to study\nthe efficiency of various foreign exchange markets around the market crisis. We\nfound that on average, the ApEn values for European and North American foreign\nexchange markets are larger than those for African and Asian ones except Japan.\nWe also found that the ApEn for Asian markets increase significantly after the\nAsian currency crisis. Our results suggest that the markets with a larger\nliquidity such as European and North American foreign exchange markets have a\nhigher market efficiency than those with a smaller liquidity such as the\nAfrican and Asian ones except Japan.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/physics/0608016v2", "link": "http://dx.doi.org/10.1016/j.physa.2007.02.032"}, {"title": "Inf-convolution of G-expectations", "summary": "In this paper we will discuss the optimal risk transfer problems when risk\nmeasures are generated by G-expectations, and we present the relationship\nbetween inf-convolution of G-expectations and the inf-convolution of drivers G.", "category": ["q-fin.RM", "math.PR"], "id": "http://arxiv.org/abs/0910.5398v1", "link": "http://dx.doi.org/10.1007/s11425-010-4031-6"}, {"title": "Relationship between Type of Risks and Income of the Rural Households in\n  the Pattani Province of Thailand", "summary": "This study examines the relationship between type of risks and income of the\nrural households in Pattani province,Thailand using the standard multiple\nregression analysis.A multi-stage sampling technique is employed to select 600\nhouseholds of 12 districts in the rural Pattani province and a structured\nquestionnaire is used for data collection.Evidences from descriptive analysis\nshow that the type of risks faced by households in rural Pattani province are\njob loss,reduction of salary,household member died,household members who work\nhave accident,marital problem and infection of crops or livestock.In\naddition,result from the regression analysis suggests that job loss,household\nmember died and marital problem have significant negative effects on the\nhouseholds income.The result suggests that job loss has adverse impact on\nhouseholds income.", "category": ["econ.GN", "q-fin.EC"], "id": "http://arxiv.org/abs/2001.03046v1", "link": "http://dx.doi.org/10.5539/ass.v10n17p204"}, {"title": "A theory for combinations of risk measures", "summary": "We study combinations of risk measures under no restrictive assumption on the\nset of alternatives. The main result is the representation for resulting risk\nmeasures from the properties of both alternative functionals and combination\nfunctions. To that, we develop a representation for arbitrary mixture of convex\nrisk measures. In this case, we obtain a penalty that recall the notion of\ninf-convolution under theoretical measure integration. As an application, we\naddress the context of probability-based risk measurements for functionals on\nthe set of distribution functions. We develop results related to this specific\ncontext. We also explore features of individual interest generated by our\nframework, such as the preservation of continuity properties, the\nrepresentation of worst-case risk measures, stochastic dominance and\nelicitability.", "category": ["q-fin.MF", "q-fin.RM"], "id": "http://arxiv.org/abs/1807.01977v5", "link": "http://arxiv.org/abs/1807.01977v5"}, {"title": "Correlation between Risk Aversion and Wealth distribution", "summary": "Different models of capital exchange among economic agents have been proposed\nrecently trying to explain the emergence of Pareto's wealth power law\ndistribution. One important factor to be considered is the existence of risk\naversion. In this paper we study a model where agents posses different levels\nof risk aversion, going from uniform to a random distribution. In all cases the\nrisk aversion level for a given agent is constant during the simulation. While\nfor a uniform and constant risk aversion the system self-organizes in a\ndistribution that goes from an unfair ``one takes all'' distribution to a\nGaussian one, a random risk aversion can produce distributions going from\nexponential to log-normal and power-law. Besides, interesting correlations\nbetween wealth and risk aversion are found.", "category": ["q-fin.GN"], "id": "http://arxiv.org/abs/cond-mat/0311127v1", "link": "http://dx.doi.org/10.1016/j.physa.2004.04.077"}, {"title": "Regional economic convergence and spatial quantile regression", "summary": "The presence of \\b{eta}-convergence in European regions is an important issue\nto be analyzed. In this paper, we adopt a quantile regression approach in\nanalyzing economic convergence. While previous work has performed quantile\nregression at the national level, we focus on 187 European NUTS2 regions for\nthe period 1981-2009 and use spatial quantile regression to account for spatial\ndependence.", "category": ["econ.EM"], "id": "http://arxiv.org/abs/1906.04613v1", "link": "http://arxiv.org/abs/1906.04613v1"}, {"title": "Market Price of Risk and Random Field Driven Models of Term Structure: A\n  Space-Time Change of Measure Look", "summary": "No-arbitrage models of term structure have the feature that the return on\nzero-coupon bonds is the sum of the short rate and the product of volatility\nand market price of risk. Well known models restrict the behavior of the market\nprice of risk so that it is not dependent on the type of asset being modeled.\nWe show that the models recently proposed by Goldstein and Santa-Clara and\nSornette, among others, allow the market price of risk to depend on\ncharacteristics of each asset, and we quantify this dependence. A key tool in\nour analysis is a very general space-time change of measure theorem, proved by\nthe first author in earlier work, and covers continuous orthogonal local\nmartingale measures including space-time white noise.", "category": ["q-fin.PR", "math.PR"], "id": "http://arxiv.org/abs/1005.3799v1", "link": "http://arxiv.org/abs/1005.3799v1"}, {"title": "Empirical Study of the GARCH model with Rational Errors", "summary": "We use the GARCH model with a fat-tailed error distribution described by a\nrational function and apply it for the stock price data on the Tokyo Stock\nExchange. To determine the model parameters we perform the Bayesian inference\nto the model. The Bayesian inference is implemented by the Metropolis-Hastings\nalgorithm with an adaptive multi-dimensional Student's t-proposal density. In\norder to compare the model with the GARCH model with the standard normal errors\nwe calculate information criterions: AIC and DIC, and find that both criterions\nfavor the GARCH model with a rational error distribution. We also calculate the\naccuracy of the volatility by using the realized volatility and find that a\ngood accuracy is obtained for the GARCH model with a rational error\ndistribution. Thus we conclude that the GARCH model with a rational error\ndistribution is superior to the GARCH model with the normal errors and it can\nbe used as an alternative GARCH model to those with other fat-tailed\ndistributions.", "category": ["q-fin.CP", "q-fin.ST"], "id": "http://arxiv.org/abs/1312.7057v1", "link": "http://dx.doi.org/10.1088/1742-6596/454/1/012040"}, {"title": "Exponential moving average versus moving exponential average", "summary": "In this note we discuss the mathematical tools to define trend indicators\nwhich are used to describe market trends. We explain the relation between\naverages and moving averages on the one hand and the so called exponential\nmoving average (EMA) on the other hand. We present a lot of examples and give\nthe definition of the most frequently used trend indicator, the MACD, and\ndiscuss its properties.", "category": ["q-fin.ST"], "id": "http://arxiv.org/abs/2001.04237v1", "link": "http://dx.doi.org/10.1007/s00591-010-0080-8"}]