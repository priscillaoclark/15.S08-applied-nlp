{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Multiclass (30 %)\n",
    "### So far we have largely focused on binary classification, where the input is a document and the output is a yes or a no (or probability of yes). \n",
    "\n",
    "### In fact, more complex tasks exist where the input is a document and the output can be multiple (more than two) classes. \n",
    "\n",
    "## In this problem we'll investigate two so-called multiclass problems\n",
    "### Multiclass: an observation is assigned inclusion in ONE of a N $N>2$ categories\n",
    " - ### E.g. is this sentence positive, negative, or neutral sentiment\n",
    " - ### E.g. is this email spam or not spam\n",
    "\n",
    "\n",
    "### Multiclass-multilabel: an observation can belong to more than one of $N>=2$ categories\n",
    " - ### E.g. is this document about `{sports, current events, Steph Curry}` ( a document can be about more than one)\n",
    " - ### E.g. is this blood sample A, B, O, $+$, $-$ (blood can be `A+` or `A-`)\n",
    "\n",
    "## We will study the metrics we can use to evaluate these classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start with multiclass by studying the 20 newsgroups data\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "# Part 0: get the data\n",
    " - ### use the builtin function `from sklearn.datasets import fetch_20newsgroups`\n",
    " - ### NB: look at the docs and use the `remove` kwarg in order to get cleaned data\n",
    "\n",
    "## TODO\n",
    " - ## fetch the data separately for the train and test data\n",
    " - ## How many classes are present? \n",
    " - ## What is the most common class- please give the name and not the number.\n",
    " - ## What is the accuracy of the best constant guess in the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(...\n",
    "data_test = fetch_20newsgroups(...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = ...\n",
    "print('found {} classes'.format(val_counts.shape[0]))\n",
    "most_common_class = ...\n",
    "print('most common class: {}'.format(most_common_class))\n",
    "\n",
    "dummy_acc = accuracy_score(...\n",
    "print('constant guess acc: {:.3f}'.format(dummy_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: fit a model\n",
    "## As we saw with mnist, logistic regression is capable of fitting multi-class data.\n",
    " - ## Encode the text with as a bag of words and fit logistic regression to the data\n",
    " - ## Calcuate the out of sample accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "# 1. make a count vectorizer with max_features=20000\n",
    "# 2. fit it\n",
    "# 3. transform the train and test data into number\n",
    "vec = ..\n",
    "\n",
    "# your code here\n",
    "xtr = ... # train data\n",
    "xte = ... # test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. fit logistic regression\n",
    "# 2. compute accuracy score\n",
    "\n",
    "# your code here\n",
    "accuracy_score(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Evaluate metrics\n",
    "### As we have seen previously, while accuracy is useful, it does not always capture all the behavior we want in a metric.\n",
    "\n",
    "### Here we will extend the concept of f1 score to the multiclass setting. There are several ways to do this\n",
    " - report a different f1 score for every class (no averaging)\n",
    " - report the mean f1 score over all classes\n",
    " - report a weighted f1 score weighted by class prevelance. \n",
    "\n",
    "### For each of these three types of f1\n",
    " - calculate the score(s) without the help of scikit learn\n",
    " - compare it to the corresponding f1 score evaluated with scikit-learn (NB you'll need to read the docs for `f1_score`. \n",
    " - Write down the pros and cons for this method of calculating multiclass f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(xte)\n",
    "\n",
    "f1s = []\n",
    "for label_index, label_name in enumerate(data_train.target_names):\n",
    "    # calculate the f1 score of one (label_index) vs rest\n",
    "    # your code here...\n",
    "    f1s.append(...\n",
    "\n",
    "for label_name, fs in zip(data_train.target_names, f1s):\n",
    "    print('fscore for {} \\t = {:.3f}'.format(label_name, fs))\n",
    "\n",
    "print('\\n\\n')\n",
    "# compare to sklearn\n",
    "success = (f1s == f1_score(data_test.target, preds, average=None)).all()\n",
    "if success:\n",
    "    print('sklearn builtin matches results')\n",
    "else:\n",
    "    print('scores do not match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pros are ...\n",
    "# The cons are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the macro f1 (the mean of the f1s for each class)\n",
    "f1_macro = ... # calculate without sklearn\n",
    "f1_macro_sk = f1_score(... # calculate with sklearn\n",
    "assert(f1_macro == f1_macro_sk)\n",
    "print('macro f1: {} \\t sklearn macro f1 {}'.format(\n",
    "    f1_macro, \n",
    "    f1_macro_sk\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pros are ...\n",
    "# The cons are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now weighted by class prevalence\n",
    "# TODO:\n",
    "#  - calculate the frequency of each class\n",
    "#  - take a weighted average of the f1s, weighted by these weights\n",
    "#  - compare to sklearn\n",
    "wts = ...\n",
    "weighted_f1 = # without sklearn\n",
    "weighted_f1_sk = f1_score(... # with sklearn\n",
    "\n",
    "print('weighted f1 {} \\t sklearn weighted f1 {}'.format(weighted_f1, weighted_f1_sk))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pros are ...\n",
    "# The cons are ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Confusion Matrix\n",
    "## The confusion matrix is a handy way to understand errors in classification problems.  It is a 2-D grid of what values were predicted and what the actual values were. \n",
    "\n",
    "See [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) in the sklearn docs. \n",
    "\n",
    "## Create a confusion matrix for the 20-newsgroups dataset and comment on the most common failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# your code here\n",
    "# NB: it's handy to call `pd.DataFrame` on the confusion matrix to print it out nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Multiclass Multilabel Problems (20 %)\n",
    "### In this problem we'll examine academic articles from the [arXiv](www.arxiv.org).\n",
    "### Authors who submit articles can attach one or more categories to the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Load the data\n",
    "## TODO\n",
    " - ### load the data\n",
    " - ### compute all of the unique categories in the train data\n",
    " - ### What are the 10 most common categories which occur together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/arxiv-qfin-train.json') as fi:\n",
    "    data_train = json.load(fi)\n",
    "\n",
    "with open('../../data/arxiv-qfin-test.json') as fi:\n",
    "    data_test = json.load(fi)\n",
    "\n",
    "    \n",
    "print(len(data_train), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the unique categories here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the co-occuring categories here\n",
    "# Hint:\n",
    "#  - loop through all the train articles\n",
    "#  - loop through all the pairs of categories\n",
    "#  - keep track of the counts of every pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Encode the data\n",
    "\n",
    "## We will encode the title of each article using a bag of words (`CountVectorizer`). Try limiting the features to about 20k. \n",
    "\n",
    "## Encoding targets is as bit trickier for multilabel problems. In this case we want our target to be a matrix of $N_{samples} x N_{categories}$ but each row does not have to sum to 1.\n",
    " - ## NB: scikit learn as a `MultiLabelBinarizer` to help here. \n",
    "\n",
    "# $ \\\\ $\n",
    "## TODO\n",
    " - ## fit a `CountVectorizer` on the titles to create `x_train` and `x_test`\n",
    " - ## create `y_train` and `y_test` to be matrices of $N_{samples} x N_{categories}$ with all 0s and 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(...\n",
    "vec.fit(...\n",
    "x_train = ...\n",
    "x_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "# your code here\n",
    "\n",
    "y_train = ...\n",
    "y_test = ...\n",
    "print(y_train.shape, y_test.shape)\n",
    "print(list(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model the data\n",
    "### While scikit-learn can't handle multilabel data in logistic regression, keras can. \n",
    "### Create and fit a multilabel logistic regression model and fit it. \n",
    "### NB: think hard about the activation function and loss function that are appropriate in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Softmax, Dropout\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "doc_input = Input( ...\n",
    "# your code here\n",
    "# dont forget to compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.history.history)[['val_loss', 'val_accuracy']].plot(\n",
    "    figsize=(12,7), secondary_y='val_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: f1 score\n",
    "## While modeling is more difficult in the multilabel case, the metrics are, oddly, simpler. Here, we can only compute metrics class by class.\n",
    "\n",
    "### For each class, print the accuracy and f1 score for the class. Comment on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(...\n",
    "# loop through all the classes\n",
    "# compute and print the accuracy and f1 for that class\n",
    "for i, class_ in enumerate(mlb.classes_):\n",
    "    acc = accuracy_score(y_test[:, i], preds[:, i])\n",
    "    f1 = f1_score(y_test[:, i], preds[:, i])\n",
    "    print('class{} \\t\\tacc: {:.3f} \\tf1: {:.3f}'.format(class_, acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: New Metrics (30%)\n",
    "## In this problem we'll explore new metrics associated with true positives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the IMDB data and fit a model\n",
    " - ### Load the imdb data\n",
    " - ### featurize the text using TFIDF\n",
    " - ### Fit logistic regression\n",
    " - ### calculate the in-sample and out of sample accuracy and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    train_dir = os.path.join(imdb_data_dir, 'train')\n",
    "    test_dir = os.path.join(imdb_data_dir, 'test')\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = (label == 'pos')\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = (label == 'pos')\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "    \n",
    "    return (train_docs, y_train), (test_docs, y_test)\n",
    "\n",
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('../../data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer...\n",
    "# more code here\n",
    "\n",
    "\n",
    "# more code here\n",
    "preds_train = ...\n",
    "preds_test = ...\n",
    "\n",
    "print('#'*20 + ' in sample ' + '#'*20 )\n",
    "print('\\t\\taccuracy: {:.3f}'.format(accuracy_score(y_train, preds_train)))\n",
    "print('\\t\\tf1: {:.3f}'.format(f1_score(y_train, preds_train)))\n",
    "print('\\n\\n')\n",
    "print('#'*20 + ' out of sample ' + '#'*20 )\n",
    "print('\\t\\taccuracy: {:.3f}'.format(accuracy_score(y_test, preds_test)))\n",
    "print('\\t\\tf1: {:.3f}'.format(f1_score(y_test, preds_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tradeoff between true positives and false positives\n",
    "Typically we take a threshold of 0.5 probability to consider something a positive example.\n",
    "However, as we change this threshold we can change the number of true positives we get.\n",
    " - Example: at a theshold of 0.0001 we will get nearly all of the true positives\n",
    " - Example: at a threshold of 0.999 we will get almost none of the true positives\n",
    "\n",
    "Notice: as we change our threshold and increase the number of true positives we will also increase the number of false positives we pick up.\n",
    "\n",
    "In this part you will create a graph of the false positive rate on the x-axis and the true positive rate on the y-axis. This is often called the `receiver operator characteristic`. Make this curve for the out of sample data below.\n",
    "\n",
    "Note: while you can use the builtin scikit-learn functionality for this, you will __not receive credit__ if you do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# hint: \n",
    "#  - loop through the thresholds\n",
    "#  - calulcate the true positives and false positives\n",
    "\n",
    "# hint: what values for thresholds should you loop through?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(true_pos_rates, index=false_pos_rates).plot(figsize=(12,8), fontsize=16)\n",
    "plt.xlabel('False Pos Rate', fontsize=16)\n",
    "plt.ylabel('True Pos Rate', fontsize=16)\n",
    "plt.title('Receiver Operator Characteristic', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Baseline\n",
    " - What does the receiver operator curve look like for a random guessing classifier? \n",
    " - Make the same plot as above but add the random guessing curve\n",
    " - Add comments about WHY the random guessing curve looks this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.Series(true_pos_rates, index=false_pos_rates, name='logistic regression').plot(\n",
    "    figsize=(12,8), fontsize=16\n",
    ")\n",
    "baseline_series = ... # your code here for the ROC for random guessing\n",
    "baseline_series.to_frame('random guess').plot(ax=ax, fontsize=16)\n",
    "plt.xlabel('False Pos Rate', fontsize=16)\n",
    "plt.ylabel('True Pos Rate', fontsize=16)\n",
    "plt.title('Receiver Operator Characteristic', fontsize=20)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Boiling it down to a single number\n",
    " - While the ROC is a useful curve and contains a lot of information, it is useful to distill in down to a single number. Typically, the area under the curve is used. Calculate the area under the curve and add it as the title to your previous plot. \n",
    " - Hint: think about approximations for integrals for finding area under a curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_under = # your code here\n",
    "\n",
    "\n",
    "# repeat the plotting code here\n",
    "\n",
    "plt.title('Area under the curve = {:.3f}'.format(area_under, fontsize=20))\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Check you work and comment on the results\n",
    " - \"There's gotta be a better way!\"\n",
    " - In fact, `scikit-learn` will take care of a lot of the headache here. \n",
    " - `from sklearn.metrics import plot_roc_curve`\n",
    " - read the docs and use this function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve, auc\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few comments:\n",
    " - The area under the ROC has a nice interpretation. It can be thought of as the probability that a randomly chosen positive example has a higher probability than a randomly chosen negative example.\n",
    " - This metric is also nice since it is independent of a threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Examining Coefficients (20%)\n",
    "In class we skipped an important step: we never made sure our models made sense. \n",
    "Logistic regression provides coefficients, which allow us to determine if a model\n",
    "if learning anything reasonable. \n",
    "\n",
    "In this problem, you'll load the imdb data, fit logistic regression and exmamine the coefficients. \n",
    "Print out the largest and smallest (largest negative) coefficients and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "# copy code from above to load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('../../data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vec = TfidfVectorizer(...\n",
    "# more code here\n",
    "                      \n",
    "lr = LogisticRegression(...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: you can call `vec.get_feature_names` to get the words in order\n",
    "# that correspond to the columns of the TFIDF matrix \n",
    "# This is useful to pass to the index of a pd.Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(...)\n",
    "\n",
    "# NB: to get the largest items in a series by abs try\n",
    "#    coefs.loc[coefs.abs().nlargest(20).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
