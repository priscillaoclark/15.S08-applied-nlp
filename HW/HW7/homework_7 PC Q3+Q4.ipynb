{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: The Company Name Comparer (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem in NLP for finance is automatically comparing two company names to see if they are the same. \n",
    "Intuitively, this is a problem that is fairly easy for humans: \n",
    " - `S and P Global` and `S&P Global`\n",
    " - `JPMorgan` and `JP Morgan`\n",
    " - `Google Inc.` and `Google`\n",
    "\n",
    "However, the number of ways in which companies can be represented, and the intricacies of corporate \n",
    "structure make it difficult to craft rules to capture this behavior.\n",
    "This makes the problem perfect for a machine learning approach. \n",
    "We will see that it is particularly well suited to deep learning, since some \n",
    "features will be difficult to craft by hand. \n",
    "In the end of this problem we will build a neural network that accepts TWO strings as input\n",
    "and produces a single floating point output, which is the probability that\n",
    "the two organizations refer to the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is an excellent source of natural language data. \n",
    "We need a data set of many ways that we can refer to the same company.\n",
    "Here we will use the page redicts- if a certain page automatically \n",
    "redirects to a second page, we can say with high confidence that the \n",
    "title of the first page unambiguously refers to the same thing as \n",
    "the second. The lack of ambiguity is important! For example, we \n",
    "do not want `Amazon` to be another way to refer to `amazon.com` since\n",
    "it can also refer to the `amazon river`. See [here](https://en.wikipedia.org/wiki/Amazon) for more.\n",
    "\n",
    "In `org_redirects.csv` you will find data on the page redirects that have been \n",
    "filtered down to everything considered an organization (this is not only companies, \n",
    "but it's close). The file has columns\n",
    " - `souce_id` a page id for the source page\n",
    " - `source_title`: the page title for the source\n",
    " - `target_id`: the page id for the page to which the source will redirect\n",
    " - `target_title`: the title of the page to which the source redirects\n",
    " - `target_qid`: the id of the page in wikidata, the accompanying knowledge graph to wikipedia\n",
    " - `edge_type`: the type of organization the target item has in wikidata\n",
    "\n",
    "For our purposes, we can consider the `source_title` and `target_title` two \n",
    "ways to unambiguously refer to the same organization. We will teach an algorithm\n",
    "to recognize the patterns in these data.\n",
    "\n",
    "\n",
    "TODO\n",
    "1. Load the data\n",
    "2. Explore the data: \n",
    "   - What is the distribution of words / characters?\n",
    "   - Are there any tokens that are particularly common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096901, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_title</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_title</th>\n",
       "      <th>target_qid</th>\n",
       "      <th>qid</th>\n",
       "      <th>edge_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>AfghanistanMilitary</td>\n",
       "      <td>20182</td>\n",
       "      <td>Afghan_Armed_Forces</td>\n",
       "      <td>11062919</td>\n",
       "      <td>11062919</td>\n",
       "      <td>8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820</td>\n",
       "      <td>Afghanistan/Military</td>\n",
       "      <td>20182</td>\n",
       "      <td>Afghan_Armed_Forces</td>\n",
       "      <td>11062919</td>\n",
       "      <td>11062919</td>\n",
       "      <td>8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8771295</td>\n",
       "      <td>Afghan_Military</td>\n",
       "      <td>20182</td>\n",
       "      <td>Afghan_Armed_Forces</td>\n",
       "      <td>11062919</td>\n",
       "      <td>11062919</td>\n",
       "      <td>8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8916870</td>\n",
       "      <td>Afghan_Military_Force</td>\n",
       "      <td>20182</td>\n",
       "      <td>Afghan_Armed_Forces</td>\n",
       "      <td>11062919</td>\n",
       "      <td>11062919</td>\n",
       "      <td>8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8916892</td>\n",
       "      <td>Afghan_Militia_Force</td>\n",
       "      <td>20182</td>\n",
       "      <td>Afghan_Armed_Forces</td>\n",
       "      <td>11062919</td>\n",
       "      <td>11062919</td>\n",
       "      <td>8473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id           source_title  target_id         target_title  \\\n",
       "0         20    AfghanistanMilitary      20182  Afghan_Armed_Forces   \n",
       "1        820   Afghanistan/Military      20182  Afghan_Armed_Forces   \n",
       "2    8771295        Afghan_Military      20182  Afghan_Armed_Forces   \n",
       "3    8916870  Afghan_Military_Force      20182  Afghan_Armed_Forces   \n",
       "4    8916892   Afghan_Militia_Force      20182  Afghan_Armed_Forces   \n",
       "\n",
       "   target_qid       qid  edge_type  \n",
       "0    11062919  11062919       8473  \n",
       "1    11062919  11062919       8473  \n",
       "2    11062919  11062919       8473  \n",
       "3    11062919  11062919       8473  \n",
       "4    11062919  11062919       8473  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"org_redirects.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Process, Clean, and Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have the data in slightly more usable form. \n",
    "For simplicity, we will want to only deal with lowercase characters. \n",
    "Additionally, we will want to remove special characters from the source\n",
    "and target titles.\n",
    "\n",
    "Additionally, some redirects require special knowledge, that we don't expect the \n",
    "algorithm to be able to learn. \n",
    "\n",
    "TODO: \n",
    " - create two new columns `source_clean` and `target_clean` which are lowercased versions of the original data with the characters `,_/` replaced by spaces\n",
    " - NB: remember to drop rows that are now duplicates because of the string cleaning we've done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping duplicates: (1096901, 9)\n"
     ]
    }
   ],
   "source": [
    "import re # probably a useful import \n",
    "\n",
    "def normalize_string(s):\n",
    "    # your code here\n",
    "    # normalize the data by lowercasing, and replace certain characters with whitespace\n",
    "    # careful, replace 1 or more whitespace characters with a single whitespace\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[,_/]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "df[\"clean_source\"] = df[\"source_title\"].apply(normalize_string)\n",
    "df[\"clean_target\"] = df[\"target_title\"].apply(normalize_string)\n",
    "\n",
    "# drop the duplicates here. \n",
    "# you should have approx 1.1 million examples before dropping duplicates\n",
    "# and approx 1.04 million examples after\n",
    "print(f\"Shape before dropping duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping duplicates: (1043573, 9)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['clean_source', 'clean_target'])\n",
    "print(f\"Shape after dropping duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Make training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have positive examples, ie two ways of referring to the same organization. \n",
    "We need negative examples in order for the network to learn what is not considered the same orgnanization. \n",
    "There are many ways to do this, but we will use the most simple one- random examples from the training data. \n",
    "\n",
    "TODO:\n",
    " - make another copy of our training data\n",
    " - shuffle either the source or target columns so that they no longer line up\n",
    " - add these new data as negative examples to the training data\n",
    " - split the data into two: train and test \n",
    "\n",
    "\n",
    "Now we have a dataset with approximately 50% positive examples\n",
    "\n",
    "TODO (answers these questions in text)\n",
    " - What are the benefits of this approach\n",
    " - What are the downsides of this appoach\n",
    " - What would be a more sophisticated way to generate negative examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangle the data here\n",
    "# you should have approximately twice as many examples now\n",
    "# the mean `y` value should be about 0.5\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "positive_pairs = df[['clean_source', 'clean_target']].copy()\n",
    "positive_pairs['y'] = 1\n",
    "\n",
    "negative_pairs = df[['clean_source', 'clean_target']].copy()\n",
    "negative_pairs['clean_target'] = np.random.permutation(negative_pairs['clean_target'])\n",
    "negative_pairs['y'] = 0\n",
    "\n",
    "all_pairs = pd.concat([positive_pairs, negative_pairs], axis=0)\n",
    "all_pairs = all_pairs.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 2087146\n",
      "Average label value: 0.500\n",
      "Training examples: 1669716\n",
      "Test examples: 417430\n"
     ]
    }
   ],
   "source": [
    "# split randomly into train and test\n",
    "train_data, test_data = train_test_split(all_pairs, test_size=0.2, random_state=42)\n",
    "print(f\"Total examples: {len(all_pairs)}\")\n",
    "print(f\"Average label value: {all_pairs['y'].mean():.3f}\")\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "Benefits:\n",
    "- Simple and fast to implement\n",
    "- Computationally efficient\n",
    "- Guarantees a balanced dataset (50-50 split between positive and negative examples)\n",
    "- Preserves the natural distribution of company name formats in both columns\n",
    "- Easy to scale - we can generate as many negative examples as needed\n",
    "\n",
    "Downsides:\n",
    "- May create \"easy\" negative examples that don't challenge the model\n",
    "- Could accidentally create some false negatives (when randomly paired companies are actually the same)\n",
    "- Doesn't capture real-world scenarios where company names are intentionally similar but different\n",
    "- May not help the model learn subtle distinctions between similar but different companies\n",
    "- Random pairing might not expose the model to the most important edge cases\n",
    "\n",
    "More sophisticated approaches to generate negative examples:\n",
    "\n",
    "Controlled Character Modification:\n",
    "- Make specific character-level changes to company names\n",
    "- Replace/add/remove legal entity suffixes (Inc, LLC, Ltd)\n",
    "- Modify spacing and punctuation in controlled ways\n",
    "- This would help the model learn about meaningful vs non-meaningful differences\n",
    "\n",
    "Similar Company Selection:\n",
    "- Use string similarity metrics (Levenshtein distance, Jaccard similarity) to find similar but different companies\n",
    "- This would create \"hard\" negative examples that force the model to learn subtle distinctions\n",
    "- Example: \"American Airlines\" vs \"American Express\"\n",
    "\n",
    "Industry-Based Pairing:\n",
    "- Use company metadata to pair companies from the same industry\n",
    "- This would create more realistic negative examples of companies that might actually be confused\n",
    "\n",
    "Subsidiary/Parent Company Relationships:\n",
    "- Use corporate structure data to create negative examples from related but distinct entities\n",
    "- Example: \"Google\" vs \"Alphabet Inc\"\n",
    "\n",
    "Known Disambiguation Cases:\n",
    "- Use Wikipedia disambiguation pages to find companies with similar names that are definitely different\n",
    "- This would provide high-quality negative examples based on real-world cases\n",
    "\n",
    "Rule-Based Generation:\n",
    "- Create systematic variations of company names\n",
    "- Change common words (e.g., \"North\" to \"South\", \"First\" to \"Second\")\n",
    "- This would help the model learn semantic differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Turn the data into numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a character level representation, since organizations have too\n",
    "many special words to use a word-level representation. \n",
    "\n",
    "Eventually, we will build a neural network that accepts TWO strings as input\n",
    "and produces a single floating point output, which is the probability that\n",
    "the source and target refer to the same thing.\n",
    "\n",
    "TODO:\n",
    " - turn the data into sequences of characters that are integer encoded\n",
    "   - use the `Tokenizer` with `char_level=True`\n",
    "   - use `tokenizer.text_to_sequences` to turn the characters into integer-encoded sequences\n",
    " - pad the sequences to a constant length of 30 characters\n",
    " - write two functions, `train_gen` and `test_gen` which yield data for the network in the right form.\n",
    "   - data should be yielded as `yield [input_1, input_2], output`\n",
    "   - you will likely find it easier to have these functions one-hot-encode the characters on the fly instead of holding very large numpy arrays in memory.\n",
    " - test your functions and make sure you can recover the original input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_iterator():\n",
    "    \"\"\"This is a helpful iterator\n",
    "    \n",
    "    It will yield all the data in `clean_source` and `clean_target` \n",
    "    in the training data, without making another copy\n",
    "    \"\"\"\n",
    "    for item in train_data[\"clean_source\"]:\n",
    "        yield item\n",
    "    for item in train_data[\"clean_target\"]:\n",
    "        yield item\n",
    "\n",
    "\n",
    "tok = Tokenizer(lower=True, char_level=True, num_words=140)\n",
    "tok.fit_on_texts(text_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1669716, 30) (417430, 30)\n"
     ]
    }
   ],
   "source": [
    "# for both train and test\n",
    "#   for the src and target\n",
    "#   - turn each into integer sequence\n",
    "#   - pad the sequences \n",
    "# make y values for both train and test\n",
    "MAX_SEQ_LEN = 30\n",
    "\n",
    "x_train_src = pad_sequences(tok.texts_to_sequences(train_data['clean_source']), \n",
    "                          maxlen=MAX_SEQ_LEN)\n",
    "x_train_targ = pad_sequences(tok.texts_to_sequences(train_data['clean_target']), \n",
    "                           maxlen=MAX_SEQ_LEN)\n",
    "y_train = train_data['y'].values\n",
    "\n",
    "\n",
    "x_test_src = pad_sequences(tok.texts_to_sequences(test_data['clean_source']), \n",
    "                         maxlen=MAX_SEQ_LEN)\n",
    "x_test_targ = pad_sequences(tok.texts_to_sequences(test_data['clean_target']), \n",
    "                          maxlen=MAX_SEQ_LEN)\n",
    "y_test = test_data['y'].values\n",
    "\n",
    "print(x_train_targ.shape, x_test_targ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = tok.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import funcy\n",
    "\n",
    "def train_gen(batch_size):\n",
    "    # your code here\n",
    "    # the last line should probably look like\n",
    "    #    `yield [batch_src, batch_targ], batch_y`\n",
    "    \n",
    "    # don't forget to one hot encode the characaters\n",
    "    #   keras.utils.to_categorical is helpful (num_classes=MAX_CHARS)\n",
    "    \n",
    "    # if you're feeling enterprising, you can randomly flip\n",
    "    # the source and target data, which serves as a way to augment\n",
    "    # the training data. Consider                \n",
    "#             if np.random.rand() > 0.5:\n",
    "#                 batch_src, batch_targ = batch_targ, batch_src\n",
    "    while True:\n",
    "        indices = np.random.permutation(len(x_train_src))\n",
    "        \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            \n",
    "            batch_src = x_train_src[batch_indices]\n",
    "            batch_targ = x_train_targ[batch_indices]\n",
    "            batch_y = y_train[batch_indices]\n",
    "\n",
    "            if np.random.rand() > 0.5:\n",
    "                batch_src, batch_targ = batch_targ, batch_src\n",
    "            \n",
    "            batch_src = to_categorical(batch_src, num_classes=MAX_CHARS).astype('float32')\n",
    "            batch_targ = to_categorical(batch_targ, num_classes=MAX_CHARS).astype('float32')\n",
    "            batch_y = batch_y.astype('float32')\n",
    "            \n",
    "            yield (batch_src, batch_targ), batch_y\n",
    "            \n",
    "\n",
    "def val_gen(batch_size):\n",
    "    # your code here, it should be very similar to the function above\n",
    "    # no need to flip the src and targets randomly\n",
    "    while True:\n",
    "        for i in range(0, len(x_test_src), batch_size):\n",
    "\n",
    "            batch_src = x_test_src[i:i + batch_size]\n",
    "            batch_targ = x_test_targ[i:i + batch_size]\n",
    "            batch_y = y_test[i:i + batch_size]\n",
    "            \n",
    "            batch_src = to_categorical(batch_src, num_classes=MAX_CHARS).astype('float32')\n",
    "            batch_targ = to_categorical(batch_targ, num_classes=MAX_CHARS).astype('float32')\n",
    "            batch_y = batch_y.astype('float32')\n",
    "            \n",
    "            yield (batch_src, batch_targ), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shapes: (32, 30, 141) (32, 30, 141) (32,)\n",
      "Sample decoded text: wmab-fm\n"
     ]
    }
   ],
   "source": [
    "# check the shapes, make sure you can recover the data and it looks normal\n",
    "train_iter = train_gen(32)\n",
    "val_iter = val_gen(32)\n",
    "\n",
    "# Get sample batches\n",
    "[src_batch, targ_batch], batch_y = next(train_iter)\n",
    "\n",
    "print(\"Sample batch shapes:\", src_batch.shape, targ_batch.shape, batch_y.shape)\n",
    "\n",
    "# Decode a sample sequence to verify data\n",
    "sample_text = ''.join([tok.index_word.get(i, '') for i in \n",
    "                      np.argmax(src_batch[0], axis=-1) if i != 0])\n",
    "print(\"Sample decoded text:\", sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build and fit a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of keras is that it is flexible enough to allow us to do almost arbitrary matrix operations. \n",
    "In this model we will use several `Conv1D`-`MaxPooling1D` combinations to\n",
    "encode or process the organization names. However, we will use the __SAME__ blocks for both \n",
    "the source and target names, in order to \"force\" the network to learn operations that\n",
    "distill the strings down into features that make them easy to compare.\n",
    "\n",
    "Build a keras model that \n",
    " - has __two__ text inputs\n",
    " - has several conv/pooling blocks to process the inputs\n",
    " - uses the __same__ blocks to process both of the text inputs\n",
    " - concatentates the result of the convolutions together into one vector\n",
    " - use one or more `Dense` layers to output a single (`float`), which is the probability that the two organizations are the same\n",
    "\n",
    "Hint: `keras.models.Model` can accept lists of inputs and outputs in addition to single tensors. \n",
    " - `model = Model([input_1, input_2], output)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    GlobalMaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">141</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">141</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">90,368</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">20,544</span> │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ max_pooling1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m141\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m141\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m90,368\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │ conv1d[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m82,048\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ max_pooling1d[\u001b[38;5;34m1\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │ conv1d_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │     \u001b[38;5;34m41,024\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │ conv1d_2[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │     \u001b[38;5;34m20,544\u001b[0m │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ max_pooling1d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │ conv1d_3[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">258,817</span> (1011.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m258,817\u001b[0m (1011.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">258,817</span> (1011.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m258,817\u001b[0m (1011.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "do = 0.05\n",
    "\n",
    "left = Input(shape=(MAX_SEQ_LEN, MAX_CHARS))\n",
    "right = Input(shape=(MAX_SEQ_LEN, MAX_CHARS))\n",
    "\n",
    "blocks = [\n",
    "    Conv1D(128, 5, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(128, 5, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(64, 5, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(64, 5, padding=\"same\", activation=\"relu\"),\n",
    "    GlobalMaxPooling1D()\n",
    "]\n",
    "\n",
    "def encode(inpt):\n",
    "    x = blocks[0](inpt)\n",
    "    for block in blocks[1:]:\n",
    "        x = block(x)\n",
    "    return x\n",
    "\n",
    "left_stream = encode(left)\n",
    "right_stream = encode(right)\n",
    "# TODO:\n",
    "# - concatenate right and left streams\n",
    "# - use 2-3 dense blocks to get the output into the right shape \n",
    "#    this is a binary classification problem\n",
    "# - compile your model\n",
    "\n",
    "# your code here\n",
    "merged = Concatenate()([left_stream, right_stream])\n",
    "x = Dense(128, activation='relu')(merged)\n",
    "x = Dropout(do)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(do)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model([left, right], output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# user these callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-5\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bluebird/develop/15.S08_applied_nlp/venv_nlp/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 112ms/step - acc: 0.7017 - loss: 0.5584 - val_acc: 0.7972 - val_loss: 0.4234 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 110ms/step - acc: 0.8072 - loss: 0.4070 - val_acc: 0.8276 - val_loss: 0.3708 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 111ms/step - acc: 0.8359 - loss: 0.3562 - val_acc: 0.8454 - val_loss: 0.3360 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 109ms/step - acc: 0.8530 - loss: 0.3234 - val_acc: 0.8532 - val_loss: 0.3240 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 106ms/step - acc: 0.8624 - loss: 0.3043 - val_acc: 0.8583 - val_loss: 0.3119 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 107ms/step - acc: 0.8707 - loss: 0.2882 - val_acc: 0.8636 - val_loss: 0.3033 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 114ms/step - acc: 0.8771 - loss: 0.2759 - val_acc: 0.8677 - val_loss: 0.2972 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 113ms/step - acc: 0.8822 - loss: 0.2658 - val_acc: 0.8700 - val_loss: 0.2919 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 120ms/step - acc: 0.8861 - loss: 0.2576 - val_acc: 0.8703 - val_loss: 0.2936 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 108ms/step - acc: 0.8899 - loss: 0.2499 - val_acc: 0.8715 - val_loss: 0.2903 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 105ms/step - acc: 0.8928 - loss: 0.2440 - val_acc: 0.8741 - val_loss: 0.2865 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 105ms/step - acc: 0.8958 - loss: 0.2379 - val_acc: 0.8746 - val_loss: 0.2870 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 107ms/step - acc: 0.8985 - loss: 0.2333 - val_acc: 0.8754 - val_loss: 0.2855 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 110ms/step - acc: 0.9003 - loss: 0.2287 - val_acc: 0.8759 - val_loss: 0.2815 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 105ms/step - acc: 0.9022 - loss: 0.2245 - val_acc: 0.8764 - val_loss: 0.2829 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 106ms/step - acc: 0.9042 - loss: 0.2207 - val_acc: 0.8775 - val_loss: 0.2813 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 106ms/step - acc: 0.9062 - loss: 0.2169 - val_acc: 0.8775 - val_loss: 0.2836 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 124ms/step - acc: 0.9076 - loss: 0.2133 - val_acc: 0.8781 - val_loss: 0.2863 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 106ms/step - acc: 0.9140 - loss: 0.1997 - val_acc: 0.8798 - val_loss: 0.2905 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 107ms/step - acc: 0.9173 - loss: 0.1928 - val_acc: 0.8789 - val_loss: 0.2901 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m1630/1630\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 107ms/step - acc: 0.9217 - loss: 0.1838 - val_acc: 0.8799 - val_loss: 0.3036 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "batch_size = 1024\n",
    "steps_per_epoch = len(x_train_src) // batch_size\n",
    "validation_steps = len(x_test_src) // batch_size\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_gen(batch_size)\n",
    "val_generator = val_gen(batch_size)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=30,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy \n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['acc'], label='Train Acc')\n",
    "plt.plot(history.history['val_acc'], label='Val Acc')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how our new model works on unseen text. \n",
    "Write a function\n",
    "```python\n",
    "def compare_inputs(a, b):\n",
    "    \"\"\"takes in two strings and returns a float\"\"\"\n",
    "    pass\n",
    "```\n",
    "that can use the model to compare two strings. \n",
    "\n",
    "TODO\n",
    " - Come up with at least 10 examples of your own and comment on the results\n",
    "\n",
    "TODO: How would this be done without deep learning? \n",
    " - Can we use the same operations on both the source and target?\n",
    " - How would we need to combine representations that come from the features we engineer from the two inputs?\n",
    " - If we had hand-engineered features, how would we add them to this network?\n",
    "\n",
    "(answer in text below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inputs(a, b):\n",
    "    # take two strings, process them the same way we process our training data\n",
    "    # use `model.predict` to get a probability that they are the same\n",
    "    # your code here\n",
    "    a_clean = normalize_string(a)\n",
    "    b_clean = normalize_string(b)\n",
    "    \n",
    "    # Convert to integer sequences\n",
    "    a_seq = pad_sequences(tok.texts_to_sequences([a_clean]), maxlen=MAX_SEQ_LEN)\n",
    "    b_seq = pad_sequences(tok.texts_to_sequences([b_clean]), maxlen=MAX_SEQ_LEN)\n",
    "    \n",
    "    # One-hot encode\n",
    "    a_one_hot = to_categorical(a_seq, num_classes=MAX_CHARS)\n",
    "    b_one_hot = to_categorical(b_seq, num_classes=MAX_CHARS)\n",
    "    \n",
    "    # Get model prediction\n",
    "    return model.predict([a_one_hot, b_one_hot], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bluebird/develop/15.S08_applied_nlp/venv_nlp/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The college of Mann\" and \"University of mann\": 0.9968\n",
      "\"S and P Global\" and \"S&P Global\": 0.9019\n",
      "\"Google Inc\" and \"Google\": 0.9965\n",
      "\"JPMorgan Chase\" and \"JP Morgan\": 0.9511\n",
      "\"Microsoft Corporation\" and \"Microsoft Corp\": 0.9995\n",
      "\"International Business Machines\" and \"IBM\": 0.9090\n",
      "\"Meta Platforms Inc\" and \"Facebook Inc\": 0.2375\n",
      "\"Walmart Inc.\" and \"Wal-Mart Stores\": 0.9332\n",
      "\"Coca Cola Co\" and \"The Coca-Cola Company\": 0.9998\n",
      "\"McDonald's Corp\" and \"McDonalds Corporation\": 0.9952\n",
      "\"Apple Inc\" and \"Alphabet Inc\": 0.4547\n",
      "\"Bank of America\" and \"Bank of China\": 0.9928\n"
     ]
    }
   ],
   "source": [
    "for pair in [\n",
    "    (\"The college of Mann\", \"University of mann\"),\n",
    "    (\"S and P Global\", \"S&P Global\"),\n",
    "    (\"Google Inc\", \"Google\"),\n",
    "    (\"JPMorgan Chase\", \"JP Morgan\"),\n",
    "    (\"Microsoft Corporation\", \"Microsoft Corp\"),\n",
    "    (\"International Business Machines\", \"IBM\"),\n",
    "    (\"Meta Platforms Inc\", \"Facebook Inc\"),\n",
    "    (\"Walmart Inc.\", \"Wal-Mart Stores\"),\n",
    "    (\"Coca Cola Co\", \"The Coca-Cola Company\"),\n",
    "    (\"McDonald's Corp\", \"McDonalds Corporation\"),\n",
    "    (\"Apple Inc\", \"Alphabet Inc\"),\n",
    "    (\"Bank of America\", \"Bank of China\")\n",
    "]:\n",
    "    print(\"\\\"{}\\\" and \\\"{}\\\": {:.4f}\".format(pair[0], pair[1], compare_inputs(*pair)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments on results**\n",
    "\n",
    "Strengths:\n",
    "1. Handles common corporate suffix variations very well (\"Inc\" vs \"Corporation\", \"Corp\" vs \"Company\")\n",
    "2. Strong on spacing/punctuation differences (\"S and P\" vs \"S&P\", \"Coca Cola\" vs \"Coca-Cola\")\n",
    "3. Excellent at minor spelling variations (\"McDonalds\" vs \"McDonald's\")\n",
    "4. Good at recognizing when company names are fundamentally different (Apple/Alphabet: 0.4447)\n",
    "\n",
    "Notable patterns:\n",
    "1. Very high confidence (>0.99) on simple variations like \"Google Inc\"/\"Google\"\n",
    "2. Does not correctly identifies complete company changes (Meta/Facebook: 0.2375)\n",
    "3. Partial match confusion on \"Bank of America\"/\"Bank of China\" (0.9928) - shows it's picking up on the shared terms\n",
    "\n",
    "Areas for improvement:\n",
    "1. The IBM case (0.9090) suggests it could better handle extreme abbreviations\n",
    "2. The \"Bank of\" case shows it might be too sensitive to shared words in different companies\n",
    "\n",
    "This suggests the character-level CNN is effectively learning:\n",
    "- Common corporate naming patterns\n",
    "- Various ways to write the same company name\n",
    "- The relative importance of different parts of company names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How would this be done without deep learning?**\n",
    "\n",
    "*Can we use the same operations on both the source and target?*\n",
    "\n",
    "Yes, using the same operations on both source and target strings is optimal through a Siamese architecture, where identical layers with shared weights process both inputs. This makes sense because what makes company names similar shouldn't depend on which name is the source or target - the features that help recognize \"Google\" or understand suffixes like \"Inc\" should work the same way in both cases. This approach is more efficient and learns better since patterns learned from one input directly apply to the other.\n",
    "\n",
    "*How would we need to combine representations that come from the features we engineer from the two inputs?*\n",
    "\n",
    "The key approaches for combining engineered features from the two inputs would be concatenation, element-wise operations, or computing a similarity/distance metric. You could concatenate the feature vectors and let the model learn the relationships, use element-wise operations like subtraction or multiplication to directly compare corresponding features, or compute similarity scores (like cosine similarity or L2 distance) between the feature vectors. For this specific task of company name matching, element-wise absolute difference or multiplication would be particularly suitable, as they preserve the symmetry of the comparison - it shouldn't matter which name is the source or target.\n",
    "\n",
    "*If we had hand-engineered features, how would we add them to this network?*\n",
    "\n",
    "Hand-engineered features could be incorporated into this network in three main ways: (1) concatenate them with the CNN encodings just before the dense layers, adding them to the learned representations after the GlobalMaxPooling1D, (2) add them as additional input channels alongside the character-level representations, expanding the input dimensionality, or (3) create a parallel branch in the network that processes these features separately and then merges with the main CNN outputs. For company name matching, useful engineered features might include things like industry codes, TF-IDF vectors, or edit distance metrics, which would complement the character-level patterns the CNN learns automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Character level word features (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we showed that neural networks can learn everything from character-level representations.\n",
    "Recall the potential advantages\n",
    " - We are much less likely to see unknown characters\n",
    " - We don't have to worry about word tokenization\n",
    " - Maybe `jump` and `jumps` can \"share\" a lot of the representation\n",
    " - There's nothing special about a space \n",
    "\n",
    "In fact, there is perhaps something special about a space. \n",
    "While the network can, in principle, learn the concept of a word boundary,\n",
    "we may be able to help the network learn by telling it that words boundaries are\n",
    "an important concept. This is especially true in smaller-data problems.\n",
    "\n",
    "In this problem we'll seek to remedy the first and third bullets above by encoding\n",
    "every __word__ as a sequence of characters. This should allow for the network to implicitly\n",
    "learn similarities between words with similar letters. In some sense, we can think of \n",
    "such an encoding as character-level method for learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Load the imdb data as we did in class. \n",
    " - Tokenize it into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(imdb_data_dir, \"train\")\n",
    "    test_dir = os.path.join(imdb_data_dir, \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('../HW6/data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the text keeping the 50k most common words\n",
    "# turn text into integer sequences\n",
    "# pad the sequences to 125 elements each\n",
    "\n",
    "# your code here\n",
    "MAX_SEQ_LEN = 125\n",
    "\n",
    "# your code here\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_docs)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_docs)\n",
    "\n",
    "x_train = pad_sequences(train_sequences, maxlen=MAX_SEQ_LEN)\n",
    "x_test = pad_sequences(test_sequences, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Wrangle the data into the correct representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to represent the data as character-level features, word by word. \n",
    "This means we will require the data to be in a shape\n",
    " - ### batch x word-position x character\n",
    "\n",
    "which is another dimension!\n",
    "### 1a\n",
    " - create a valid set of characters from `string.ascii_lowercase + string.digits + string.punctuation`\n",
    " - create character to int and int to character lookups\n",
    " - create word to int and int to word lookups\n",
    "\n",
    "### 1b\n",
    " - for every word (int) in a sequence, resolve it to a list of integers, which encode characters\n",
    " - pad each character sequence (which encodes a single word) to a constant length of 10 chars / word\n",
    "\n",
    "### 1c\n",
    " - Pad every word sequence (each now encoded as a list of integer sequences) to 125 words\n",
    "\n",
    "This should yield training data of the shape `(25000, 125, 10)`  `(examples x words x characters)` of type `int32`\n",
    "\n",
    "### 1d\n",
    " - make sure you can recover a coherent IMDB review from an element in `x_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "valid_chars = string.ascii_lowercase + string.digits + string.punctuation\n",
    "valid_chars = list(set(valid_chars))\n",
    "char_to_int = {c: i + 1 for i, c in enumerate(valid_chars)}  # save 0 for padding\n",
    "int_to_char = {i + 1: c for i, c in enumerate(valid_chars)}\n",
    "\n",
    "word_to_int = tokenizer.word_index\n",
    "int_to_word = {v: k for k, v in word_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS_PER_WORD = 10\n",
    "MAX_SEQ_LEN_WORDS = 125\n",
    "UNK_CHAR = len(valid_chars) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_char_seq(word):\n",
    "    # your code here\n",
    "    if word not in int_to_word:\n",
    "        return [UNK_CHAR] * MAX_CHARS_PER_WORD\n",
    "    \n",
    "    # Get the word string and convert to lowercase\n",
    "    word_str = int_to_word[word].lower()\n",
    "    \n",
    "    # Convert characters to integers\n",
    "    char_seq = []\n",
    "    for char in word_str:\n",
    "        if char in char_to_int:\n",
    "            char_seq.append(char_to_int[char])\n",
    "        else:\n",
    "            char_seq.append(UNK_CHAR)\n",
    "            \n",
    "    # Pad or truncate to MAX_CHARS_PER_WORD\n",
    "    if len(char_seq) > MAX_CHARS_PER_WORD:\n",
    "        char_seq = char_seq[:MAX_CHARS_PER_WORD]\n",
    "    else:\n",
    "        char_seq = char_seq + [0] * (MAX_CHARS_PER_WORD - len(char_seq))\n",
    "        \n",
    "    return char_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_seq_to_char_seq(word_seq):\n",
    "    # your code here\n",
    "    return [word_to_char_seq(word) for word in word_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 16:25:25.056076: W tensorflow/core/framework/op_kernel.cc:1828] INVALID_ARGUMENT: ValueError: Could not find callback with key=pyfunc_2 in the registry.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/bluebird/develop/15.S08_applied_nlp/venv_nlp/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 258, in __call__\n",
      "    raise ValueError(f\"Could not find callback with key={token} in the \"\n",
      "\n",
      "ValueError: Could not find callback with key=pyfunc_2 in the registry.\n",
      "\n",
      "\n",
      "2024-12-04 16:25:25.056802: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] Error occurred when finalizing GeneratorDataset iterator: INVALID_ARGUMENT: ValueError: Could not find callback with key=pyfunc_2 in the registry.\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/bluebird/develop/15.S08_applied_nlp/venv_nlp/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 258, in __call__\n",
      "    raise ValueError(f\"Could not find callback with key={token} in the \"\n",
      "\n",
      "ValueError: Could not find callback with key=pyfunc_2 in the registry.\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 364 ms, total: 12.6 s\n",
      "Wall time: 12.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000, 125, 10), (25000, 125, 10))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "char_seqs_train = [word_seq_to_char_seq(ts) for ts in train_sequences]\n",
    "x_train = pad_sequences(char_seqs_train, maxlen=MAX_SEQ_LEN_WORDS)\n",
    "\n",
    "char_seqs_test = [word_seq_to_char_seq(ts) for ts in test_sequences]\n",
    "x_test = pad_sequences(char_seqs_test, maxlen=MAX_SEQ_LEN_WORDS)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_text(char_sequence):\n",
    "    \"\"\"Recover original text from a character sequence\"\"\"\n",
    "    words = []\n",
    "    for word_chars in char_sequence:\n",
    "        # Skip if all zeros (padding)\n",
    "        if not np.any(word_chars):  # more efficient than all(c == 0)\n",
    "            continue\n",
    "            \n",
    "        # Convert character indices back to characters\n",
    "        chars = [int_to_char[idx] for idx in word_chars if idx != 0]\n",
    "        \n",
    "        if chars:  # only append if we have characters\n",
    "            words.append(''.join(chars))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered text: that my son and an 11 year old friend myself and my daughters 23 year old boyfriend went to see the movie the next day for a guys day out we had even more fun the second time around and everyone raved about it it's clean and delightful acted by a pre adolescent cast reminiscen of the tv classic freaks and geeks we all feel it will become a sleeper hit not unlike the freaks geeks which didn't survive its first season but sold out its dvd release do see it especially if you have boys and you'll find it stimulates conversati about fun and safety girls will love it because of the opportunit it affords to say boys are so weird don't miss it\n"
     ]
    }
   ],
   "source": [
    "# make sure you can recover the original text\n",
    "recovered_words = recover_text(x_train[0])\n",
    "recovered_text = \" \".join(recovered_words)\n",
    "print(\"Recovered text:\", recovered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, keras cannot handle our extra dimension gracefully, so we will do a trick.\n",
    "\n",
    "TODO\n",
    " - One-hot-encode characters for the input layer, so that the network accepts data in a shape `batch x word-seq-index x char-seq-index x char-encoding`. \n",
    " - Reshape this into `(batch x word-seq-index x char-seq-index * char-encodeing)` in the network\n",
    " - Use a recurrent network or conv net of your choice for the rest of the modeling\n",
    " - fit the model\n",
    "\n",
    "__NB__: one hot encode the characters on the fly (in the data generators) for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Reshape, LSTM, Dense, Dropout, TimeDistributed, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = UNK_CHAR + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">710</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">429,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m71\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m710\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m429,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">483,201</span> (1.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m483,201\u001b[0m (1.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">483,201</span> (1.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m483,201\u001b[0m (1.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "char_token_input = Input(shape=(MAX_SEQ_LEN_WORDS, MAX_CHARS_PER_WORD))\n",
    "\n",
    "# your model definition here\n",
    "embedded = TimeDistributed(Lambda(lambda x: tf.one_hot(tf.cast(x, 'int32'), MAX_CHARS)))(char_token_input)\n",
    "char_encoding_dim = MAX_CHARS\n",
    "reshaped = Reshape((MAX_SEQ_LEN_WORDS, -1))(embedded)\n",
    "x = LSTM(128, return_sequences=True)(reshaped)\n",
    "x = Dropout(0.5)(x)\n",
    "x = LSTM(64)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(char_token_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import funcy\n",
    "\n",
    "def train_gen(batch_size):\n",
    "    # your code here, be sure to one-hot encode characters on the fly\n",
    "    n_samples = len(x_train)\n",
    "    while True:\n",
    "        # Shuffle the data each epoch\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Generate batches\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            batch_x = x_train[batch_indices]\n",
    "            batch_y = y_train[batch_indices]\n",
    "            \n",
    "            yield batch_x, batch_y\n",
    "\n",
    "\n",
    "def val_gen(batch_size):\n",
    "    # your code here, be sure to one-hot encode characters on the fly\n",
    "    n_samples = len(x_test)\n",
    "    while True:\n",
    "        # Generate sequential batches for validation\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            \n",
    "            batch_x = x_test[start:end]\n",
    "            batch_y = y_test[start:end]\n",
    "            \n",
    "            yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 533ms/step - acc: 0.5245 - loss: 0.6889 - val_acc: 0.5886 - val_loss: 0.6599 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 503ms/step - acc: 0.6543 - loss: 0.6278 - val_acc: 0.7142 - val_loss: 0.5598 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 492ms/step - acc: 0.7130 - loss: 0.5723 - val_acc: 0.7298 - val_loss: 0.5290 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x377de78b0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "model.fit(\n",
    "    train_gen(batch_size),\n",
    "    validation_data=val_gen(batch_size),\n",
    "    epochs=5,\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    validation_steps=x_test.shape[0] // batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Discuss results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the advantages and disadvantages of using character-level word features\n",
    "\n",
    "**Character-level Word Features**\n",
    "\n",
    "Advantages:\n",
    "- Better handling of out-of-vocabulary (OOV) words\n",
    "    - Can capture partial word matches\n",
    "    - Can handle misspellings and variants\n",
    "    - More robust to rare words\n",
    "-  Parameter efficiency\n",
    "    - Vocabulary size is limited to character set (much smaller than word vocabulary)\n",
    "    - Shared parameters across similar words\n",
    "- Morphological awareness\n",
    "    - Can capture prefixes/suffixes\n",
    "    - Natural handling of word variations (e.g., \"jump\", \"jumped\", \"jumping\")\n",
    "\n",
    "Disadvantages:\n",
    "- Increased sequence length\n",
    "    - Each word becomes a sequence of characters\n",
    "    - More time steps for the model to process\n",
    "- More complex patterns to learn\n",
    "    - Model must learn both character-level and word-level patterns\n",
    "    - May require more training data\n",
    "    - May need deeper architectures\n",
    "\n",
    "### What are the advantages and disadvantages of the reshaping trick that we did\n",
    "\n",
    "**The Reshaping Trick**\n",
    "\n",
    "Advantages:\n",
    "- Framework compatibility\n",
    "    - Allows us to use standard Keras layers\n",
    "    - Avoids need for custom layer implementations\n",
    "- Computational efficiency\n",
    "    - Single matrix multiplication instead of nested operations\n",
    "    - Better parallelization potential\n",
    "\n",
    "Disadvantages:\n",
    "- Loss of hierarchical structure\n",
    "    - Flattens the character-level information\n",
    "    - May make it harder for model to learn natural character groupings\n",
    "- Less interpretable\n",
    "    - Harder to analyze what the model learns at character level\n",
    "    - Feature interactions become more opaque\n",
    "- Memory intensive\n",
    "    - One-hot encoding increases dimensionality significantly\n",
    "    - Full matrix must be stored in memory during forward/backward passes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Improving our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we don't need to use the trick to reshape our data.\n",
    "What we really want is a character-level convolution model that we can apply word by word. \n",
    "Luckily, keras has a Layer (really a layer wrapper) called `TimeDistributed`, which will\n",
    "apply an operation element by element in a sequence. \n",
    "\n",
    "TODO\n",
    " - create a model that is __only__ the character-level encoding of a word.\n",
    "   - accept a single word as an input (shape=`(MAX_CHARS_PER_WORD, MAX_CHARS)`)\n",
    "   - the model should reshape the data to `(MAX_CHARS_PER_WORD * MAX_CHARS)`\n",
    "   - after reshaping the model should apply several convultional blocks to find the features that best represent the word.\n",
    "   - output should be a single vector of shape `(hidden,)` (try 128)\n",
    " - Use the `TimeDistributed` function to apply this entire model, word by word, to a sequence\n",
    " - This will yield a `hidden`-d digest of every word\n",
    " - Use LSTMs and Dense layers to complete the network and classify the sentiment\n",
    "\n",
    "\n",
    "__NB__: a model can be applied like any other layer or operation: `ouput = my_model(input_tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">186,112</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m71\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m186,112\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m197,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785,793</span> (3.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m785,793\u001b[0m (3.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785,793</span> (3.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m785,793\u001b[0m (3.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# define the encoder\n",
    "#(this should work, but feel free to play with it)\n",
    "conv_layers = [\n",
    "    Reshape((MAX_CHARS_PER_WORD, MAX_CHARS)),\n",
    "    Conv1D(256, 3, activation=\"relu\"),\n",
    "    Conv1D(128, 3, activation=\"relu\"),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 2, activation=\"relu\"),\n",
    "    GlobalMaxPooling1D(),\n",
    "]\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "encoder = Sequential()\n",
    "for cl in conv_layers:\n",
    "    encoder.add(cl)\n",
    "\n",
    "\n",
    "input_shape = (MAX_SEQ_LEN_WORDS, MAX_CHARS_PER_WORD)\n",
    "inpt = Input(shape=input_shape)\n",
    "\n",
    "hidden = Lambda(lambda x: tf.one_hot(tf.cast(x, 'int32'), MAX_CHARS))(inpt)\n",
    "hidden = TimeDistributed(encoder)(hidden)\n",
    "\n",
    "# more model definition here\n",
    "hidden = LSTM(256, return_sequences=True)(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "hidden = LSTM(128)(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "hidden = Dense(64, activation='relu')(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "output = Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "model = Model(inputs=inpt, outputs=output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - acc: 0.4960 - loss: 0.6942 - val_acc: 0.5424 - val_loss: 0.6917 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - acc: 0.5684 - loss: 0.6751 - val_acc: 0.7509 - val_loss: 0.5235 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - acc: 0.7418 - loss: 0.5327 - val_acc: 0.8067 - val_loss: 0.4252 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "batch_size = 256\n",
    "history = model.fit(\n",
    "    train_gen(batch_size),\n",
    "    validation_data=val_gen(batch_size),\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(x_train) // batch_size,\n",
    "    validation_steps=len(x_test) // batch_size,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, our new model which we apply word by word can be thought of as a \n",
    "learned, non-linear (and somewhat complex) digest or \"embedding\" of each word\n",
    "that is learned from the characters that comprise the word.\n",
    "\n",
    "We can explore what this embedding gives us. \n",
    "\n",
    "TODO:\n",
    " - pick a few words and apply the model to them to get their vector representation\n",
    " - be sure to include words not in the vocabulary\n",
    " - pick a few words and calculate the similarity between their vector representation\n",
    " - comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jump</th>\n",
       "      <th>jumped</th>\n",
       "      <th>jumps</th>\n",
       "      <th>leap</th>\n",
       "      <th>leaps</th>\n",
       "      <th>tuesday</th>\n",
       "      <th>wednesday</th>\n",
       "      <th>bad</th>\n",
       "      <th>funny</th>\n",
       "      <th>hilarious</th>\n",
       "      <th>scary</th>\n",
       "      <th>exemplary</th>\n",
       "      <th>exemplerey</th>\n",
       "      <th>happy</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>wrogye</th>\n",
       "      <th>unwrogye</th>\n",
       "      <th>wrogyed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jump</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.764495</td>\n",
       "      <td>0.598764</td>\n",
       "      <td>0.647045</td>\n",
       "      <td>0.508508</td>\n",
       "      <td>0.482536</td>\n",
       "      <td>0.435503</td>\n",
       "      <td>0.679658</td>\n",
       "      <td>0.622398</td>\n",
       "      <td>0.245666</td>\n",
       "      <td>0.386312</td>\n",
       "      <td>0.303388</td>\n",
       "      <td>0.372699</td>\n",
       "      <td>0.596177</td>\n",
       "      <td>0.414633</td>\n",
       "      <td>0.551997</td>\n",
       "      <td>0.231156</td>\n",
       "      <td>0.542999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumped</th>\n",
       "      <td>0.764495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739674</td>\n",
       "      <td>0.516270</td>\n",
       "      <td>0.656761</td>\n",
       "      <td>0.545505</td>\n",
       "      <td>0.526633</td>\n",
       "      <td>0.703731</td>\n",
       "      <td>0.550829</td>\n",
       "      <td>0.257609</td>\n",
       "      <td>0.351338</td>\n",
       "      <td>0.237852</td>\n",
       "      <td>0.263628</td>\n",
       "      <td>0.732327</td>\n",
       "      <td>0.546637</td>\n",
       "      <td>0.573726</td>\n",
       "      <td>0.261591</td>\n",
       "      <td>0.569360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumps</th>\n",
       "      <td>0.598764</td>\n",
       "      <td>0.739674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604761</td>\n",
       "      <td>0.872101</td>\n",
       "      <td>0.736139</td>\n",
       "      <td>0.668463</td>\n",
       "      <td>0.617459</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.459188</td>\n",
       "      <td>0.574944</td>\n",
       "      <td>0.442298</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.784060</td>\n",
       "      <td>0.735388</td>\n",
       "      <td>0.680620</td>\n",
       "      <td>0.508542</td>\n",
       "      <td>0.686031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leap</th>\n",
       "      <td>0.647045</td>\n",
       "      <td>0.516270</td>\n",
       "      <td>0.604761</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710745</td>\n",
       "      <td>0.657450</td>\n",
       "      <td>0.588291</td>\n",
       "      <td>0.658806</td>\n",
       "      <td>0.676989</td>\n",
       "      <td>0.401585</td>\n",
       "      <td>0.618175</td>\n",
       "      <td>0.438449</td>\n",
       "      <td>0.497757</td>\n",
       "      <td>0.618177</td>\n",
       "      <td>0.507051</td>\n",
       "      <td>0.561576</td>\n",
       "      <td>0.407996</td>\n",
       "      <td>0.536329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leaps</th>\n",
       "      <td>0.508508</td>\n",
       "      <td>0.656761</td>\n",
       "      <td>0.872101</td>\n",
       "      <td>0.710745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792859</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>0.625241</td>\n",
       "      <td>0.707990</td>\n",
       "      <td>0.548274</td>\n",
       "      <td>0.679824</td>\n",
       "      <td>0.498977</td>\n",
       "      <td>0.530550</td>\n",
       "      <td>0.781736</td>\n",
       "      <td>0.752838</td>\n",
       "      <td>0.715136</td>\n",
       "      <td>0.590492</td>\n",
       "      <td>0.705173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tuesday</th>\n",
       "      <td>0.482536</td>\n",
       "      <td>0.545505</td>\n",
       "      <td>0.736139</td>\n",
       "      <td>0.657450</td>\n",
       "      <td>0.792859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816948</td>\n",
       "      <td>0.466394</td>\n",
       "      <td>0.803416</td>\n",
       "      <td>0.708286</td>\n",
       "      <td>0.831116</td>\n",
       "      <td>0.730272</td>\n",
       "      <td>0.723972</td>\n",
       "      <td>0.686308</td>\n",
       "      <td>0.772738</td>\n",
       "      <td>0.807921</td>\n",
       "      <td>0.747137</td>\n",
       "      <td>0.809694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wednesday</th>\n",
       "      <td>0.435503</td>\n",
       "      <td>0.526633</td>\n",
       "      <td>0.668463</td>\n",
       "      <td>0.588291</td>\n",
       "      <td>0.700500</td>\n",
       "      <td>0.816948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457778</td>\n",
       "      <td>0.725921</td>\n",
       "      <td>0.682577</td>\n",
       "      <td>0.707195</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.757291</td>\n",
       "      <td>0.634834</td>\n",
       "      <td>0.745216</td>\n",
       "      <td>0.812392</td>\n",
       "      <td>0.780464</td>\n",
       "      <td>0.844875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.679658</td>\n",
       "      <td>0.703731</td>\n",
       "      <td>0.617459</td>\n",
       "      <td>0.658806</td>\n",
       "      <td>0.625241</td>\n",
       "      <td>0.466394</td>\n",
       "      <td>0.457778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.609455</td>\n",
       "      <td>0.268065</td>\n",
       "      <td>0.355165</td>\n",
       "      <td>0.209031</td>\n",
       "      <td>0.260042</td>\n",
       "      <td>0.725315</td>\n",
       "      <td>0.517727</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.231749</td>\n",
       "      <td>0.461342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0.622398</td>\n",
       "      <td>0.550829</td>\n",
       "      <td>0.679487</td>\n",
       "      <td>0.676989</td>\n",
       "      <td>0.707990</td>\n",
       "      <td>0.803416</td>\n",
       "      <td>0.725921</td>\n",
       "      <td>0.609455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628628</td>\n",
       "      <td>0.860292</td>\n",
       "      <td>0.645266</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.712556</td>\n",
       "      <td>0.760877</td>\n",
       "      <td>0.841804</td>\n",
       "      <td>0.702459</td>\n",
       "      <td>0.819477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hilarious</th>\n",
       "      <td>0.245666</td>\n",
       "      <td>0.257609</td>\n",
       "      <td>0.459188</td>\n",
       "      <td>0.401585</td>\n",
       "      <td>0.548274</td>\n",
       "      <td>0.708286</td>\n",
       "      <td>0.682577</td>\n",
       "      <td>0.268065</td>\n",
       "      <td>0.628628</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758445</td>\n",
       "      <td>0.757707</td>\n",
       "      <td>0.766904</td>\n",
       "      <td>0.407102</td>\n",
       "      <td>0.684366</td>\n",
       "      <td>0.732405</td>\n",
       "      <td>0.784929</td>\n",
       "      <td>0.750654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scary</th>\n",
       "      <td>0.386312</td>\n",
       "      <td>0.351338</td>\n",
       "      <td>0.574944</td>\n",
       "      <td>0.618175</td>\n",
       "      <td>0.679824</td>\n",
       "      <td>0.831116</td>\n",
       "      <td>0.707195</td>\n",
       "      <td>0.355165</td>\n",
       "      <td>0.860292</td>\n",
       "      <td>0.758445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.765851</td>\n",
       "      <td>0.796123</td>\n",
       "      <td>0.555943</td>\n",
       "      <td>0.741954</td>\n",
       "      <td>0.824137</td>\n",
       "      <td>0.838842</td>\n",
       "      <td>0.798931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exemplary</th>\n",
       "      <td>0.303388</td>\n",
       "      <td>0.237852</td>\n",
       "      <td>0.442298</td>\n",
       "      <td>0.438449</td>\n",
       "      <td>0.498977</td>\n",
       "      <td>0.730272</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.209031</td>\n",
       "      <td>0.645266</td>\n",
       "      <td>0.757707</td>\n",
       "      <td>0.765851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.913328</td>\n",
       "      <td>0.388756</td>\n",
       "      <td>0.611034</td>\n",
       "      <td>0.674163</td>\n",
       "      <td>0.793534</td>\n",
       "      <td>0.687530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exemplerey</th>\n",
       "      <td>0.372699</td>\n",
       "      <td>0.263628</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.497757</td>\n",
       "      <td>0.530550</td>\n",
       "      <td>0.723972</td>\n",
       "      <td>0.757291</td>\n",
       "      <td>0.260042</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.766904</td>\n",
       "      <td>0.796123</td>\n",
       "      <td>0.913328</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.385585</td>\n",
       "      <td>0.638494</td>\n",
       "      <td>0.726414</td>\n",
       "      <td>0.827886</td>\n",
       "      <td>0.744962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>0.596177</td>\n",
       "      <td>0.732327</td>\n",
       "      <td>0.784060</td>\n",
       "      <td>0.618177</td>\n",
       "      <td>0.781736</td>\n",
       "      <td>0.686308</td>\n",
       "      <td>0.634834</td>\n",
       "      <td>0.725315</td>\n",
       "      <td>0.712556</td>\n",
       "      <td>0.407102</td>\n",
       "      <td>0.555943</td>\n",
       "      <td>0.388756</td>\n",
       "      <td>0.385585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.764617</td>\n",
       "      <td>0.654738</td>\n",
       "      <td>0.498364</td>\n",
       "      <td>0.620998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unhappy</th>\n",
       "      <td>0.414633</td>\n",
       "      <td>0.546637</td>\n",
       "      <td>0.735388</td>\n",
       "      <td>0.507051</td>\n",
       "      <td>0.752838</td>\n",
       "      <td>0.772738</td>\n",
       "      <td>0.745216</td>\n",
       "      <td>0.517727</td>\n",
       "      <td>0.760877</td>\n",
       "      <td>0.684366</td>\n",
       "      <td>0.741954</td>\n",
       "      <td>0.611034</td>\n",
       "      <td>0.638494</td>\n",
       "      <td>0.764617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819042</td>\n",
       "      <td>0.770136</td>\n",
       "      <td>0.800284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrogye</th>\n",
       "      <td>0.551997</td>\n",
       "      <td>0.573726</td>\n",
       "      <td>0.680620</td>\n",
       "      <td>0.561576</td>\n",
       "      <td>0.715136</td>\n",
       "      <td>0.807921</td>\n",
       "      <td>0.812392</td>\n",
       "      <td>0.499135</td>\n",
       "      <td>0.841804</td>\n",
       "      <td>0.732405</td>\n",
       "      <td>0.824137</td>\n",
       "      <td>0.674163</td>\n",
       "      <td>0.726414</td>\n",
       "      <td>0.654738</td>\n",
       "      <td>0.819042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855048</td>\n",
       "      <td>0.976910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unwrogye</th>\n",
       "      <td>0.231156</td>\n",
       "      <td>0.261591</td>\n",
       "      <td>0.508542</td>\n",
       "      <td>0.407996</td>\n",
       "      <td>0.590492</td>\n",
       "      <td>0.747137</td>\n",
       "      <td>0.780464</td>\n",
       "      <td>0.231749</td>\n",
       "      <td>0.702459</td>\n",
       "      <td>0.784929</td>\n",
       "      <td>0.838842</td>\n",
       "      <td>0.793534</td>\n",
       "      <td>0.827886</td>\n",
       "      <td>0.498364</td>\n",
       "      <td>0.770136</td>\n",
       "      <td>0.855048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrogyed</th>\n",
       "      <td>0.542999</td>\n",
       "      <td>0.569360</td>\n",
       "      <td>0.686031</td>\n",
       "      <td>0.536329</td>\n",
       "      <td>0.705173</td>\n",
       "      <td>0.809694</td>\n",
       "      <td>0.844875</td>\n",
       "      <td>0.461342</td>\n",
       "      <td>0.819477</td>\n",
       "      <td>0.750654</td>\n",
       "      <td>0.798931</td>\n",
       "      <td>0.687530</td>\n",
       "      <td>0.744962</td>\n",
       "      <td>0.620998</td>\n",
       "      <td>0.800284</td>\n",
       "      <td>0.976910</td>\n",
       "      <td>0.845746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                jump    jumped     jumps      leap     leaps   tuesday  \\\n",
       "jump        1.000000  0.764495  0.598764  0.647045  0.508508  0.482536   \n",
       "jumped      0.764495  1.000000  0.739674  0.516270  0.656761  0.545505   \n",
       "jumps       0.598764  0.739674  1.000000  0.604761  0.872101  0.736139   \n",
       "leap        0.647045  0.516270  0.604761  1.000000  0.710745  0.657450   \n",
       "leaps       0.508508  0.656761  0.872101  0.710745  1.000000  0.792859   \n",
       "tuesday     0.482536  0.545505  0.736139  0.657450  0.792859  1.000000   \n",
       "wednesday   0.435503  0.526633  0.668463  0.588291  0.700500  0.816948   \n",
       "bad         0.679658  0.703731  0.617459  0.658806  0.625241  0.466394   \n",
       "funny       0.622398  0.550829  0.679487  0.676989  0.707990  0.803416   \n",
       "hilarious   0.245666  0.257609  0.459188  0.401585  0.548274  0.708286   \n",
       "scary       0.386312  0.351338  0.574944  0.618175  0.679824  0.831116   \n",
       "exemplary   0.303388  0.237852  0.442298  0.438449  0.498977  0.730272   \n",
       "exemplerey  0.372699  0.263628  0.495845  0.497757  0.530550  0.723972   \n",
       "happy       0.596177  0.732327  0.784060  0.618177  0.781736  0.686308   \n",
       "unhappy     0.414633  0.546637  0.735388  0.507051  0.752838  0.772738   \n",
       "wrogye      0.551997  0.573726  0.680620  0.561576  0.715136  0.807921   \n",
       "unwrogye    0.231156  0.261591  0.508542  0.407996  0.590492  0.747137   \n",
       "wrogyed     0.542999  0.569360  0.686031  0.536329  0.705173  0.809694   \n",
       "\n",
       "            wednesday       bad     funny  hilarious     scary  exemplary  \\\n",
       "jump         0.435503  0.679658  0.622398   0.245666  0.386312   0.303388   \n",
       "jumped       0.526633  0.703731  0.550829   0.257609  0.351338   0.237852   \n",
       "jumps        0.668463  0.617459  0.679487   0.459188  0.574944   0.442298   \n",
       "leap         0.588291  0.658806  0.676989   0.401585  0.618175   0.438449   \n",
       "leaps        0.700500  0.625241  0.707990   0.548274  0.679824   0.498977   \n",
       "tuesday      0.816948  0.466394  0.803416   0.708286  0.831116   0.730272   \n",
       "wednesday    1.000000  0.457778  0.725921   0.682577  0.707195   0.718300   \n",
       "bad          0.457778  1.000000  0.609455   0.268065  0.355165   0.209031   \n",
       "funny        0.725921  0.609455  1.000000   0.628628  0.860292   0.645266   \n",
       "hilarious    0.682577  0.268065  0.628628   1.000000  0.758445   0.757707   \n",
       "scary        0.707195  0.355165  0.860292   0.758445  1.000000   0.765851   \n",
       "exemplary    0.718300  0.209031  0.645266   0.757707  0.765851   1.000000   \n",
       "exemplerey   0.757291  0.260042  0.690446   0.766904  0.796123   0.913328   \n",
       "happy        0.634834  0.725315  0.712556   0.407102  0.555943   0.388756   \n",
       "unhappy      0.745216  0.517727  0.760877   0.684366  0.741954   0.611034   \n",
       "wrogye       0.812392  0.499135  0.841804   0.732405  0.824137   0.674163   \n",
       "unwrogye     0.780464  0.231749  0.702459   0.784929  0.838842   0.793534   \n",
       "wrogyed      0.844875  0.461342  0.819477   0.750654  0.798931   0.687530   \n",
       "\n",
       "            exemplerey     happy   unhappy    wrogye  unwrogye   wrogyed  \n",
       "jump          0.372699  0.596177  0.414633  0.551997  0.231156  0.542999  \n",
       "jumped        0.263628  0.732327  0.546637  0.573726  0.261591  0.569360  \n",
       "jumps         0.495845  0.784060  0.735388  0.680620  0.508542  0.686031  \n",
       "leap          0.497757  0.618177  0.507051  0.561576  0.407996  0.536329  \n",
       "leaps         0.530550  0.781736  0.752838  0.715136  0.590492  0.705173  \n",
       "tuesday       0.723972  0.686308  0.772738  0.807921  0.747137  0.809694  \n",
       "wednesday     0.757291  0.634834  0.745216  0.812392  0.780464  0.844875  \n",
       "bad           0.260042  0.725315  0.517727  0.499135  0.231749  0.461342  \n",
       "funny         0.690446  0.712556  0.760877  0.841804  0.702459  0.819477  \n",
       "hilarious     0.766904  0.407102  0.684366  0.732405  0.784929  0.750654  \n",
       "scary         0.796123  0.555943  0.741954  0.824137  0.838842  0.798931  \n",
       "exemplary     0.913328  0.388756  0.611034  0.674163  0.793534  0.687530  \n",
       "exemplerey    1.000000  0.385585  0.638494  0.726414  0.827886  0.744962  \n",
       "happy         0.385585  1.000000  0.764617  0.654738  0.498364  0.620998  \n",
       "unhappy       0.638494  0.764617  1.000000  0.819042  0.770136  0.800284  \n",
       "wrogye        0.726414  0.654738  0.819042  1.000000  0.855048  0.976910  \n",
       "unwrogye      0.827886  0.498364  0.770136  0.855048  1.000000  0.845746  \n",
       "wrogyed       0.744962  0.620998  0.800284  0.976910  0.845746  1.000000  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# here are some good words to try\n",
    "# add a few others\n",
    "words = [\n",
    "    \"jump\",\n",
    "    \"jumped\",\n",
    "    \"jumps\",\n",
    "    \"leap\",\n",
    "    \"leaps\",\n",
    "    \"tuesday\",\n",
    "    \"wednesday\",\n",
    "    \"bad\",\n",
    "    \"funny\",\n",
    "    \"hilarious\",\n",
    "    \"scary\",\n",
    "    \"exemplary\",\n",
    "    \"exemplerey\", #spelling error\n",
    "    \"happy\", \n",
    "    \"unhappy\",  # added a prefix\n",
    "    \"wrogye\", # made up\n",
    "    \"unwrogye\", # opposite of made up\n",
    "    \"wrogyed\", # past tense made up\n",
    "]\n",
    "\n",
    "# your code to get the encoder inputs\n",
    "# don't forget to pad the encoder inputs\n",
    "# your code here\n",
    "encoder_inputs = []\n",
    "for word in words:\n",
    "    # Convert to character sequence\n",
    "    char_seq = []\n",
    "    for char in word.lower():\n",
    "        if char in char_to_int:\n",
    "            char_seq.append(char_to_int[char])\n",
    "        else:\n",
    "            char_seq.append(UNK_CHAR)\n",
    "    \n",
    "    # Pad to MAX_CHARS_PER_WORD\n",
    "    if len(char_seq) > MAX_CHARS_PER_WORD:\n",
    "        char_seq = char_seq[:MAX_CHARS_PER_WORD]\n",
    "    else:\n",
    "        char_seq = char_seq + [0] * (MAX_CHARS_PER_WORD - len(char_seq))\n",
    "    \n",
    "    # One-hot encode\n",
    "    char_one_hot = tf.one_hot(char_seq, MAX_CHARS)\n",
    "    encoder_inputs.append(char_one_hot)\n",
    "\n",
    "# Convert to numpy array\n",
    "encoder_inputs = np.array(encoder_inputs)\n",
    "\n",
    "extracted_word_vectors = encoder.predict(encoder_inputs, batch_size=batch_size)\n",
    "cs = cosine_similarity(extracted_word_vectors)\n",
    "similarity_df = pd.DataFrame(cs, index=words, columns=words)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "Tense Variants:\n",
    "- jump/jumped: 0.764 (high similarity for past tense)\n",
    "- jump/jumps: 0.599 (moderate similarity for plural)\n",
    "- leap/leaps: 0.711 (similar pattern to jump/jumps)\n",
    "\n",
    "Semantic Relationships:\n",
    "- funny/hilarious: 0.629 (moderate similarity for synonyms)\n",
    "- bad/scary: 0.355 (low similarity for different negative words)\n",
    "\n",
    "Spelling Differences:\n",
    "- exemplary/exemplerey: 0.913 (very high similarity despite misspelling)\n",
    "\n",
    "Prefixs:\n",
    "- happy/unhappy: 0.765 (high similarity despite opposite meaning)\n",
    "- wrogye/unwrogye: 0.855 (similar pattern with made-up words)\n",
    "\n",
    "Days of Week:\n",
    "- tuesday/wednesday: 0.817 (high similarity for related concepts)\n",
    "\n",
    "Overall:\n",
    "- Tense and spelling variations are captured well\n",
    "- The model learned that adding prefixes (un-) or suffixes (-ed) creates related words\n",
    "- Made-up words follow similar patterns to real words\n",
    "- Similar word types (like days of week) cluster together\n",
    "- Character-level patterns are more influential than semantic meaning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
