{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: More Powerful RNNs (20%)\n",
    "RNNs are powerful tools for processing sequences. \n",
    "In this problem we'll explore a powerful variant/addition.\n",
    "\n",
    "\n",
    "While powerful sequences processors, RNNs work one element at a time.\n",
    "Therefore, RNNs can \"miss out\" on understanding a given word, if important context comes after \n",
    "that word in the sequence. \n",
    "\n",
    "\n",
    "Also, notice, that while reading backwards is more difficult, we can still learn a lot. For example\n",
    "\n",
    " - ```sanitizer hand bought and store the to went I yesterday```\n",
    "\n",
    "\n",
    "In this problem, we'll make more powerful RNNs that can gain additional context by reading \n",
    "documents from beginning to end, as well as from end to beginning. \n",
    "\n",
    "\n",
    "## Part 0 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text\n",
    "\n",
    "\n",
    "#(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text(...\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the text keeping the 50k most common words\n",
    "# turn text into integer sequences\n",
    "# pad the sequences to 125 elements each\n",
    "\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Running in reverse\n",
    "Create two identical LSTM models and fit them on the IMDB data. \n",
    " - For the first, use our regular recipe\n",
    " - For the second, flip every document around so that the sequences are processed in reverse\n",
    "   - the easiest way to flip the data is to use `np.fliplr` on the data as you feed it to your model\n",
    "\n",
    "\n",
    "Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "\n",
    "# text_input = Input(shape=(MAX_SEQ_LEN,)....\n",
    "# the model should\n",
    "# - embed the word sequences to make a dense representation\n",
    "# - use an LSTM (or two) to process the sequence\n",
    "# - use dropout to help with overfitting\n",
    "# - use one (or more) dense layers to get the output into the right shape\n",
    "\n",
    "# compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy\n",
    "\n",
    "\n",
    "# here is a function you might find handy\n",
    "def gen_data(x, y, batch_size, fliplr=False):\n",
    "    \"\"\"Generate data for our recurrent network\n",
    "    \n",
    "    Args:\n",
    "        x: model inputs encoded as integer sequences\n",
    "        y: targets (0/1)\n",
    "        batch size: size of batch to yield\n",
    "        fliplr: whether to flip the input data\n",
    "    \"\"\"\n",
    "    idx = np.arange(x.shape[0])\n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        for chunk in funcy.chunks(batch_size, idx):\n",
    "            _x = x[chunk]\n",
    "            _y = y[chunk]\n",
    "            if fliplr:\n",
    "                _x = np.flip(_x, axis=-1)\n",
    "            yield _x, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a batch of data with fliplr=True\n",
    "# turn it back into words (you can use `tokenizer.word_index`)\n",
    "# make sure it looks like normal, backword english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model, preserving the original order of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an identical model and fit it, this time flipping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The bidirectional LSTM\n",
    "The idea behdind the bidirectional LSTM is to combine a forward and backward LSTM \n",
    "in order to reap the benefits of each. In such a scheme, we use two different LSTM\n",
    "one that processes the sequence in the forward direction and one that processes it \n",
    "backward. We combine (concatenate) their outputs to get a more complex and hopefully \n",
    "better represntation of the sequence we're trying to process. \n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "![bidrectional lstm](../lectures/09_intro_to_neural/bi-lstm.jpg)\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $\n",
    "\n",
    "TODO: Make a model\n",
    " - One branch will take the embedded sequence and process it with an LSTM\n",
    " - Another branch will use a `Lambda` layer to flip the sequence and then process it with a different LSTM\n",
    "   - __NB__: `keras.backed.reverse` is a helpful tensor operation\n",
    " - Concatenate the branches together\n",
    " - Use 1-2 dense layers to get the output into the right shape. \n",
    "\n",
    "This recipe will not require flipping the data before it goes into the model. \n",
    "\n",
    "### Plot the model in the notebook to confirm two branches that get combined\n",
    "\n",
    "NB: you could also make __two__ different inputs to the model (one that gets flipped and one that doesn't) and process both with forward LSTMs. This approach is equivalent, but likely harder to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# model code here\n",
    "text_input = Input(shape=(MAX_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "# embed the sequence\n",
    "# process the sequence with two different branches\n",
    "#  - regular (forward) LSTM\n",
    "#  - a layer that first flips the sequence and then an LSTM\n",
    "# Dense layers to get the output in the right shape\n",
    "\n",
    "embedded_sequence = Embedding(tokenizer.num_words + 1, 100)(text_input)\n",
    "\n",
    "flip_layer = Lambda(... name=\"flipper\")  # fix this line\n",
    "flipped_sequence = flip_layer(embedded_sequence)\n",
    "\n",
    "# more modeling code here\n",
    "\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from tensorflow.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def plot_model_in_notebook(model):\n",
    "    return Image(model_to_dot(model, show_shapes=True).create(prog='dot', format='png'))\n",
    "\n",
    "plot_model_in_notebook(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fit the model\n",
    " - fit the model feeding in forward sequences. \n",
    " - repeat for backward sequences (remember to re-initialize the model)\n",
    "\n",
    "Comment on the results\n",
    " - how is the performance?\n",
    " - does the direction matter when we use a bidirectional lstm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize the model\n",
    "# (just re-make it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model with reversed sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Use the keras builtin\n",
    "In fact, keras has a builtin called `Bidirectional` which takes an RNN as an input\n",
    "and returns a bidirectional version as the output. \n",
    "\n",
    "Use this to confirm that it works correctly. \n",
    "Comment on the number of parameters in the bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "# make a model with a single Bidirectional LSTM using the keras builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Data Augmentation (10%)\n",
    "Another way to prevent overfitting is to augment the data.\n",
    "More data is always better, but sometimes we can't easily collect more data. \n",
    "A set of techniques to turn our current data set into a bigger one are called `data augmentation`. \n",
    "\n",
    "Data augmentation can take many forms, and are specific to the data and problem being solve. \n",
    "For example, in an image recognition problem, it is very common to rotate, crop, and zoom\n",
    "images to generate new ones. We can think of this as a form of regularization, since we are, \n",
    "in some sense, forcing a pentalty if the model does not have rotation /scale invariance. \n",
    "In speech recognition, this can take the form of distoring an audio clip to have higher pitches\n",
    "(e.g. speeding it up), which should \"teach\" a model that it should be pitch invariant. \n",
    "\n",
    "In text classification problems, it typcially a little more difficult to augment data. \n",
    "One common method is known as back-translation: if an autmated machine translation model is \n",
    "available, we can translate our text into one language (e.g. english to french) and then back\n",
    "to the original language again (french to english). This typically yields a very similar \n",
    "piece of text to the original, but with different words. \n",
    "\n",
    "Here we'll try a simpler approach. In a low-data setting, we do not want the model to be too sensitive\n",
    "to any given word. Accordingly, we can augment our data by creating additional examples which are \n",
    "identical to our current example, but with some words set to unknown words.\n",
    "\n",
    "This problem is more opened ended.\n",
    "TODO:\n",
    " - Load and process the IMDB sentiment data\n",
    " - train two identical models. In one of them, replace some fraction of the words with an unknown token (try `MAX_WORDS + 1`) as the integer encoding for the unknown token)\n",
    " - Discuss the results. \n",
    "   - What is the result of dropping words.\n",
    "   - How does it compare to the image / audio methods described here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text\n",
    "\n",
    "\n",
    "#(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text(...\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the text keeping the 50k most common words\n",
    "# turn text into integer sequences\n",
    "# pad the sequences to 125 elements each\n",
    "\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKNOWN_WORD = tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Dropout, GRU, Embedding, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "\n",
    "# modeling code here\n",
    "# consider using\n",
    "# - the right input shape\n",
    "# - an embedding that takes us from an integer encoded word \n",
    "#   - NB max value is now tokenizer.num_words + 2 because of the unknown word\n",
    "# - 1-2 LSTMs or GRUs\n",
    "#  - remember, the last one needs `return_sequences=False`, the rest need it True\n",
    "# - a Dense layer\n",
    "\n",
    "\n",
    "# model code here\n",
    "text_input = Input(shape=(MAX_SEQ_LEN,), dtype=\"int32\")\n",
    "embedded_sequence = Embedding(tokenizer.num_words + 2, 100)(text_input)\n",
    "# more code here\n",
    "\n",
    "model = Model(...)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy\n",
    "def train_data_gen(batch_size, dropout_frac=0.):\n",
    "    \"\"\"training data geneator\n",
    "    \n",
    "    Args:\n",
    "        batch size: size of batch to yield\n",
    "        dropout_frac: fraction of words to replace with UKNOWN_WORD\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# use `dropout_frac=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# identical model code here\n",
    "model2 = Model(...)\n",
    "model2.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "# fit it with dropout_frac > 0 (try 0.2 or 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: The Company Name Comparer (40%)\n",
    "A common problem in NLP for finance is automatically comparing two company names to see if they are the same. \n",
    "Intuitively, this is a problem that is fairly easy for humans: \n",
    " - `S and P Global` and `S&P Global`\n",
    " - `JPMorgan` and `JP Morgan`\n",
    " - `Google Inc.` and `Google`\n",
    "\n",
    "However, the number of ways in which companies can be represented, and the intricacies of corporate \n",
    "structure make it difficult to craft rules to capture this behavior.\n",
    "This makes the problem perfect for a machine learning approach. \n",
    "We will see that it is particularly well suited to deep learning, since some \n",
    "features will be difficult to craft by hand. \n",
    "In the end of this problem we will build a neural network that accepts TWO strings as input\n",
    "and produces a single floating point output, which is the probability that\n",
    "the two organizations refer to the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the data\n",
    "Wikipedia is an excellent source of natural language data. \n",
    "We need a data set of many ways that we can refer to the same company.\n",
    "Here we will use the page redicts- if a certain page automatically \n",
    "redirects to a second page, we can say with high confidence that the \n",
    "title of the first page unambiguously refers to the same thing as \n",
    "the second. The lack of ambiguity is important! For example, we \n",
    "do not want `Amazon` to be another way to refer to `amazon.com` since\n",
    "it can also refer to the `amazon river`. See [here](https://en.wikipedia.org/wiki/Amazon) for more.\n",
    "\n",
    "In `org_redirects.csv` you will find data on the page redirects that have been \n",
    "filtered down to everything considered an organization (this is not only companies, \n",
    "but it's close). The file has columns\n",
    " - `souce_id` a page id for the source page\n",
    " - `source_title`: the page title for the source\n",
    " - `target_id`: the page id for the page to which the source will redirect\n",
    " - `target_title`: the title of the page to which the source redirects\n",
    " - `target_qid`: the id of the page in wikidata, the accompanying knowledge graph to wikipedia\n",
    " - `edge_type`: the type of organization the target item has in wikidata\n",
    "\n",
    "For our purposes, we can consider the `source_title` and `target_title` two \n",
    "ways to unambiguously refer to the same organization. We will teach an algorithm\n",
    "to recognize the patterns in these data.\n",
    "\n",
    "\n",
    "## TODO\n",
    " - ## 1. load the data\n",
    " - ## 2. Explore the data: \n",
    "   - ## what is the distribution of words / characters?\n",
    "   - ## are there any tokens that are particularly common?\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/org_redirects.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Process, Clean, and Filter\n",
    "We need to have the data in slightly more usable form. \n",
    "For simplicity, we will want to only deal with lowercase characters. \n",
    "Additionally, we will want to remove special characters from the source\n",
    "and target titles.\n",
    "\n",
    "Additionally, some redirects require special knowledge, that we don't expect the \n",
    "algorithm to be able to learn. \n",
    "\n",
    "## TODO: \n",
    " - create two new columns `source_clean` and `target_clean` which are lowercased versions of the original data with the characters `,_/` replaced by spaces\n",
    " - NB: remember to drop rows that are now duplicates because of the string cleaning we've done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # probably a useful import \n",
    "\n",
    "def normalize_string(s):\n",
    "    # your code here\n",
    "    # normalize the data by lowercasing, and replace certain characters with whitespace\n",
    "    # careful, replace 1 or more whitespace characters with a single whitespace\n",
    "\n",
    "df[\"clean_source\"] = df[\"source_title\"].apply(normalize_string)\n",
    "df[\"clean_target\"] = df[\"target_title\"].apply(normalize_string)\n",
    "\n",
    "# drop the duplicates here. \n",
    "# you should have approx 1.1 million examples before dropping duplicates\n",
    "# and approx 1.04 million examples after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Make training data\n",
    "We only have positive examples, ie two ways of referring to the same organization. \n",
    "We need negative examples in order for the network to learn what is not considered the same orgnanization. \n",
    "There are many ways to do this, but we will use the most simple one- random examples from the training data. \n",
    "\n",
    "## TODO:\n",
    " - ## make another copy of our training data\n",
    " - ## shuffle either the source or target columns so that they no longer line up\n",
    " - ## add these new data as negative examples to the training data\n",
    " - ## split the data into two: train and test \n",
    "\n",
    "\n",
    "### Now we have a dataset with approximately 50% positive examples\n",
    "\n",
    "## TODO (answers these questions in text)\n",
    " - ## What are the benefits of this approach\n",
    " - ## What are the downsides of this appoach\n",
    " - ## What would be a more sophisticated way to generate negative examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangle the data here\n",
    "# you should have approximately twice as many examples now\n",
    "# the mean `y` value should be about 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split randomly into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Turn the data into numpy arrays\n",
    "We will use a character level representation, since organizations have too\n",
    "many special words to use a word-level representation. \n",
    "\n",
    "Eventually, we will build a neural network that accepts TWO strings as input\n",
    "and produces a single floating point output, which is the probability that\n",
    "the source and target refer to the same thing.\n",
    "\n",
    "### TODO:\n",
    " - turn the data into sequences of characters that are integer encoded\n",
    "   - use the `Tokenizer` with `char_level=True`\n",
    "   - use `tokenizer.text_to_sequences` to turn the characters into integer-encoded sequences\n",
    " - pad the sequences to a constant length of 30 characters\n",
    " - write two functions, `train_gen` and `test_gen` which yield data for the network in the right form.\n",
    "   - data should be yielded as `yield [input_1, input_2], output`\n",
    "   - you will likely find it easier to have these functions one-hot-encode the characters on the fly instead of holding very large numpy arrays in memory.\n",
    " - test your functions and make sure you can recover the original input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_iterator():\n",
    "    \"\"\"This is a helpful iterator\n",
    "    \n",
    "    It will yield all the data in `clean_source` and `clean_target` \n",
    "    in the training data, without making another copy\n",
    "    \"\"\"\n",
    "    for item in df_train[\"clean_source\"]:\n",
    "        yield item\n",
    "    for item in df_train[\"clean_target\"]:\n",
    "        yield item\n",
    "\n",
    "\n",
    "tok = Tokenizer(lower=True, char_level=True, num_words=140)\n",
    "tok.fit_on_texts(text_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for both train and test\n",
    "#   for the src and target\n",
    "#   - turn each into integer sequence\n",
    "#   - pad the sequences \n",
    "# make y values for both train and test\n",
    "print(x_train_targ.shape, x_test_targ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = tok.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import funcy\n",
    "\n",
    "\n",
    "def train_gen(batch_size):\n",
    "    # your code here\n",
    "    # the last line should probably look like\n",
    "    #    `yield [batch_src, batch_targ], batch_y`\n",
    "    \n",
    "    # don't forget to one hot encode the characaters\n",
    "    #   keras.utils.to_categorical is helpful (num_classes=MAX_CHARS)\n",
    "    \n",
    "    # if you're feeling enterprising, you can randomly flip\n",
    "    # the source and target data, which serves as a way to augment\n",
    "    # the training data. Consider                \n",
    "#             if np.random.rand() > 0.5:\n",
    "#                 batch_src, batch_targ = batch_targ, batch_src\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "def val_gen(batch_size):\n",
    "    # your code here, it should be very similar to the function above\n",
    "    # no need to flip the src and targets randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shapes, make sure you can recover the data and it looks normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Build and fit a model\n",
    "The beauty of keras is that it is flexible enough to allow us to do almost arbitrary matrix operations. \n",
    "In this model we will use several `Conv1D`-`MaxPooling1D` combinations to\n",
    "encode or process the organization names. However, we will use the __SAME__ blocks for both \n",
    "the source and target names, in order to \"force\" the network to learn operations that\n",
    "distill the strings down into features that make them easy to compare.\n",
    "\n",
    "Build a keras model that \n",
    " - has __two__ text inputs\n",
    " - has several conv/pooling blocks to process the inputs\n",
    " - uses the __same__ blocks to process both of the text inputs\n",
    " - concatentates the result of the convolutions together into one vector\n",
    " - use one or more `Dense` layers to output a single (`float`), which is the probability that the two organizations are the same\n",
    "\n",
    "Hint: `keras.models.Model` can accept lists of inputs and outputs in addition to single tensors. \n",
    " - `model = Model([input_1, input_2], output)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    GlobalMaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "do = 0.05\n",
    "\n",
    "\n",
    "\n",
    "left = Input(shape=(MAX_SEQ_LEN, MAX_CHARS))\n",
    "right = Input(shape=(MAX_SEQ_LEN, MAX_CHARS))\n",
    "\n",
    "\n",
    "blocks = [\n",
    "    Conv1D(128, 5, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(128, 5, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(64, 5, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPooling1D(3),\n",
    "    Conv1D(64, 5, padding=\"same\", activation=\"relu\"),\n",
    "    GlobalMaxPooling1D()\n",
    "]\n",
    "\n",
    "def encode(inpt):\n",
    "    x = blocks[0](inpt)\n",
    "    for block in blocks[1:]:\n",
    "        x = block(x)\n",
    "    return x\n",
    "\n",
    "left_stream = encode(left)\n",
    "right_stream = encode(right)\n",
    "# TODO:\n",
    "# - concatenate right and left streams\n",
    "# - use 2-3 dense blocks to get the output into the right shape \n",
    "#    this is a binary classification problem\n",
    "# - compile your model\n",
    "\n",
    "# your code here\n",
    "\n",
    "model = Model([left, right], your_output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# user these callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis\n",
    "We want to see how our new model works on unseen text. \n",
    "Write a function\n",
    "```python\n",
    "def compare_inputs(a, b):\n",
    "    \"\"\"takes in two strings and returns a float\"\"\"\n",
    "    pass\n",
    "```\n",
    "that can use the model to compare two strings. \n",
    "\n",
    "TODO\n",
    " - Come up with at least 10 examples of your own and comment on the results\n",
    "\n",
    "TODO: How would this be done without deep learning? \n",
    " - Can we use the same operations on both the source and target?\n",
    " - How would we need to combine representations that come from the features we engineer from the two inputs?\n",
    " - If we had hand-engineered features, how would we add them to this network?\n",
    "\n",
    "(answer in text below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_inputs(a, b):\n",
    "    # take two strings, process them the same way we process our training data\n",
    "    # use `model.predict` to get a probability that they are the same\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in [\n",
    "    (\"The college of Mann\", \"University of mann\"),\n",
    "    (...) # add lost more examples here\n",
    "]:\n",
    "    print(\"\\\"{}\\\" and \\\"{}\\\": {:.4f}\".format(pair[0], pair[1], compare_inputs(*pair)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put analysis / comments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Character level word features (30%)\n",
    "In class we showed that neural networks can learn everything from character-level representations.\n",
    "Recall the potential advantages\n",
    " - We are much less likely to see unknown characters\n",
    " - We don't have to worry about word tokenization\n",
    " - Maybe `jump` and `jumps` can \"share\" a lot of the representation\n",
    " - There's nothing special about a space \n",
    "\n",
    "# $ \\\\ $\n",
    "In fact, there is perhaps something special about a space. \n",
    "While the network can, in principle, learn the concept of a word boundary,\n",
    "we may be able to help the network learn by telling it that words boundaries are\n",
    "an important concept. This is especially true in smaller-data problems.\n",
    "# $ \\\\ $\n",
    "In this problem we'll seek to remedy the first and third bullets above by encoding\n",
    "every __word__ as a sequence of characters. This should allow for the network to implicitly\n",
    "learn similarities between words with similar letters. In some sense, we can think of \n",
    "such an encoding as character-level method for learning word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the data\n",
    " - ## Load the imdb data as we did in class. \n",
    " - ## Tokenize it into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "%pylab inline\n",
    "np.random.seed(1234)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text\n",
    "\n",
    "\n",
    "#(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text(...\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the text keeping the 50k most common words\n",
    "# turn text into integer sequences\n",
    "# pad the sequences to 125 elements each\n",
    "\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Wrangle the data into the correct representation\n",
    "Our goal is to represent the data as character-level features, word by word. \n",
    "This means we will require the data to be in a shape\n",
    " - ### batch x word-position x character\n",
    "\n",
    "which is another dimension!\n",
    "### 1a\n",
    " - create a valid set of characters from `string.ascii_lowercase + string.digits + string.punctuation`\n",
    " - create character to int and int to character lookups\n",
    " - create word to int and int to word lookups\n",
    "\n",
    "### 1b\n",
    " - for every word (int) in a sequence, resolve it to a list of integers, which encode characters\n",
    " - pad each character sequence (which encodes a single word) to a constant length of 10 chars / word\n",
    "\n",
    "### 1c\n",
    " - Pad every word sequence (each now encoded as a list of integer sequences) to 125 words\n",
    "\n",
    "This should yield training data of the shape `(25000, 125, 10)`  `(examples x words x characters)` of type `int32`\n",
    "\n",
    "### 1d\n",
    " - make sure you can recover a coherent IMDB review from an element in `x_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "valid_chars = string.ascii_lowercase + string.digits + string.punctuation\n",
    "valid_chars = list(set(valid_chars))\n",
    "char_to_int = {c: i + 1 for i, c in enumerate(valid_chars)}  # save 0 for padding\n",
    "int_to_char = ...\n",
    "\n",
    "word_to_int = ...\n",
    "int_to_word = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS_PER_WORD = 10\n",
    "MAX_SEQ_LEN_WORDS = 125\n",
    "UNK_CHAR = len(valid_chars) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_char_seq(word):\n",
    "    # your code here\n",
    "\n",
    "\n",
    "def word_seq_to_char_seq(word_seq):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "char_seqs_train = [word_seq_to_char_seq(ts) for ts in train_sequences]\n",
    "x_train = pad_sequences(char_seqs_train, maxlen=MAX_SEQ_LEN_WORDS)\n",
    "\n",
    "char_seqs_test = [word_seq_to_char_seq(ts) for ts in test_sequences]\n",
    "x_test = pad_sequences(char_seqs_test, maxlen=MAX_SEQ_LEN_WORDS)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you can recover the original text\n",
    "\" \".join(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create a network\n",
    "Sadly, keras cannot handle our extra dimension gracefully, so we will do a trick.\n",
    "\n",
    "### TODO\n",
    " - One-hot-encode characters for the input layer, so that the network accepts data in a shape `batch x word-seq-index x char-seq-index x char-encoding`. \n",
    " - Reshape this into `(batch x word-seq-index x char-seq-index * char-encodeing)` in the network\n",
    " - Use a recurrent network or conv net of your choice for the rest of the modeling\n",
    " - fit the model\n",
    "\n",
    "__NB__: one hot encode the characters on the fly (in the data generators) for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Reshape, ...\n",
    "...\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = UNK_CHAR + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "char_token_input = Input(...\n",
    "# your model definition here\n",
    "\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import funcy\n",
    "\n",
    "def train_gen(batch_size):\n",
    "    # your code here, be sure to one-hot encode characters on the fly\n",
    "\n",
    "\n",
    "def val_gen(batch_size):\n",
    "    # your code here, be sure to one-hot encode characters on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "model.fit(\n",
    "    train_gen(batch_size),\n",
    "    validation_data=val_gen(batch_size),\n",
    "    epochs=5,\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    validation_steps=x_test.shape[0] // batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Discuss results\n",
    " - What are the advantages and disadvantages of using character-level word features\n",
    " - What are the advantages and disadvantages of the reshaping trick that we did\n",
    "\n",
    "Put your answers here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "## Part 4: Improving our model\n",
    "\n",
    "In fact, we don't need to use the trick to reshape our data.\n",
    "What we really want is a character-level convolution model that we can apply word by word. \n",
    "Luckily, keras has a Layer (really a layer wrapper) called `TimeDistributed`, which will\n",
    "apply an operation element by element in a sequence. \n",
    "\n",
    "### TODO\n",
    " - create a model that is __only__ the character-level encoding of a word.\n",
    "   - accept a single word as an input (shape=`(MAX_CHARS_PER_WORD, MAX_CHARS)`)\n",
    "   - the model should reshape the data to `(MAX_CHARS_PER_WORD * MAX_CHARS)`\n",
    "   - after reshaping the model should apply several convultional blocks to find the features that best represent the word.\n",
    "   - output should be a single vector of shape `(hidden,)` (try 128)\n",
    " - Use the `TimeDistributed` function to apply this entire model, word by word, to a sequence\n",
    " - This will yield a `hidden`-d digest of every word\n",
    " - Use LSTMs and Dense layers to complete the network and classify the sentiment\n",
    "\n",
    "\n",
    "__NB__: a model can be applied like any other layer or operation: `ouput = my_model(input_tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "\n",
    "# define the encoder\n",
    "#(this should work, but feel free to play with it)\n",
    "conv_layers = [\n",
    "    Reshape((MAX_CHARS_PER_WORD, MAX_CHARS)),\n",
    "    Conv1D(256, 3, activation=\"relu\"),\n",
    "    Conv1D(128, 3, activation=\"relu\"),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 2, activation=\"relu\"),\n",
    "    GlobalMaxPooling1D(),\n",
    "]\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "encoder = Sequential()\n",
    "for cl in conv_layers:\n",
    "    encoder.add(cl)\n",
    "\n",
    "\n",
    "input_shape = (...\n",
    "inpt = Input(shape=input_shape)\n",
    "hidden = Reshape(...\n",
    "hidden = TimeDistributed(encoder)(hidden)\n",
    "# more model definition here\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis\n",
    "In fact, our new model which we apply word by word can be thought of as a \n",
    "learned, non-linear (and somewhat complex) digest or \"embedding\" of each word\n",
    "that is learned from the characters that comprise the word.\n",
    "\n",
    "We can explore what this embedding gives us. \n",
    "\n",
    "### TODO:\n",
    " - pick a few words and apply the model to them to get their vector representation\n",
    " - be sure to include words not in the vocabulary\n",
    " - pick a few words and calculate the similarity between their vector representation\n",
    " - comment on the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# here are some good words to try\n",
    "# add a few others\n",
    "words = [\n",
    "    \"jump\",\n",
    "    \"jumped\",\n",
    "    \"jumps\",\n",
    "    \"leap\",\n",
    "    \"leaps\",\n",
    "    \"tuesday\",\n",
    "    \"wednesday\",\n",
    "    \"bad\",\n",
    "    \"funny\",\n",
    "    \"hilarious\",\n",
    "    \"scary\",\n",
    "    \"exemplary\",\n",
    "    \"exemplerey\", #spelling error\n",
    "    \"happy\", \n",
    "    \"unhappy\",  # added a prefix\n",
    "    \"wrogye\", # made up\n",
    "    \"unwrogye\", # opposite of made up\n",
    "    \"wrogyed\", # past tense made up\n",
    "]\n",
    "\n",
    "# your code to get the encoder inputs\n",
    "# don't forget to pad the encoder inputs\n",
    "# your code here\n",
    "# ...\n",
    "extracted_word_vectors = encoder.predict(encoder_inputs, batch_size=batch_size)\n",
    "cs = cosine_similarity(extracted_word_vectors)\n",
    "pd.DataFrame(cs, index=words, columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
