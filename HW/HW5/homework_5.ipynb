{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Affinity Propagation\n",
    "We mostly looked at clustering methods which found cluster centers by taking the mean of all the points in the cluster. \n",
    "While this is reasonable, it means that the cluster center itself may not be a valid data point, especially for integer-valued data like the bag of words representation of text.\n",
    "\n",
    "Another approach is to find __exemplar__ data points that are particularly representative of the rest of the data in the cluster, and to use those as cluster centers.\n",
    "\n",
    "One such algorithm is affinity propagation. The original paper from 2007 can be found [here](http://utstat.toronto.edu/reid/sta414/frey-affinity.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Read the paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the advantages\n",
    "### Two of the advantages of affinity propagation include:\n",
    " - ### cluster centers are actual data points\n",
    " - ### it only requires that we have pre-computed pairwise similarity between data points\n",
    "\n",
    "# $ \\\\ $\n",
    "### Give examples of cases where each of these advantages are desirable.\n",
    "### Give examples of cases where each these advantages are NOT desirable.\n",
    " - ## Write your answers here\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Document Summarization\n",
    "One of the benefits of having data points as cluster centers is that, for NLP, the centers are sure to be valid languge. \n",
    "\n",
    "Consider a single document that is broken into sentences. If we can identify certain sentences which are representative of other similar sentences, we can, perhaps effectively summarize a document!\n",
    "This type of summarization where parts of a document are copied verbatim is called __extractive summarization__ and is different from __abstractive summarization__ which would not attempt to sentences found in the text. \n",
    "\n",
    "In this part you will\n",
    " - locate a few documents of your choosing\n",
    " - break each document into sentences\n",
    " - use the bag of words or TFIDF to represent the text\n",
    " - use [affinity propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) to find the examplar sentences\n",
    " - print them out and comment on the results. \n",
    "   - what works, what doesn't work? \n",
    "   - is TFIDF better than BOW?\n",
    "   - etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./causes_of_world_war_I\") as fi:\n",
    "    data = fi.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "# convert documents into sentences using `sent_tokenize`\n",
    "# make sure to only including sentences of >50 characters and not starting with '=='\n",
    "\n",
    "# your code here\n",
    "\n",
    "assert(len(sentences) == 482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences using tfidf representation\n",
    "\n",
    "# your code here\n",
    "\n",
    "# fit AffinityPropagation\n",
    "# ap = AffinityPropagation(...)\n",
    "\n",
    "# your code here\n",
    "\n",
    "print(ap.cluster_centers_indices_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print cluster centers using `ap.cluster_centers_indices_`\n",
    "# comment on results. what works, what doesn't work? is TFIDF better than BOW?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Another use for Unsupervised Data\n",
    "\n",
    "As mentioned in class, we can use unlabled text in order to make better text representations. \n",
    "\n",
    "This is a large part of cutting edge NLP, and we'll cover this in a few lectures.\n",
    "In the meantime, we can do a simple version. In low data settings, TFIDF vectors can \n",
    "be misleading, since we don't have enough observations to get good statistics on document frequencies. \n",
    "\n",
    "In this problem, we'll use the [20 newsgroups data](https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset), which is a 20-class classification\n",
    "problem on email subject matter. In real-world applications, it is typically costly and time \n",
    "consuming to label data. In this problem we'll simulate that by keeping only the first 1000 labels.\n",
    "However, we can use the rest of the data set (and other data sets) in order to get better statistics\n",
    "on document frequencies, and therefore better text representations that make better use of our\n",
    "few labeled examples\n",
    "\n",
    "## Part 0: Load the data\n",
    " - read the docs about the dataset\n",
    " - load the 20newsgrousp data with `sklearn.datasets.fetch_20newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# ok to restart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "fetch_20newsgroups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"), subset=\"train\")\n",
    "data_test = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"), subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the remove kwarg do in this function?\n",
    "\n",
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the in-sample accuracy of a dummy model that always guesses this class?\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many training examples are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fit a baseline\n",
    "\n",
    "Fit a baseline on the first 1000 examples\n",
    " - Fit a `TfidfVectorizer` on the first 1000 examples\n",
    "   - use only 10k features\n",
    " - transform the first 1000 training examples and the test data\n",
    " - fit logistic regression and report the accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED_EXAMPLES = 1000\n",
    "MAX_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# your code here\n",
    "\n",
    "# lr.score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use more unsupervised data\n",
    "Repeat part 1, but fit the `TfidfVectorizer` on more data.\n",
    "\n",
    "You should include\n",
    " - all of the training docs (even though we'll only use the first 1000 labels)\n",
    " - load the train set from the IMDB sentiment analysis dataset\n",
    "\n",
    "\n",
    "Then\n",
    " - Fit the same logistic regression model and report the accuracy\n",
    " - Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(imdb_data_dir, \"train\")\n",
    "    test_dir = os.path.join(imdb_data_dir, \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(extra_docs, _), _ = load_imdb_data_text(\"../../data/aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# lr.score(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Backoff language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know that having unknown words in text is a problem for a language model. Any estimate of probability is difficult in such a scenario. \n",
    "\n",
    "### In class, we saw a simple way of smoothing probabilities by adding count 1 to every occuring ngram. While this can be a simple and effective technique we can do something a bit more clever. In this exercise we will implement two such techniques. \n",
    "\n",
    "### 1) to deal with unknown unigrams we will introduce a special `<unk>` token in our training data to represent rare tokens\n",
    "\n",
    "### 2) for unknown bigrams we will use a technique called backoff. The idea is to \"backoff\" to a lower order n-gram estimate for the probability if the n-gram is unknown. For example the probability of an unknown bigram `w_1 w_2` can be estimated by looking at the unigram probability of `w_2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "wiki_df = pd.read_csv('../../data/kdwd_r1k_articles.csv')\n",
    "\n",
    "def get_tokens(text):\n",
    "    return ['<s>'] + re.findall(r'\\w+', text.lower()) + ['</s>']\n",
    "\n",
    "train_sentences_list = ' '.join(wiki_df['intro_text'].iloc[:-100].tolist()).split('.')\n",
    "test_sentences_list = ' '.join(wiki_df['intro_text'].iloc[-100:].tolist()).split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's build a basic 1-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_list = [get_tokens(text) for text in train_sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = Counter()\n",
    "# your code here\n",
    "        \n",
    "n_unigrams = np.sum([v for _, v in unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_unigrams == 95491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_token_prob(token):\n",
    "    return unigram_counts[token] / n_unigrams\n",
    "\n",
    "def get_text_prob_unigram(text):\n",
    "    tokens = get_tokens(text)\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        # code here\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_unigram('the company').round(9) == 2.455e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we haven't yet introduced any smoothing, meaning, out-of-vocabulary words will have a probability of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georg/venvs/mit/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_unigram(\"onomatopoeia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have learned that we can simply add 1 to every word count to prevent this (ref: laplace smoothing). Another way however is to mark rare words within our training set as unknown words. The idea is that the model will then learn how to deal with unknown/rare words, to more correctly evaluate a test text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this, let us first identify all unigrams that occur fewer or equal than k times. Let's use k=1 to start out with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens = set()\n",
    "# you loop code here\n",
    "        rare_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(rare_tokens) == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's create a new counter `filtered_unigram_counts` where every token that appears in `rare_tokens` is recorded as the special token `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unigram_counts = Counter()\n",
    "for token_list in train_token_list:\n",
    "# your code here\n",
    "        \n",
    "n_filtered_unigrams = np.sum([v for _, v in filtered_unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filtered_unigram_counts['<unk>'] == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use these new counts, let's modify our text probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_unigram_token_prob(token):\n",
    "    return filtered_unigram_counts[token] / n_filtered_unigrams\n",
    "\n",
    "def get_text_prob_filtered_unigram(text):\n",
    "    tokens = # get tokens and convert to <unk> if needed\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        logp += np.log(get_filtered_unigram_token_prob(t))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_filtered_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_filtered_unigram('the company').round(9) == 2.455e-06)\n",
    "assert(get_text_prob_filtered_unigram(\"onomatopoeia\").round(5) == 0.00016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that now unknown words actually have a probability higher than some of the rare words that we have already seen before like `apple`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of count 1 to label words as `<unk>`was arbitrary. How could we tune is if we had more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your text answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's expand our model to bigrams now. Make sure to check if each component in a bigram exists and label it as `<unk>` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bigram_counts = Counter()\n",
    "for token_list in train_token_list:\n",
    "    # your loop and 'unk' conversion here\n",
    "        filtered_bigram_counts[t1 + ' ' + t2] += 1\n",
    "\n",
    "def get_filtered_bigram_token_prob(token1, token2):\n",
    "    return filtered_bigram_counts[token1 + ' ' + token2] / filtered_unigram_counts[token1]\n",
    "        \n",
    "def get_text_prob_filtered_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_filtered_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_filtered_bigram('the company').round(5) == 0.00148)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We correctly get a higher probabiliy for `the company`, now that we are respecting bigrams.\n",
    "### However:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georg/venvs/mit/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_filtered_bigram('company the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that we still get 0 for unknown bigrams. Let's fix this via Backoff. To reiterate: the idea is to default to unigram probabilities if the bigram is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backoff_bigram_token_prob(token1, token2):\n",
    "    # check if bigram exists and if not return unigram token2 prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_prob_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_backoff_bigram('company the').round(8) == 1.1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can happily now estimate any input text we can think of with running into issues with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if this was all worth it. Let's evaluate perplexity.\n",
    "### Specifically compare the perplexity of our filtered unigram model `get_filtered_unigram_token_prob` to our new and improved backoff bigram model `get_backoff_bigram_token_prob`\n",
    "\n",
    "### Note: For easy comparison let's only evaluate `tokens[1:]` for both models such that even the first token can already form a correct bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_ppl_filtered_unigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    # your code\n",
    "\n",
    "def get_text_ppl_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_filtered_unigram(text))\n",
    "model_unigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_backoff_bigram(text))\n",
    "model_bigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(model_bigram_ppl < model_unigram_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems like it worked very well. Try to find one or two examples of short strings that clearly show that our bigram model is better and why. (Short answer is OK here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
