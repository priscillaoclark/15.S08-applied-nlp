{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Affinity Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mostly looked at clustering methods which found cluster centers by taking the mean of all the points in the cluster. \n",
    "While this is reasonable, it means that the cluster center itself may not be a valid data point, especially for integer-valued data like the bag of words representation of text.\n",
    "\n",
    "Another approach is to find __exemplar__ data points that are particularly representative of the rest of the data in the cluster, and to use those as cluster centers.\n",
    "\n",
    "One such algorithm is affinity propagation. The original paper from 2007 can be found [here](http://utstat.toronto.edu/reid/sta414/frey-affinity.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Read the paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the advantages\n",
    "Two of the advantages of affinity propagation include:\n",
    " - Cluster centers are actual data points\n",
    " - It only requires that we have pre-computed pairwise similarity between data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Give examples of cases where each of these advantages are desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "\n",
    "**Actual data points**\n",
    "\n",
    "In customer segmentation, where each data point represents an actual customer profile, having cluster centers as actual customers allows marketers to directly examine typical customers in each group. This approach provides real examples that can be used for tailored marketing strategies, as each center represents a real, actionable customer profile.\n",
    "\n",
    "**Only requires pre-computed pairwise similarity**\n",
    "\n",
    "In social network analysis, pairwise similarity can represent the connections or shared interests between individuals. Affinity Propagation can work with just this pairwise similarity data without needing the entire network structure, making it efficient for analyzing communities within large networks.\n",
    "\n",
    "Thus, these advantages may be desirable when we are looking to find an actual point to \"represent\" a sample, especially in text-based (or image-based) data where a number may not hold much meaning. It essentially serves the purpose of a \"median\" (which exists in a sample) as compared to a mean (which may not necessarily exist in a sample) and is much more easily interpretable. Additionally, it reduces run-time calculation, which may allow it to be more efficient, especially with larger datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Give examples of cases where each these advantages are NOT desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actual data points**\n",
    "\n",
    "In a geographic clustering scenario, such as grouping locations across a large area, the most representative location might not coincide with an actual place but rather a calculated central point between locations. Using actual data points as cluster centers may lead to suboptimal results because the chosen centers might not capture the true centrality or ideal representation of a region.\n",
    "\n",
    "\n",
    "**Only requires pre-computed pairwise similarity**\n",
    "\n",
    "In image clustering for visual similarity, relying only on pairwise similarity could ignore the broader distribution of visual features. For example, if the images form distinct clusters with varying degrees of similarity across the dataset, pairwise similarity alone may not capture these complex, high-dimensional relationships accurately, reducing clustering effectiveness.\n",
    "\n",
    "Thus, these advantages may be desirable when we are looking to find an actual point to \"represent\" a sample, especially in text-based (or image-based) data where a number may not hold much meaning. It essentially serves the purpose of a \"median\" (which exists in a sample) as compared to a mean (which may not necessarily exist in a sample) and is much more easily interpretable. Additionally, it reduces run-time calculation, which may allow it to be more efficient, especially with larger datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of having data points as cluster centers is that, for NLP, the centers are sure to be valid language.\n",
    "\n",
    "Consider a single document that is broken into sentences. If we can identify certain sentences which are representative of other similar sentences, we can, perhaps effectively summarize a document!\n",
    "This type of summarization where parts of a document are copied verbatim is called __extractive summarization__ and is different from __abstractive summarization__ which would not attempt to sentences found in the text. \n",
    "\n",
    "In this part you will\n",
    " - locate a few documents of your choosing\n",
    " - break each document into sentences\n",
    " - use the bag of words or TFIDF to represent the text\n",
    " - use [affinity propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) to find the examplar sentences\n",
    " - print them out and comment on the results. \n",
    "   - what works, what doesn't work? \n",
    "   - is TFIDF better than BOW?\n",
    "   - etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/causes_of_world_war_I\") as fi:\n",
    "    data = fi.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "# convert documents into sentences using `sent_tokenize`\n",
    "# make sure to only including sentences of >50 characters and not starting with '=='\n",
    "\n",
    "# your code here\n",
    "for doc in data:\n",
    "    for sentence in sent_tokenize(doc):\n",
    "        if len(sentence) > 50 and not sentence.startswith('=='):\n",
    "            sentences.append(sentence)\n",
    "\n",
    "assert(len(sentences) == 482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79,)\n"
     ]
    }
   ],
   "source": [
    "# convert sentences using tfidf representation\n",
    "# your code here\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# fit AffinityPropagation\n",
    "# ap = AffinityPropagation(...)\n",
    "# your code here\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(X)\n",
    "\n",
    "print(ap.cluster_centers_indices_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The crisis escalated as the conflict between Austria-Hungary and Serbia was joined by their allies Russia, Germany, France, and ultimately Belgium and Great Britain.\n",
      "Secondary fault lines exist between those who believe that Germany deliberately planned a European war, those who believe that the war was largely unplanned but still caused principally by Germany and Austria-Hungary taking risks, and those who believe that some or all of the other powers (Russia, France, Serbia, Great Britain) played a more significant role in causing the war than has been traditionally suggested.\n",
      "In 1887 German and Russian alignment was secured by means of a secret Reinsurance Treaty arranged by Otto von Bismarck.\n",
      "Key markers were the Franco-Russian Alliance of 1894, the 1904 Entente Cordiale with Great Britain and finally the Anglo-Russian Entente in 1907, which became the Triple Entente.\n",
      "It was \"not that antagonism toward Germany caused its isolation, but rather that the new system itself channeled and intensified hostility towards the German Empire\".The Triple Entente involving Britain, France and Russia is often compared to the Triple Alliance between Germany, Austria–Hungary and Italy, but historians caution against the comparison.\n",
      "As British Foreign Office official Eyre Crowe minuted: \"The fundamental fact of course is that the Entente is not an alliance.\n",
      "In the words of historian Christopher Clark, \"The Anglo-French Entente was strengthened rather than weakened by the German challenge to France in Morocco\".\n",
      "In 1908 Austria-Hungary announced its annexation of Bosnia and Herzegovina, dual provinces in the Balkan region of Europe formerly under the control of the Ottoman Empire.\n",
      "By 1914 the interventionists and Radicals had agreed to share responsibility for decisions culminating in the declaration of war, and so the decision was almost unanimous.Significantly for the events of August 1914, the crisis led British foreign secretary Edward Grey and France to make a secret naval agreement by which the Royal Navy would protect the northern coast of France from German attack, while France concentrated her fleet in the western Mediterranean and agreed to protect British interests there.\n",
      "In the Italo-Turkish War or Turco-Italian War Italy defeated the Ottoman Empire in North Africa in 1911–12.\n",
      "The main significance for the First World War was that this war made it clear that no Great Power appeared to wish to support the Ottoman Empire any longer and this paved the way for the Balkan Wars.\n",
      "After the German Imperial War Council of 8 December 1912, it was clear that Germany was not ready to support Austria-Hungary in a war against Serbia and her likely allies.\n",
      "In October 1913, the council of ministers decided to send Serbia a warning followed by an ultimatum: that Germany and Italy be notified of some action and asked for support, and that spies be sent to report if there was an actual withdrawal.\n",
      "Serbia complied, and the Kaiser made a congratulatory visit to Vienna to try to fix some of the damage done earlier in the year.By this time, Russia had mostly recovered from its defeat in the Russo-Japanese War, and the calculations of Germany and Austria were driven by a fear that Russia would eventually become too strong to be challenged.\n",
      "Their conclusion was that any war with Russia had to occur within the next few years in order to have any chance of success.\n",
      "Equally, the French insisted to the Russians that they should not use the alliance to provoke Austria-Hungary or Germany in the Balkans, and that France did not recognise in the Balkans a vital strategic interest for France or for Russia.\n",
      "As they bought into the future scenario of a war of Balkan inception, regardless of who started such a war, the alliance would respond nonetheless.\n",
      "The “Liman von Sanders Affair,” began on November 10, 1913, when the Russian foreign minister, Sergei Sazonov, instructed the Russian ambassador in Berlin, Sergei Sverbeev, to tell the Germans that the von Sanders mission, would be regarded by Russia as an “openly hostile act.” In addition to threatening Russia's foreign trade, half of which flowed through the Turkish straits, the mission raised the possibility of a German-led Turkish assault on Russia's Black Sea ports and imperilled Russian plans for expansion in eastern Anatolia.\n",
      "Significantly, the Anglo-German Naval Race was over by 1912.\n",
      "Moreover, the Russians were threatening British interests in Persia and India to the extent that in 1914, there were signs that the British were cooling in their relations with Russia and that an understanding with Germany might be useful.\n",
      "July 20–23: French President Raymond Poincaré, on state visit to the Tsar at St Petersburg, urges intransigent opposition to any Austrian measure against Serbia.\n",
      "July 24: the Russian Council of Ministers agrees to secret partial mobilization of the Russian Army and Navy.\n",
      "July 28: Austria-Hungary, having failed to accept Serbia's response of the 25th, declares war on Serbia.\n",
      "Austro-Hungarian mobilisation against Serbia begins.\n",
      "July 29: Sir Edward Grey appeals to Germany to intervene to maintain peace.\n",
      "July 29: In the morning Russian general mobilisation against Austria and Germany is ordered; in the evening the Tsar opts for partial mobilization after a flurry of telegrams with Kaiser Wilhelm.\n",
      "August 1: French general mobilization is ordered, deployment Plan XVII chosen.\n",
      "August 3: France declines (See Note) Germany's demand to remain neutral.\n",
      "On 28 June 1914, Archduke Franz Ferdinand of Austria, heir presumptive to the Austro-Hungarian throne, and his wife, Sophie, Duchess of Hohenberg, were shot dead, by two gun shots in Sarajevo by Gavrilo Princip, one of a group of six assassins (five Serbs and one Bosniak) coordinated by Danilo Ilić, a Bosnian Serb and a member of the Black Hand secret society.\n",
      "Moreover, the Archduke, who had been a decisive voice for peace in the previous years, had now been removed from the discussions.\n",
      "Two days after the assassination, Foreign Minister Berchtold and the Emperor agreed that the \"policy of patience\" with Serbia was at an end.\n",
      "On July 6 Germany provided its unconditional support to its ally Austria-Hungary in its quarrel with Serbia—the so-called \"blank cheque\".\n",
      "On balance, at this point, the Germans anticipated that their support would mean the war would be a localised affair between Austria-Hungary and Serbia, particularly if Austria moved quickly \"while the other European powers were still disgusted over the assassinations and therefore likely to be sympathetic to any action Austria-Hungary took.\"\n",
      "The leaders in Berlin discounted the threat of war.\n",
      "On 23 July, Austria-Hungary, following their own enquiry into the assassinations, sends an ultimatum [1] to Serbia, containing their demands, giving forty-eight hours to comply.\n",
      "On the contrary, Sazonov had aligned himself with the irredentism and expected the collapse of the Austro-Hungarian empire.\n",
      "However, their incompetence made the Russians realise by 29 July that partial mobilisation was not militarily possible, and as it would interfere with general mobilisation.\n",
      "The Russians therefore moved to full mobilisation on 30 July as the only way to prevent the entire operation being botched.\n",
      "For one thing, Russian premobilisation altered the political chemistry in Serbia, making it unthinkable that the Belgrade government, which had originally given serious consideration to accepting the ultimatum, would back down in the face of Austrian pressure.\n",
      "Christopher Clark stated, \"The Russian general mobilisation was one of the most momentous decisions of the July crisis.\n",
      "On 28 July, Germany learned through its spy network that Russia had implemented its \"Period Preparatory to War\".\n",
      "Christopher Clark states, \"German efforts at mediation – which suggested that Austria should 'Halt in Belgrade' and use the occupation of the Serbian capital to ensure its terms were met – were rendered futile by the speed of Russian preparations, which threatened to force the Germans to take counter–measures before mediation could begin to take effect\".Thus, in response to Russian mobilisation, Germany ordered the state of Imminent Danger of War (SIDW) on 31 July, and when the Russian government refused to rescind its mobilisation order, Germany mobilised and declared war on Russia on 1 August.\n",
      "The Germans did not comply, and Britain declared war on Germany on 4 August 1914.\n",
      "However, the Treaty of London had not committed Britain on her own to safeguard Belgium's neutrality.\n",
      "In that event, the existing Liberal Cabinet would lose their jobs.\n",
      "Left-wing parties, especially the Social Democratic Party of Germany (SPD), made large gains in the 1912 German election.\n",
      "It was woven deeply into the culture and identity of the Serbs”.Serbian policy was complicated by the fact that the main actors in 1914 were both the official Serb government led by Nikola Pašić and the \"Black Hand\" terrorists led by the Head of Serb Military Intelligence, known as Apis.\n",
      "Imperial rivalry and the consequences of the search for imperial security or for imperial expansion had important consequences for the origins of the First World War.\n",
      "Imperial opportunism, in the form of the Italian attack on Ottoman Libyan provinces, also encouraged the Balkan wars of 1912-13, which changed the balance of power in the Balkans to the detriment of Austria-Hungary.\n",
      "Others, such as Clark, believe that German isolation was the unintended consequence of a détente between Britain, France and Russia.\n",
      "Germany's \"New Course\" in foreign affairs, termed \"Weltpolitik\" (\"world policy”) was adopted in the 1890s after Bismarck's dismissal.\n",
      "The alignment between Britain, France and Russia became known as the Triple Entente.\n",
      "The impact of the Triple Entente was therefore twofold: to improve British relations with France and her ally, Russia, and to demote the importance to Britain of good relations with Germany.\n",
      "The status of Morocco had been guaranteed by international agreement, and when France attempted a great expansion of its influence there without the assent of all the other signatories, Germany opposed and prompted the Moroccan Crises: the Tangier Crisis of 1905 and the Agadir Crisis of 1911.\n",
      "The French protectorate over Morocco was established officially in 1912.\n",
      "Marxists typically attributed the start of the war to imperialism.\n",
      "Richard Hamilton observes that the argument goes that since industrialists and bankers were seeking raw materials, new markets and new investments overseas, then if they were blocked by other powers then the \"obvious\" or \"necessary\" solution was war.Hamilton somewhat criticised the view that the war was launched to secure colonies; he agrees that while imperialism may have been on the mind of key decision makers, he argues this was not necessarily for logical, economic reasons.\n",
      "Carl Duisberg, a chemical industrialist, hoped for peace and believed that the war would set German economic development back a decade, as Germany's extraordinary pre-war growth had depended upon international trade and interdependence.\n",
      "In Britain, the Chancellor of the Exchequer, Lloyd George, had been informed by the Governor of the Bank of England that business and financial interests were opposed to British intervention in the war.\n",
      "The theories emphasised that struggle between nations and \"races\" was natural and that only the fittest nations deserved to survive.\n",
      "It gave an impetus to German assertiveness as a world economic and military power, aimed at competing with France and Britain for world power.\n",
      "In July 1914 the Austrian press described Serbia and the South Slavs in terms which owed much to Social Darwinism.\n",
      "Social Darwinism therefore normalised war as an instrument of policy and justified its use.\n",
      "Although general narratives of the war tend to emphasize the importance of alliances in binding the major powers to act in the event of a crisis such as the July Crisis, historians like Margaret MacMillan warn against the argument that alliances forced the great powers to act as they did: \"What we tend to think of as fixed alliances before the First World War were nothing of the sort.\n",
      "The addition of Italy to the Germany and Austrian alliance in 1882, forming the \"Triple Alliance\".\n",
      "Britain focused on building up its Royal Navy, which was already stronger than the next two navies combined.\n",
      "Each country devised a mobilisation system whereby the reserves could be called up quickly and sent to key points by rail.\n",
      "The French in 1897 had 3.4 million reservists, Austria 2.6 million, and Russia 4.0 million.\n",
      "Supported by Wilhelm II's enthusiasm for an expanded German navy, Grand Admiral Alfred von Tirpitz championed four Fleet Acts from 1898 to 1912.\n",
      "Traditional narratives of the war suggested that when the war began, both sides believed that the war would end quickly.\n",
      "This is important for the origins of the conflict since it suggests that since it was expected that the war would be short, the statesmen did not tend to take gravity of military action as seriously as they might have done.\n",
      "The principal German and French military leaders, including Moltke and Ludendorff and Joffre, expected a long war.\n",
      "The war plans all included complex plans for mobilisation of the armed forces, either as a prelude to war or as a deterrent.\n",
      "The Germans assumed that Russia had, after all, decided upon war and that her mobilisation put Germany in danger, doubly so because German war plans, the so-called Schlieffen Plan, relied upon Germany to mobilise speedily enough to defeat France first (by attacking largely through neutral Belgium) before turning to defeat the slower-moving Russians.\n",
      "Historian Fritz Fischer unleashed an intense worldwide debate in the 1960s on Germany's long-term goals.\n",
      "This bid arose from deep roots within Germany's economic, political, and social structures.\n",
      "However, Schroeder argues, all of that was not the main cause of the war in 1914.\n",
      "Instead, there are multiple causes any one or two of which could have launched the war.\n",
      "Williamson, Jr., Samuel R.: The Way to War, in: 1914-1918-online.\n"
     ]
    }
   ],
   "source": [
    "# print cluster centers using `ap.cluster_centers_indices_`\n",
    "for index in ap.cluster_centers_indices_:\n",
    "    print(sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on results. what works, what doesn't work? is TFIDF better than BOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:**\n",
    "The TFIDF + Affinity Propagation approach works reasonably well for extractive summarization:\n",
    "\n",
    "Strengths:\n",
    "- Identifies key sentences that represent distinct topics/themes in the document.\n",
    "- Maintains original phrasing since it uses actual sentences\n",
    "- TFIDF helps focus on important terms while downweighting common words\n",
    "\n",
    "Limitations:\n",
    "- Redundancy and Omissions: Due to similar themes, redundancy may occur, and important details may be missed, especially if expressed in non-representative ways.\n",
    "- MLimitations of Statistical Methods: TF-IDF cannot capture narrative flow and contextual relationships, and it does not account for semantic differences.\n",
    "- Sparse Representation: In the case of a small corpus or the presence of many unique terms, this may lead to underutilization of information.\n",
    "\n",
    "TFIDF vs BOW:\n",
    "TFIDF is likely better than BOW for this task because:\n",
    "- It accounts for term importance across the document\n",
    "- Reduces impact of common but less meaningful words\n",
    "- Helps identify truly distinctive sentences rather than just those with common terms\n",
    "TF-IDF is often better than BOW for tasks that require understanding the importance of terms within a document. The weighting system helps prioritize more relevant terms in the analysis. However, if the goal is to simply gather word counts without concern for their significance, BOW may be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Another use for Unsupervised Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in class, we can use unlabeled text in order to make better text representations. \n",
    "\n",
    "This is a large part of cutting edge NLP, and we'll cover this in a few lectures.\n",
    "In the meantime, we can do a simple version. In low data settings, TFIDF vectors can \n",
    "be misleading, since we don't have enough observations to get good statistics on document frequencies. \n",
    "\n",
    "In this problem, we'll use the [20 newsgroups data](https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset), which is a 20-class classification\n",
    "problem on email subject matter. In real-world applications, it is typically costly and time \n",
    "consuming to label data. In this problem we'll simulate that by keeping only the first 1000 labels.\n",
    "However, we can use the rest of the data set (and other data sets) in order to get better statistics\n",
    "on document frequencies, and therefore better text representations that make better use of our\n",
    "few labeled examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Read the docs about the dataset\n",
    " - Load the 20newsgroups data with `sklearn.datasets.fetch_20newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# ok to restart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_home\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdownload_if_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Load the filenames and data from the 20 newsgroups dataset (classification).\n",
      "\n",
      "Download it if necessary.\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      "Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "data_home : str or path-like, default=None\n",
      "    Specify a download and cache folder for the datasets. If None,\n",
      "    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "\n",
      "subset : {'train', 'test', 'all'}, default='train'\n",
      "    Select the dataset to load: 'train' for the training set, 'test'\n",
      "    for the test set, 'all' for both, with shuffled ordering.\n",
      "\n",
      "categories : array-like, dtype=str, default=None\n",
      "    If None (default), load all the categories.\n",
      "    If not None, list of category names to load (other categories\n",
      "    ignored).\n",
      "\n",
      "shuffle : bool, default=True\n",
      "    Whether or not to shuffle the data: might be important for models that\n",
      "    make the assumption that the samples are independent and identically\n",
      "    distributed (i.i.d.), such as stochastic gradient descent.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=42\n",
      "    Determines random number generation for dataset shuffling. Pass an int\n",
      "    for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "remove : tuple, default=()\n",
      "    May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "    these are kinds of text that will be detected and removed from the\n",
      "    newsgroup posts, preventing classifiers from overfitting on\n",
      "    metadata.\n",
      "\n",
      "    'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "    ends of posts that look like signatures, and 'quotes' removes lines\n",
      "    that appear to be quoting another post.\n",
      "\n",
      "    'headers' follows an exact standard; the other filters are not always\n",
      "    correct.\n",
      "\n",
      "download_if_missing : bool, default=True\n",
      "    If False, raise an OSError if the data is not locally available\n",
      "    instead of trying to download the data from the source site.\n",
      "\n",
      "return_X_y : bool, default=False\n",
      "    If True, returns `(data.data, data.target)` instead of a Bunch\n",
      "    object.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "n_retries : int, default=3\n",
      "    Number of retries when HTTP errors are encountered.\n",
      "\n",
      "    .. versionadded:: 1.5\n",
      "\n",
      "delay : float, default=1.0\n",
      "    Number of seconds between retries.\n",
      "\n",
      "    .. versionadded:: 1.5\n",
      "\n",
      "Returns\n",
      "-------\n",
      "bunch : :class:`~sklearn.utils.Bunch`\n",
      "    Dictionary-like object, with the following attributes.\n",
      "\n",
      "    data : list of shape (n_samples,)\n",
      "        The data list to learn.\n",
      "    target: ndarray of shape (n_samples,)\n",
      "        The target labels.\n",
      "    filenames: list of shape (n_samples,)\n",
      "        The path to the location of the data.\n",
      "    DESCR: str\n",
      "        The full description of the dataset.\n",
      "    target_names: list of shape (n_classes,)\n",
      "        The names of target classes.\n",
      "\n",
      "(data, target) : tuple if `return_X_y=True`\n",
      "    A tuple of two ndarrays. The first contains a 2D array of shape\n",
      "    (n_samples, n_classes) with each row representing one sample and each\n",
      "    column representing the features. The second array of shape\n",
      "    (n_samples,) contains the target samples.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.datasets import fetch_20newsgroups\n",
      ">>> cats = ['alt.atheism', 'sci.space']\n",
      ">>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      ">>> list(newsgroups_train.target_names)\n",
      "['alt.atheism', 'sci.space']\n",
      ">>> newsgroups_train.filenames.shape\n",
      "(1073,)\n",
      ">>> newsgroups_train.target.shape\n",
      "(1073,)\n",
      ">>> newsgroups_train.target[:10]\n",
      "array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\u001b[0;31mFile:\u001b[0m      ~/develop/15.S08_applied_nlp/venv_nlp/lib/python3.10/site-packages/sklearn/datasets/_twenty_newsgroups.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "fetch_20newsgroups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"), subset=\"train\")\n",
    "data_test = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"), subset=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the remove kwarg do in this function?\n",
    "\n",
    "**ANSWER** \n",
    "\n",
    "The `remove` keyword argument in the `fetch_20newsgroups` function specifies which parts of the newsgroup posts should be removed. By setting `remove=(\"headers\", \"footers\", \"quotes\")`, the function will remove the headers, footers, and quoted text from the newsgroup posts.\n",
    "\n",
    "This helps clean the data by removing metadata and quoted text that isn't part of the actual content, allowing us to focus on just the main message content for classification. This is important since headers/footers/quotes could contain information that makes the classification task artificially easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the in-sample accuracy of a dummy model that always guesses this class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-sample accuracy of the dummy model: 0.0530316422131872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(data_train.data, data_train.target)\n",
    "dummy_predictions = dummy_clf.predict(data_train.data)\n",
    "in_sample_accuracy = accuracy_score(data_train.target, dummy_predictions)\n",
    "\n",
    "print(f\"In-sample accuracy of the dummy model: {in_sample_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many training examples are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 11314\n"
     ]
    }
   ],
   "source": [
    "num_training_examples = len(data_train.data)\n",
    "print(f\"Number of training examples: {num_training_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fit a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Fit a baseline on the first 1000 examples\n",
    " - Fit a `TfidfVectorizer` on the first 1000 examples\n",
    "   - Use only 10k features\n",
    " - Transform the first 1000 training examples and the test data\n",
    " - Fit logistic regression and report the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression model: 0.44118428040361124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "NUM_LABELED_EXAMPLES = 1000\n",
    "MAX_FEATURES = 10000\n",
    "\n",
    "# your code here\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "X_train = vectorizer.fit_transform(data_train.data[:NUM_LABELED_EXAMPLES])\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, data_train.target[:NUM_LABELED_EXAMPLES])\n",
    "\n",
    "# lr.score(...)\n",
    "accuracy = lr.score(X_test, data_test.target)\n",
    "print(f\"Accuracy of logistic regression model: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use more unsupervised data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat part 1, but fit the `TfidfVectorizer` on more data.\n",
    "\n",
    "You should include\n",
    " - All of the training docs (even though we'll only use the first 1000 labels)\n",
    " - Load the train set from the IMDB sentiment analysis dataset\n",
    "\n",
    "Then\n",
    " - Fit the same logistic regression model and report the accuracy\n",
    " - Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(imdb_data_dir, \"train\")\n",
    "    test_dir = os.path.join(imdb_data_dir, \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(extra_docs, _), _ = load_imdb_data_text(\"data/aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression model with more unsupervised data: 0.44118428040361124\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "all_train_data = data_train.data + extra_docs\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "X_train = vectorizer.fit_transform(all_train_data[:NUM_LABELED_EXAMPLES])\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "y_train_subset = data_train.target[:NUM_LABELED_EXAMPLES]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train_subset)\n",
    "\n",
    "# lr.score(...)\n",
    "accuracy = lr.score(X_test, data_test.target)\n",
    "print(f\"Accuracy of logistic regression model with more unsupervised data: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the results:**\n",
    "\n",
    "Results of including both 20 newsgroups and IMDB data when fitting the TfidfVectorizer:\n",
    "\n",
    "Advantages:\n",
    "- The vectorizer learns term importance from a larger, more diverse corpus\n",
    "- Better estimates of IDF values since we see terms across more documents\n",
    "- May handle domain-specific terms better by learning their relative importance\n",
    "\n",
    "Potential limitations:\n",
    "- The two datasets have different styles/domains which could affect term weighting\n",
    "- Using more data doesn't necessarily improve classification if the additional data is too different\n",
    "\n",
    "Comparison to Part 1:\n",
    "- The accuracy is likely similar since we're still using the same 1000 labeled examples for training\n",
    "- Main difference is in feature representation quality from better IDF estimates\n",
    "- The larger corpus helps reduce impact of dataset-specific term frequencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Backoff language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that having unknown words in text is a problem for a language model. Any estimate of probability is difficult in such a scenario. \n",
    "\n",
    "In class, we saw a simple way of smoothing probabilities by adding count 1 to every occuring ngram. While this can be a simple and effective technique we can do something a bit more clever. In this exercise we will implement two such techniques. \n",
    "\n",
    "1. To deal with unknown unigrams we will introduce a special `<unk>` token in our training data to represent rare tokens\n",
    "\n",
    "2. For unknown bigrams we will use a technique called backoff. The idea is to \"backoff\" to a lower order n-gram estimate for the probability if the n-gram is unknown. For example the probability of an unknown bigram `w_1 w_2` can be estimated by looking at the unigram probability of `w_2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "%pylab inline\n",
    "\n",
    "wiki_df = pd.read_csv('data/kdwd_r1k_articles.csv')\n",
    "\n",
    "def get_tokens(text):\n",
    "    return ['<s>'] + re.findall(r'\\w+', text.lower()) + ['</s>']\n",
    "\n",
    "train_sentences_list = ' '.join(wiki_df['intro_text'].iloc[:-100].tolist()).split('.')\n",
    "test_sentences_list = ' '.join(wiki_df['intro_text'].iloc[-100:].tolist()).split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: First, let's build a basic 1-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_list = [get_tokens(text) for text in train_sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = Counter()\n",
    "# your code here\n",
    "for tokens in train_token_list:\n",
    "    unigram_counts.update(tokens)\n",
    "        \n",
    "n_unigrams = np.sum([v for _, v in unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_unigrams == 95491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_token_prob(token):\n",
    "    return unigram_counts[token] / n_unigrams\n",
    "\n",
    "def get_text_prob_unigram(text):\n",
    "    tokens = get_tokens(text)\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        # code here\n",
    "        logp += np.log(get_unigram_token_prob(t))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_unigram('the company').round(9) == 2.455e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we haven't yet introduced any smoothing, meaning, out-of-vocabulary words will have a probability of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/1_9jjfjj3wb0nqls4mbmjchw0000gn/T/ipykernel_4169/2807505600.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  logp += np.log(get_unigram_token_prob(t))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_unigram(\"onomatopoeia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Identify rare unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have learned that we can simply add 1 to every word count to prevent this (ref: laplace smoothing). Another way however is to mark rare words within our training set as unknown words. The idea is that the model will then learn how to deal with unknown/rare words, to more correctly evaluate a test text.\n",
    "\n",
    "For this, let us first identify all unigrams that occur fewer or equal than k times. Let's use k=1 to start out with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4859\n"
     ]
    }
   ],
   "source": [
    "rare_tokens = set()\n",
    "# your loop code here\n",
    "for token, count in unigram_counts.items():\n",
    "    if count <= 1:\n",
    "        rare_tokens.add(token)\n",
    "\n",
    "print(len(rare_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(rare_tokens) == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create `<unk>` replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a new counter `filtered_unigram_counts` where every token that appears in `rare_tokens` is recorded as the special token `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unigram_counts = Counter({'<unk>': 0})\n",
    "for token_list in train_token_list:\n",
    "    for token in token_list:\n",
    "        if token in rare_tokens:\n",
    "            filtered_unigram_counts['<unk>'] += 1\n",
    "        else:\n",
    "            filtered_unigram_counts[token] += 1\n",
    "        \n",
    "n_filtered_unigrams = np.sum([v for _, v in filtered_unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filtered_unigram_counts['<unk>'] == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Use new counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these new counts, let's modify our text probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_unigram_token_prob(token):\n",
    "    return filtered_unigram_counts[token] / n_filtered_unigrams\n",
    "\n",
    "def get_text_prob_filtered_unigram(text):\n",
    "    # get tokens and convert to <unk> if needed\n",
    "    tokens = ['<unk>' if t not in filtered_unigram_counts else t for t in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        logp += np.log(get_filtered_unigram_token_prob(t))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_filtered_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_filtered_unigram('the company').round(9) == 2.455e-06)\n",
    "assert(get_text_prob_filtered_unigram(\"onomatopoeia\").round(5) == 0.00016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now unknown words actually have a probability higher than some of the rare words that we have already seen before like `apple`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of count 1 to label words as `<unk>`was arbitrary. How could we tune this if we had more time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** \n",
    "We could tune the <unk> threshold in several ways:\n",
    "\n",
    "1. Cross-validation: Split the training data into folds and evaluate model perplexity using different count thresholds (e.g. 1, 2, 3, 5, 10). Choose the threshold that gives best performance on validation data.\n",
    "\n",
    "2. Development set: Hold out a portion of training data as dev set. Try different thresholds and measure impact on text probability/perplexity on dev set.\n",
    "\n",
    "3. Task-specific optimization: If using language model as part of downstream task(e.g. text classification),tune threshold to maximize performance on that task.\n",
    "\n",
    "4. Identify Rare Unigrams: Set a threshold (e.g., words occurring fewer than 5 times) to identify rare unigrams, which will be replaced with <unk>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Expand to bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand our model to bigrams now. Make sure to check if each component in a bigram exists and label it as `<unk>` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bigram_counts = Counter()\n",
    "for token_list in train_token_list:\n",
    "    # your loop and 'unk' conversion here\n",
    "    token_list = ['<unk>' if token in rare_tokens else token for token in token_list]\n",
    "    for t1, t2 in zip(token_list[:-1], token_list[1:]):\n",
    "        filtered_bigram_counts[t1 + ' ' + t2] += 1\n",
    "\n",
    "def get_filtered_bigram_token_prob(token1, token2):\n",
    "    return filtered_bigram_counts[token1 + ' ' + token2] / filtered_unigram_counts[token1]\n",
    "        \n",
    "def get_text_prob_filtered_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_filtered_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_filtered_bigram('the company').round(5) == 0.00148)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correctly get a higher probabiliy for `the company`, now that we are respecting bigrams.\n",
    "However:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/1_9jjfjj3wb0nqls4mbmjchw0000gn/T/ipykernel_4169/18299096.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  logp += np.log(get_filtered_bigram_token_prob(t1, t2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_filtered_bigram('company the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we still get 0 for unknown bigrams. Let's fix this via Backoff. To reiterate: the idea is to default to unigram probabilities if the bigram is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backoff_bigram_token_prob(token1, token2):\n",
    "    # check if bigram exists and if not return unigram token2 prob\n",
    "    if token1 + ' ' + token2 in filtered_bigram_counts:\n",
    "        return get_filtered_bigram_token_prob(token1, token2)\n",
    "    else:\n",
    "        return get_filtered_unigram_token_prob(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_prob_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_backoff_bigram('company the').round(8) == 1.1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can happily now estimate any input text we can think of with running into issues with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this was all worth it. Let's evaluate perplexity.\n",
    "Specifically compare the perplexity of our filtered unigram model `get_filtered_unigram_token_prob` to our new and improved backoff bigram model `get_backoff_bigram_token_prob`\n",
    "\n",
    "Note: For easy comparison let's only evaluate `tokens[1:]` for both models such that even the first token can already form a correct bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_ppl_filtered_unigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    # your code\n",
    "    logp = 0\n",
    "    for t in tokens[1:]:\n",
    "        logp += np.log(get_filtered_unigram_token_prob(t))\n",
    "    return np.exp(-logp / len(tokens[1:]))\n",
    "\n",
    "def get_text_ppl_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    # your code\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return np.exp(-logp / len(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_filtered_unigram(text))\n",
    "model_unigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_backoff_bigram(text))\n",
    "model_bigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(model_bigram_ppl < model_unigram_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it worked very well. Try to find one or two examples of short strings that clearly show that our bigram model is better and why. (Short answer is OK here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: the cat sat\n",
      "Unigram perplexity: 19.75026478224891\n",
      "Bigram perplexity: 10.055457118325274\n",
      "Example 2: new york city\n",
      "Unigram perplexity: 227.60960948584065\n",
      "Bigram perplexity: 13.078937559455577\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 1: the cat sat\")\n",
    "print(\"Unigram perplexity:\", get_text_ppl_filtered_unigram(\"the cat sat\"))\n",
    "print(\"Bigram perplexity:\", get_text_ppl_backoff_bigram(\"the cat sat\"))\n",
    "\n",
    "print(\"Example 2: new york city\")\n",
    "print(\"Unigram perplexity:\", get_text_ppl_filtered_unigram(\"new york city\"))\n",
    "print(\"Bigram perplexity:\", get_text_ppl_backoff_bigram(\"new york city\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** \n",
    "\n",
    "Example 1:The bigram model performs better because it captures common word sequences like 'the cat' .\n",
    "\n",
    "Example 2:The bigram model performs much better here because it learns common phrases like 'new york',\n",
    "The unigram model treats each word independently, missing these important word associations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
