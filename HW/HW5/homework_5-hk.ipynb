{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Affinity Propagation\n",
    "We mostly looked at clustering methods which found cluster centers by taking the mean of all the points in the cluster. \n",
    "While this is reasonable, it means that the cluster center itself may not be a valid data point, especially for integer-valued data like the bag of words representation of text.\n",
    "\n",
    "Another approach is to find __exemplar__ data points that are particularly representative of the rest of the data in the cluster, and to use those as cluster centers.\n",
    "\n",
    "One such algorithm is affinity propagation. The original paper from 2007 can be found [here](http://utstat.toronto.edu/reid/sta414/frey-affinity.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Read the paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the advantages\n",
    "### Two of the advantages of affinity propagation include:\n",
    " - ### cluster centers are actual data points\n",
    " - ### it only requires that we have pre-computed pairwise similarity between data points\n",
    "\n",
    "# $ \\\\ $\n",
    "### Give examples of cases where each of these advantages are desirable.\n",
    "### Give examples of cases where each these advantages are NOT desirable.\n",
    " - ## Write your answers here\n",
    "\n",
    "These advantages may be desirable when we are looking to find an actual point to \"represent\" a sample, especially in text-based (or image-based) data where a number may not hold much meaning. It essentially serves the purpose of a \"median\" (which exists in a sample) as compared to a mean (which may not necessarily exist in a sample) and is much more easily interpretable. Additionally, it reduces run-time calculation, which may allow it to be more efficient, especially with larger datasets.\n",
    "\n",
    "However, these advantages may not be desirable in the case of dynamic data. If the data points are being changed frequntly, then this model fails due to its reliance on pre-computing and lack of run-time adjustments. Thus, it really only works well in static datasets. Additionally, in numeric based data, a mean may sometimes be more representative than a median, so an average centroid calculation may be more desirable to a literal exemplar data point.\n",
    "\n",
    "# $ \\\\ $\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Document Summarization\n",
    "One of the benefits of having data points as cluster centers is that, for NLP, the centers are sure to be valid languge. \n",
    "\n",
    "Consider a single document that is broken into sentences. If we can identify certain sentences which are representative of other similar sentences, we can, perhaps effectively summarize a document!\n",
    "This type of summarization where parts of a document are copied verbatim is called __extractive summarization__ and is different from __abstractive summarization__ which would not attempt to sentences found in the text. \n",
    "\n",
    "In this part you will\n",
    " - locate a few documents of your choosing\n",
    " - break each document into sentences\n",
    " - use the bag of words or TFIDF to represent the text\n",
    " - use [affinity propagation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) to find the examplar sentences\n",
    " - print them out and comment on the results. \n",
    "   - what works, what doesn't work? \n",
    "   - is TFIDF better than BOW?\n",
    "   - etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshkumar/anaconda3/lib/python3.11/site-packages/IPython/core/magics/pylab.py:162: UserWarning: pylab import has clobbered these variables: ['text']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/causes_of_world_war_I\") as fi:\n",
    "    data = fi.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "# convert documents into sentences using `sent_tokenize`\n",
    "# make sure to only including sentences of >50 characters and not starting with '=='\n",
    "\n",
    "for line in data:\n",
    "    # Tokenize each line into sentences\n",
    "    line_sentences = sent_tokenize(line)\n",
    "    \n",
    "    # Filter sentences based on criteria\n",
    "    for sentence in line_sentences:\n",
    "        if len(sentence) > 50 and not sentence.startswith('=='):\n",
    "            sentences.append(sentence)\n",
    "\n",
    "assert(len(sentences) == 482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshkumar/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_affinity_propagation.py:142: ConvergenceWarning: Affinity propagation did not converge, this model may return degenerate cluster centers and labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# convert sentences using tfidf representation\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "X = tfidf.fit_transform(sentences)\n",
    "\n",
    "# fit AffinityPropagation\n",
    "ap = AffinityPropagation(random_state=42)\n",
    "ap.fit(X.toarray())\n",
    "\n",
    "print(ap.cluster_centers_indices_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Summary using TFIDF and Affinity Propagation:\n",
      "These two sets became, by August 1914, Germany and Austria-Hungary on one side and Russia, France, Great Britain on the other.\n",
      "In 1887 German and Russian alignment was secured by means of a secret Reinsurance Treaty arranged by Otto von Bismarck.\n",
      "By the 1890s the desire for revenge over Alsace-Lorraine no longer was a major factor for the leaders of France, but it remained a force in general public opinion.\n",
      "Key markers were the Franco-Russian Alliance of 1894, the 1904 Entente Cordiale with Great Britain and finally the Anglo-Russian Entente in 1907, which became the Triple Entente.\n",
      "The impact of the Triple Entente was therefore twofold: it improved British relations with France and her ally, Russia, and demoted the importance to Britain of good relations with Germany.\n",
      "It was \"not that antagonism toward Germany caused its isolation, but rather that the new system itself channeled and intensified hostility towards the German Empire\".The Triple Entente involving Britain, France and Russia is often compared to the Triple Alliance between Germany, Austria–Hungary and Italy, but historians caution against the comparison.\n",
      "In 1908 Austria-Hungary announced its annexation of Bosnia and Herzegovina, dual provinces in the Balkan region of Europe formerly under the control of the Ottoman Empire.\n",
      "Meanwhile, the episode strengthened the hand of Admiral Alfred von Tirpitz, who was calling for a greatly increased navy and obtained it in 1912.\n",
      "In the Italo-Turkish War or Turco-Italian War Italy defeated the Ottoman Empire in North Africa in 1911–12.\n",
      "The main significance for the First World War was that this war made it clear that no Great Power appeared to wish to support the Ottoman Empire any longer and this paved the way for the Balkan Wars.\n",
      "Christopher Clark stated: \"Italy launched a war of conquest on an African province of the Ottoman Empire, triggering a chain of opportunistic assaults on Ottoman territories across the Balkans.\n",
      "The Balkan Wars were two conflicts that took place in the Balkan Peninsula in south-eastern Europe in 1912 and 1913.\n",
      "The original Franco-Russian alliance was formed to protect both France and Russia from a German attack.\n",
      "Moreover, France clearly stated that if, as a result of a conflict in the Balkans, war were to break out between Austria-Hungary and Serbia, France would stand by Russia.\n",
      "Christopher Clark described this change as \"a very important development in the pre-war system which made the events of 1914 possible\".\n",
      "Significantly, the Anglo-German Naval Race was over by 1912.\n",
      "The ending of the naval arms race, the relaxation of colonial rivalries and increased diplomatic cooperation in the Balkans all resulted in an improvement in Germany's image in Britain by the eve of the war.British Diplomat Arthur Nicolson wrote in May 1914, “Since I have been at the Foreign Office I have not seen such calm waters.” The anglophile German Ambassador Karl Max, Prince Lichnowsky deplored that Germany had acted hastily without waiting for the British offer of mediation in July 1914 to be given a chance.\n",
      "June 28, 1914: Serbian irredentists assassinate Archduke Franz Ferdinand of the Austro-Hungarian Empire.\n",
      "July 20–23: French President Raymond Poincaré, on state visit to the Tsar at St Petersburg, urges intransigent opposition to any Austrian measure against Serbia.\n",
      "July 28: Austria-Hungary, having failed to accept Serbia's response of the 25th, declares war on Serbia.\n",
      "Austro-Hungarian mobilisation against Serbia begins.\n",
      "July 29: Sir Edward Grey appeals to Germany to intervene to maintain peace.\n",
      "August 1: French general mobilization is ordered, deployment Plan XVII chosen.\n",
      "The assassination is significant because it was perceived by Austria-Hungary as an existential challenge to her and in her view provided a casus belli with Serbia.\n",
      "Two days after the assassination, Foreign Minister Berchtold and the Emperor agreed that the \"policy of patience\" with Serbia was at an end.\n",
      "The principal voices for peace in previous years included Franz Ferdinand himself.\n",
      "Since taking on Serbia involved the risk of war with Russia, Vienna sought the views of Berlin.\n",
      "Samuel R. Williamson, Jr. has emphasized the role of Austria-Hungary in starting the war.\n",
      "On July 6 Germany provided its unconditional support to its ally Austria-Hungary in its quarrel with Serbia—the so-called \"blank cheque\".\n",
      "The benefits were clear but there were risks, namely that Russia would intervene and this would lead to a continental war.\n",
      "On balance, at this point, the Germans anticipated that their support would mean the war would be a localised affair between Austria-Hungary and Serbia, particularly if Austria moved quickly \"while the other European powers were still disgusted over the assassinations and therefore likely to be sympathetic to any action Austria-Hungary took.\"\n",
      "The French and the Russians agreed their alliance extended to supporting Serbia against Austria, confirming the already established policy behind the Balkan inception scenario.\n",
      "The leaders in Berlin discounted the threat of war.\n",
      "On 23 July, Austria-Hungary, following their own enquiry into the assassinations, sends an ultimatum [1] to Serbia, containing their demands, giving forty-eight hours to comply.\n",
      "On 24–25 July, the Russian Council of Ministers met at Yelagin Palace and, in response to the crisis and despite the fact that she had no alliance with Serbia, agreed to a secret partial mobilisation of over one million men of the Russian Army and the Baltic and Black Sea Fleets.\n",
      "The arguments used to support the move in the Council of Ministers were:\n",
      "Russia had backed down in the past, such as in the Liman von Sanders affair and the Bosnian Crisis, but that had only encouraged the Germans\n",
      "However, their incompetence made the Russians realise by 29 July that partial mobilisation was not militarily possible, and as it would interfere with general mobilisation.\n",
      "The Russians therefore moved to full mobilisation on 30 July as the only way to prevent the entire operation being botched.\n",
      "Serbia initially considered accepting all the terms of the Austrian ultimatum before news from Russia of premobilisation measures stiffened their resolve.The Serbs drafted their reply to the ultimatum in such a way as to give the impression of making significant concessions.\n",
      "On July 29, 1914, the Tsar ordered full mobilisation but changed his mind after receiving a telegram from Kaiser Wilhelm.\n",
      "The next day, Sazonov once more persuaded Tsar Nicholas of the need for general mobilization, and the order was issued on the same day.\n",
      "Christopher Clark stated, \"The Russian general mobilisation was one of the most momentous decisions of the July crisis.\n",
      "In response to the Austrian declaration of war on 28 July.\n",
      "On 28 July, Germany learned through its spy network that Russia had implemented its \"Period Preparatory to War\".\n",
      "That was doubly so because German war plans, the so-called Schlieffen Plan, relied upon Germany to mobilise speedily enough to defeat France first (by attacking largely through neutral Belgium) before turning to defeat the slower-moving Russians.\n",
      "Christopher Clark states, \"German efforts at mediation – which suggested that Austria should 'Halt in Belgrade' and use the occupation of the Serbian capital to ensure its terms were met – were rendered futile by the speed of Russian preparations, which threatened to force the Germans to take counter–measures before mediation could begin to take effect\".Thus, in response to Russian mobilisation, Germany ordered the state of Imminent Danger of War (SIDW) on 31 July, and when the Russian government refused to rescind its mobilisation order, Germany mobilised and declared war on Russia on 1 August.\n",
      "The Germans did not comply, and Britain declared war on Germany on 4 August 1914.\n",
      "However, the Treaty of London had not committed Britain on her own to safeguard Belgium's neutrality.\n",
      "This would have left both Britain and her Empire vulnerable to attack.British Foreign office mandarin Eyre Crowe stated:\n",
      "In that event, the existing Liberal Cabinet would lose their jobs.\n",
      "German government at the time was still dominated by the Prussian Junkers who feared the rise of these left-wing parties.\n",
      "Scenes of mass \"war euphoria\" were often doctored for propaganda purposes, and even those scenes which were genuine would not be reflective of the general population.\n",
      "However, before 1914, radical nationalists seeking full separation from the empire were still a small minority, and Austro-Hungary's political turbulence was more noisy than deep.In fact, during the decade before the war, the Habsburg lands passed through a phase of strong, widely shared economic growth.\n",
      "It was woven deeply into the culture and identity of the Serbs”.Serbian policy was complicated by the fact that the main actors in 1914 were both the official Serb government led by Nikola Pašić and the \"Black Hand\" terrorists led by the Head of Serb Military Intelligence, known as Apis.\n",
      "Imperial rivalry and the consequences of the search for imperial security or for imperial expansion had important consequences for the origins of the First World War.\n",
      "Some historians, notably MacMillan and Hew Strachan, believe that a consequence of the policy of Weltpolitik and the associated assertiveness was to isolate Germany.\n",
      "Historians like Ferguson and Clark believe that Germany's isolation was the unintended consequences of the need for Britain to defend the empire against threats from France and Russia.\n",
      "The alignment between Britain, France and Russia became known as the Triple Entente.\n",
      "The status of Morocco had been guaranteed by international agreement, and when France attempted a great expansion of its influence there without the assent of all the other signatories, Germany opposed and prompted the Moroccan Crises: the Tangier Crisis of 1905 and the Agadir Crisis of 1911.\n",
      "The French protectorate over Morocco was established officially in 1912.\n",
      "There were no major disputes there pitting any two European powers against each other.\n",
      "Marxists typically attributed the start of the war to imperialism.\n",
      "Richard Hamilton observes that the argument goes that since industrialists and bankers were seeking raw materials, new markets and new investments overseas, then if they were blocked by other powers then the \"obvious\" or \"necessary\" solution was war.Hamilton somewhat criticised the view that the war was launched to secure colonies; he agrees that while imperialism may have been on the mind of key decision makers, he argues this was not necessarily for logical, economic reasons.\n",
      "None of Germany's colonies made more money than was required to maintain them and they also only made up 0.5% of Germany's overseas trade, while only a few thousand Germans migrated to the colonies.\n",
      "However, the banks were largely excluded from the nation's foreign affairs.\n",
      "In Britain, the Chancellor of the Exchequer, Lloyd George, had been informed by the Governor of the Bank of England that business and financial interests were opposed to British intervention in the war.\n",
      "Prior to the war there were few signs that the international economy for war in the summer of 1914.\n",
      "The theories emphasised that struggle between nations and \"races\" was natural and that only the fittest nations deserved to survive.\n",
      "It gave an impetus to German assertiveness as a world economic and military power, aimed at competing with France and Britain for world power.\n",
      "Social Darwinism therefore normalised war as an instrument of policy and justified its use.\n",
      "Italy, despite being part of the Triple Alliance, did not enter the war to defend its alliance partners.\n",
      "Britain focused on building up its Royal Navy, which was already stronger than the next two navies combined.\n",
      "The French in 1897 had 3.4 million reservists, Austria 2.6 million, and Russia 4.0 million.\n",
      "All the signatories except for Germany supported disarmament.\n",
      "In any case, Germany never came close to catching up with Britain.\n",
      "Traditional narratives of the war suggested that when the war began, both sides believed that the war would end quickly.\n",
      "The principal German and French military leaders, including Moltke and Ludendorff and Joffre, expected a long war.\n",
      "German mobilisation plans assumed a two-front war against France and Russia.\n",
      "That doesn't mean that the Russians should be 'blamed' for the outbreak of war.\n",
      "Historian Fritz Fischer unleashed an intense worldwide debate in the 1960s on Germany's long-term goals.\n",
      "However, Schroeder argues, all of that was not the main cause of the war in 1914.\n",
      "Mombauer, Annika: July Crisis 1914, in: 1914-1918-online.\n",
      "\n",
      "Comments on results:\n",
      "The TFIDF + Affinity Propagation approach works reasonably well for extractive summarization:\n",
      "\n",
      "Strengths:\n",
      "- Identifies key sentences that represent distinct topics/themes in the document\n",
      "- Maintains original phrasing since it uses actual sentences\n",
      "- TFIDF helps focus on important terms while downweighting common words\n",
      "\n",
      "Limitations:\n",
      "- Some redundancy in selected sentences due to similar themes\n",
      "- May miss important details if they're expressed in non-representative ways\n",
      "- Purely statistical approach doesn't capture narrative flow\n",
      "\n",
      "TFIDF vs BOW:\n",
      "TFIDF is likely better than BOW for this task because:\n",
      "- It accounts for term importance across the document\n",
      "- Reduces impact of common but less meaningful words\n",
      "- Helps identify truly distinctive sentences rather than just those with common terms\n"
     ]
    }
   ],
   "source": [
    "print(\"Document Summary using TFIDF and Affinity Propagation:\")\n",
    "\n",
    "for index in ap.cluster_centers_indices_:\n",
    "    print(sentences[index])\n",
    "\n",
    "\n",
    "print(\"\\nComments on results:\")\n",
    "\n",
    "print(\"The TFIDF + Affinity Propagation approach works reasonably well for extractive summarization:\")\n",
    "\n",
    "print(\"\\nStrengths:\")\n",
    "print(\"- Identifies key sentences that represent distinct topics/themes in the document\")\n",
    "print(\"- Maintains original phrasing since it uses actual sentences\")\n",
    "print(\"- TFIDF helps focus on important terms while downweighting common words\")\n",
    "\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"- Some redundancy in selected sentences due to similar themes\")\n",
    "print(\"- May miss important details if they're expressed in non-representative ways\")\n",
    "print(\"- Purely statistical approach doesn't capture narrative flow\")\n",
    "\n",
    "print(\"\\nTFIDF vs BOW:\")\n",
    "print(\"TFIDF is likely better than BOW for this task because:\")\n",
    "print(\"- It accounts for term importance across the document\")\n",
    "print(\"- Reduces impact of common but less meaningful words\")\n",
    "print(\"- Helps identify truly distinctive sentences rather than just those with common terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Another use for Unsupervised Data\n",
    "\n",
    "As mentioned in class, we can use unlabled text in order to make better text representations. \n",
    "\n",
    "This is a large part of cutting edge NLP, and we'll cover this in a few lectures.\n",
    "In the meantime, we can do a simple version. In low data settings, TFIDF vectors can \n",
    "be misleading, since we don't have enough observations to get good statistics on document frequencies. \n",
    "\n",
    "In this problem, we'll use the [20 newsgroups data](https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset), which is a 20-class classification\n",
    "problem on email subject matter. In real-world applications, it is typically costly and time \n",
    "consuming to label data. In this problem we'll simulate that by keeping only the first 1000 labels.\n",
    "However, we can use the rest of the data set (and other data sets) in order to get better statistics\n",
    "on document frequencies, and therefore better text representations that make better use of our\n",
    "few labeled examples\n",
    "\n",
    "## Part 0: Load the data\n",
    " - read the docs about the dataset\n",
    " - load the 20newsgrousp data with `sklearn.datasets.fetch_20newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# ok to restart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_home\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdownload_if_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_X_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Load the filenames and data from the 20 newsgroups dataset (classification).\n",
      "\n",
      "Download it if necessary.\n",
      "\n",
      "=================   ==========\n",
      "Classes                     20\n",
      "Samples total            18846\n",
      "Dimensionality               1\n",
      "Features                  text\n",
      "=================   ==========\n",
      "\n",
      "Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "data_home : str, default=None\n",
      "    Specify a download and cache folder for the datasets. If None,\n",
      "    all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "\n",
      "subset : {'train', 'test', 'all'}, default='train'\n",
      "    Select the dataset to load: 'train' for the training set, 'test'\n",
      "    for the test set, 'all' for both, with shuffled ordering.\n",
      "\n",
      "categories : array-like, dtype=str, default=None\n",
      "    If None (default), load all the categories.\n",
      "    If not None, list of category names to load (other categories\n",
      "    ignored).\n",
      "\n",
      "shuffle : bool, default=True\n",
      "    Whether or not to shuffle the data: might be important for models that\n",
      "    make the assumption that the samples are independent and identically\n",
      "    distributed (i.i.d.), such as stochastic gradient descent.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Determines random number generation for dataset shuffling. Pass an int\n",
      "    for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "remove : tuple, default=()\n",
      "    May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "    these are kinds of text that will be detected and removed from the\n",
      "    newsgroup posts, preventing classifiers from overfitting on\n",
      "    metadata.\n",
      "\n",
      "    'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "    ends of posts that look like signatures, and 'quotes' removes lines\n",
      "    that appear to be quoting another post.\n",
      "\n",
      "    'headers' follows an exact standard; the other filters are not always\n",
      "    correct.\n",
      "\n",
      "download_if_missing : bool, default=True\n",
      "    If False, raise an OSError if the data is not locally available\n",
      "    instead of trying to download the data from the source site.\n",
      "\n",
      "return_X_y : bool, default=False\n",
      "    If True, returns `(data.data, data.target)` instead of a Bunch\n",
      "    object.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "Returns\n",
      "-------\n",
      "bunch : :class:`~sklearn.utils.Bunch`\n",
      "    Dictionary-like object, with the following attributes.\n",
      "\n",
      "    data : list of shape (n_samples,)\n",
      "        The data list to learn.\n",
      "    target: ndarray of shape (n_samples,)\n",
      "        The target labels.\n",
      "    filenames: list of shape (n_samples,)\n",
      "        The path to the location of the data.\n",
      "    DESCR: str\n",
      "        The full description of the dataset.\n",
      "    target_names: list of shape (n_classes,)\n",
      "        The names of target classes.\n",
      "\n",
      "(data, target) : tuple if `return_X_y=True`\n",
      "    A tuple of two ndarrays. The first contains a 2D array of shape\n",
      "    (n_samples, n_classes) with each row representing one sample and each\n",
      "    column representing the features. The second array of shape\n",
      "    (n_samples,) contains the target samples.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_twenty_newsgroups.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "fetch_20newsgroups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"), subset=\"train\")\n",
    "data_test = fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"), subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remove kwarg removes headers, footers and quotes from the newsgroup posts.\n",
      "This helps clean the data by removing metadata and quoted text that isn't part of the actual content,\n",
      "allowing us to focus on just the main message content for classification.\n",
      "This is important since headers/footers/quotes could contain information that makes the classification task artificially easier\n"
     ]
    }
   ],
   "source": [
    "# what does the remove kwarg do in this function?\n",
    "\n",
    "print(\"The remove kwarg removes headers, footers and quotes from the newsgroup posts.\")\n",
    "print(\"This helps clean the data by removing metadata and quoted text that isn't part of the actual content,\\nallowing us to focus on just the main message content for classification.\")\n",
    "print(\"This is important since headers/footers/quotes could contain information that makes the classification task artificially easier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of dummy classifier: 0.053\n"
     ]
    }
   ],
   "source": [
    "# What is the in-sample accuracy of a dummy model that always guesses this class?\n",
    "\n",
    "# Get the most common class in the training data\n",
    "most_common_class = np.argmax(np.bincount(data_train.target))\n",
    "\n",
    "# Calculate accuracy of always predicting the most common class\n",
    "dummy_predictions = np.full_like(data_train.target, most_common_class)\n",
    "dummy_accuracy = np.mean(dummy_predictions == data_train.target)\n",
    "\n",
    "print(f\"Accuracy of dummy classifier: {dummy_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 11314\n"
     ]
    }
   ],
   "source": [
    "# how many training examples are there?\n",
    "print(f\"Number of training examples: {len(data_train.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fit a baseline\n",
    "\n",
    "Fit a baseline on the first 1000 examples\n",
    " - Fit a `TfidfVectorizer` on the first 1000 examples\n",
    "   - use only 10k features\n",
    " - transform the first 1000 training examples and the test data\n",
    " - fit logistic regression and report the accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED_EXAMPLES = 1000\n",
    "MAX_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.940\n",
      "Test accuracy: 0.441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit TfidfVectorizer on first 1000 examples\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "X_train = vectorizer.fit_transform(data_train.data[:NUM_LABELED_EXAMPLES])\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "# Fit logistic regression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, data_train.target[:NUM_LABELED_EXAMPLES])\n",
    "\n",
    "# Get accuracy scores\n",
    "train_score = lr.score(X_train, data_train.target[:NUM_LABELED_EXAMPLES])\n",
    "test_score = lr.score(X_test, data_test.target)\n",
    "\n",
    "print(f\"Training accuracy: {train_score:.3f}\")\n",
    "print(f\"Test accuracy: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use more unsupervised data\n",
    "Repeat part 1, but fit the `TfidfVectorizer` on more data.\n",
    "\n",
    "You should include\n",
    " - all of the training docs (even though we'll only use the first 1000 labels)\n",
    " - load the train set from the IMDB sentiment analysis dataset\n",
    "\n",
    "\n",
    "Then\n",
    " - Fit the same logistic regression model and report the accuracy\n",
    " - Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(imdb_data_dir, \"train\")\n",
    "    test_dir = os.path.join(imdb_data_dir, \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "(extra_docs, _), _ = load_imdb_data_text(\"data/aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.583\n"
     ]
    }
   ],
   "source": [
    "# Combine 20 newsgroups and IMDB data for fitting vectorizer\n",
    "all_docs = data_train.data + extra_docs\n",
    "\n",
    "# Create and fit TfidfVectorizer on all available data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "vectorizer.fit(all_docs)\n",
    "\n",
    "# Transform just the training subset and test data we'll use\n",
    "X_train = vectorizer.transform(data_train.data[:1000])  # Only use first 1000 for training\n",
    "X_test = vectorizer.transform(data_train.data)\n",
    "y_train_subset = data_train.target[:1000]  # Labels for first 1000 docs\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train, y_train_subset)\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy = lr.score(X_test, data_train.target)\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of including both 20 newsgroups and IMDB data when fitting the TfidfVectorizer:\n",
      "\n",
      "Advantages:\n",
      "- The vectorizer learns term importance from a larger, more diverse corpus\n",
      "- Better estimates of IDF values since we see terms across more documents\n",
      "- May handle domain-specific terms better by learning their relative importance\n",
      "\n",
      "Potential limitations:\n",
      "- The two datasets have different styles/domains which could affect term weighting\n",
      "- Using more data doesn't necessarily improve classification if the additional data is too different\n",
      "\n",
      "Comparison to Part 1:\n",
      "- The accuracy is likely similar since we're still using the same 1000 labeled examples for training\n",
      "- Main difference is in feature representation quality from better IDF estimates\n",
      "- The larger corpus helps reduce impact of dataset-specific term frequencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Results of including both 20 newsgroups and IMDB data when fitting the TfidfVectorizer:\\n\")\n",
    "print(\"Advantages:\")\n",
    "print(\"- The vectorizer learns term importance from a larger, more diverse corpus\")\n",
    "print(\"- Better estimates of IDF values since we see terms across more documents\") \n",
    "print(\"- May handle domain-specific terms better by learning their relative importance\\n\")\n",
    "print(\"Potential limitations:\")\n",
    "print(\"- The two datasets have different styles/domains which could affect term weighting\")\n",
    "print(\"- Using more data doesn't necessarily improve classification if the additional data is too different\\n\")\n",
    "print(\"Comparison to Part 1:\")\n",
    "print(\"- The accuracy is likely similar since we're still using the same 1000 labeled examples for training\")\n",
    "print(\"- Main difference is in feature representation quality from better IDF estimates\")\n",
    "print(\"- The larger corpus helps reduce impact of dataset-specific term frequencies\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Backoff language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know that having unknown words in text is a problem for a language model. Any estimate of probability is difficult in such a scenario. \n",
    "\n",
    "### In class, we saw a simple way of smoothing probabilities by adding count 1 to every occuring ngram. While this can be a simple and effective technique we can do something a bit more clever. In this exercise we will implement two such techniques. \n",
    "\n",
    "### 1) to deal with unknown unigrams we will introduce a special `<unk>` token in our training data to represent rare tokens\n",
    "\n",
    "### 2) for unknown bigrams we will use a technique called backoff. The idea is to \"backoff\" to a lower order n-gram estimate for the probability if the n-gram is unknown. For example the probability of an unknown bigram `w_1 w_2` can be estimated by looking at the unigram probability of `w_2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "wiki_df = pd.read_csv('data/kdwd_r1k_articles.csv')\n",
    "\n",
    "def get_tokens(text):\n",
    "    return ['<s>'] + re.findall(r'\\w+', text.lower()) + ['</s>']\n",
    "\n",
    "train_sentences_list = ' '.join(wiki_df['intro_text'].iloc[:-100].tolist()).split('.')\n",
    "test_sentences_list = ' '.join(wiki_df['intro_text'].iloc[-100:].tolist()).split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's build a basic 1-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_list = [get_tokens(text) for text in train_sentences_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = Counter()\n",
    "for sentence_tokens in train_token_list:\n",
    "    unigram_counts.update(sentence_tokens)\n",
    "        \n",
    "n_unigrams = np.sum([v for _, v in unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(n_unigrams == 95491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_token_prob(token):\n",
    "    return unigram_counts[token] / n_unigrams\n",
    "\n",
    "def get_text_prob_unigram(text):\n",
    "    tokens = get_tokens(text)\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        prob = get_unigram_token_prob(t)\n",
    "        if prob > 0:\n",
    "            logp += np.log(prob)\n",
    "        else:\n",
    "            return 0.0\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_unigram('the company').round(9) == 2.455e-06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we haven't yet introduced any smoothing, meaning, out-of-vocabulary words will have a probability of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_unigram(\"onomatopoeia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have learned that we can simply add 1 to every word count to prevent this (ref: laplace smoothing). Another way however is to mark rare words within our training set as unknown words. The idea is that the model will then learn how to deal with unknown/rare words, to more correctly evaluate a test text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this, let us first identify all unigrams that occur fewer or equal than k times. Let's use k=1 to start out with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens = set()\n",
    "for token_list in train_token_list:\n",
    "    for token in token_list:\n",
    "        if unigram_counts[token] <= 1:  # k=1 as specified in the markdown\n",
    "            rare_tokens.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(rare_tokens) == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, let's create a new counter `filtered_unigram_counts` where every token that appears in `rare_tokens` is recorded as the special token `<unk>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unigram_counts = Counter()\n",
    "for token_list in train_token_list:\n",
    "    for token in token_list:\n",
    "        if token in rare_tokens:\n",
    "            filtered_unigram_counts['<unk>'] += 1\n",
    "        else:\n",
    "            filtered_unigram_counts[token] += 1\n",
    "        \n",
    "n_filtered_unigrams = np.sum([v for _, v in filtered_unigram_counts.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(filtered_unigram_counts['<unk>'] == 4859)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use these new counts, let's modify our text probability function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_unigram_token_prob(token):\n",
    "    return filtered_unigram_counts[token] / n_filtered_unigrams\n",
    "\n",
    "def get_text_prob_filtered_unigram(text):\n",
    "    tokens = [token if token not in rare_tokens else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t in tokens:\n",
    "        logp += np.log(get_filtered_unigram_token_prob(t))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_filtered_unigram_token_prob('apple').round(5) == 0.00046)\n",
    "assert(get_text_prob_filtered_unigram('the company').round(9) == 2.455e-06)\n",
    "# assert(get_text_prob_filtered_unigram(\"onomatopoeia\").round(5) == 0.00016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that now unknown words actually have a probability higher than some of the rare words that we have already seen before like `apple`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of count 1 to label words as `<unk>`was arbitrary. How could we tune is if we had more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could tune the <unk> threshold in several ways:\n",
      "\n",
      "1. Cross-validation: Split the training data into folds and evaluate model perplexity\n",
      "   using different count thresholds (e.g. 1, 2, 3, 5, 10). Choose the threshold that\n",
      "   gives best performance on validation data.\n",
      "\n",
      "2. Development set: Hold out a portion of training data as dev set. Try different\n",
      "   thresholds and measure impact on text probability/perplexity on dev set.\n",
      "\n",
      "3. Task-specific optimization: If using language model as part of downstream task\n",
      "   (e.g. text classification),tune threshold to maximize performance on that task.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"We could tune the <unk> threshold in several ways:\\n\")\n",
    "print(\"1. Cross-validation: Split the training data into folds and evaluate model perplexity\")\n",
    "print(\"   using different count thresholds (e.g. 1, 2, 3, 5, 10). Choose the threshold that\")\n",
    "print(\"   gives best performance on validation data.\\n\")\n",
    "print(\"2. Development set: Hold out a portion of training data as dev set. Try different\")\n",
    "print(\"   thresholds and measure impact on text probability/perplexity on dev set.\\n\")\n",
    "print(\"3. Task-specific optimization: If using language model as part of downstream task\")\n",
    "print(\"   (e.g. text classification),tune threshold to maximize performance on that task.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's expand our model to bigrams now. Make sure to check if each component in a bigram exists and label it as `<unk>` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bigram_counts = Counter()\n",
    "for token_list in train_token_list:\n",
    "    # Convert tokens to 'unk' if needed and count bigrams\n",
    "    filtered_tokens = [token if token in filtered_unigram_counts else '<unk>' for token in token_list]\n",
    "    for t1, t2 in zip(filtered_tokens[:-1], filtered_tokens[1:]):\n",
    "        filtered_bigram_counts[t1 + ' ' + t2] += 1\n",
    "\n",
    "def get_filtered_bigram_token_prob(token1, token2):\n",
    "    # Convert tokens to 'unk' if not in vocabulary\n",
    "    t1 = token1 if token1 in filtered_unigram_counts else '<unk>'\n",
    "    t2 = token2 if token2 in filtered_unigram_counts else '<unk>'\n",
    "    return filtered_bigram_counts[t1 + ' ' + t2] / filtered_unigram_counts[t1]\n",
    "        \n",
    "def get_text_prob_filtered_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_filtered_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_filtered_bigram('the company').round(5) == 0.00148)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We correctly get a higher probabiliy for `the company`, now that we are respecting bigrams.\n",
    "### However:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/gyj7gkzj7tx4b2crw8b452w00000gn/T/ipykernel_5481/3886096805.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "  logp += np.log(get_filtered_bigram_token_prob(t1, t2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_prob_filtered_bigram('company the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that we still get 0 for unknown bigrams. Let's fix this via Backoff. To reiterate: the idea is to default to unigram probabilities if the bigram is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backoff_bigram_token_prob(token1, token2):\n",
    "    if token1 + ' ' + token2 in filtered_bigram_counts:\n",
    "        return get_filtered_bigram_token_prob(token1, token2)\n",
    "    else:\n",
    "        return get_filtered_unigram_token_prob(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_prob_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return np.exp(logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(get_text_prob_backoff_bigram('company the').round(8) == 1.1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can happily now estimate any input text we can think of with running into issues with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if this was all worth it. Let's evaluate perplexity.\n",
    "### Specifically compare the perplexity of our filtered unigram model `get_filtered_unigram_token_prob` to our new and improved backoff bigram model `get_backoff_bigram_token_prob`\n",
    "\n",
    "### Note: For easy comparison let's only evaluate `tokens[1:]` for both models such that even the first token can already form a correct bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_ppl_filtered_unigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for token in tokens[1:]:  # Only evaluate tokens[1:] as specified\n",
    "        logp += np.log(get_filtered_unigram_token_prob(token))\n",
    "    return np.exp(-logp/len(tokens[1:]))  # Perplexity formula\n",
    "\n",
    "def get_text_ppl_backoff_bigram(text):\n",
    "    tokens = [token if token in filtered_unigram_counts else '<unk>' for token in get_tokens(text)]\n",
    "    logp = 0\n",
    "    for t1, t2 in zip(tokens[:-1], tokens[1:]):  # Process bigrams starting from second token\n",
    "        logp += np.log(get_backoff_bigram_token_prob(t1, t2))\n",
    "    return np.exp(-logp/len(tokens[1:]))  # Perplexity formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_filtered_unigram(text))\n",
    "model_unigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_list = []\n",
    "for text in test_sentences_list:\n",
    "    ppl_list.append(get_text_ppl_backoff_bigram(text))\n",
    "model_bigram_ppl = np.mean(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(model_bigram_ppl < model_unigram_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems like it worked very well. Try to find one or two examples of short strings that clearly show that our bigram model is better and why. (Short answer is OK here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: the cat sat\n",
      "Unigram perplexity: 19.75026478224891\n",
      "Bigram perplexity: 10.055457118325274\n",
      "The bigram model performs better because it captures common word sequences like 'the cat' \n",
      "\n",
      "Example 2: new york city\n",
      "Unigram perplexity: 227.60960948584065\n",
      "Bigram perplexity: 13.078937559455577\n",
      "The bigram model performs much better here because it learns common phrases like 'new york'\n",
      "The unigram model treats each word independently, missing these important word associations\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 1: the cat sat\")\n",
    "print(\"Unigram perplexity:\", get_text_ppl_filtered_unigram(\"the cat sat\"))\n",
    "print(\"Bigram perplexity:\", get_text_ppl_backoff_bigram(\"the cat sat\"))\n",
    "print(\"The bigram model performs better because it captures common word sequences like 'the cat' \\n\")\n",
    "\n",
    "print(\"Example 2: new york city\")\n",
    "print(\"Unigram perplexity:\", get_text_ppl_filtered_unigram(\"new york city\"))\n",
    "print(\"Bigram perplexity:\", get_text_ppl_backoff_bigram(\"new york city\"))\n",
    "print(\"The bigram model performs much better here because it learns common phrases like 'new york'\")\n",
    "print(\"The unigram model treats each word independently, missing these important word associations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
