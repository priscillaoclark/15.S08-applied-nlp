{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0: Other activation functions (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The leaky Relu is defined as $max(0.1x, x)$.\n",
    " - What is its derivative? (Please express in \"easy\" format\")\n",
    " - Is it suitable for back propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ANSWERS**\n",
    "\n",
    "1) The derivative of the leaky ReLU function can be expressed as:\n",
    "\n",
    "   f′(x)=0.1x if x＜0\n",
    "\n",
    "   f′(x)=x if x＞0\n",
    "\n",
    "2) Yes, leaky ReLU is suitable for backpropagation. Its derivative is simple and computationally efficient, allowing for effective gradient flow \n",
    "during training, especially in deep networks where traditional ReLU might fail due to the dying ReLU issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $tanh$ is defined as $\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    " - What is its derivative? (Please express in \"easy\" format\")\n",
    " - Is it suitable for back propagation?\n",
    " - How is it different from the sigmoid activation\n",
    " - What is an example of when to use it? When should you not use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**ANSWERS**\n",
    "\n",
    "1) The derivative of the tanh function can be derived and simplified to:\n",
    " \n",
    "   f′(x)=1−tanh^2(x)\n",
    "\n",
    "2) Yes, the tanh function is suitable for backpropagation. Its derivative is straightforward to compute, and the function itself is differentiable everywhere, which is crucial for gradient-based optimization methods.\n",
    "\n",
    "3) Difference from Sigmoid Activation:\n",
    "   - Range: The tanh function outputs values between -1 and 1, making it zero-centered. In contrast, the sigmoid function outputs values between 0 and 1.\n",
    "   - Gradient: Tanh has a stronger gradient for most inputs compared to the sigmoid function, which can be beneficial for learning but also makes it more prone to the vanishing gradient problem in very deep networks.\n",
    "\n",
    "4) Use tanh in hidden layers of neural networks where you need a non-linear activation function that is zero-centered, which can help in stabilizing the learning process.\n",
    "Avoid using tanh in very deep networks or when dealing with problems where the vanishing gradient problem might be a significant issue. In such cases, ReLU or its variants (like leaky ReLU) might be more appropriate. Additionally, for binary classification problems where the output needs to be interpreted as a probability, the sigmoid function is typically preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: The Deep Learning Recipe (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we'll follow the \"deep learning recipe\" covered in class on the IMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: load the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text\n",
    "# or copy the loading function from the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(imdb_data_dir, \"train\")\n",
    "    test_dir = os.path.join(imdb_data_dir, \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps \n",
    " - be one with data\n",
    " - set up e2e harness + get dumb baselines\n",
    " - overfit\n",
    " - regualarize\n",
    " - tune\n",
    " - squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: be one with the data\n",
    " - make some histograms\n",
    " - calculate some summary statistics\n",
    " - read a bunch of training examples and discuss any oddities you find\n",
    " - finally, turn the data into count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# make some plots, calculate some summary stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_text_lengths(docs):\n",
    "    lengths = [len(doc.split()) for doc in docs]\n",
    "    return {\n",
    "        'mean_length': np.mean(lengths),\n",
    "        'median_length': np.median(lengths),\n",
    "        'std_length': np.std(lengths),\n",
    "        'min_length': min(lengths),\n",
    "        'max_length': max(lengths),\n",
    "        'total_words': sum(lengths)\n",
    "    }\n",
    "    \n",
    "def plot_histograms(train_lengths: list, test_lengths: list):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(train_lengths, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title('Training Document Lengths')\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(test_lengths, bins=50, color='green', alpha=0.7)\n",
    "    plt.title('Test Document Lengths')\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def find_anomalies(docs: list, min_length: int = 15, max_length: int = 1500):\n",
    "    anomalies = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        length = len(doc.split())\n",
    "        if length < min_length or length > max_length:\n",
    "            anomalies.append((i, doc))\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train stats: {'mean_length': 233.7872, 'median_length': 174.0, 'std_length': 173.72955740506563, 'min_length': 10, 'max_length': 2470, 'total_words': 5844680}\n",
      "test stats: {'mean_length': 228.52668, 'median_length': 172.0, 'std_length': 168.88031504049724, 'min_length': 4, 'max_length': 2278, 'total_words': 5713167}\n"
     ]
    }
   ],
   "source": [
    "train_stats = analyze_text_lengths(train_docs)\n",
    "test_stats = analyze_text_lengths(test_docs)\n",
    "print('train stats:', train_stats)\n",
    "print('test stats:', test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lengths = [len(doc.split()) for doc in train_docs]\n",
    "test_lengths = [len(doc.split()) for doc in test_docs]\n",
    "plot_histograms(train_lengths, test_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies found:\n",
      "Document 1632: This movie is terrible but it has some good effects.\n",
      "Document 6939: You'd better choose Paul Verhoeven's even if you have watched it.\n",
      "Document 8655: Ming The Merciless does a little Bardwork and a movie most foul!\n",
      "Document 15726: Warning: Does contain spoilers.<br /><br />Open Your Eyes<br /><br />If you have not seen this film and plan on doing so, just stop reading here and take my word for it. You have to see this film. I have seen it four times so far and I still haven't made up my mind as to what exactly happened in the film. That is all I am going to say because if you have not seen this film, then stop reading right now.<br /><br />If you are still reading then I am going to pose some questions to you and maybe if anyone has any answers you can email me and let me know what you think.<br /><br />I remember my Grade 11 English teacher quite well. His name was Mr. Krisak. To me, he was wise beyond his years and he always had this circuitous way of teaching you things that perhaps you weren't all too keen on. If we didn't like Shakespeare, then he turned the story into a modern day romance with modern day language so we could understand it. Our class room was never a room, it was a cottage and we were on the lake reading a book at our own leisure time. This was his own indelible way of branding something into our sponge-like minds. <br /><br />I begin this review of Vanilla Sky with a description of this brilliant man because he once gave us an assignment that has been firmly etched in my mind, like the phone number of a long lost best friend, and it finally made some sense to me after watching The Matrix. Now if I didn't know better, I would have thought that the Wachowski brothers were really just an alias for my teacher Mr. Krisak. But giving them the benefit of the doubt, we'll assume it wasn't him. But that was the first time this assignment was anything more than impalpable. <br /><br />He had asked us to prove to him and to ourselves that were real. Show me how you can tell that you are real. This got the class spouting off all of the usual ideas that I'm sure you can imagine. Everything from pain, to sense of touch to sense of loss to sense of hunger were spouted off to our teacher to prove to him that we were real. After every scenario that we gave him, he would come back with the one answer that would leave us speechless.<br /><br />\"What if you are nothing but someone else's dream?\"<br /><br />What if you were someone else's dream? What a messed up question that is. This was a question/scenario posed to us about 15 years ago, before the astronomical use of the Internet and rapid advancement of computers. How possible could it seem back then? But if you look at today's technology, now ask yourself, what it you were a part of someone else's dream.<br /><br />Another brilliant but surreal film this year, David Lynch's Mulholland Drive explored similar areas. But Vanilla Sky goes deeper than any other film could hope to. In short this is one film that will literally (if you let it) blow your mind from all of the possibilities that surround you.<br /><br />Open your Eyes.<br /><br />Tom Cruise plays David Aames, a young, hot shot, righteous, full of himself publisher and owner of several magazines. He inherited this from his father and although he has talent and business savvy, his board of governers, the Seven Dwarfs, think he is a rich dink born with a silver spoon in his mouth. They feel he has done nothing to deserve the pinnacle of success that each and every one of them believes should go to them. <br /><br />Early in the film we meet one of David's gorgeous toys named Julie Gianni, played with pernicious but bombastic perfection by Cameron Diaz. David and Julie play a good game, both claiming they are just there to use each other and are not the slightest bit interested in a monogamous, committed relationship. This is the type of relationship commensurate with David's other flings he's had in the sexual prime of his life. And although both talk a good game, we can tell that only one is really telling the truth. <br /><br />Next we meet Brian Shelby, played with a stroke of genius by Kevin Smith's good buddy Jason Lee. Brian is writing a book that David is going to publish but they are also very good friends. This is something that David has very little of in his life and you can sense a real caring for one another early on in the film. Brian has one famous line that he keeps telling David over and over again. And that is \" the sweet ain't so sweet without the bitter.\" He goes on to tell him that one day he will find true love and not just this part time lover status that he seems to perpetrate with all of the floozies who inhabit his bed for a night or two.<br /><br />At David's huge birthday bash, (so huge that the likes of Steven Spielberg wish him a happy birthday) Brian enters with his date, Sofia Sorrano, played of course by Penelope Cruz with what has to be the best performance of this year by an actress. This is a bash by invite only and at first David and Sofia seem intrigued with one another. And in typical David fashion, despite his best friend being there, he begins to flirt with Sofia. To complicate things, Julie shows up uninvited and begins spying on David. David then spends the night with Sofia, but they only talk and draw caricatures of one another. There is no hanky panky. The next day, as David is leaving Sofia's apartment, he is greeted by Julie, who offers him a ride and from there.......well, I think we have all seen the commercials.<br /><br />That is all I will really say about the plot, because from here the film teases us with what is reality and what is blurred perception. We are introduced to a character played by Kurt Russel and a few other shady characters that all play a part in this labyrinth like haze. There is a subtext of death and possible panacea-like cure-alls that may or not be able to create the possibility of eternal life. This is just one of the intriguing possibilities the movie offers us, but it doesn't end there.<br /><br />Like many movies seem to thrive on today, this film has a secret. Sixth Sense may have began this craze, but look even further back and you can maybe thank Angel Heart for starting the craze. Regardless of how it originated, Vanilla Sky has one of it's own surreptitious gut busters. And what makes this one so much fun is that the film gives you many obvious clues along the way but not enough to give you an apodictic solution to the gauntlet of truth and lies you have just put yourself through. I have seen this film four times and every time it has been because I want to see if there is something more I can pick up, something more I can understand. To be able to work your mind in the theater, to enable it to open up to new possibilities is something rare in a film. All of the ersatz so called \"Best Pictures of the year\" have been good but nothing spectacular. They lack substance. A Beautiful Mind was intriguing but flat, The Royal Tenenbaums was interesting but uneven. Vanilla Sky is a rarity because it is a film that leaves you yearning for more yet guarantees your satisfaction because the film and those that made it care about it. I know this film has received mixed reviews but I just think that those who don't like it don't quite understand it. <br /><br />This is what film making is supposed to be like. This is what a film is supposed to do to you. It is supposed to make you feel something. Most of the other films this year have been just empty spaces. This one isn't.<br /><br />10 out of 10 The best film of the year. I would love to see this get nominated for best picture and I would love to see Cruz up for best Actress, Diaz for best supporting, Cruise for best actor and Jason Mewes should be a shoe in for best supporting actor. Cameron Crowe should there as well. None of this may come to pass, and that is a shame. This is one film that should not be missed. <br /><br />And on a final note, I am quite sure Mr. Krisak would like this film and maybe this is the one film that may answer his question. Can you prove you are real? Or are we just a figment of someone's imagination? Are we artificially transplanted for someone else's bemusement? This is a film that spawns more questions than it does answers. And I'm sure that is just fine with him.<br /><br />Open Your Eyes\n",
      "Document 15959: Adrian Pasdar is excellent is this film. He makes a fascinating woman.\n",
      "Document 17384: I wouldn't rent this one even on dollar rental night.\n",
      "Document 18387: Long, boring, blasphemous. Never have I been so glad to see ending credits roll.\n",
      "Document 18792: **Attention Spoilers**<br /><br />First of all, let me say that Rob Roy is one of the best films of the 90's. It was an amazing achievement for all those involved, especially the acting of Liam Neeson, Jessica Lange, John Hurt, Brian Cox, and Tim Roth. Michael Canton Jones painted a wonderful portrait of the honor and dishonor that men can represent in themselves. But alas...<br /><br />it constantly, and unfairly gets compared to \"Braveheart\". These are two entirely different films, probably only similar in the fact that they are both about Scots in historical Scotland. Yet, this comparison frequently bothers me because it seems to be almost assumed that \"Braveheart\" is a better film than \"Rob Roy\". I like \"Braveheart\" a lot, but the idea of comparing it to \"Rob Roy\" is a little insulting to me. To put quite simply, I love \"Braveheart\", but it is a pale shadow to how much I love \"Rob Roy\". Here are my particular reasons...<br /><br />-\"Rob Roy\" is about real people.<br /><br />Let's face it, the William Wallace in \"Braveheart\" is not a real person. He's a legend, a martyr, a larger than life figurehead. Because of this depiction, he is also a perfect person, never doing wrong, and basically showing his Scot countrymen to the promised land. When he finally does fail, it is not to his fault. Like Jesus, he is betrayed by the very people he trusted most. He even goes through the worst kind of torture because he wants freedom so much.<br /><br />The depiction of Wallace is very well done and effective. But it really doesn't inspire or intrigue me. I find human ambiguity far more facinating than human perfection. That is why \"The Last Temptation of Christ\" is a better film than \"King of Kings\", and that is also one of the reasons why I think \"Rob Roy\" is better than \"Braveheart\". Rob Roy may be heroic and brave, but he is far from perfect. He makes several mistakes that affected the lives of many of his loved ones. Now sure, not bearing false claim against the Duke of Argyll was an act of nobility and courage, but it was also an act of egoism and self centeredness. Let us not forget that the kinfolk that he had claimed to protect were driven homeless by the end of the film because of this act. But Rob did the best he could, and that was all you could ask of him.<br /><br />Rob's Wife Mary, is also a normal, ambigious person. Let us start though, with how she looks in this film. Sure, she's beautiful, but she doesn't wear makeup and she basically allows her natural beauty to show. Compare this with the two loves (or one, depending on your point of view) of William Wallace in \"Braveheart\". Now these two ladies are hot, but hardly indicitive of how women looked at the time (especially the lay persons). Maybe not a fair comparison, but just another example of how Rob Roy's attempts for accuracy are far more effective.<br /><br />Throughout \"Rob Roy\", Mary has to live with her vicious rape by the dastardly carrion, Cunningham. She feels compelled to tell Rob of her struggle, but doesn't because she knows that Rob must seek revenge for her rape. Such revenge would surely mean the death of Rob, and Mary is not prepared for such a sacrifice.<br /><br />The villains in \"Rob Roy\" are equally as compelling. Although the enemies in \"Braveheart\" are well written, they are hardly original. Robert the Bruce, a man both brave and cowardly, is plagued by moral decisions that are all to familar in the fictional realm. Should he take his claim as the king of Scotland, or should he betray Wallace in order to ensure the safety of his family name? Bruce is the most ambigious character in \"Braveheart\", but from Brutus in \"Julius Ceasar\" to Fredo in \"The Godfather Part II\", these types of characters are hardly original. Longshanks, although a compelling villain in his own right, is very one dimensional. He is the epidemy of evil, and his tyrant ways stand in direct contrast to Wallace's heroism.<br /><br />\"Rob Roy\" has three villains that are wonderful in their chicanery. First of all, let's start with Marquis of Montrose. He is a man who is so obsessed with his self image, that he's willing to let an innocent man suffer because of it. \"See to it that I am not mocked\" are his favorite words to his \"factor\". He is a man obsessed with power, upset that a man of great noble bearing as the Duke of Argyll can be considered of greater providency then him. He is shamefully self obsessed and insecure. He is an evil aristocrat, but in ways that make him unique.<br /><br />Cunningham and Callarn are the conspirators in \"Rob Roy\", and are also Roy's direct assailants. Callarn is so cunning in his cowardace that he is almost comical. He will do anything to maintain the good will of the Marquis, which includes backstabbing and trickery. Cunningham is a compelling character in that he seems to have been raised to do whatever he can to obtain status and the affection of the Marquis. He needs a father, little does he know that the Marquis is his real father. Therefore, when the opportunity to obtain wealth comes from Callarn, he grabs it without even questioning it. He is very much like the evil of modern man, so self centered and vain that he cares not about the consequences of his actions on others.<br /><br />Many have criticized Tim Roth's performance in this film as overacting. Hogwash I say. It is clear that Cunningham is not simply evil but also psychopath throughout the film. In a world where a man and his stepson can go around shooting random people for amusement, is Cunningham too much of an unbelievable character? We live in a society where people seem to have decreased the value of human life. \"Rob Roy\" simply teaches us that only the circumstances of this decreased value has changed. It is a problem throughout human history that the vanity of the human heart will not allow for the capacity for compassion. Rob Roy and Mary give us hope that goodness will prevail, but snakes will always exist in our world.<br /><br />Another character that I find fascinating is the Duke of Argyll. He is a true nobleman, and his values of honesty and courtesy are in direct contrast to the Marquis. He appreciates the bravery of Rob Roy and Mary, and has a direct vexation for the Marquis and his factor. He gives the world hope for the people of power. Hopefully, people like the Marquis are an exception and not the rule.<br /><br />- The final duel in \"Rob Roy\" is more exciting then 10 of the battle scenes in \"Braveheart\".<br /><br />One thing I get tired of is people telling me that \"Braveheart\" is a better film because of the battle scenes. First of all, battle scenes are hardly original. From \"Spartacus\" to \"Gladiator\", Hollywood has had a long tradition of historical European battle scenes. \"Braveheart\" has some of the best battle scenes ever put on film, but they suffer from one important problem. These battle scenes have no context except for the fight for freedom.<br /><br />Now, don't get me wrong, duels are hardly original either. In fact, there are probably 10 times as many films with duels as there are with battle scenes. But the context of the duel between Cunningham and Rob Roy is a beauty to behold. It is one of the greatest scenes in film history. Let me explain why...<br /><br />First of all, the fighting style and the bearing of the two characters in this duel describe the characters perfectly. Cunningham is effette and dangerous, Rob Roy is strong and courageous. Cunningham uses a fencing sword while Rob uses a broadsword. Cunningham fights with quick tricky movements, while Roy's fighting style is more obvious.<br /><br />The whole film, from the deliberately slow first half to the exciting second half, is leading up to this moment. It is powerful stuff, and it is clear that Rob must exterminate this menacing evil that has plagued his whole world. When Rob finally gets the upper hand (literally and figuratively,) it is one of the greatest moments in film history. Rob wins because he has more to live for, and his honor is more powerful than 10 Cunningham's. The use of music is absolutely chilling in this scene. Good prevailing against a real evil is more powerful to me than seeing a dude get disemboweled just so he can yell \"FREEDOM!\". But hey, maybe that's just me.<br /><br />- \"Rob Roy\" is more realistic than \"Braveheart\"<br /><br />I don't know that people in the aristocracy or Scotsmen talked like the people in \"Rob Roy\", but I do feel that it clearly an attempt to capture their speech patterns. I feel that many people are bored by \"Rob Roy\" simply because they can't understand what the characters are saying. If this is the case, then read some Shakesphere, or put on the close-captioning. \"Rob Roy\" is actually one of the greatest written films of the 90's. Many of the dialogue in this film is clever, but maybe you have to watch the film a couple of times to understand it.<br /><br />By contrast, the dialogue in \"Braveheart\" is hardly very interesting. Of course, what do you expect when the main character is a Scotsman played by an Australian? This is a legend, and there was clearly not an attempt to capture the speech of the times. This film takes place several centuries before \"Rob Roy\", and yet they talk like the people today. Thus the reason that many people like it better. Audiences today have become increasingly lazy, and they don't want to take the time or patience to understand things that are complex. Therefore, as with many epic films, they expect to see the villians speak a recognizable English accent while the heroes speak in a vernacular not too far away from our American language. Sure, it is clear that the Wallace is Scotish, but other than sounding like Scotty from Star Trek and a couple of \"Aye\"s for acknowledgement, the Scots in this film fit into the Hollywood tradition of how we believe Scots should sound.<br /><br />So, do these descriptions prove that \"Rob Roy\" is a better film than \"Braveheart\"? Hardly. But if it proves one thing, it shows that it is hardly common knowledge that \"Braveheart\" is a better film than \"Rob Roy\". To put simply, \"Rob Roy\" is a film that has themes that are very apropos in today's world. \"Braveheart\" is a film about a legend that is inspiring but hardly realistic. You can make a decision on what you think is better...<br /><br />Grade - A Score - 9\n",
      "Document 20139: Titanic directed by James Cameron presents a fictional love story on the historical setting of the Titanic. The plot is simple, noncomplicated, or not for those who love plots that twist and turn and keep you in suspense. The end of the movie can be figured out within minutes of the start of the film, but the love story is an interesting one, however. Kate Winslett is wonderful as Rose, an aristocratic young lady betrothed by Cal (Billy Zane). Early on the voyage Rose meets Jack (Leonardo DiCaprio), a lower class artist on his way to America after winning his ticket aboard Titanic in a poker game. If he wants something, he goes and gets it unlike the upper class who are so concerned with their social worries. The two fall in love and the audience sees the sinking of the Titanic primarily through their eyes.<br /><br />The movie begins in modern times with the exploration of the wreck by a group searching for treasures, that sunk with the Titanic, which has recently occurred. One of the survivors of the Titanic, Rose DeWitt Bukater, who had heard of the exploration of the wreck on television and is flown to the boat where the search is being led from to tell of what she remembers to help the search. She gets to telling her memory of the one and only voyage of the Titanic. With this, the scene shifts to Southhampton, Ireland where the Titanic set sail from on April 10, 1912 as all the passengers are boarding. After another stop on the Irish coast Titanic went out to see on its maiden voyage across the Atlantic bound for New York. Historically the first few days of the voyage went by uneventful, but the fictional plot of the story is developed during this time as Rose sees the hopeless entrapement of an engagement that she is in to the wealthy Cal Hockley and falls in love with third class passenger, Jack Dawson. Captain Smith alledgedly as shown in the movie was urged by the White Star Line Director to increase the speed of the ship so they would make the newspaper headlines and receive extra publicity by arriving in New York on Thursday night and not on Friday morning as planned. Smith then ordered the fateful decision going against his thirty-two years of experience to stretch the Titanic's legs out to full speed. The Titanic had reports that the waters in the Atlantic they were sailing in were full of icebergs, but they ignored these warnings and proceeded at full speed as shown in the movie. On April 15, 1912 at 11:39, an iceberg was sighted. They attempted to shut off the engines and turn the ship out of the path of the iceberg but there was not enough time and the ship hit the iceberg on the starboard side as depicted in the film. The portrayal of the many small holes in the side of the ship and not one large gash along the side is accurate. The crew of Titanic sent out distress calls and set off distress rockets as shown until 2:18 when the lights finally failed. The lights of the California were spotted six miles away but they failed to realize what was going on and did not respond to Titanic's many pleas for help. The California had tried earlier in the day to warn Titanic of the severe ice that had caused them to stop their trip but Titanic had brushed them off causing the California to turn off its radio and leave the Titanic on its own. The first class women and children were the first as depicted to be put into the twenty lifeboats that were on the ship. Overwhelmingly the third class passengers suffered the most amount of deaths of any class and the crew was hit hard in this tragedy too. The word of White Star Line employees and first class passengers was believed over that of second and third class passengers when authorities were trying to gain information of the sinking. Also, the metal that was used to build the Titanic has been found in recent years under conditions of severe cold, which were experienced the night Titanic sank to be extremely brittle. Overall, the basic plot is very accurate in its portayal of the events and the times at which these events took place on the Titanic.<br /><br />Many of the characters in the story were not real and created simply for the purpose of the movie or as composite characters to represent possible characteristics and ideas of people on the ship. The core group of Rose, Jack, Cal, and Rose's mother all were fictional characters added into the story as they represent different groups of people from the time. Yet many characters such as the Unsinkable Molly Brown; Captain Edward Smith; the ship designer, Thomas Andrew; the White Star Line Representative, Bruce Ismay; and all of the Titanic's officers were real. The maiden voyage was going to be Captain Edward Smith's last voyage anyway as he planned to retire afterwards. He had been a part of the White Star Line since 1880 where he worked his way up to his status as the Millionaire's Captain when the Titanic sunk. The portrayals of the officers is accurate as only four survived the tragedy except for the officer who threatened to kill all of the passengers of the ship with his pistol. He is on record as acting heroicly and was misportrayed to the point that James Cameron apologized and evoked a monument in his honor in the officer's former Canadian hometown. As shown in the movie there was a language problem between the crew and many of the lower-class passengers from non-English speaking nations. In addition, Officer Lowe was the only officer who came back in the lifeboat as depicted. The old people shown in their bed as the water came in their room were based on the Strauss'. Not wanting to leave her husband's side Mrs. Strauss refused to get in her lifeboat and died with her husband on the Titanic. Furthermore, Mr. Goggenheim who was shown sipping his brandy and smoking a cigar reportedly did go out like this dressed in his best. The richest man on the ship, John Jacob Astor, who owned most of Manhattan died nonetheless as well, but his much younger wife was saved in a lifeboat. In addition, Molly Brown was saved and later had medals made up for the crew of the Carpethia that picked the survivors of Titanic up from the water. Her ticket on the Titanic had cost over four-thousand dollars and by the end of her life she ended up broke. All of the interiors of the ship were masterfully replacated down to the last pieces of china and silverware. The gymnasium, which is hardly seen is recreated perfectly with all of the machines reproduced to match those seen in old photographs. The wonderful outfits and costuming were an excellent re-creation of the Post-Victorian era of 1912. The rich at this time practically ruled everything, as the women's suffrage movement had not quite gotten moving yet. Women during this time often married for financial security as Rose was considering doing and normally took a back seast status to their husbands as Cal wished for Rose to do. The rich did not take well to `new money' such as Molly Brown as depicted. Everything of the time was very formal. Women had to be escorted to dinner by a male figure as seen with in the dining scenes. Smoking was not very common among women of the time but holders of cigarettes, which were just coming in at the time were used as seen with Rose in the movie. Men of the time generally smoked cigars not cigarettes. Women were constained physically by their corsets and socially by society. Although James Cameron had no background in historical films he brought in experts of Titanic coupled with two years spent cross-referencing the history of the Titanic and few liberties were taken. The beautiful cinematography and special effects also helped to make the film even more breathtaking.<br /><br />A recognizable message can be seen in the movie Titanic as the people on the ship had about three hours to contemplate their demise. The director, James Cameron, shows the various reactions to this time of crisis in people's lives. Everyone reacts differently and he gets you to think of how you might have reacted had you been in that situation on the Titanic on that fateful night. In addition, this film is a reflection of the 1990's when it was produced as it gives a look into the wreck of the Titanic. Only in the past fifteen years has the site of the actual Titanic been found and explored. This movie was able to give us a deeper look into a disaster that many would not have viewed. However, the moral question of whether people today should be taking treasures from the wreck of an underwater graveyard is posed. There have been attempts to stop treasure seeking missions such as the one portrayed in Titanic but all have failed. As it stands today anyone can make a voyage to the Titanic and take whatever valuables they as portrayed in the film showing the general values of our time on this matter.<br /><br />Technically the film is very well done. To get footage of the wreck at the bottom of the ocean it took twelve dives to get all of the footage needed for the movie. In addition, a special camera had to be created to withstand the intense pressure at the bottom of the ocean. Cameron did not plan on using the probe to go as far inside Titanic as anyone has in the 88 years since the ship sunk but it worked out that this provided an unique perspective into the ship. Furthermore, throughout the film fade ins and outs from the wreck of Titanic to the scene of Titanic during its actual voyage. This shift between the modern scene to the past scene during the voyage works as an excellent transition that makes the story easy to follow in aclear manner. At the very beginning of the movie a septune recreation is used to recreate the scene when the actual people left the European coast on Titanic giving it distinction from the rest of the events of the film.<br /><br />Titanic plays almost like a historical biography and is like a work of art, a true epic. Like most history novels, we know the ending, but it doesn't take away from the wonderful treats that can be found in this picture. Certain aspects of this film are Academy Award material including costuming, sound, cintematography, and editing. If you like interesting characters that will give you an insight into the life of characters in the early 1900's and how they face disaster, then this movie definitely is for you.<br /><br />\n",
      "Document 22009: By now you've probably heard a bit about the new Disney dub of Miyazaki's classic film, Laputa: Castle In The Sky. During late summer of 1998, Disney released \"Kiki's Delivery Service\" on video which included a preview of the Laputa dub saying it was due out in \"1999\". It's obviously way past that year now, but the dub has been finally completed. And it's not \"Laputa: Castle In The Sky\", just \"Castle In The Sky\" for the dub, since Laputa is not such a nice word in Spanish (even though they use the word Laputa many times throughout the dub). You've also probably heard that world renowned composer, Joe Hisaishi, who scored the movie originally, went back to rescore the excellent music with new arrangements. Laputa came out before My Neighbor Totoro and after Nausicaa of the Valley of the Wind, which began Studio Ghibli and it's long string of hits. And in my opinion, I think it's one of Miyazaki's best films with a powerful lesson tuckered inside this two hour and four minute gem. Laputa: Castle in the Sky is a film for all ages and I urge everyone to see it.<br /><br />For those unfamiliar with Castle in the Sky's story, it begins right at the start and doesn't stop for the next two hours. The storytelling is so flawless and masterfully crafted, you see Miyazaki's true vision. And believe me, it's one fantastic one. The film begins with Sheeta, a girl with one helluva past as she is being held captive by the government on an airship. Sheeta holds the key to Laputa, the castle in the sky and a long lost civilization. The key to Laputa is a sacred pendant she has which is sought by many, namely the government, the military and the air pirate group, the Dola gang (who Sheeta and Pazu later befriend). Soon, the pirates attack the ship and she escapes during the raid. She falls a few thousand feet, but the fall is soft and thanks to her pendant. As she floats down from the sky, Pazu, an orphan boy who survives by working in the mines, sees Sheeta and catches her. The two become fast friends, but thanks to her pendant, the two get caught up in one huge thrill ride as the Dola gang and government try to capture Sheeta. One action sequence after another, we learn all of the character's motives and identities as we build to the emotional and action packed climax which will surely please all with it's fantastic animation and wonderful dialogue. Plus somewhat twisty surprise. I think this film is simply remarkable and does hold for the two hour and four minute run time. The story is wonderful, as we peak into Hayao Miyazaki's animation which has no limits. The setting of the film is a combo of many time periods. It does seem to take place at the end of the 1800s, but it is some alternante universe which has advanced technology and weapons. Laputa is also surprisingly a funny film. The film has tons of hilarious moments, almost equal to the drama and action the film holds. I think the funniest part is a fight scene where Pazu's boss faces off against a pirate, and soon after a riot breaks out. It's funny as we see the men compare their strength and the music fits right in with it perfectly.<br /><br />Now let's talk about how the dub rates. An excellent cast give some great performances to bring these characters to life. Teen heartthrob James Van Der Beek plays the hero Pazu, who has a much more mature voice then in the Japanese version, where in the original he sounded more childlike. Either way, I think his voice is a nice fit with Pazu. Anna Paquin, the young Oscar winner from \"The Piano\", plays Sheeta. This is also a nice performance, but the voice is a bit uneven, she doesn't stay true to one accent. At times she sounds as American as apple pie, but at other times she sounds like someone from New Zealand. The performance I most enjoyed however was of Coris Leachman, who played Mama Dola. Not only is this an excellent performance, but the voice and emotion she gives the character really brings it to life. If there was ever a live action Laputa movie (G-d forbid), she would be the one to play her, you can just imagine her in the role (well, somewhat). Luke Skywalker himself, Mark Hamill is Muska, and this is another top rate Hamill performance. You may be familiar with Hamill from a long line of voice work after he did the original Star Wars movies, but he renders Muska to full evil. His voice sounds like his regular voice and mix of the Joker, who he played for many episodes on the animated Batman series. Rounding out the cast is voice character actor Jim Cummings, who does a great, gruff job as the general and Andy Dick and Mandy Patakin as members of the Dola gang.<br /><br />Now let me talk about what really makes this dub special, Joe Hisaishi's newly arranged music! For those who have never heard of him, Mr. Hisaishi does the music and like all of Miyazaki's films, the music is very memorable. Each of his scores has it's own personas which fits the particular film perfectly. Now, these new arrangements he has done are more \"American like\", which I think was the goal of the new recordings. Don't worry, the classic tunes of the Japanese version are still here in great form. The score, to me, sounds to be arranged like this is a Hollywood blockbuster. It has more power, it has more emphasis, it's clearer and deeper. The film's prologue, the first seconds where we are introduced to the airships, has some new music (I am not sure, but I believe when we first saw the ships there was no music at all). But a majority of the music has new backdrops and more background music to enjoy. Things seem very enhanced. In a powerful scene, the music is more stronger then in the original versions. In a calm scene, it's more calmer. Overall, I think many of you will be pleased with the new arrangements an mixes, I highly did myself, and personally think it helps improve the film. I prefer the new score over the old one, and I hope Disney will release or license the music rights to a full blown soundtrack.<br /><br />Another plus side to the dub is that the story remains faithful, and much of the original Japanese lines are intact. In Kiki, I'm sure a few lines where changed, and this is the same way, lines have been changed. But a majority are close or exactly the original lines and dialogue Miyazaki has written. I was afraid some excellent lines would be butchered, but they were there intact. Some new lines have been added as well which help out. But I am not sure whether to consider this a good thing or a bad thing, Disney DID NOT translate the ending song, it was in Japanese. I was mortified when they did completely new songs for the Kiki dub, but with this version it's the original song... in Japanese. So I guess it's good it's still the original, but bad since a majority of people seeing this dub speak English.<br /><br />There is a big down side to this dub, and it deals with how the voices match the character's lips. Of course in any dub it won't be perfect, but I think in Kiki and Mononoke the dubbing of lines to match were much better executed (and Disney had a little bit more time with this one...). Some of the time everything matches perfect, some of the time it doesn't completley match, and in a rare case, someone says something and the lips don't move at all (there's a scene where Sheeta chuckles and her mouth doesn't move one bit).<br /><br />As far as things about the film itself, these are my thoughts. I thought the most amazing part of Laputa was the animation. From the opening sequence to the ending, the animation is so lush and detailed, you just have to watch in awe. You see the true nature of each character, true detail to their face with extreme close ups and action. You have to give a ton of credit for the effort that these animators put into this film. Everything is so well done and beautifully hand drawn, it's like a moving piece of art. And to think, this was done in the mid 1980's. The animation is quite different from Disney, Ghibli has it's own distinctive flare which is very different, but very good. And after all these years, the colors look as vibrant as ever. Laputa also has tons of action sequences, lots of plane dogfights plus a few on ground. These sequences are so well done and so intriguing, it's scary that they are comparable to a big budget action film. And the finale is just something you MUST see. The sound effects are pure and classic and fit explosions, guns firing and everything else well. And like all Miyazaki films, each one focuses on a different theme (i.g. Kiki: Confidence). This one has a great a lesson on greed and power. People don't realize how greed can take over you, and how having too much power isn't good. People are obsessed with power, and are greedy, and the main villian, Muska, greatly shows this.<br /><br />All in all, Laputa: Castle In The Sky was a great film to begin with, and is now improved for the most part. I am glad a more mainstream audience now have the chance to see this classic animated film in all it's glory. With a great voice cast who put a lot into the film with the excellent redone musical score from Joe Hisaishi, Disney has done a nice job on this dub and is quite worthy. Though I think the voices matched the mouths better in the Kiki and Princess Mononoke Disney dubs, Castle In The Sky is still a great dub and is worth the long delays because now more can expierence a fantastic film.\n",
      "Document 22441: *!!- SPOILERS - !!*<br /><br />Before I begin this, let me say that I have had both the advantages of seeing this movie on the big screen and of having seen the \"Authorized Version\" of this movie, remade by Stephen King, himself, in 1997.<br /><br />Both advantages made me appreciate this version of \"The Shining,\" all the more.<br /><br />Also, let me say that I've read Mr. King's book, \"The Shining\" on many occasions over the years, and while I love the book and am a huge fan of his work, Stanley Kubrick's retelling of this story is far more compelling ... and SCARY.<br /><br />Kubrick really knows how to convey the terror of the psyche straight to film. In the direction of the movie AND the writing of the screenplay, itself, he acquired the title \"Magus\" beyond question. Kubrick's genius is like magic. The movie world lost a great director when he died in 1999. Among his other outstanding credits are: Eyes Wide Shut, 1999; Full Metal Jacket, 1987; Barry Lyndon, 1975; A Clockwork Orange, 1971; 2001: A Space Odyssey, 1968; Spartacus, 1960 and many more.<br /><br />The Torrences (Jack, Wendy his wife and Danny, their son) are living in the Overlook Hotel for the winter; Jack has been hired as the caretaker. It is his job to oversee the upkeep of the hotel during the several months of hard snow, until spring when the Overlook reopens its doors. It seems there are many wealthy and jaded tourists who will flock to the Colorado Mountains for a snow-filled summer getaway.<br /><br />The Hotel was an impressive piece of architecture and staging. It lent to the atmosphere, by having a dark, yet at the same time \"welcoming\" atmosphere, itself. The furnishings and furniture was all period (late 70's - early 80's), and the filmography of the landscape approaching the hotel in the opening scene is brilliant. It not only lets you enjoy the approach to the Overlook, it also fixes in your mind how deserted and isolated the Hotel is from the rest of the world.<br /><br />The introduction of Wendy and Danny's characters was a stroke of genius. You get the whole story of their past, Danny's \"imaginary friend,\" Tony, and the story of Jack's alcoholism all rolled into this nice, neat introductory scene. There was no need in stretching the past history out over two hours of the movie; obviously, Kubrick saw that from the beginning.<br /><br />Closing Day. Again, the scenic drive up the mountains to the Hotel (this time, with family in tow), the interaction between Jack and Danny was hilarious while also portraying a very disturbing exchange.<br /><br />The initial tour through the Overlook is quite breathtaking, even as the \"staff\" is moving things out, you get a chance to see the majestic fire places, the high cathedral ceilings and expensive furnishings, dormants and crown moldings in the architecture. \"They did a good job! Pink and gold are my favorite colors.\" (Wendy Torrence) Even the \"staff wing\" is well designed and beautifully built.<br /><br />The maze was a magnificent touch, reminiscent of the Labyrinth in which the Minotaur of Crete was Guardian. When Jack Nicholson stands at the scaled model of the maze and stares into the center, seeing Wendy and Danny entering, it's a magickal moment; one that tells you right away, there are heavy energies in that house; there's something seriously wrong, already starting. \"I wouldn't want to go in there unless I had at least an hour to find my way out.\" (The Hotel Manager)<br /><br />Scatman Cruthers, as Dick Halloran, was genuine and open in his performance. His smiles were natural and his performance was wonderful. You could actually believe you were there in the hotel, taking the tour of the kitchen with Wendy and \"Doc.\" His explanation of \"the shining\" to Danny was very well delivered, as was his conversation with the child about Tony and the Hotel. It was believable and sincere.<br /><br />The cut out and pan scan of the hotel itself, with the mountains looming behind, the cold air swirling about, mist coming up from the warm roof of the snowbound hotel, adds so MUCH to the atmosphere of the movie. It also marks the \"half-way-to-hell\" point, so to speak; the turning point in the movie.<br /><br />Shelley Duvall's portrayal of Wendy Torrence was masterful. (So WHAT if she also played Olive Oyl?! It just shows her marvelous diversity!) Honestly, before I saw the movie on the big screen in 1980, I said,\" What? Olive Oyl? *lol* (Popeye was also released in 1980.) But I took that back as soon as the movie started. She's brilliant. In this Fiend's opinion, this is her best performance, to date! (Although I did love her in Steve Martin's \"Roxanne,\" 1987.)<br /><br />Once Kubrick has established the pearly bits of information of which you, the viewer, need to be in possession: the Torrence's past; Danny's broken arm; Tony; the history of the Hotel itself; the fact that Danny is not \"mental,\" but rather clairvoyant instead, and the general layout of the Hotel; all of which you get in the opening 3 sequences; the movie never stops scaring you.<br /><br />The two butchered daughters of the previous caretaker, Delbert Grady (the girls having appeared several times to Danny, first by way of Tony in the apartment before the family ever left for the hotel) were icons with which Danny could identify, and of which he was afraid, at the same time. They were haunting (and haunted), themselves and showed Danny how and where they were killed, in a rather graphic and material way.<br /><br />Kubrick's Tony was written as an attendant spirit, like a spirit guide which he acquired as a result of his arm nearly being wrenched off his body by his own father. He was...\"the little boy who lives in my mouth.\" He would manifest in the end of Danny's finger and physically spoke through Danny in order to speak TO Danny. NOT like in the book, I realize, where Tony was intended by Stephen King to be the projection of Danny as an older boy, trying to save his father. Kubrick left out that little twist and it somehow made it more frightening when Tony \"took ... Danny ... over.\" The idea of Danny's older self projecting back to his younger self isn't...scary.<br /><br />The \"Woman in the Shower\" scene, done by Lia Beldan (about whom I can find no other credits for having done anything before, or since) as the younger woman and Billie Gibson (who ALSO appears to suffer from a lack of credits for works before or since), was seductively obnoxious and thoroughly disgusting. It was dramatic, and frightening. Abhorrent and scary. When Nicholson looks into the mirror and sees her decomposing flesh beneath his hands; the look of sheer terror on his face was so complete and REAL.<br /><br />Jack quickly embarks on his trek from the \"jonesing\" alcoholic to a certifiable insane person. The degradation of his character's mental state is carefully and thoroughly documented by Kubrick. Jack's instant friendship with Lloyd the Bartender (as only alcoholics, would-be mental patients and drug addicts do) portrays his pressing NEED of the atmosphere to which Lloyd avails him; namely, alcohol ...\"hair of the dog that bit me.\" (Jack Torrence) In Jack's case, it's bourbon on the rocks, at no charge to Jack. \"Orders from the house.\" (Lloyd the Bartender) Nice play on words.<br /><br />When Wendy find's Jack's \"screenplay\" is nothing more than page after page of the same line typed over and over, albeit in 8 or 9 different creative styles...when he asks from the shadows, \"How do you like it?\" and Wendy whirls and screams with the baseball bat in her hand...is so poignant. It's the point where she realizes how messed up the whole situation is...how messed up Jack is. It's very scary, dramatic and delivers a strong presence. That coupled with Danny's visions of the hotel lobby filling with blood, imposed over the scene between Jack and Wendy, and with the confrontational ending to this scene, make this possibly THE strongest scene of the movie.<br /><br />The \"REDRUM\" scene. Wow. What do I say? What mother would not be totally freaked by awakening to find their young, troubled son standing over them with a huge knife, talking in that freaky little voice, exclaiming \"REDRUM\" over and over? Even if it HAD no meaning, it would still be as scary as the 7th level of HELL. It was something everyone could (and has) remember(ed). Speaking of memorable scenes...<br /><br />Nicholson's final assault on his family with an axe was perhaps one of the scariest scenes of movie history. His ad-libbed line, \"Heeeeere's Johnny!\" was a stroke of brilliance and is one of the most memorable scenes in the history of horror. It also goes down in horror movie history.<br /><br />The ending..? Kubrick's ending was perfection. I felt it ended beautifully. No smarm, no platitudinous whining, no tearfully idiotic ending for THIS movie. Just epitomized perfection. That's all I'll say on the subject of the ending.<br /><br />Who cares what was taken out?! Look what Kubrick put IN. Rent it, watch it, BUY IT. It's a classic in the horror genre, and for good reason. IT RAWKS!!<br /><br />*Me being Me* ... Take this movie, and sitck it in your Stephen King collection, and take the 1997 \"Authorized\" version done by King and stick it down in the kiddie section. That's where it belongs. .: This movie rates a 9.98 from the Fiend :.\n",
      "Document 23088: Match 1: Tag Team Table Match Bubba Ray and Spike Dudley vs Eddie Guerrero and Chris Benoit Bubba Ray and Spike Dudley started things off with a Tag Team Table Match against Eddie Guerrero and Chris Benoit. According to the rules of the match, both opponents have to go through tables in order to get the win. Benoit and Guerrero heated up early on by taking turns hammering first Spike and then Bubba Ray. A German suplex by Benoit to Bubba took the wind out of the Dudley brother. Spike tried to help his brother, but the referee restrained him while Benoit and Guerrero ganged up on him in the corner. With Benoit stomping away on Bubba, Guerrero set up a table outside. Spike dashed into the ring and somersaulted over the top rope onto Guerrero on the outside! After recovering and taking care of Spike, Guerrero slipped a table into the ring and helped the Wolverine set it up. The tandem then set up for a double superplex from the middle rope which would have put Bubba through the table, but Spike knocked the table over right before his brother came crashing down! Guerrero and Benoit propped another table in the corner and tried to Irish Whip Spike through it, but Bubba dashed in and blocked his brother. Bubba caught fire and lifted both opponents into back body drops! Bubba slammed Guerrero and Spike stomped on the Wolverine from off the top rope. Bubba held Benoit at bay for Spike to soar into the Wassup! headbutt! Shortly after, Benoit latched Spike in the Crossface, but the match continued even after Spike tapped out. Bubba came to his brother's rescue and managed to sprawl Benoit on a table. Bubba leapt from the middle rope, but Benoit moved and sent Bubba crashing through the wood! But because his opponents didn't force him through the table, Bubba was allowed to stay in the match. The first man was eliminated shortly after, though, as Spike put Eddie through a table with a Dudley Dawg from the ring apron to the outside! Benoit put Spike through a table moments later to even the score. Within seconds, Bubba nailed a Bubba Bomb that put Benoit through a table and gave the Dudleys the win! Winner: Bubba Ray and Spike Dudley<br /><br />Match 2: Cruiserweight Championship Jamie Noble vs Billy Kidman Billy Kidman challenged Jamie Noble, who brought Nidia with him to the ring, for the Cruiserweight Championship. Noble and Kidman locked up and tumbled over the ring, but raced back inside and grappled some more. When Kidman thwarted all Noble's moves, Noble fled outside the ring where Nidia gave him some encouragement. The fight spread outside the ring and Noble threw his girlfriend into the challenger. Kidman tossed Nidia aside but was taken down with a modified arm bar. Noble continued to attack Kidman's injured arm back in the ring. Kidman's injured harm hampered his offense, but he continued to battle hard. Noble tried to put Kidman away with a powerbomb but the challenger countered into a facebuster. Kidman went to finish things with a Shooting Star Press, but Noble broke up the attempt. Kidman went for the Shooting Star Press again, but this time Noble just rolled out of harm's way. Noble flipped Kidman into a power bomb soon after and got the pin to retain his WWE Cruiserweight Championship! Winner: Jamie Noble<br /><br />Match 3: European Championship William Regal vs Jeff Hardy William Regal took on Jeff Hardy next in an attempt to win back the European Championship. Jeff catapulted Regal over the top rope then took him down with a hurracanrana off the ring apron. Back in the ring, Jeff hit the Whisper in the wind to knock Regal for a loop. Jeff went for the Swanton Bomb, but Regal got his knees up to hit Jeff with a devastating shot. Jeff managed to surprise Regal with a quick rollup though and got the pin to keep the European Championship! Regal started bawling at seeing Hardy celebrate on his way back up the ramp. Winner: Jeff Hardy<br /><br />Match 4: Chris Jericho vs John Cena Chris Jericho had promised to end John Cena's career in their match at Vengeance, which came up next. Jericho tried to teach Cena a lesson as their match began by suplexing him to the mat. Jericho continued to knock Cena around the ring until his cockiness got the better of him. While on the top rope, Jericho began to showboat and allowed Cena to grab him for a superplex! Cena followed with a tilt-a-whirl slam but was taken down with a nasty dropkick to the gut. The rookie recovered and hit a belly to belly suplex but couldn't put Y2J away. Jericho launched into the Lionsault but Cena dodged the move. Jericho nailed a bulldog and then connected on the Lionsault, but did not go for the cover. He goaded Cena to his feet so he could put on the Walls of Jericho. Cena had other ideas, reversing the move into a pin attempt and getting the 1-2-3! Jericho went berserk after the match. Winner: John Cena<br /><br />Match 5: Intercontinental Championship RVD vs Brock Lesnar via disqualification The Next Big Thing and Mr. Pay-Per-View tangled with the Intercontinental Championship on the line. Brock grabbed the title from the ref and draped it over his shoulder momentarily while glaring at RVD. Van Dam 's quickness gave Brock fits early on. The big man rolled out of the ring and kicked the steel steps out of frustration. Brock pulled himself together and began to take charge. With Paul Heyman beaming at ringside, Brock slammed RVD to the hard floor outside the ring. From there, Brock began to overpower RVD, throwing him with ease over the top rope. RVD landed painfully on his back, then had to suffer from having his spine cracked against the steel ring steps. The fight returned to the ring with Brock squeezing RVD around the ribs. RVD broke away and soon after leveled Brock with a kick to the temple. RVD followed with the Rolling Thunder but Brock managed to kick out after a two-count. The fight looked like it might be over soon as RVD went for a Five-Star Frog Splash. Brock, though, hoisted Van Dam onto his shoulder and went for the F-5, but RVD whirled Brock into a DDT and followed with the Frog Splash! He went for the pin, but Heyman pulled the ref from the ring! The ref immediately called for a disqualification and soon traded blows with Heyman! After, RVD leapt onto Brock from the top rope and then threatened to hit the Van Terminator! Heyman grabbed RVD's leg and Brock picked up the champ and this time connected with the F-5 onto a steel chair! Winner: RVD<br /><br />Match 6: Booker T vs the Big Show Booker T faced the Big Show one-on-one next. Show withstood Booker T's kicks and punches and slapped Booker into the corner. After being thrown from the ring, Booker picked up a chair at ringside, but Big Show punched it back into Booker's face. Booker tried to get back into the game by choking Show with a camera cable at ringside. Booker smashed a TV monitor from the Spanish announcers' position into Show's skull, then delivered a scissors kick that put both men through the table! Booker crawled back into the ring and Big Show staggered in moments later. Show grabbed Booker's throat but was met by a low blow and a kick to the face. Booker climbed the top rope and nailed a somersaulting leg drop to get the pin! Winner: Booker T<br /><br />Announcement: Triple H entered the ring to a thunderous ovation as fans hoped to learn where The Game would end up competing. Before he could speak, Eric Bishoff stopped The Game to apologize for getting involved in his personal business. If Triple H signed with RAW, Bischoff promised his personal life would never come into play again. Bischoff said he's spent the past two years networking in Hollywood. He said everyone was looking for the next breakout WWE Superstar, and they were all talking about Triple H. Bischoff guaranteed that if Triple H signed with RAW, he'd be getting top opportunities coming his way. Stephanie McMahon stepped out to issue her own pitch. She said that because of her personal history with Triple H, the two of them know each other very well. She said the two of them were once unstoppable and they can be again. Bischoff cut her off and begged her to stop. Stephanie cited that Triple H once told her how Bischoff said Triple H had no talent and no charisma. Bischoff said he was young at the time and didn't know what he had, but he still has a lot more experience that Stephanie. The two continued to bicker back and forth, until Triple H stepped up with his microphone. The Game said it would be easy to say \"screw you\" to either one of them. Triple H went to shake Bischoff's hand, but pulled it away. He said he would rather go with the devil he knows, rather than the one he doesn't know. Before he could go any further, though, Shawn Michaels came out to shake things up. HBK said the last thing he wanted to do was cause any trouble. He didn't want to get involved, but he remembered pledging to bring Triple H to the nWo. HBK said there's nobody in the world that Triple H is better friends with. HBK told his friend to imagine the two back together again, making Bischoff's life a living hell. Triple H said that was a tempting offer. He then turned and hugged HBK, making official his switch to RAW! Triple H and HBK left, and Bischoff gloated over his victory. Bischoff said the difference between the two of them is that he's got testicles and she doesn't. Stephanie whacked Bischoff on the side of the head and left!<br /><br />Match 7: Tag Team Championship Match Christian and Lance Storm vs Hollywood Hogan and Edge The match started with loud \"USA\" chants and with Hogan shoving Christian through the ropes and out of the ring. The Canadians took over from there. But Edge scored a kick to Christian's head and planted a facebuster on Storm to get the tag to Hogan. Hogan began to Hulk up and soon caught Christian with a big boot and a leg drop! Storm broke up the count and Christian tossed Hogan from the ring where Storm superkicked the icon. Edge tagged in soon after and dropped both opponents. He speared both of them into the corner turnbuckles, but missed a spear on Strom and hit the ref hard instead. Edge nailed a DDT, but the ref was down and could not count. Test raced down and took down Hogan then leveled Edge with a boot. Storm tried to get the pin, but Edge kicked out after two. Riksihi sprinted in to fend off Test, allowing Edge to recover and spear Storm. Christian distracted the ref, though, and Y2J dashed in and clocked Edge with the Tag Team Championship! Storm rolled over and got the pinfall to win the title! Winners and New Tag Team Champions: Christian and Lance Storm<br /><br />Match 8: WWE Undisputed Championship Triple Threat Match. The Rock vs Kurt Angle and the Undertaker Three of WWE's most successful superstars lined up against each other in a Triple Threat Match with the Undisputed Championship hanging in the balance. Taker and The Rock got face to face with Kurt Angle begging for some attention off to the side. He got attention in the form of a beat down form the two other men. Soon after, Taker spilled out of the ring and The Rock brawled with Angle. Angle gave a series of suplexes that took down Rock, but the Great One countered with a DDT that managed a two-count. The fight continued outside the ring with Taker coming to life and clotheslining Angle and repeatedly smacking The Rock. Taker and Rock got into it back into the ring, and Taker dropped The Rock with a sidewalk slam to get a two-count. Rock rebounded, grabbed Taker by the throat and chokeslammed him! Angle broke up the pin attempt that likely would have given The Rock the title. The Rock retaliated by latching on the ankle lock to Kurt Angle. Angle reversed the move and Rock Bottomed the People's Champion. Soon after, The Rock disposed of Angle and hit the People's Elbow on the Undertaker. Angle tried to take advantage by disabling the Great One outside the ring and covering Taker, who kicked out after a two count. Outside the ring, Rock took a big swig from a nearby water bottle and spewed the liquid into Taker's face to blind the champion. Taker didn't stay disabled for long, and managed to overpower Rock and turn his attention to Angle. Taker landed a guillotine leg drop onto Angle, laying on the ring apron. The Rock picked himself up just in time to break up a pin attempt on Kurt Angle. Taker nailed Rock with a DDT and set him up for a chokeslam. ANgle tried sneaking up with a steel chair, but Taker caught on to that tomfoolery and smacked it out of his hands. The referee got caught in the ensuing fire and didn't see Angle knock Taker silly with a steel chair. Angle went to cover Taker as The Rock lay prone, but the Dead Man somehow got his shoulder up. Angle tried to pin Rock, but he too kicked out. The Rock got up and landed Angle in the sharpshooter! Angle looked like he was about to tap, but Taker kicked The Rock out of the submission hold. Taker picked Rock up and crashed him with the Last Ride. While the Dead Man covered him for the win, Angle raced in and picked Taker up in the ankle lock! Taker went delirious with pain, but managed to counter. He picked Angle up for the last ride, but Angle put on a triangle choke! It looked like Taker was about to pass out, but The Rock broke Angle's hold only to find himself caught in the ankle lock. Rock got out of the hold and watched Taker chokeslam Angle. Rocky hit the Rock Bottom, but Taker refused to go down and kicked out. Angle whirled Taker up into the Angle Slam but was Rock Bottomed by the Great One and pinned! Winner and New WWE Champion: The Rock<br /><br />~Finally there is a decent PPV! Lately the PPV weren't very good, but this one was a winner. I give this PPV a A-<br /><br />\n",
      "Document 24257: Some have praised _Atlantis:_The_Lost_Empire_ as a Disney adventure for adults. I don't think so--at least not for thinking adults.<br /><br />This script suggests a beginning as a live-action movie, that struck someone as the type of crap you cannot sell to adults anymore. The \"crack staff\" of many older adventure movies has been done well before, (think _The Dirty Dozen_) but _Atlantis_ represents one of the worse films in that motif. The characters are weak. Even the background that each member trots out seems stock and awkward at best. An MD/Medicine Man, a tomboy mechanic whose father always wanted sons, if we have not at least seen these before, we have seen mix-and-match quirks before. The story about how one companion, Vinny played by Don Novello (Fr. Guido Sarducci), went from flower stores to demolitions totally unconvincing.<br /><br />Only the main character, Milo Thatch, a young Atlantis-obsessed academic voiced by Michael J. Fox, has any depth to him. Milo's search for Atlantis continues that of his grandfather who raised him. The opening scene shows a much younger Milo giddily perched on a knee, as his grandfather places his pith helmet on his head.<br /><br />And while the characters were thin at best, the best part about _Atlantis_ was the voice talent. Perhaps Milo's depth is no thicker than Fox's charm. Commander Rourke loses nothing being voiced by James Garner. Although Rourke is a pretty stock military type, Garner shows his ability to breath life into characters simply by his delivery. Garner's vocal performance is the high point. I'm sorry to say Leonard Nimoy's Dying King is nothing more than obligatory. Additionally, Don Novello as the demolition expert, Vinny Santorini, was also notable for one or two well-done, funny lines--but I've always liked Father Guido Sarducci, anyway. <br /><br />Also well done was the Computer Animation. The BACKGROUND animation, that is. The character animation has not been this bad for Disney since the minimalism that drove Don Bluth out the door. The character animation does nothing if not make already flat characters appear even flatter. Aside from landscapes, buildings and vehicles there isn't much to impress.<br /><br />The plot was the worst. Some say hackneyed or trite. I'm not so sure about that. Any serviceable plot can be made into something new with the proper treatment. Shakespeare often started from a known story and plot and was famous only for putting on a new coat of paint. So the treatment is the thing. And _Atlantis_ obviously lacks that.<br /><br />I cannot begin to go into all the logic gaps without a spoiler section. The plot was bad. The plot's bridges snap like twine and the ending does not make sense. To add to that, the script and the animation is peppered with annoying sloppiness.<br /><br />** SPOILERS **<br /><br />1. Right at the beginning when Milo reveals that runic or Celtic symbols have been wrongly transliterated and the \"Coast of Ireland\" should read the \"Coast of Iceland\", we begin to have problems. The writers of the script would need to know the British take for Eire or Eireann as \"Ireland\", and completely ignore the older, Latin term Hibernia. But more than this, they need to know of the Vikings conspiracy to call the greener island Iceland and the icier island Greenland. <br /><br />By making it the matter of a mis-tranliterated \"letter\", the writers have doomed themselves to requiring a runic version of English and a post-Roman date on the script. Since this is long after Atlantis was supposed to have sunk into its undersea cave. And without visible clues and less technology than Milo had, made the inscription far less trustworthy.<br /><br />2. The Shepard's Journal could not be written before the sinking of Atlantis, or it would know nothing about the cave or the crystal lying \"in the King's eye\". It must have been written after the sinking, but without even the technology that Milo's expedition had, how the heck did anybody get by the Leviathan. So how could it know more about anything after that? And why would it be written in Atlantian? <br /><br />Automatic writing and clairvoyance or astral travel can explain these things. However clairvoyance and astral travel do not require the shepard to write in Atlantian. So it's got to be some sort automatic writing. Since noone left in Atlantis can read, it must be the spirits of the crystal beaming messages to the surface. This would have made more sense. But could also have been explained within the movie: Milo could have discovered that this power had been calling him all his life--appeared in dreams, etc. This needed to be explored in the movie.<br /><br />3. The Atlantians should simply not be able to comprehend modern languages. No-one expects that the original Indo-Europeans would be able to converse in Europe, anymore than Romans would understand that hard \"c\"s or their day became French \"ch\"s (pronounced like \"sh\"s, no less!)<br /><br />4. Current Atlantians were alive before the cataclysm--when apparently they *could* read, yet now are unable to read what they used to, or operate similar machinery. <br /><br />5. The Mass Illiteracy points out a crucial flaw in the movie. NOTHING seems to have happened to this culture. It seems suspended in air until Milo can rescue it. Even though it appears that life is not a constant struggle for survival, no-one wants to compose poetry or write novels and perhaps it is a combination of Atlantian school systems going downhill toward the end and lack of good fiction that caused Atlantis to fall into illiteracy.<br /><br />5. Kida can be excused for not knowing how to read or operate the machinery if she was so young when the Cataclysm of Stupidity set in--But ANY OF IT **HARDLY** qualifies her father for Deification!! Kashakim's foolishness almost single-handedly wiped his people from existence. Killed a bunch in the cataclysm, stalled progress (not a lot killed here, but he oversaw a massive slide in culture and progress) until someone could take the crystal to kill everybody, if they weren't boiled in lava first because the Giant Robots weren't there to protect them. <br /><br />A bolt of blue electricity should have shattered Kashakim's likeness, when Kida tried joining her father's image to the circle of GREAT Kings of Atlantis! <br /><br />6. Even though Milo was the only one who could read Atlantian, Rourke and others knew enough to look through a book of gibberish and find a page on a crystal--which he knew to be a crystal and not some stylized astrological or \"phases of the sun\" diagram.<br /><br />7. If Milo's grandfather had told Rourke about it, it still does not explain what how Rourke would have suffered from Milo's reading it as part of the book. Ripping out the page--which was dog-eared in Rourke's hand, even though Milo found NO sign of a torn page in the book apparently--only was there to tip off the viewer that \"something was not quite right\". Unless the word \"crystal\" would have set alarms off in Milo's head that somebody would try to steal it, Milo would have suspected nothing. It's just thick-headed foreshadowing.<br /><br />8. The crew's \"double-cross\" was not a character change. We learned that Vinny, Sweet, Audrey and Cookie had been going along with Rourke from the beginning. However, the \"change of heart\" falls flat. It was a change, and needed to be better motivated. Hard to do with characters who weren't given anything to begin with.<br /><br />9. Niggling little bit that the lava flows up over the dome, instead of filling in the rest of the area that we view the sequence from. It's liquid; it will not flow over the protective dome until it fills up all lower areas.<br /><br />10. The ending STINKS!-- and makes no sense other than to appease political correctness. With it's powersource restored, Atlantis is no longer a weak power, needing coddling. The giant robot guardians and the sky-cycles shooting blue lightning suggest that they have less to fear from us than they might. The technology is superior to ours, and definitely to early 20th-century. In the end Milo needs to teach the Atlantians to read, for what? The whole idea is to leave their little quiet, chastened culture alone, not to send it into hyperdrive. <br /><br />** END SPOILERS **<br /><br />Perhaps, the Lost World plot and the turn-of-the-century setting should give me a hint that this is more an homage to pulps. The failures I find with the film agree with this idea. But I am at a loss why I should pay to see thin characters and plot holes simply because many dime novels had them as well. And pulp stories is part of the \"crap they can't sell adults anymore\", anyway. We have become a bit more sophisticated and our pulp needs to grow up as well. Raiders of the Lost Ark lost none of its pulp feel and avoided so much badness.<br /><br />4 out of 10--the movie is enjoyable but as I think about the plot, it seeps ever lower.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print out some documents, find some anomalies\n",
    "anomalies = find_anomalies(train_docs)\n",
    "print(\"Anomalies found:\")\n",
    "for idx, doc in anomalies:\n",
    "    print(f\"Document {idx}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50000) (25000, 50000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(lowercase=True, max_features=50000)\n",
    "vec.fit(train_docs)\n",
    "term_document_matrix_train = vec.transform(train_docs)\n",
    "term_document_matrix_test = vec.transform(test_docs)\n",
    "print(term_document_matrix_train.shape, term_document_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: set up test harness and get baselines\n",
    " - state your baseline models and calculate the log loss and accuracy\n",
    "   - what is the best constant guess?\n",
    "   - what about a rules-based model? (e.g. checking if one of a few known words is present)\n",
    " - make a function that calculates model performance on the test set\n",
    "   - `def eval_model(your_model):`\n",
    " - make a keras model\n",
    "   - try to initialize the last layer appropriately (see [here](https://keras.io/api/layers/initializers/))\n",
    "     - `bias_initializer=Constant(some_constant)`\n",
    "   - evaluate the model with your function BEFORE training\n",
    " - examine data exactly as it is presented to the network\n",
    " - make sure you can memorize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant guess log loss: 0.6931471805599453\n",
      "Constant guess accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy and log loss for a constant guess\n",
    "constant_guess = np.ones_like(y_test) * np.mean(y_train)\n",
    "constant_log_loss = log_loss(y_test, constant_guess)\n",
    "constant_accuracy = accuracy_score(y_test, np.round(constant_guess))\n",
    "print(f'Constant guess log loss: {constant_log_loss}')\n",
    "print(f'Constant guess accuracy: {constant_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rules-based model log loss: 17.35429823379212\n",
      "Rules-based model accuracy: 0.51852\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy and log loss for a rules-based approach\n",
    "def rules_based_model(docs):\n",
    "    predictions = []\n",
    "    for doc in docs:\n",
    "        if 'good' in doc or 'excellent' in doc:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return np.array(predictions)\n",
    "\n",
    "rules_based_predictions = rules_based_model(test_docs)\n",
    "rules_based_log_loss = log_loss(y_test, rules_based_predictions)\n",
    "rules_based_accuracy = accuracy_score(y_test, rules_based_predictions)\n",
    "print(f'Rules-based model log loss: {rules_based_log_loss}')\n",
    "print(f'Rules-based model accuracy: {rules_based_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    # your code here\n",
    "    # print or return the accuracy and log loss on the test data\n",
    "    predictions = model.predict(term_document_matrix_test)\n",
    "    log_loss_value = log_loss(y_test, predictions)\n",
    "    accuracy_value = accuracy_score(y_test, np.round(predictions))\n",
    "    print(f'Model log loss: {log_loss_value}')\n",
    "    print(f'Model accuracy: {accuracy_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other keras imports\n",
    "import keras.backend as K\n",
    "from keras.initializers import Constant # for last layer initialization\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: what value of X do I need for $\\sigma(x)$ to be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">400,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │       \u001b[38;5;34m400,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">400,089</span> (1.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m400,089\u001b[0m (1.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">400,089</span> (1.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m400,089\u001b[0m (1.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a model\n",
    "# inpt = Input(shape=...)\n",
    "# hidden = ... (inpt)\n",
    "# hidden = ...(hidden)\n",
    "# ...\n",
    "# model = ...\n",
    "# model.compile... # don't forget to compile it\n",
    "\n",
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state_1 = Dense(8, activation=\"relu\")(text_input)\n",
    "hidden_state_2 = Dense(8, activation=\"relu\")(hidden_state_1)\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state_2)\n",
    "\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Model log loss: 0.6935904376604634\n",
      "Model accuracy: 0.50476\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model before training it\n",
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example training data (vectorized):\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Corresponding document:\n",
      "My wife and I took our 13 year old son to see this film and were absolutely delighted with the winsome fun of the film. It has extra appeal to boys and men who remember their childhood, but even women enjoy the film and especially Hallie Kate Eisenberg's refrain, \"Boys are so weird.\" It's refreshing to see a film that unapologetically shows that boys and girls are indeed different in their emotional and social makeup. Boys really do these kinds of strange things and usually survive to tell the story and scare their mothers silly! We enjoyed the film so much that my son and an 11 year old friend, myself and my daughters 23 year old boyfriend went to see the movie the next day for a guys day out. We had even more fun the second time around and everyone raved about it. It's clean and delightfully acted by a pre-adolescent cast reminiscent of the TV Classic \"Freaks and Geeks\". We all feel it will become a sleeper hit not unlike the \"Freaks & Geeks\" which didn't survive its first season but sold-out its DVD release. Do see it especially if you have boys and you'll find it stimulates conversation about fun and safety! Girls will love it because of the opportunity it affords to say, \"Boys are so weird!\" Don't miss it...\n"
     ]
    }
   ],
   "source": [
    "# examine data as it is presented to the network\n",
    "print(\"Example training data (vectorized):\")\n",
    "print(term_document_matrix_train[0].toarray())\n",
    "print(\"Corresponding document:\")\n",
    "print(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example 0:\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Corresponding document:\n",
      "My wife and I took our 13 year old son to see this film and were absolutely delighted with the winsome fun of the film. It has extra appeal to boys and men who remember their childhood, but even women enjoy the film and especially Hallie Kate Eisenberg's refrain, \"Boys are so weird.\" It's refreshing to see a film that unapologetically shows that boys and girls are indeed different in their emotional and social makeup. Boys really do these kinds of strange things and usually survive to tell the story and scare their mothers silly! We enjoyed the film so much that my son and an 11 year old friend, myself and my daughters 23 year old boyfriend went to see the movie the next day for a guys day out. We had even more fun the second time around and everyone raved about it. It's clean and delightfully acted by a pre-adolescent cast reminiscent of the TV Classic \"Freaks and Geeks\". We all feel it will become a sleeper hit not unlike the \"Freaks & Geeks\" which didn't survive its first season but sold-out its DVD release. Do see it especially if you have boys and you'll find it stimulates conversation about fun and safety! Girls will love it because of the opportunity it affords to say, \"Boys are so weird!\" Don't miss it...\n",
      "\n",
      "Training example 1:\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Corresponding document:\n",
      "\"54\" is a film based on the infamous \"Studio 54\" of the 1970s - the hangout for the social elite and party clubbers. In the film, Ryan Phillippe is the main character, based on an actual employee of Studio from 1977 - 1982.<br /><br />The film's problem is that it's all glitter and style and no substance. It tries to be a really grimy and probing satire like \"Boogie Nights\" but ultimately comes across as an inferior wannabe. Mike Myers is given the thankless task of playing cocaine-snorting club owner Steve Rubell. It's only a slightly comedic role and if this was Myers' best attempts at sliding into drama like Lemmon and other comedic actors did in their time, it's a total failure.<br /><br />\"54\" could have been insightful and interesting but instead it's just another dumb teen flick that isn't entertaining or even remotely engaging. View at your own peril.\n",
      "\n",
      "Training example 2:\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Corresponding document:\n",
      "What a wonderful, fanciful movie \"Stardust\" is.<br /><br />I could easily end it with that one statement and suffice to say, one could take it as a very strong recommendation to go see it.<br /><br />At a time when Hollywood seems bent on forcing remakes and sequels down our throats, \"Stardust\" makes us remember why we go to the movies in the first place - to escape reality for a couple of hours and explore other lives, other times, or other planets. Ironically, \"Stardust\" takes us to all three places effortlessly and with a childlike glee we all long for.<br /><br />\"Stardust\" is full of all the characters we remember as children: princes, witches, pirates, ghosts and scoundrels. It has the damsel in distress, the hero, the rogues, the obstacles, spells, antidotes, charms, and even a touch of light-speed to make it quasi modern.<br /><br />\"Stardust\" is about a man from the town of Wall, which is conveniently situated next to a wall that separates their town from a magical kingdom. The only way past the wall is through a breech that is diligently guarded by a scruffy old codger (played wonderfully by David Kelly). One day a young man from Wall named Ben Barnes out maneuvers the old guard and escapes through the breech. He happens upon an enchanted kingdom called Stormhold where he meets a chained (and very sexy) young lady named Una. She is held captive by a witch and leashed by an unbreakable chain. While the witch is away, Una seduces Ben and sends him on his way. Ben returns to Wall without incident and continues his life. But nine months later he is summoned to the wall breech where the old guard hands him what you might expect - a baby boy.<br /><br />The boy, named Tristan grows up to be a rather hapless young man (Charlie Cox) who is smitten with a girl way out of his league and also betrothed to another. Nevertheless, the young lady (named Victoria and played Sienna Miller) goes out once with Tristain and he confesses his love to her. After they espy a falling star, she tells him he can have her if he retrieves the star and brings it back to her. He agrees and sets out on his quest, which will take him to the other side of the wall.<br /><br />Meanwhile in the kingdom of Stormhold, the old king (perfectly played by Peter O'Toole) is dying. He calls his remaining living sons to tell them who shall succeed him to the throne. His sons' names are Primus, Secondus, Sextmus, and Septimus. The other sons where killed by the other brothers in a humorous competition to see who lives to get the throne.<br /><br />Anyway, he tosses his ruby charm to the sky and Voila, that what brings the star to earth.<br /><br />The star crashes in the form of a beautiful woman named Yvaine (Clare Danes) and she, of course, is wearing the charm. But little does she know she is now being persuaded by Tristain, the Princes, and also an aging witch named Lamia (Michelle Pfeiffer) who wants to cut out the stars heart to regain her own youth.<br /><br />Complicated? Yes. But it all comes together as the adventure unfolds.<br /><br />Tristain is the first to find Yvaine but is so blinded by his devotion to Victoria he doesn't recognize the growing bond between he and Yvaine. His initial interest lie only in returning Yvaine to Victoria as proof of his love. But he must get past the princes and Lamia first. The princes aren't that big an issue as they are constantly trying to kill each other - and just as in \"Pirates of the Caribbean\" - never has death been so funny. <br /><br />But Tristain also encounters the witch who enslaved his mother (though he doesn't know it's his mother) and a band of flying pirates led by Robert DeNiro.<br /><br />His is the most important character in the movie and DeNiro plays it to a tee. He steals the movie with his toughness and soon we learn an undercover secret that will leave audiences on the floor with laughter. Though his role is small in length, DeNiro is extraordinary!<br /><br />Michelle Pfeiffer is wonderful as Lamia - a sexy evil witch. Claire Danes is most appropriate as the confused and distressed Yvaine. She makes a perfect damsel. Jason Flemyng, Adam Buston, Rupert Everett, and Mark Strong add the perfect dose of levity as the fighting princes whom, as they die return as ghosts ala \"Blithe Spirit\" and \"High Spirits\".<br /><br />Moreover director Matthew Vaughn, whose only other directing experience was \"Layer Cake\", weaves an enchanting tale that everyone will enjoy.<br /><br />\"Stardust\" may be too complex for young children, but anyone over the age of 13 will want to see this movie multiple times. It's that good. \"Stardust\" is what movies are supposed to be. Perfectly written, perfectly cast, perfectly directed, and perfectly acted. In other words...perfect.\n",
      "\n",
      "Training example 3:\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Corresponding document:\n",
      "This movie is nothing like \"Office Space\" except in the premise. Office Space was hilarious. I would not recommend this movie to anyone, as I laughed not once during the entire film. Mr. Cornbluth's self-indulgent tirades quickly become more annoying than 15 Jason Alexanders in the same room. If you decide to see this movie, use a free rental or watch it at someone else's house so you can leave if necessary.\n",
      "\n",
      "Training example 4:\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "Corresponding document:\n",
      "First off, the movie was not true to facts at all. I just saw the documentary a few days earlier and the movie wasn't anything like it. First of all Nash was a genius at mathematics and this is what the movie should have been about not a story about a man who was cured and who found love at the end and so on. Also there are a lot of scenes that were just plain wrong - the scene where he rode around with a bike at the campus happened in his early university years not after it. In my opinion Russell Crowe didn't fit to this part at all since he doesn't look the intelligent/individualist type, therefore he really couldn't play one. It would have been great if it would have focused more on the mathematics (similar to Pi) and not the over-dramatized lovelife. At this level ABM was too hollywood-ish and too superficial to be great. Personally I think he wasn't mad nor paranoid and he was onto something since people of that caliber tend to know more than we \"lesser mortals\". 5/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print out a few training examples\n",
    "# they should be vectors of counts.\n",
    "# turn them back into words\n",
    "for i in range(5):\n",
    "    print(f\"Training example {i}:\")\n",
    "    print(term_document_matrix_train[i].toarray())\n",
    "    print(\"Corresponding document:\")\n",
    "    print(train_docs[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - acc: 0.6000 - loss: 0.6840 - val_acc: 0.5043 - val_loss: 0.6935\n",
      "Epoch 2/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.6700 - loss: 0.6509 - val_acc: 0.5026 - val_loss: 0.6972\n",
      "Epoch 3/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.6900 - loss: 0.6140 - val_acc: 0.5057 - val_loss: 0.6936\n",
      "Epoch 4/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.8400 - loss: 0.5716 - val_acc: 0.5107 - val_loss: 0.6885\n",
      "Epoch 5/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.9200 - loss: 0.5331 - val_acc: 0.5108 - val_loss: 0.6894\n",
      "Epoch 6/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.9600 - loss: 0.4971 - val_acc: 0.5069 - val_loss: 0.6992\n",
      "Epoch 7/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.9700 - loss: 0.4626 - val_acc: 0.5048 - val_loss: 0.7114\n",
      "Epoch 8/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.9700 - loss: 0.4314 - val_acc: 0.5043 - val_loss: 0.7218\n",
      "Epoch 9/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.9800 - loss: 0.4024 - val_acc: 0.5044 - val_loss: 0.7313\n",
      "Epoch 10/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 0.9800 - loss: 0.3762 - val_acc: 0.5052 - val_loss: 0.7402\n",
      "Epoch 11/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 1.0000 - loss: 0.3518 - val_acc: 0.5053 - val_loss: 0.7509\n",
      "Epoch 12/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 1.0000 - loss: 0.3297 - val_acc: 0.5054 - val_loss: 0.7625\n",
      "Epoch 13/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - acc: 1.0000 - loss: 0.3092 - val_acc: 0.5056 - val_loss: 0.7725\n",
      "Epoch 14/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 1.0000 - loss: 0.2901 - val_acc: 0.5062 - val_loss: 0.7817\n",
      "Epoch 15/15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - acc: 1.0000 - loss: 0.2725 - val_acc: 0.5064 - val_loss: 0.7906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x37d696740>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you can \"memorize\" or complete fit a small batch of data\n",
    "# try the first 100 training examples\n",
    "# the loss should go to near 0 pretty quickly\n",
    "#model.fit(...)\n",
    "model.fit(term_document_matrix_train[:100], \n",
    "          y_train[:100], \n",
    "          epochs=15,\n",
    "          batch_size=512,\n",
    "          validation_data=(term_document_matrix_test, y_test), \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Model log loss: 0.7906306528667495\n",
      "Model accuracy: 0.50644\n"
     ]
    }
   ],
   "source": [
    "eval_model(model)\n",
    "# at this point, the model is probably over fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Overfit\n",
    " - make the network large, and convince yourself you can overfit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m12,800,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,833,281</span> (48.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,833,281\u001b[0m (48.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,833,281</span> (48.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,833,281\u001b[0m (48.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state_1 = Dense(256, activation=\"relu\")(text_input)\n",
    "hidden_state_2 = Dense(128, activation=\"relu\")(hidden_state_1)\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state_2)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - acc: 0.7756 - loss: 0.4684 - val_acc: 0.8724 - val_loss: 0.3330\n",
      "Epoch 2/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 135ms/step - acc: 0.9601 - loss: 0.1202 - val_acc: 0.8590 - val_loss: 0.4248\n",
      "Epoch 3/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 136ms/step - acc: 0.9874 - loss: 0.0507 - val_acc: 0.8644 - val_loss: 0.4900\n",
      "Epoch 4/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 134ms/step - acc: 0.9955 - loss: 0.0191 - val_acc: 0.8626 - val_loss: 0.6029\n",
      "Epoch 5/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 134ms/step - acc: 0.9993 - loss: 0.0050 - val_acc: 0.8602 - val_loss: 0.6944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x34e2c6b30>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "#model.fit(...)\n",
    "model.fit(term_document_matrix_train, \n",
    "          y_train, \n",
    "          epochs=5,\n",
    "          batch_size=512,\n",
    "          validation_data=(term_document_matrix_test, y_test), \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularize\n",
    " - use regularizers, dropout, network size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,032</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │     \u001b[38;5;34m1,600,032\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model code here\n",
    "# just like you did in the previous part\n",
    "# add dropout, regularization, maybe remove a Dense layer\n",
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(\n",
    "    32,\n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-4),\n",
    ")(text_input)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "hidden_state = Dense(\n",
    "    16,\n",
    "    activation=\"relu\",\n",
    ")(hidden_state)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - acc: 0.7088 - loss: 0.5847 - val_acc: 0.8673 - val_loss: 0.3657\n",
      "Epoch 2/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - acc: 0.9027 - loss: 0.3049 - val_acc: 0.8843 - val_loss: 0.3170\n",
      "Epoch 3/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - acc: 0.9455 - loss: 0.1998 - val_acc: 0.8857 - val_loss: 0.3210\n",
      "Epoch 4/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - acc: 0.9651 - loss: 0.1480 - val_acc: 0.8760 - val_loss: 0.3671\n",
      "Epoch 5/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 72ms/step - acc: 0.9800 - loss: 0.1091 - val_acc: 0.8756 - val_loss: 0.4028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x37d56dea0>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(term_document_matrix_train, \n",
    "          y_train, \n",
    "          epochs=5,\n",
    "          batch_size=512,\n",
    "          validation_data=(term_document_matrix_test, y_test), \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - 6: Tune and Squeeze\n",
    "It will take a long time to tune the number of units in the Dense layers, so we will skip the tune phase. \n",
    "\n",
    "### Todo\n",
    " - Retrain the model\n",
    " - Make sure let it train enough\n",
    " - use callbacks to make sure the network stops before overfitting too much \n",
    " - use callbacks to reduce the learning rate appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,032</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │     \u001b[38;5;34m1,600,032\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model code here\n",
    "K.clear_session()\n",
    "text_input = Input(shape=(term_document_matrix_train.shape[1],))\n",
    "hidden_state = Dense(\n",
    "    32,\n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-4),\n",
    ")(text_input)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "hidden_state = Dense(\n",
    "    16,\n",
    "    activation=\"relu\",\n",
    ")(hidden_state)\n",
    "hidden_state = Dropout(0.2)(hidden_state)\n",
    "output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "model = Model(text_input, output)\n",
    "model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# add these callbacks just like we did in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 74ms/step - acc: 0.7098 - loss: 0.5891 - val_acc: 0.8756 - val_loss: 0.3516 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - acc: 0.9103 - loss: 0.2834 - val_acc: 0.8889 - val_loss: 0.3049 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m48/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - acc: 0.9518 - loss: 0.1767\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - acc: 0.9517 - loss: 0.1766 - val_acc: 0.8848 - val_loss: 0.3286 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - acc: 0.9722 - loss: 0.1205\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - acc: 0.9722 - loss: 0.1206 - val_acc: 0.8860 - val_loss: 0.3317 - learning_rate: 1.0000e-04\n",
      "Epoch 4: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x31f25b910>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "lr = ReduceLROnPlateau(patience=1, verbose=True)\n",
    "es = EarlyStopping(patience=2, verbose=True)\n",
    "\n",
    "model.fit(\n",
    "    term_document_matrix_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=512,\n",
    "    validation_data=(term_document_matrix_test, y_test),\n",
    "    callbacks=[lr, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Model log loss: 0.31718860503451907\n",
      "Model accuracy: 0.88596\n"
     ]
    }
   ],
   "source": [
    "eval_model(model)\n",
    "# you should be able to get > 88% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Transfer Learning (30 %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will explore a technique called transfer learning. Often, we don't have very much labeled data for the problem at hand (we call it __data-poor__), but we can find labeled data for a similar problem (which we call ___data-rich__). \n",
    "\n",
    "In transfer learning, we use the __data-rich problem__ to train an network with good performance. We then make a similar network for the __data-poor problem__ but use the weights learned from the first problem in this network. This greatly reduces the amount of data needed to train the data-poor problem. You can think of this as reducing the number of free parameters. \n",
    "\n",
    "Here, we will use the mnist digit recognition problem. We will pretend that we are interested in telling the difference between the digits `4` and `9`, but we only have 10 labeled examples. We will pretend that we have tons of labeled examples of all of the other digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some imports\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Subset the data into two datasets\n",
    " 1. One part will have `x_train_49`, `y_train_49`, etc. which has only `4`s and `9`s. \n",
    " 2. The second part will have variables `x_train_rest` etc, which will have the rest of the data and none of the digits `4` and `9`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9457, 784)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_training_data(data):\n",
    "    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "    data = data.astype('float32') / 255\n",
    "    return data\n",
    "\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "\n",
    "def subset_to_9_and_4(x, y):  # this is a new function\n",
    "    mask = (y == 9) | (y == 4)\n",
    "    new_x = x[mask]\n",
    "    new_y = (y[mask] == 4).astype('int64')\n",
    "    return new_x, new_y\n",
    "\n",
    "def subset_to_rest(x, y):  # this is a new function\n",
    "    mask = ~((y == 9) | (y == 4))\n",
    "    new_x = x[mask]\n",
    "    new_y = y[mask]\n",
    "    return new_x, new_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_training_data(x_train)\n",
    "x_test = preprocess_training_data(x_test)\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "y_train_ohe = preprocess_targets(y_train, num_classes)\n",
    "y_test_ohe = preprocess_targets(y_test, num_classes)\n",
    "\n",
    "train_frac = 0.8\n",
    "cutoff = int(x_train.shape[0] * train_frac)\n",
    "x_train, x_val = x_train[:cutoff], x_train[cutoff:]\n",
    "y_train, y_val = y_train[:cutoff], y_train[cutoff:]\n",
    "y_train_ohe, y_val_ohe = y_train_ohe[:cutoff], y_train_ohe[cutoff:]\n",
    "\n",
    "x_train_49, y_train_49 = subset_to_9_and_4(x_train, y_train)\n",
    "x_val_49, y_val_49 = subset_to_9_and_4(x_val, y_val)\n",
    "x_test_49, y_test_49 = subset_to_9_and_4(x_test, y_test)\n",
    "\n",
    "print(x_train_49.shape)\n",
    "\n",
    "x_train_rest, y_train_rest = subset_to_rest(x_train, y_train)\n",
    "x_test_rest, y_test_rest = subset_to_rest(x_test, y_test)\n",
    "\n",
    "y_train_rest_ohe = to_categorical(y_train_rest, num_classes)\n",
    "y_test_rest_ohe = to_categorical(y_test_rest, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(y_train_rest_ohe[0])\n",
    "print(y_train_rest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "# show distinct classes in x_train_rest\n",
    "print(np.unique(y_train_rest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will throw away most of the training data for the 4-9 problem\n",
    " - we will keep only 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 10\n",
    "x_train_49, y_train_49 = x_train_49[:num_points], y_train_49[:num_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build a neural network to fit the `rest` data.\n",
    " - Include 2 densely connected hidden layers with 256 neurons each.\n",
    " - The output dimension should be either 8 or 10, depending on how you do the problem\n",
    " - Compute the accuracy score for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "num_hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ digit_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ digit_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">269,322</span> (1.03 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m269,322\u001b[0m (1.03 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit_input = Input(shape=(x_train_rest.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model_rest = ...\n",
    "#model_rest.compile( ... # to be removed\n",
    "hidden_1 = Dense(num_hidden_units, activation='relu')(digit_input)\n",
    "hidden_2 = Dense(num_hidden_units, activation='relu')(hidden_1)\n",
    "output = Dense(num_classes, activation='softmax')(hidden_2)\n",
    "\n",
    "model_rest = Model(digit_input, output)\n",
    "model_rest.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rest.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model for 10 epochs and compute the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7527 - loss: 0.8757 - val_accuracy: 0.9474 - val_loss: 0.1792\n",
      "Epoch 2/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1556 - val_accuracy: 0.9640 - val_loss: 0.1226\n",
      "Epoch 3/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9707 - loss: 0.0998 - val_accuracy: 0.9690 - val_loss: 0.0978\n",
      "Epoch 4/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9792 - loss: 0.0748 - val_accuracy: 0.9735 - val_loss: 0.0859\n",
      "Epoch 5/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9830 - loss: 0.0610 - val_accuracy: 0.9742 - val_loss: 0.0862\n",
      "Epoch 6/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9857 - loss: 0.0470 - val_accuracy: 0.9767 - val_loss: 0.0725\n",
      "Epoch 7/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0352 - val_accuracy: 0.9789 - val_loss: 0.0714\n",
      "Epoch 8/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0274 - val_accuracy: 0.9803 - val_loss: 0.0645\n",
      "Epoch 9/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9943 - loss: 0.0221 - val_accuracy: 0.9801 - val_loss: 0.0669\n",
      "Epoch 10/10\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9950 - loss: 0.0196 - val_accuracy: 0.9821 - val_loss: 0.0616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3c2c35690>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#model_rest.fit(...\n",
    "model_rest.fit(x_train_rest, \n",
    "               y_train_rest_ohe, \n",
    "               epochs=10, \n",
    "               batch_size=512, \n",
    "               validation_data=(x_test_rest, y_test_rest_ohe), \n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9982616901397705\n",
      "Test accuracy: 0.9821450710296631\n"
     ]
    }
   ],
   "source": [
    "#accuracy_score(...\n",
    "# Compute accuracy on rest data\n",
    "train_acc = model_rest.evaluate(x_train_rest, y_train_rest_ohe, verbose=0)[1]\n",
    "test_acc = model_rest.evaluate(x_test_rest, y_test_rest_ohe, verbose=0)[1]\n",
    "print(f'Training accuracy: {train_acc}')\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fit a model on the `4`-`9` data\n",
    " - ### Use the same 2 densely-connected layers with 256 hidden units\n",
    " - ### Here the output layer could have 1 or two units, depending on how you set up the problem\n",
    " - ### NB: DO NOT use `K.clear_session()` because we need stuff for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ digit_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ digit_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,009</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m267,009\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,009</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m267,009\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit_input_49 = Input(shape=(x_train_49.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model49 = Model(...\n",
    "#model49.compile( ...\n",
    "hidden_1 = Dense(256, activation='relu')(digit_input_49)\n",
    "hidden_2 = Dense(256, activation='relu')(hidden_1)\n",
    "output = Dense(1, activation='sigmoid')(hidden_2)\n",
    "\n",
    "model49 = Model(digit_input_49, output)\n",
    "model49.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model49.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 0.4000 - loss: 0.6950 - val_accuracy: 0.5681 - val_loss: 0.6633\n",
      "Epoch 2/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9000 - loss: 0.4835 - val_accuracy: 0.5737 - val_loss: 0.6502\n",
      "Epoch 3/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.3439 - val_accuracy: 0.5848 - val_loss: 0.6410\n",
      "Epoch 4/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.2442 - val_accuracy: 0.6058 - val_loss: 0.6355\n",
      "Epoch 5/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1706 - val_accuracy: 0.6230 - val_loss: 0.6358\n",
      "Epoch 6/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1160 - val_accuracy: 0.6422 - val_loss: 0.6450\n",
      "Epoch 7/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0765 - val_accuracy: 0.6487 - val_loss: 0.6648\n",
      "Epoch 8/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0492 - val_accuracy: 0.6560 - val_loss: 0.6945\n",
      "Epoch 9/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0313 - val_accuracy: 0.6628 - val_loss: 0.7310\n",
      "Epoch 10/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0197 - val_accuracy: 0.6714 - val_loss: 0.7709\n",
      "Epoch 11/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0125 - val_accuracy: 0.6718 - val_loss: 0.8123\n",
      "Epoch 12/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.6744 - val_loss: 0.8532\n",
      "Epoch 13/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6757 - val_loss: 0.8932\n",
      "Epoch 14/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.6761 - val_loss: 0.9318\n",
      "Epoch 15/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.6774 - val_loss: 0.9687\n",
      "Epoch 16/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6778 - val_loss: 1.0037\n",
      "Epoch 17/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.6787 - val_loss: 1.0371\n",
      "Epoch 18/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.1255e-04 - val_accuracy: 0.6787 - val_loss: 1.0689\n",
      "Epoch 19/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.1220e-04 - val_accuracy: 0.6799 - val_loss: 1.0989\n",
      "Epoch 20/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.7604e-04 - val_accuracy: 0.6804 - val_loss: 1.1273\n",
      "Epoch 21/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.8169e-04 - val_accuracy: 0.6804 - val_loss: 1.1541\n",
      "Epoch 22/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.1495e-04 - val_accuracy: 0.6808 - val_loss: 1.1793\n",
      "Epoch 23/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.6689e-04 - val_accuracy: 0.6804 - val_loss: 1.2030\n",
      "Epoch 24/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.3219e-04 - val_accuracy: 0.6808 - val_loss: 1.2252\n",
      "Epoch 25/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.0647e-04 - val_accuracy: 0.6804 - val_loss: 1.2460\n",
      "Epoch 26/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.7061e-05 - val_accuracy: 0.6799 - val_loss: 1.2655\n",
      "Epoch 27/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.2267e-05 - val_accuracy: 0.6804 - val_loss: 1.2836\n",
      "Epoch 28/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 6.0836e-05 - val_accuracy: 0.6808 - val_loss: 1.3005\n",
      "Epoch 29/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.1784e-05 - val_accuracy: 0.6804 - val_loss: 1.3163\n",
      "Epoch 30/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.4588e-05 - val_accuracy: 0.6808 - val_loss: 1.3310\n",
      "Epoch 31/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 3.8811e-05 - val_accuracy: 0.6799 - val_loss: 1.3447\n",
      "Epoch 32/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 3.4133e-05 - val_accuracy: 0.6804 - val_loss: 1.3574\n",
      "Epoch 33/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.0320e-05 - val_accuracy: 0.6804 - val_loss: 1.3692\n",
      "Epoch 34/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.7183e-05 - val_accuracy: 0.6808 - val_loss: 1.3802\n",
      "Epoch 35/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.4569e-05 - val_accuracy: 0.6808 - val_loss: 1.3904\n",
      "Epoch 36/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.2373e-05 - val_accuracy: 0.6821 - val_loss: 1.3998\n",
      "Epoch 37/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.0516e-05 - val_accuracy: 0.6825 - val_loss: 1.4086\n",
      "Epoch 38/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.8934e-05 - val_accuracy: 0.6825 - val_loss: 1.4167\n",
      "Epoch 39/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.7580e-05 - val_accuracy: 0.6825 - val_loss: 1.4242\n",
      "Epoch 40/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.6412e-05 - val_accuracy: 0.6834 - val_loss: 1.4312\n",
      "Epoch 41/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.5400e-05 - val_accuracy: 0.6834 - val_loss: 1.4377\n",
      "Epoch 42/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.4519e-05 - val_accuracy: 0.6834 - val_loss: 1.4437\n",
      "Epoch 43/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.3748e-05 - val_accuracy: 0.6829 - val_loss: 1.4493\n",
      "Epoch 44/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.3071e-05 - val_accuracy: 0.6829 - val_loss: 1.4545\n",
      "Epoch 45/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.2472e-05 - val_accuracy: 0.6829 - val_loss: 1.4593\n",
      "Epoch 46/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 1.1944e-05 - val_accuracy: 0.6829 - val_loss: 1.4637\n",
      "Epoch 47/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 1.1479e-05 - val_accuracy: 0.6829 - val_loss: 1.4678\n",
      "Epoch 48/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 1.1066e-05 - val_accuracy: 0.6825 - val_loss: 1.4716\n",
      "Epoch 49/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 1.0696e-05 - val_accuracy: 0.6817 - val_loss: 1.4752\n",
      "Epoch 50/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.0365e-05 - val_accuracy: 0.6817 - val_loss: 1.4785\n",
      "Epoch 51/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.0066e-05 - val_accuracy: 0.6817 - val_loss: 1.4816\n",
      "Epoch 52/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 9.7948e-06 - val_accuracy: 0.6817 - val_loss: 1.4845\n",
      "Epoch 53/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.5476e-06 - val_accuracy: 0.6817 - val_loss: 1.4872\n",
      "Epoch 54/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 9.3223e-06 - val_accuracy: 0.6812 - val_loss: 1.4897\n",
      "Epoch 55/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 9.1156e-06 - val_accuracy: 0.6812 - val_loss: 1.4921\n",
      "Epoch 56/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 8.9253e-06 - val_accuracy: 0.6817 - val_loss: 1.4943\n",
      "Epoch 57/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 8.7496e-06 - val_accuracy: 0.6817 - val_loss: 1.4964\n",
      "Epoch 58/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.5864e-06 - val_accuracy: 0.6817 - val_loss: 1.4984\n",
      "Epoch 59/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 8.4344e-06 - val_accuracy: 0.6817 - val_loss: 1.5003\n",
      "Epoch 60/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.2924e-06 - val_accuracy: 0.6817 - val_loss: 1.5021\n",
      "Epoch 61/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.1594e-06 - val_accuracy: 0.6812 - val_loss: 1.5038\n",
      "Epoch 62/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 8.0346e-06 - val_accuracy: 0.6812 - val_loss: 1.5054\n",
      "Epoch 63/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 7.9172e-06 - val_accuracy: 0.6817 - val_loss: 1.5069\n",
      "Epoch 64/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.8064e-06 - val_accuracy: 0.6817 - val_loss: 1.5084\n",
      "Epoch 65/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.7015e-06 - val_accuracy: 0.6817 - val_loss: 1.5098\n",
      "Epoch 66/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.6018e-06 - val_accuracy: 0.6817 - val_loss: 1.5111\n",
      "Epoch 67/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.5076e-06 - val_accuracy: 0.6817 - val_loss: 1.5124\n",
      "Epoch 68/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.4211e-06 - val_accuracy: 0.6817 - val_loss: 1.5136\n",
      "Epoch 69/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.3368e-06 - val_accuracy: 0.6812 - val_loss: 1.5148\n",
      "Epoch 70/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.2542e-06 - val_accuracy: 0.6812 - val_loss: 1.5159\n",
      "Epoch 71/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.1737e-06 - val_accuracy: 0.6812 - val_loss: 1.5170\n",
      "Epoch 72/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.0990e-06 - val_accuracy: 0.6812 - val_loss: 1.5181\n",
      "Epoch 73/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.0267e-06 - val_accuracy: 0.6812 - val_loss: 1.5191\n",
      "Epoch 74/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.9565e-06 - val_accuracy: 0.6812 - val_loss: 1.5201\n",
      "Epoch 75/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.8882e-06 - val_accuracy: 0.6812 - val_loss: 1.5211\n",
      "Epoch 76/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.8217e-06 - val_accuracy: 0.6812 - val_loss: 1.5220\n",
      "Epoch 77/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.7573e-06 - val_accuracy: 0.6812 - val_loss: 1.5229\n",
      "Epoch 78/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.6940e-06 - val_accuracy: 0.6812 - val_loss: 1.5238\n",
      "Epoch 79/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.6336e-06 - val_accuracy: 0.6817 - val_loss: 1.5247\n",
      "Epoch 80/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.5730e-06 - val_accuracy: 0.6812 - val_loss: 1.5255\n",
      "Epoch 81/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.5122e-06 - val_accuracy: 0.6817 - val_loss: 1.5264\n",
      "Epoch 82/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.4553e-06 - val_accuracy: 0.6817 - val_loss: 1.5273\n",
      "Epoch 83/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.3993e-06 - val_accuracy: 0.6817 - val_loss: 1.5281\n",
      "Epoch 84/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.3444e-06 - val_accuracy: 0.6817 - val_loss: 1.5289\n",
      "Epoch 85/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.2901e-06 - val_accuracy: 0.6817 - val_loss: 1.5297\n",
      "Epoch 86/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.2364e-06 - val_accuracy: 0.6817 - val_loss: 1.5305\n",
      "Epoch 87/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.1832e-06 - val_accuracy: 0.6817 - val_loss: 1.5313\n",
      "Epoch 88/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.1314e-06 - val_accuracy: 0.6817 - val_loss: 1.5321\n",
      "Epoch 89/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.0806e-06 - val_accuracy: 0.6812 - val_loss: 1.5329\n",
      "Epoch 90/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.0302e-06 - val_accuracy: 0.6812 - val_loss: 1.5337\n",
      "Epoch 91/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 5.9800e-06 - val_accuracy: 0.6812 - val_loss: 1.5344\n",
      "Epoch 92/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.9299e-06 - val_accuracy: 0.6812 - val_loss: 1.5352\n",
      "Epoch 93/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.8799e-06 - val_accuracy: 0.6812 - val_loss: 1.5360\n",
      "Epoch 94/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.8301e-06 - val_accuracy: 0.6812 - val_loss: 1.5368\n",
      "Epoch 95/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.7810e-06 - val_accuracy: 0.6808 - val_loss: 1.5376\n",
      "Epoch 96/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.7324e-06 - val_accuracy: 0.6808 - val_loss: 1.5383\n",
      "Epoch 97/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.6856e-06 - val_accuracy: 0.6808 - val_loss: 1.5391\n",
      "Epoch 98/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.6376e-06 - val_accuracy: 0.6808 - val_loss: 1.5400\n",
      "Epoch 99/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.5897e-06 - val_accuracy: 0.6808 - val_loss: 1.5408\n",
      "Epoch 100/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.5431e-06 - val_accuracy: 0.6808 - val_loss: 1.5416\n",
      "Epoch 101/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.4964e-06 - val_accuracy: 0.6808 - val_loss: 1.5424\n",
      "Epoch 102/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.4498e-06 - val_accuracy: 0.6808 - val_loss: 1.5432\n",
      "Epoch 103/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.4033e-06 - val_accuracy: 0.6808 - val_loss: 1.5439\n",
      "Epoch 104/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.3573e-06 - val_accuracy: 0.6808 - val_loss: 1.5447\n",
      "Epoch 105/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.3113e-06 - val_accuracy: 0.6808 - val_loss: 1.5454\n",
      "Epoch 106/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.2655e-06 - val_accuracy: 0.6808 - val_loss: 1.5462\n",
      "Epoch 107/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.2197e-06 - val_accuracy: 0.6808 - val_loss: 1.5469\n",
      "Epoch 108/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.1742e-06 - val_accuracy: 0.6808 - val_loss: 1.5477\n",
      "Epoch 109/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.1292e-06 - val_accuracy: 0.6808 - val_loss: 1.5484\n",
      "Epoch 110/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.0844e-06 - val_accuracy: 0.6808 - val_loss: 1.5492\n",
      "Epoch 111/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.0413e-06 - val_accuracy: 0.6808 - val_loss: 1.5499\n",
      "Epoch 112/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 4.9967e-06 - val_accuracy: 0.6808 - val_loss: 1.5508\n",
      "Epoch 113/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 4.9527e-06 - val_accuracy: 0.6812 - val_loss: 1.5516\n",
      "Epoch 114/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 4.9089e-06 - val_accuracy: 0.6812 - val_loss: 1.5524\n",
      "Epoch 115/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.8647e-06 - val_accuracy: 0.6812 - val_loss: 1.5531\n",
      "Epoch 116/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.8201e-06 - val_accuracy: 0.6812 - val_loss: 1.5539\n",
      "Epoch 117/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 4.7752e-06 - val_accuracy: 0.6812 - val_loss: 1.5547\n",
      "Epoch 118/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.7301e-06 - val_accuracy: 0.6812 - val_loss: 1.5555\n",
      "Epoch 119/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.6847e-06 - val_accuracy: 0.6817 - val_loss: 1.5562\n",
      "Epoch 120/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.6392e-06 - val_accuracy: 0.6821 - val_loss: 1.5570\n",
      "Epoch 121/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.5936e-06 - val_accuracy: 0.6821 - val_loss: 1.5577\n",
      "Epoch 122/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.5479e-06 - val_accuracy: 0.6821 - val_loss: 1.5585\n",
      "Epoch 123/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.5022e-06 - val_accuracy: 0.6821 - val_loss: 1.5592\n",
      "Epoch 124/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.4564e-06 - val_accuracy: 0.6821 - val_loss: 1.5600\n",
      "Epoch 125/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.4106e-06 - val_accuracy: 0.6821 - val_loss: 1.5607\n",
      "Epoch 126/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.3650e-06 - val_accuracy: 0.6821 - val_loss: 1.5614\n",
      "Epoch 127/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 4.3217e-06 - val_accuracy: 0.6821 - val_loss: 1.5623\n",
      "Epoch 128/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 4.2759e-06 - val_accuracy: 0.6821 - val_loss: 1.5632\n",
      "Epoch 129/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.2292e-06 - val_accuracy: 0.6825 - val_loss: 1.5641\n",
      "Epoch 130/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.1843e-06 - val_accuracy: 0.6825 - val_loss: 1.5650\n",
      "Epoch 131/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.1394e-06 - val_accuracy: 0.6825 - val_loss: 1.5659\n",
      "Epoch 132/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.0946e-06 - val_accuracy: 0.6825 - val_loss: 1.5668\n",
      "Epoch 133/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.0497e-06 - val_accuracy: 0.6825 - val_loss: 1.5677\n",
      "Epoch 134/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.0049e-06 - val_accuracy: 0.6825 - val_loss: 1.5686\n",
      "Epoch 135/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.9601e-06 - val_accuracy: 0.6825 - val_loss: 1.5695\n",
      "Epoch 136/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.9155e-06 - val_accuracy: 0.6825 - val_loss: 1.5704\n",
      "Epoch 137/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.8709e-06 - val_accuracy: 0.6825 - val_loss: 1.5713\n",
      "Epoch 138/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.8263e-06 - val_accuracy: 0.6825 - val_loss: 1.5723\n",
      "Epoch 139/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.7828e-06 - val_accuracy: 0.6825 - val_loss: 1.5733\n",
      "Epoch 140/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 3.7397e-06 - val_accuracy: 0.6825 - val_loss: 1.5742\n",
      "Epoch 141/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.6969e-06 - val_accuracy: 0.6825 - val_loss: 1.5753\n",
      "Epoch 142/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.6540e-06 - val_accuracy: 0.6825 - val_loss: 1.5763\n",
      "Epoch 143/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.6117e-06 - val_accuracy: 0.6825 - val_loss: 1.5774\n",
      "Epoch 144/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.5693e-06 - val_accuracy: 0.6825 - val_loss: 1.5783\n",
      "Epoch 145/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.5279e-06 - val_accuracy: 0.6825 - val_loss: 1.5794\n",
      "Epoch 146/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.4855e-06 - val_accuracy: 0.6825 - val_loss: 1.5805\n",
      "Epoch 147/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.4445e-06 - val_accuracy: 0.6825 - val_loss: 1.5816\n",
      "Epoch 148/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.4034e-06 - val_accuracy: 0.6825 - val_loss: 1.5827\n",
      "Epoch 149/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.3623e-06 - val_accuracy: 0.6825 - val_loss: 1.5838\n",
      "Epoch 150/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.3215e-06 - val_accuracy: 0.6825 - val_loss: 1.5849\n",
      "Epoch 151/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.2809e-06 - val_accuracy: 0.6825 - val_loss: 1.5859\n",
      "Epoch 152/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.2405e-06 - val_accuracy: 0.6825 - val_loss: 1.5870\n",
      "Epoch 153/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.2003e-06 - val_accuracy: 0.6825 - val_loss: 1.5881\n",
      "Epoch 154/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.1604e-06 - val_accuracy: 0.6825 - val_loss: 1.5891\n",
      "Epoch 155/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.1208e-06 - val_accuracy: 0.6825 - val_loss: 1.5902\n",
      "Epoch 156/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.0813e-06 - val_accuracy: 0.6825 - val_loss: 1.5913\n",
      "Epoch 157/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.0424e-06 - val_accuracy: 0.6825 - val_loss: 1.5925\n",
      "Epoch 158/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 3.0035e-06 - val_accuracy: 0.6825 - val_loss: 1.5936\n",
      "Epoch 159/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.9652e-06 - val_accuracy: 0.6825 - val_loss: 1.5948\n",
      "Epoch 160/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.9272e-06 - val_accuracy: 0.6825 - val_loss: 1.5959\n",
      "Epoch 161/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.8894e-06 - val_accuracy: 0.6825 - val_loss: 1.5971\n",
      "Epoch 162/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.8523e-06 - val_accuracy: 0.6825 - val_loss: 1.5983\n",
      "Epoch 163/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.8152e-06 - val_accuracy: 0.6825 - val_loss: 1.5996\n",
      "Epoch 164/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.7787e-06 - val_accuracy: 0.6825 - val_loss: 1.6008\n",
      "Epoch 165/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 2.7426e-06 - val_accuracy: 0.6825 - val_loss: 1.6020\n",
      "Epoch 166/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.7067e-06 - val_accuracy: 0.6825 - val_loss: 1.6032\n",
      "Epoch 167/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.6711e-06 - val_accuracy: 0.6825 - val_loss: 1.6044\n",
      "Epoch 168/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 2.6359e-06 - val_accuracy: 0.6825 - val_loss: 1.6056\n",
      "Epoch 169/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.6009e-06 - val_accuracy: 0.6825 - val_loss: 1.6068\n",
      "Epoch 170/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.5664e-06 - val_accuracy: 0.6825 - val_loss: 1.6080\n",
      "Epoch 171/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.5329e-06 - val_accuracy: 0.6825 - val_loss: 1.6093\n",
      "Epoch 172/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 2.4986e-06 - val_accuracy: 0.6825 - val_loss: 1.6106\n",
      "Epoch 173/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.4654e-06 - val_accuracy: 0.6825 - val_loss: 1.6119\n",
      "Epoch 174/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.4329e-06 - val_accuracy: 0.6825 - val_loss: 1.6132\n",
      "Epoch 175/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 2.4003e-06 - val_accuracy: 0.6825 - val_loss: 1.6144\n",
      "Epoch 176/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.3680e-06 - val_accuracy: 0.6825 - val_loss: 1.6156\n",
      "Epoch 177/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.3360e-06 - val_accuracy: 0.6825 - val_loss: 1.6168\n",
      "Epoch 178/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.3044e-06 - val_accuracy: 0.6825 - val_loss: 1.6179\n",
      "Epoch 179/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.2732e-06 - val_accuracy: 0.6825 - val_loss: 1.6191\n",
      "Epoch 180/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.2423e-06 - val_accuracy: 0.6825 - val_loss: 1.6202\n",
      "Epoch 181/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.2122e-06 - val_accuracy: 0.6825 - val_loss: 1.6214\n",
      "Epoch 182/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.1821e-06 - val_accuracy: 0.6825 - val_loss: 1.6228\n",
      "Epoch 183/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.1525e-06 - val_accuracy: 0.6821 - val_loss: 1.6241\n",
      "Epoch 184/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.1234e-06 - val_accuracy: 0.6821 - val_loss: 1.6254\n",
      "Epoch 185/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.0946e-06 - val_accuracy: 0.6821 - val_loss: 1.6267\n",
      "Epoch 186/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.0661e-06 - val_accuracy: 0.6821 - val_loss: 1.6279\n",
      "Epoch 187/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.0380e-06 - val_accuracy: 0.6821 - val_loss: 1.6292\n",
      "Epoch 188/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.0102e-06 - val_accuracy: 0.6821 - val_loss: 1.6304\n",
      "Epoch 189/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.9827e-06 - val_accuracy: 0.6821 - val_loss: 1.6317\n",
      "Epoch 190/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.9556e-06 - val_accuracy: 0.6821 - val_loss: 1.6329\n",
      "Epoch 191/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.9290e-06 - val_accuracy: 0.6821 - val_loss: 1.6341\n",
      "Epoch 192/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.9025e-06 - val_accuracy: 0.6821 - val_loss: 1.6353\n",
      "Epoch 193/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.8764e-06 - val_accuracy: 0.6821 - val_loss: 1.6365\n",
      "Epoch 194/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.8507e-06 - val_accuracy: 0.6821 - val_loss: 1.6376\n",
      "Epoch 195/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.8261e-06 - val_accuracy: 0.6821 - val_loss: 1.6389\n",
      "Epoch 196/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.8011e-06 - val_accuracy: 0.6821 - val_loss: 1.6403\n",
      "Epoch 197/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.7761e-06 - val_accuracy: 0.6821 - val_loss: 1.6416\n",
      "Epoch 198/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 1.7521e-06 - val_accuracy: 0.6821 - val_loss: 1.6429\n",
      "Epoch 199/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.7283e-06 - val_accuracy: 0.6821 - val_loss: 1.6442\n",
      "Epoch 200/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.7049e-06 - val_accuracy: 0.6821 - val_loss: 1.6455\n",
      "Epoch 201/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.6819e-06 - val_accuracy: 0.6821 - val_loss: 1.6468\n",
      "Epoch 202/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.6590e-06 - val_accuracy: 0.6821 - val_loss: 1.6480\n",
      "Epoch 203/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.6365e-06 - val_accuracy: 0.6825 - val_loss: 1.6492\n",
      "Epoch 204/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.6142e-06 - val_accuracy: 0.6825 - val_loss: 1.6504\n",
      "Epoch 205/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.5924e-06 - val_accuracy: 0.6825 - val_loss: 1.6517\n",
      "Epoch 206/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.5709e-06 - val_accuracy: 0.6825 - val_loss: 1.6530\n",
      "Epoch 207/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.5498e-06 - val_accuracy: 0.6825 - val_loss: 1.6542\n",
      "Epoch 208/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.5289e-06 - val_accuracy: 0.6825 - val_loss: 1.6555\n",
      "Epoch 209/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.5086e-06 - val_accuracy: 0.6825 - val_loss: 1.6568\n",
      "Epoch 210/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.4883e-06 - val_accuracy: 0.6829 - val_loss: 1.6581\n",
      "Epoch 211/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.4685e-06 - val_accuracy: 0.6829 - val_loss: 1.6593\n",
      "Epoch 212/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.4490e-06 - val_accuracy: 0.6829 - val_loss: 1.6606\n",
      "Epoch 213/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.4296e-06 - val_accuracy: 0.6829 - val_loss: 1.6618\n",
      "Epoch 214/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.4106e-06 - val_accuracy: 0.6829 - val_loss: 1.6630\n",
      "Epoch 215/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.3923e-06 - val_accuracy: 0.6829 - val_loss: 1.6642\n",
      "Epoch 216/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.3737e-06 - val_accuracy: 0.6829 - val_loss: 1.6656\n",
      "Epoch 217/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.3556e-06 - val_accuracy: 0.6829 - val_loss: 1.6669\n",
      "Epoch 218/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.3379e-06 - val_accuracy: 0.6829 - val_loss: 1.6681\n",
      "Epoch 219/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.3204e-06 - val_accuracy: 0.6829 - val_loss: 1.6694\n",
      "Epoch 220/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.3031e-06 - val_accuracy: 0.6829 - val_loss: 1.6706\n",
      "Epoch 221/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.2863e-06 - val_accuracy: 0.6829 - val_loss: 1.6718\n",
      "Epoch 222/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.2698e-06 - val_accuracy: 0.6829 - val_loss: 1.6730\n",
      "Epoch 223/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.2534e-06 - val_accuracy: 0.6829 - val_loss: 1.6742\n",
      "Epoch 224/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.2373e-06 - val_accuracy: 0.6829 - val_loss: 1.6754\n",
      "Epoch 225/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2215e-06 - val_accuracy: 0.6829 - val_loss: 1.6767\n",
      "Epoch 226/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.2057e-06 - val_accuracy: 0.6829 - val_loss: 1.6781\n",
      "Epoch 227/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1904e-06 - val_accuracy: 0.6829 - val_loss: 1.6794\n",
      "Epoch 228/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1753e-06 - val_accuracy: 0.6829 - val_loss: 1.6808\n",
      "Epoch 229/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.1606e-06 - val_accuracy: 0.6829 - val_loss: 1.6821\n",
      "Epoch 230/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1461e-06 - val_accuracy: 0.6834 - val_loss: 1.6833\n",
      "Epoch 231/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.1318e-06 - val_accuracy: 0.6834 - val_loss: 1.6846\n",
      "Epoch 232/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 1.1177e-06 - val_accuracy: 0.6834 - val_loss: 1.6858\n",
      "Epoch 233/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.1038e-06 - val_accuracy: 0.6834 - val_loss: 1.6870\n",
      "Epoch 234/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.0901e-06 - val_accuracy: 0.6834 - val_loss: 1.6881\n",
      "Epoch 235/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.0766e-06 - val_accuracy: 0.6834 - val_loss: 1.6893\n",
      "Epoch 236/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.0633e-06 - val_accuracy: 0.6834 - val_loss: 1.6904\n",
      "Epoch 237/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.0503e-06 - val_accuracy: 0.6834 - val_loss: 1.6915\n",
      "Epoch 238/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.0378e-06 - val_accuracy: 0.6834 - val_loss: 1.6928\n",
      "Epoch 239/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.0252e-06 - val_accuracy: 0.6834 - val_loss: 1.6940\n",
      "Epoch 240/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 1.0126e-06 - val_accuracy: 0.6838 - val_loss: 1.6953\n",
      "Epoch 241/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.0004e-06 - val_accuracy: 0.6838 - val_loss: 1.6965\n",
      "Epoch 242/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 9.8854e-07 - val_accuracy: 0.6838 - val_loss: 1.6977\n",
      "Epoch 243/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 9.7681e-07 - val_accuracy: 0.6842 - val_loss: 1.6989\n",
      "Epoch 244/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 9.6519e-07 - val_accuracy: 0.6842 - val_loss: 1.7000\n",
      "Epoch 245/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.5374e-07 - val_accuracy: 0.6842 - val_loss: 1.7011\n",
      "Epoch 246/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 9.4268e-07 - val_accuracy: 0.6842 - val_loss: 1.7023\n",
      "Epoch 247/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 9.3151e-07 - val_accuracy: 0.6838 - val_loss: 1.7035\n",
      "Epoch 248/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.2058e-07 - val_accuracy: 0.6834 - val_loss: 1.7047\n",
      "Epoch 249/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 9.0997e-07 - val_accuracy: 0.6834 - val_loss: 1.7059\n",
      "Epoch 250/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 8.9947e-07 - val_accuracy: 0.6834 - val_loss: 1.7070\n",
      "Epoch 251/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.8905e-07 - val_accuracy: 0.6829 - val_loss: 1.7081\n",
      "Epoch 252/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 8.7883e-07 - val_accuracy: 0.6829 - val_loss: 1.7092\n",
      "Epoch 253/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.6875e-07 - val_accuracy: 0.6829 - val_loss: 1.7102\n",
      "Epoch 254/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 8.5883e-07 - val_accuracy: 0.6829 - val_loss: 1.7113\n",
      "Epoch 255/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 8.4920e-07 - val_accuracy: 0.6829 - val_loss: 1.7124\n",
      "Epoch 256/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 8.3957e-07 - val_accuracy: 0.6829 - val_loss: 1.7136\n",
      "Epoch 257/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 8.3008e-07 - val_accuracy: 0.6829 - val_loss: 1.7147\n",
      "Epoch 258/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.2083e-07 - val_accuracy: 0.6829 - val_loss: 1.7159\n",
      "Epoch 259/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.1171e-07 - val_accuracy: 0.6829 - val_loss: 1.7170\n",
      "Epoch 260/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.0271e-07 - val_accuracy: 0.6829 - val_loss: 1.7181\n",
      "Epoch 261/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.9383e-07 - val_accuracy: 0.6829 - val_loss: 1.7191\n",
      "Epoch 262/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.8509e-07 - val_accuracy: 0.6829 - val_loss: 1.7202\n",
      "Epoch 263/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 7.7646e-07 - val_accuracy: 0.6829 - val_loss: 1.7213\n",
      "Epoch 264/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 7.6795e-07 - val_accuracy: 0.6829 - val_loss: 1.7223\n",
      "Epoch 265/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.5956e-07 - val_accuracy: 0.6829 - val_loss: 1.7233\n",
      "Epoch 266/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.5148e-07 - val_accuracy: 0.6829 - val_loss: 1.7244\n",
      "Epoch 267/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 7.4336e-07 - val_accuracy: 0.6829 - val_loss: 1.7255\n",
      "Epoch 268/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.3527e-07 - val_accuracy: 0.6825 - val_loss: 1.7266\n",
      "Epoch 269/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.2747e-07 - val_accuracy: 0.6825 - val_loss: 1.7277\n",
      "Epoch 270/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.1974e-07 - val_accuracy: 0.6825 - val_loss: 1.7287\n",
      "Epoch 271/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.1208e-07 - val_accuracy: 0.6825 - val_loss: 1.7297\n",
      "Epoch 272/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 7.0452e-07 - val_accuracy: 0.6825 - val_loss: 1.7307\n",
      "Epoch 273/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 6.9726e-07 - val_accuracy: 0.6825 - val_loss: 1.7317\n",
      "Epoch 274/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 6.8995e-07 - val_accuracy: 0.6825 - val_loss: 1.7328\n",
      "Epoch 275/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 6.8266e-07 - val_accuracy: 0.6825 - val_loss: 1.7338\n",
      "Epoch 276/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.7562e-07 - val_accuracy: 0.6825 - val_loss: 1.7349\n",
      "Epoch 277/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.6866e-07 - val_accuracy: 0.6825 - val_loss: 1.7359\n",
      "Epoch 278/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.6179e-07 - val_accuracy: 0.6825 - val_loss: 1.7369\n",
      "Epoch 279/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.5501e-07 - val_accuracy: 0.6825 - val_loss: 1.7379\n",
      "Epoch 280/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.4834e-07 - val_accuracy: 0.6825 - val_loss: 1.7388\n",
      "Epoch 281/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.4172e-07 - val_accuracy: 0.6825 - val_loss: 1.7398\n",
      "Epoch 282/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.3524e-07 - val_accuracy: 0.6825 - val_loss: 1.7408\n",
      "Epoch 283/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.2881e-07 - val_accuracy: 0.6825 - val_loss: 1.7418\n",
      "Epoch 284/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.2253e-07 - val_accuracy: 0.6825 - val_loss: 1.7429\n",
      "Epoch 285/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.1636e-07 - val_accuracy: 0.6821 - val_loss: 1.7439\n",
      "Epoch 286/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 6.1025e-07 - val_accuracy: 0.6825 - val_loss: 1.7449\n",
      "Epoch 287/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 6.0421e-07 - val_accuracy: 0.6825 - val_loss: 1.7458\n",
      "Epoch 288/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.9827e-07 - val_accuracy: 0.6825 - val_loss: 1.7468\n",
      "Epoch 289/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.9238e-07 - val_accuracy: 0.6825 - val_loss: 1.7477\n",
      "Epoch 290/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.8659e-07 - val_accuracy: 0.6825 - val_loss: 1.7486\n",
      "Epoch 291/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.8086e-07 - val_accuracy: 0.6825 - val_loss: 1.7496\n",
      "Epoch 292/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.7520e-07 - val_accuracy: 0.6825 - val_loss: 1.7505\n",
      "Epoch 293/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.6976e-07 - val_accuracy: 0.6825 - val_loss: 1.7514\n",
      "Epoch 294/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.6430e-07 - val_accuracy: 0.6825 - val_loss: 1.7524\n",
      "Epoch 295/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.5880e-07 - val_accuracy: 0.6825 - val_loss: 1.7534\n",
      "Epoch 296/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.5354e-07 - val_accuracy: 0.6825 - val_loss: 1.7544\n",
      "Epoch 297/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.4834e-07 - val_accuracy: 0.6825 - val_loss: 1.7553\n",
      "Epoch 298/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 5.4317e-07 - val_accuracy: 0.6825 - val_loss: 1.7562\n",
      "Epoch 299/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 5.3803e-07 - val_accuracy: 0.6825 - val_loss: 1.7571\n",
      "Epoch 300/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.3299e-07 - val_accuracy: 0.6825 - val_loss: 1.7579\n",
      "Epoch 301/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.2800e-07 - val_accuracy: 0.6825 - val_loss: 1.7588\n",
      "Epoch 302/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.2309e-07 - val_accuracy: 0.6825 - val_loss: 1.7597\n",
      "Epoch 303/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.1826e-07 - val_accuracy: 0.6825 - val_loss: 1.7606\n",
      "Epoch 304/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 5.1351e-07 - val_accuracy: 0.6825 - val_loss: 1.7615\n",
      "Epoch 305/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.0880e-07 - val_accuracy: 0.6825 - val_loss: 1.7624\n",
      "Epoch 306/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.0416e-07 - val_accuracy: 0.6825 - val_loss: 1.7633\n",
      "Epoch 307/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.9958e-07 - val_accuracy: 0.6825 - val_loss: 1.7642\n",
      "Epoch 308/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.9506e-07 - val_accuracy: 0.6825 - val_loss: 1.7650\n",
      "Epoch 309/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.9059e-07 - val_accuracy: 0.6825 - val_loss: 1.7659\n",
      "Epoch 310/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.8617e-07 - val_accuracy: 0.6825 - val_loss: 1.7667\n",
      "Epoch 311/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.8190e-07 - val_accuracy: 0.6825 - val_loss: 1.7676\n",
      "Epoch 312/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 4.7760e-07 - val_accuracy: 0.6825 - val_loss: 1.7685\n",
      "Epoch 313/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.7330e-07 - val_accuracy: 0.6825 - val_loss: 1.7694\n",
      "Epoch 314/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 4.6915e-07 - val_accuracy: 0.6825 - val_loss: 1.7703\n",
      "Epoch 315/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.6503e-07 - val_accuracy: 0.6825 - val_loss: 1.7711\n",
      "Epoch 316/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.6094e-07 - val_accuracy: 0.6825 - val_loss: 1.7720\n",
      "Epoch 317/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.5692e-07 - val_accuracy: 0.6825 - val_loss: 1.7728\n",
      "Epoch 318/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.5293e-07 - val_accuracy: 0.6825 - val_loss: 1.7736\n",
      "Epoch 319/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.4901e-07 - val_accuracy: 0.6825 - val_loss: 1.7745\n",
      "Epoch 320/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.4511e-07 - val_accuracy: 0.6825 - val_loss: 1.7754\n",
      "Epoch 321/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.4127e-07 - val_accuracy: 0.6825 - val_loss: 1.7762\n",
      "Epoch 322/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 4.3750e-07 - val_accuracy: 0.6825 - val_loss: 1.7771\n",
      "Epoch 323/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.3377e-07 - val_accuracy: 0.6825 - val_loss: 1.7779\n",
      "Epoch 324/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.3010e-07 - val_accuracy: 0.6825 - val_loss: 1.7788\n",
      "Epoch 325/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.2646e-07 - val_accuracy: 0.6825 - val_loss: 1.7796\n",
      "Epoch 326/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.2285e-07 - val_accuracy: 0.6825 - val_loss: 1.7804\n",
      "Epoch 327/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.1933e-07 - val_accuracy: 0.6821 - val_loss: 1.7812\n",
      "Epoch 328/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.1582e-07 - val_accuracy: 0.6821 - val_loss: 1.7820\n",
      "Epoch 329/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.1236e-07 - val_accuracy: 0.6821 - val_loss: 1.7829\n",
      "Epoch 330/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.0894e-07 - val_accuracy: 0.6821 - val_loss: 1.7837\n",
      "Epoch 331/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 4.0559e-07 - val_accuracy: 0.6821 - val_loss: 1.7845\n",
      "Epoch 332/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.0224e-07 - val_accuracy: 0.6821 - val_loss: 1.7853\n",
      "Epoch 333/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.9893e-07 - val_accuracy: 0.6821 - val_loss: 1.7861\n",
      "Epoch 334/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.9567e-07 - val_accuracy: 0.6821 - val_loss: 1.7869\n",
      "Epoch 335/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 3.9245e-07 - val_accuracy: 0.6821 - val_loss: 1.7877\n",
      "Epoch 336/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.8927e-07 - val_accuracy: 0.6821 - val_loss: 1.7885\n",
      "Epoch 337/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.8613e-07 - val_accuracy: 0.6821 - val_loss: 1.7893\n",
      "Epoch 338/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.8301e-07 - val_accuracy: 0.6821 - val_loss: 1.7901\n",
      "Epoch 339/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.7993e-07 - val_accuracy: 0.6821 - val_loss: 1.7909\n",
      "Epoch 340/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.7691e-07 - val_accuracy: 0.6821 - val_loss: 1.7917\n",
      "Epoch 341/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.7391e-07 - val_accuracy: 0.6821 - val_loss: 1.7925\n",
      "Epoch 342/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.7094e-07 - val_accuracy: 0.6821 - val_loss: 1.7932\n",
      "Epoch 343/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.6802e-07 - val_accuracy: 0.6821 - val_loss: 1.7940\n",
      "Epoch 344/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 3.6514e-07 - val_accuracy: 0.6821 - val_loss: 1.7948\n",
      "Epoch 345/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.6229e-07 - val_accuracy: 0.6821 - val_loss: 1.7956\n",
      "Epoch 346/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.5946e-07 - val_accuracy: 0.6821 - val_loss: 1.7963\n",
      "Epoch 347/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.5666e-07 - val_accuracy: 0.6821 - val_loss: 1.7970\n",
      "Epoch 348/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.5393e-07 - val_accuracy: 0.6821 - val_loss: 1.7978\n",
      "Epoch 349/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.5121e-07 - val_accuracy: 0.6821 - val_loss: 1.7986\n",
      "Epoch 350/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4851e-07 - val_accuracy: 0.6821 - val_loss: 1.7994\n",
      "Epoch 351/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.4584e-07 - val_accuracy: 0.6821 - val_loss: 1.8001\n",
      "Epoch 352/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.4320e-07 - val_accuracy: 0.6821 - val_loss: 1.8008\n",
      "Epoch 353/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.4059e-07 - val_accuracy: 0.6821 - val_loss: 1.8015\n",
      "Epoch 354/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.3801e-07 - val_accuracy: 0.6821 - val_loss: 1.8022\n",
      "Epoch 355/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.3547e-07 - val_accuracy: 0.6821 - val_loss: 1.8030\n",
      "Epoch 356/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.3293e-07 - val_accuracy: 0.6821 - val_loss: 1.8038\n",
      "Epoch 357/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.3043e-07 - val_accuracy: 0.6821 - val_loss: 1.8045\n",
      "Epoch 358/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 3.2797e-07 - val_accuracy: 0.6821 - val_loss: 1.8052\n",
      "Epoch 359/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.2553e-07 - val_accuracy: 0.6821 - val_loss: 1.8060\n",
      "Epoch 360/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.2312e-07 - val_accuracy: 0.6821 - val_loss: 1.8067\n",
      "Epoch 361/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.2073e-07 - val_accuracy: 0.6821 - val_loss: 1.8074\n",
      "Epoch 362/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.1836e-07 - val_accuracy: 0.6821 - val_loss: 1.8081\n",
      "Epoch 363/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.1602e-07 - val_accuracy: 0.6821 - val_loss: 1.8088\n",
      "Epoch 364/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.1373e-07 - val_accuracy: 0.6821 - val_loss: 1.8094\n",
      "Epoch 365/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.1146e-07 - val_accuracy: 0.6821 - val_loss: 1.8102\n",
      "Epoch 366/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.0919e-07 - val_accuracy: 0.6821 - val_loss: 1.8109\n",
      "Epoch 367/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.0697e-07 - val_accuracy: 0.6821 - val_loss: 1.8116\n",
      "Epoch 368/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.0477e-07 - val_accuracy: 0.6821 - val_loss: 1.8123\n",
      "Epoch 369/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 3.0258e-07 - val_accuracy: 0.6821 - val_loss: 1.8130\n",
      "Epoch 370/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.0042e-07 - val_accuracy: 0.6821 - val_loss: 1.8136\n",
      "Epoch 371/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.9832e-07 - val_accuracy: 0.6821 - val_loss: 1.8143\n",
      "Epoch 372/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.9619e-07 - val_accuracy: 0.6821 - val_loss: 1.8150\n",
      "Epoch 373/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.9410e-07 - val_accuracy: 0.6821 - val_loss: 1.8158\n",
      "Epoch 374/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.9204e-07 - val_accuracy: 0.6821 - val_loss: 1.8165\n",
      "Epoch 375/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.8999e-07 - val_accuracy: 0.6821 - val_loss: 1.8171\n",
      "Epoch 376/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.8797e-07 - val_accuracy: 0.6821 - val_loss: 1.8178\n",
      "Epoch 377/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.8596e-07 - val_accuracy: 0.6821 - val_loss: 1.8185\n",
      "Epoch 378/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.8397e-07 - val_accuracy: 0.6817 - val_loss: 1.8191\n",
      "Epoch 379/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.8199e-07 - val_accuracy: 0.6817 - val_loss: 1.8198\n",
      "Epoch 380/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.8008e-07 - val_accuracy: 0.6817 - val_loss: 1.8204\n",
      "Epoch 381/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.7815e-07 - val_accuracy: 0.6817 - val_loss: 1.8211\n",
      "Epoch 382/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.7622e-07 - val_accuracy: 0.6817 - val_loss: 1.8218\n",
      "Epoch 383/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.7434e-07 - val_accuracy: 0.6821 - val_loss: 1.8225\n",
      "Epoch 384/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.7246e-07 - val_accuracy: 0.6821 - val_loss: 1.8232\n",
      "Epoch 385/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.7060e-07 - val_accuracy: 0.6821 - val_loss: 1.8238\n",
      "Epoch 386/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.6879e-07 - val_accuracy: 0.6825 - val_loss: 1.8244\n",
      "Epoch 387/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.6696e-07 - val_accuracy: 0.6825 - val_loss: 1.8251\n",
      "Epoch 388/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.6515e-07 - val_accuracy: 0.6825 - val_loss: 1.8258\n",
      "Epoch 389/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.6338e-07 - val_accuracy: 0.6825 - val_loss: 1.8265\n",
      "Epoch 390/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.6162e-07 - val_accuracy: 0.6825 - val_loss: 1.8271\n",
      "Epoch 391/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.5987e-07 - val_accuracy: 0.6825 - val_loss: 1.8277\n",
      "Epoch 392/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.5815e-07 - val_accuracy: 0.6825 - val_loss: 1.8284\n",
      "Epoch 393/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.5643e-07 - val_accuracy: 0.6825 - val_loss: 1.8290\n",
      "Epoch 394/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.5477e-07 - val_accuracy: 0.6825 - val_loss: 1.8296\n",
      "Epoch 395/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.5310e-07 - val_accuracy: 0.6825 - val_loss: 1.8303\n",
      "Epoch 396/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 2.5143e-07 - val_accuracy: 0.6825 - val_loss: 1.8309\n",
      "Epoch 397/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.4981e-07 - val_accuracy: 0.6825 - val_loss: 1.8315\n",
      "Epoch 398/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.4820e-07 - val_accuracy: 0.6825 - val_loss: 1.8321\n",
      "Epoch 399/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.4659e-07 - val_accuracy: 0.6825 - val_loss: 1.8327\n",
      "Epoch 400/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.4501e-07 - val_accuracy: 0.6825 - val_loss: 1.8334\n",
      "Epoch 401/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.4344e-07 - val_accuracy: 0.6825 - val_loss: 1.8340\n",
      "Epoch 402/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.4189e-07 - val_accuracy: 0.6825 - val_loss: 1.8346\n",
      "Epoch 403/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.4034e-07 - val_accuracy: 0.6825 - val_loss: 1.8352\n",
      "Epoch 404/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.3882e-07 - val_accuracy: 0.6825 - val_loss: 1.8358\n",
      "Epoch 405/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.3729e-07 - val_accuracy: 0.6825 - val_loss: 1.8364\n",
      "Epoch 406/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.3580e-07 - val_accuracy: 0.6825 - val_loss: 1.8370\n",
      "Epoch 407/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.3432e-07 - val_accuracy: 0.6825 - val_loss: 1.8377\n",
      "Epoch 408/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.3285e-07 - val_accuracy: 0.6825 - val_loss: 1.8383\n",
      "Epoch 409/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.3139e-07 - val_accuracy: 0.6825 - val_loss: 1.8389\n",
      "Epoch 410/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.2995e-07 - val_accuracy: 0.6825 - val_loss: 1.8395\n",
      "Epoch 411/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.2852e-07 - val_accuracy: 0.6825 - val_loss: 1.8401\n",
      "Epoch 412/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.2710e-07 - val_accuracy: 0.6825 - val_loss: 1.8407\n",
      "Epoch 413/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.2568e-07 - val_accuracy: 0.6825 - val_loss: 1.8413\n",
      "Epoch 414/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.2429e-07 - val_accuracy: 0.6825 - val_loss: 1.8419\n",
      "Epoch 415/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.2291e-07 - val_accuracy: 0.6825 - val_loss: 1.8425\n",
      "Epoch 416/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.2155e-07 - val_accuracy: 0.6825 - val_loss: 1.8431\n",
      "Epoch 417/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 2.2019e-07 - val_accuracy: 0.6829 - val_loss: 1.8437\n",
      "Epoch 418/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.1885e-07 - val_accuracy: 0.6829 - val_loss: 1.8442\n",
      "Epoch 419/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.1753e-07 - val_accuracy: 0.6829 - val_loss: 1.8448\n",
      "Epoch 420/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.1620e-07 - val_accuracy: 0.6829 - val_loss: 1.8454\n",
      "Epoch 421/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.1489e-07 - val_accuracy: 0.6829 - val_loss: 1.8460\n",
      "Epoch 422/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.1360e-07 - val_accuracy: 0.6829 - val_loss: 1.8466\n",
      "Epoch 423/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.1231e-07 - val_accuracy: 0.6829 - val_loss: 1.8472\n",
      "Epoch 424/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.1102e-07 - val_accuracy: 0.6829 - val_loss: 1.8478\n",
      "Epoch 425/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.0975e-07 - val_accuracy: 0.6829 - val_loss: 1.8484\n",
      "Epoch 426/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.0849e-07 - val_accuracy: 0.6829 - val_loss: 1.8490\n",
      "Epoch 427/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.0723e-07 - val_accuracy: 0.6829 - val_loss: 1.8495\n",
      "Epoch 428/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.0601e-07 - val_accuracy: 0.6829 - val_loss: 1.8501\n",
      "Epoch 429/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.0479e-07 - val_accuracy: 0.6829 - val_loss: 1.8507\n",
      "Epoch 430/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.0357e-07 - val_accuracy: 0.6829 - val_loss: 1.8513\n",
      "Epoch 431/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 2.0237e-07 - val_accuracy: 0.6829 - val_loss: 1.8519\n",
      "Epoch 432/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.0119e-07 - val_accuracy: 0.6829 - val_loss: 1.8524\n",
      "Epoch 433/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 2.0003e-07 - val_accuracy: 0.6829 - val_loss: 1.8530\n",
      "Epoch 434/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.9886e-07 - val_accuracy: 0.6829 - val_loss: 1.8536\n",
      "Epoch 435/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.9772e-07 - val_accuracy: 0.6829 - val_loss: 1.8541\n",
      "Epoch 436/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.9658e-07 - val_accuracy: 0.6829 - val_loss: 1.8547\n",
      "Epoch 437/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.9545e-07 - val_accuracy: 0.6829 - val_loss: 1.8552\n",
      "Epoch 438/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.9433e-07 - val_accuracy: 0.6829 - val_loss: 1.8557\n",
      "Epoch 439/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.9322e-07 - val_accuracy: 0.6829 - val_loss: 1.8563\n",
      "Epoch 440/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.9212e-07 - val_accuracy: 0.6829 - val_loss: 1.8568\n",
      "Epoch 441/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 1.9104e-07 - val_accuracy: 0.6829 - val_loss: 1.8573\n",
      "Epoch 442/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.8997e-07 - val_accuracy: 0.6829 - val_loss: 1.8579\n",
      "Epoch 443/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.8890e-07 - val_accuracy: 0.6829 - val_loss: 1.8584\n",
      "Epoch 444/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.8785e-07 - val_accuracy: 0.6829 - val_loss: 1.8589\n",
      "Epoch 445/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.8680e-07 - val_accuracy: 0.6829 - val_loss: 1.8594\n",
      "Epoch 446/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.8577e-07 - val_accuracy: 0.6829 - val_loss: 1.8599\n",
      "Epoch 447/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.8474e-07 - val_accuracy: 0.6829 - val_loss: 1.8605\n",
      "Epoch 448/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.8372e-07 - val_accuracy: 0.6829 - val_loss: 1.8610\n",
      "Epoch 449/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.8271e-07 - val_accuracy: 0.6829 - val_loss: 1.8616\n",
      "Epoch 450/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.8171e-07 - val_accuracy: 0.6829 - val_loss: 1.8621\n",
      "Epoch 451/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.8071e-07 - val_accuracy: 0.6829 - val_loss: 1.8626\n",
      "Epoch 452/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.7972e-07 - val_accuracy: 0.6829 - val_loss: 1.8632\n",
      "Epoch 453/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.7873e-07 - val_accuracy: 0.6829 - val_loss: 1.8637\n",
      "Epoch 454/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.7775e-07 - val_accuracy: 0.6829 - val_loss: 1.8642\n",
      "Epoch 455/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.7678e-07 - val_accuracy: 0.6829 - val_loss: 1.8647\n",
      "Epoch 456/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.7583e-07 - val_accuracy: 0.6829 - val_loss: 1.8652\n",
      "Epoch 457/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.7488e-07 - val_accuracy: 0.6829 - val_loss: 1.8657\n",
      "Epoch 458/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.7393e-07 - val_accuracy: 0.6829 - val_loss: 1.8663\n",
      "Epoch 459/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.7300e-07 - val_accuracy: 0.6829 - val_loss: 1.8668\n",
      "Epoch 460/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 1.7208e-07 - val_accuracy: 0.6829 - val_loss: 1.8673\n",
      "Epoch 461/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.7116e-07 - val_accuracy: 0.6829 - val_loss: 1.8679\n",
      "Epoch 462/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.7025e-07 - val_accuracy: 0.6829 - val_loss: 1.8684\n",
      "Epoch 463/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.6934e-07 - val_accuracy: 0.6829 - val_loss: 1.8689\n",
      "Epoch 464/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.6844e-07 - val_accuracy: 0.6829 - val_loss: 1.8694\n",
      "Epoch 465/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.6754e-07 - val_accuracy: 0.6829 - val_loss: 1.8699\n",
      "Epoch 466/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.6665e-07 - val_accuracy: 0.6829 - val_loss: 1.8703\n",
      "Epoch 467/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.6577e-07 - val_accuracy: 0.6829 - val_loss: 1.8709\n",
      "Epoch 468/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.6489e-07 - val_accuracy: 0.6829 - val_loss: 1.8714\n",
      "Epoch 469/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.6402e-07 - val_accuracy: 0.6829 - val_loss: 1.8719\n",
      "Epoch 470/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.6315e-07 - val_accuracy: 0.6829 - val_loss: 1.8725\n",
      "Epoch 471/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.6230e-07 - val_accuracy: 0.6829 - val_loss: 1.8730\n",
      "Epoch 472/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.6144e-07 - val_accuracy: 0.6829 - val_loss: 1.8735\n",
      "Epoch 473/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.6059e-07 - val_accuracy: 0.6829 - val_loss: 1.8740\n",
      "Epoch 474/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.5975e-07 - val_accuracy: 0.6829 - val_loss: 1.8745\n",
      "Epoch 475/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.5891e-07 - val_accuracy: 0.6829 - val_loss: 1.8750\n",
      "Epoch 476/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 1.5809e-07 - val_accuracy: 0.6829 - val_loss: 1.8756\n",
      "Epoch 477/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.5728e-07 - val_accuracy: 0.6829 - val_loss: 1.8761\n",
      "Epoch 478/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.5647e-07 - val_accuracy: 0.6829 - val_loss: 1.8766\n",
      "Epoch 479/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.5567e-07 - val_accuracy: 0.6829 - val_loss: 1.8770\n",
      "Epoch 480/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.5486e-07 - val_accuracy: 0.6829 - val_loss: 1.8775\n",
      "Epoch 481/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.5407e-07 - val_accuracy: 0.6829 - val_loss: 1.8780\n",
      "Epoch 482/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.5328e-07 - val_accuracy: 0.6829 - val_loss: 1.8785\n",
      "Epoch 483/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.5250e-07 - val_accuracy: 0.6829 - val_loss: 1.8791\n",
      "Epoch 484/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.5172e-07 - val_accuracy: 0.6829 - val_loss: 1.8796\n",
      "Epoch 485/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.5095e-07 - val_accuracy: 0.6829 - val_loss: 1.8801\n",
      "Epoch 486/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.5018e-07 - val_accuracy: 0.6829 - val_loss: 1.8805\n",
      "Epoch 487/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.4941e-07 - val_accuracy: 0.6829 - val_loss: 1.8810\n",
      "Epoch 488/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.4865e-07 - val_accuracy: 0.6829 - val_loss: 1.8815\n",
      "Epoch 489/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.4790e-07 - val_accuracy: 0.6829 - val_loss: 1.8821\n",
      "Epoch 490/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.4716e-07 - val_accuracy: 0.6829 - val_loss: 1.8826\n",
      "Epoch 491/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.4640e-07 - val_accuracy: 0.6829 - val_loss: 1.8831\n",
      "Epoch 492/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.4566e-07 - val_accuracy: 0.6829 - val_loss: 1.8836\n",
      "Epoch 493/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.4492e-07 - val_accuracy: 0.6829 - val_loss: 1.8841\n",
      "Epoch 494/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.4419e-07 - val_accuracy: 0.6829 - val_loss: 1.8845\n",
      "Epoch 495/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 1.4348e-07 - val_accuracy: 0.6829 - val_loss: 1.8850\n",
      "Epoch 496/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.4277e-07 - val_accuracy: 0.6829 - val_loss: 1.8855\n",
      "Epoch 497/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.4208e-07 - val_accuracy: 0.6829 - val_loss: 1.8859\n",
      "Epoch 498/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.4139e-07 - val_accuracy: 0.6829 - val_loss: 1.8864\n",
      "Epoch 499/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.4070e-07 - val_accuracy: 0.6829 - val_loss: 1.8868\n",
      "Epoch 500/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.4003e-07 - val_accuracy: 0.6829 - val_loss: 1.8873\n",
      "Epoch 501/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.3936e-07 - val_accuracy: 0.6829 - val_loss: 1.8877\n",
      "Epoch 502/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.3869e-07 - val_accuracy: 0.6829 - val_loss: 1.8882\n",
      "Epoch 503/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.3804e-07 - val_accuracy: 0.6829 - val_loss: 1.8886\n",
      "Epoch 504/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.3738e-07 - val_accuracy: 0.6829 - val_loss: 1.8891\n",
      "Epoch 505/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.3674e-07 - val_accuracy: 0.6829 - val_loss: 1.8895\n",
      "Epoch 506/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.3609e-07 - val_accuracy: 0.6829 - val_loss: 1.8899\n",
      "Epoch 507/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.3546e-07 - val_accuracy: 0.6829 - val_loss: 1.8903\n",
      "Epoch 508/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.3482e-07 - val_accuracy: 0.6829 - val_loss: 1.8908\n",
      "Epoch 509/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.3419e-07 - val_accuracy: 0.6829 - val_loss: 1.8912\n",
      "Epoch 510/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 1.3356e-07 - val_accuracy: 0.6829 - val_loss: 1.8917\n",
      "Epoch 511/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.3294e-07 - val_accuracy: 0.6829 - val_loss: 1.8921\n",
      "Epoch 512/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.3232e-07 - val_accuracy: 0.6829 - val_loss: 1.8925\n",
      "Epoch 513/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 1.3171e-07 - val_accuracy: 0.6829 - val_loss: 1.8930\n",
      "Epoch 514/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.3109e-07 - val_accuracy: 0.6829 - val_loss: 1.8935\n",
      "Epoch 515/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.3049e-07 - val_accuracy: 0.6829 - val_loss: 1.8939\n",
      "Epoch 516/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.2988e-07 - val_accuracy: 0.6829 - val_loss: 1.8943\n",
      "Epoch 517/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.2928e-07 - val_accuracy: 0.6829 - val_loss: 1.8948\n",
      "Epoch 518/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2869e-07 - val_accuracy: 0.6829 - val_loss: 1.8952\n",
      "Epoch 519/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2810e-07 - val_accuracy: 0.6829 - val_loss: 1.8956\n",
      "Epoch 520/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.2751e-07 - val_accuracy: 0.6829 - val_loss: 1.8960\n",
      "Epoch 521/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2693e-07 - val_accuracy: 0.6829 - val_loss: 1.8965\n",
      "Epoch 522/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2636e-07 - val_accuracy: 0.6829 - val_loss: 1.8969\n",
      "Epoch 523/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.2579e-07 - val_accuracy: 0.6829 - val_loss: 1.8973\n",
      "Epoch 524/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.2523e-07 - val_accuracy: 0.6829 - val_loss: 1.8977\n",
      "Epoch 525/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2467e-07 - val_accuracy: 0.6829 - val_loss: 1.8981\n",
      "Epoch 526/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2411e-07 - val_accuracy: 0.6829 - val_loss: 1.8985\n",
      "Epoch 527/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.2356e-07 - val_accuracy: 0.6829 - val_loss: 1.8990\n",
      "Epoch 528/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.2301e-07 - val_accuracy: 0.6829 - val_loss: 1.8994\n",
      "Epoch 529/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2246e-07 - val_accuracy: 0.6829 - val_loss: 1.8998\n",
      "Epoch 530/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.2193e-07 - val_accuracy: 0.6829 - val_loss: 1.9002\n",
      "Epoch 531/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.2139e-07 - val_accuracy: 0.6829 - val_loss: 1.9006\n",
      "Epoch 532/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.2085e-07 - val_accuracy: 0.6829 - val_loss: 1.9011\n",
      "Epoch 533/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 1.2032e-07 - val_accuracy: 0.6829 - val_loss: 1.9015\n",
      "Epoch 534/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.1979e-07 - val_accuracy: 0.6829 - val_loss: 1.9019\n",
      "Epoch 535/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1926e-07 - val_accuracy: 0.6829 - val_loss: 1.9023\n",
      "Epoch 536/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1874e-07 - val_accuracy: 0.6829 - val_loss: 1.9027\n",
      "Epoch 537/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.1821e-07 - val_accuracy: 0.6829 - val_loss: 1.9031\n",
      "Epoch 538/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.1770e-07 - val_accuracy: 0.6829 - val_loss: 1.9035\n",
      "Epoch 539/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1718e-07 - val_accuracy: 0.6829 - val_loss: 1.9040\n",
      "Epoch 540/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.1667e-07 - val_accuracy: 0.6829 - val_loss: 1.9044\n",
      "Epoch 541/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.1616e-07 - val_accuracy: 0.6829 - val_loss: 1.9048\n",
      "Epoch 542/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.1565e-07 - val_accuracy: 0.6829 - val_loss: 1.9053\n",
      "Epoch 543/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.1514e-07 - val_accuracy: 0.6829 - val_loss: 1.9057\n",
      "Epoch 544/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1463e-07 - val_accuracy: 0.6829 - val_loss: 1.9061\n",
      "Epoch 545/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.1413e-07 - val_accuracy: 0.6829 - val_loss: 1.9065\n",
      "Epoch 546/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.1363e-07 - val_accuracy: 0.6829 - val_loss: 1.9070\n",
      "Epoch 547/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.1314e-07 - val_accuracy: 0.6829 - val_loss: 1.9074\n",
      "Epoch 548/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1264e-07 - val_accuracy: 0.6829 - val_loss: 1.9078\n",
      "Epoch 549/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.1215e-07 - val_accuracy: 0.6829 - val_loss: 1.9083\n",
      "Epoch 550/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.1167e-07 - val_accuracy: 0.6829 - val_loss: 1.9087\n",
      "Epoch 551/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 1.1118e-07 - val_accuracy: 0.6829 - val_loss: 1.9091\n",
      "Epoch 552/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.1071e-07 - val_accuracy: 0.6829 - val_loss: 1.9095\n",
      "Epoch 553/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.1023e-07 - val_accuracy: 0.6829 - val_loss: 1.9099\n",
      "Epoch 554/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0977e-07 - val_accuracy: 0.6829 - val_loss: 1.9103\n",
      "Epoch 555/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.0931e-07 - val_accuracy: 0.6829 - val_loss: 1.9107\n",
      "Epoch 556/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.0884e-07 - val_accuracy: 0.6829 - val_loss: 1.9111\n",
      "Epoch 557/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0838e-07 - val_accuracy: 0.6829 - val_loss: 1.9116\n",
      "Epoch 558/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0793e-07 - val_accuracy: 0.6829 - val_loss: 1.9120\n",
      "Epoch 559/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 1.0748e-07 - val_accuracy: 0.6829 - val_loss: 1.9124\n",
      "Epoch 560/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0703e-07 - val_accuracy: 0.6829 - val_loss: 1.9128\n",
      "Epoch 561/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0658e-07 - val_accuracy: 0.6829 - val_loss: 1.9132\n",
      "Epoch 562/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.0613e-07 - val_accuracy: 0.6829 - val_loss: 1.9136\n",
      "Epoch 563/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.0569e-07 - val_accuracy: 0.6829 - val_loss: 1.9140\n",
      "Epoch 564/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0525e-07 - val_accuracy: 0.6829 - val_loss: 1.9143\n",
      "Epoch 565/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.0481e-07 - val_accuracy: 0.6834 - val_loss: 1.9148\n",
      "Epoch 566/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 1.0438e-07 - val_accuracy: 0.6834 - val_loss: 1.9152\n",
      "Epoch 567/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0394e-07 - val_accuracy: 0.6834 - val_loss: 1.9156\n",
      "Epoch 568/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0351e-07 - val_accuracy: 0.6834 - val_loss: 1.9160\n",
      "Epoch 569/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0309e-07 - val_accuracy: 0.6834 - val_loss: 1.9165\n",
      "Epoch 570/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 1.0266e-07 - val_accuracy: 0.6834 - val_loss: 1.9169\n",
      "Epoch 571/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.0224e-07 - val_accuracy: 0.6834 - val_loss: 1.9173\n",
      "Epoch 572/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.0182e-07 - val_accuracy: 0.6834 - val_loss: 1.9177\n",
      "Epoch 573/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 1.0139e-07 - val_accuracy: 0.6834 - val_loss: 1.9181\n",
      "Epoch 574/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 1.0097e-07 - val_accuracy: 0.6834 - val_loss: 1.9185\n",
      "Epoch 575/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.0055e-07 - val_accuracy: 0.6834 - val_loss: 1.9189\n",
      "Epoch 576/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 1.0014e-07 - val_accuracy: 0.6834 - val_loss: 1.9193\n",
      "Epoch 577/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.9720e-08 - val_accuracy: 0.6834 - val_loss: 1.9197\n",
      "Epoch 578/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.9308e-08 - val_accuracy: 0.6834 - val_loss: 1.9201\n",
      "Epoch 579/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.8897e-08 - val_accuracy: 0.6834 - val_loss: 1.9206\n",
      "Epoch 580/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.8491e-08 - val_accuracy: 0.6834 - val_loss: 1.9210\n",
      "Epoch 581/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.8084e-08 - val_accuracy: 0.6834 - val_loss: 1.9214\n",
      "Epoch 582/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.7676e-08 - val_accuracy: 0.6834 - val_loss: 1.9218\n",
      "Epoch 583/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.7271e-08 - val_accuracy: 0.6834 - val_loss: 1.9222\n",
      "Epoch 584/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 9.6868e-08 - val_accuracy: 0.6834 - val_loss: 1.9226\n",
      "Epoch 585/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.6472e-08 - val_accuracy: 0.6834 - val_loss: 1.9231\n",
      "Epoch 586/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 9.6071e-08 - val_accuracy: 0.6834 - val_loss: 1.9235\n",
      "Epoch 587/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 9.5673e-08 - val_accuracy: 0.6834 - val_loss: 1.9239\n",
      "Epoch 588/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.5285e-08 - val_accuracy: 0.6834 - val_loss: 1.9243\n",
      "Epoch 589/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.4899e-08 - val_accuracy: 0.6834 - val_loss: 1.9247\n",
      "Epoch 590/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.4514e-08 - val_accuracy: 0.6834 - val_loss: 1.9251\n",
      "Epoch 591/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.4135e-08 - val_accuracy: 0.6834 - val_loss: 1.9255\n",
      "Epoch 592/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.3762e-08 - val_accuracy: 0.6834 - val_loss: 1.9259\n",
      "Epoch 593/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 9.3387e-08 - val_accuracy: 0.6834 - val_loss: 1.9263\n",
      "Epoch 594/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.3010e-08 - val_accuracy: 0.6834 - val_loss: 1.9267\n",
      "Epoch 595/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.2642e-08 - val_accuracy: 0.6834 - val_loss: 1.9271\n",
      "Epoch 596/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 9.2280e-08 - val_accuracy: 0.6834 - val_loss: 1.9275\n",
      "Epoch 597/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.1917e-08 - val_accuracy: 0.6834 - val_loss: 1.9279\n",
      "Epoch 598/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 9.1553e-08 - val_accuracy: 0.6834 - val_loss: 1.9282\n",
      "Epoch 599/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.1193e-08 - val_accuracy: 0.6834 - val_loss: 1.9286\n",
      "Epoch 600/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.0835e-08 - val_accuracy: 0.6834 - val_loss: 1.9290\n",
      "Epoch 601/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 9.0484e-08 - val_accuracy: 0.6834 - val_loss: 1.9294\n",
      "Epoch 602/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 9.0132e-08 - val_accuracy: 0.6834 - val_loss: 1.9298\n",
      "Epoch 603/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 8.9778e-08 - val_accuracy: 0.6834 - val_loss: 1.9302\n",
      "Epoch 604/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.9431e-08 - val_accuracy: 0.6834 - val_loss: 1.9306\n",
      "Epoch 605/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.9084e-08 - val_accuracy: 0.6834 - val_loss: 1.9309\n",
      "Epoch 606/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.8736e-08 - val_accuracy: 0.6834 - val_loss: 1.9313\n",
      "Epoch 607/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.8390e-08 - val_accuracy: 0.6834 - val_loss: 1.9317\n",
      "Epoch 608/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 8.8045e-08 - val_accuracy: 0.6834 - val_loss: 1.9321\n",
      "Epoch 609/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.7708e-08 - val_accuracy: 0.6834 - val_loss: 1.9325\n",
      "Epoch 610/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.7367e-08 - val_accuracy: 0.6834 - val_loss: 1.9329\n",
      "Epoch 611/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.7027e-08 - val_accuracy: 0.6834 - val_loss: 1.9333\n",
      "Epoch 612/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.6690e-08 - val_accuracy: 0.6834 - val_loss: 1.9336\n",
      "Epoch 613/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.6356e-08 - val_accuracy: 0.6834 - val_loss: 1.9340\n",
      "Epoch 614/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 8.6022e-08 - val_accuracy: 0.6834 - val_loss: 1.9344\n",
      "Epoch 615/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.5691e-08 - val_accuracy: 0.6834 - val_loss: 1.9348\n",
      "Epoch 616/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.5363e-08 - val_accuracy: 0.6834 - val_loss: 1.9351\n",
      "Epoch 617/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.5037e-08 - val_accuracy: 0.6834 - val_loss: 1.9355\n",
      "Epoch 618/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.4717e-08 - val_accuracy: 0.6834 - val_loss: 1.9359\n",
      "Epoch 619/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.4399e-08 - val_accuracy: 0.6834 - val_loss: 1.9363\n",
      "Epoch 620/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.4085e-08 - val_accuracy: 0.6834 - val_loss: 1.9366\n",
      "Epoch 621/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 8.3772e-08 - val_accuracy: 0.6834 - val_loss: 1.9369\n",
      "Epoch 622/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.3462e-08 - val_accuracy: 0.6834 - val_loss: 1.9373\n",
      "Epoch 623/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 8.3155e-08 - val_accuracy: 0.6834 - val_loss: 1.9376\n",
      "Epoch 624/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 8.2850e-08 - val_accuracy: 0.6834 - val_loss: 1.9380\n",
      "Epoch 625/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 8.2547e-08 - val_accuracy: 0.6834 - val_loss: 1.9383\n",
      "Epoch 626/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.2246e-08 - val_accuracy: 0.6834 - val_loss: 1.9387\n",
      "Epoch 627/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.1945e-08 - val_accuracy: 0.6834 - val_loss: 1.9390\n",
      "Epoch 628/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.1647e-08 - val_accuracy: 0.6834 - val_loss: 1.9394\n",
      "Epoch 629/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.1350e-08 - val_accuracy: 0.6834 - val_loss: 1.9397\n",
      "Epoch 630/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.1056e-08 - val_accuracy: 0.6834 - val_loss: 1.9400\n",
      "Epoch 631/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.0763e-08 - val_accuracy: 0.6834 - val_loss: 1.9404\n",
      "Epoch 632/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.0472e-08 - val_accuracy: 0.6834 - val_loss: 1.9407\n",
      "Epoch 633/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 8.0182e-08 - val_accuracy: 0.6834 - val_loss: 1.9410\n",
      "Epoch 634/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.9891e-08 - val_accuracy: 0.6834 - val_loss: 1.9414\n",
      "Epoch 635/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.9607e-08 - val_accuracy: 0.6834 - val_loss: 1.9417\n",
      "Epoch 636/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.9320e-08 - val_accuracy: 0.6834 - val_loss: 1.9420\n",
      "Epoch 637/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.9034e-08 - val_accuracy: 0.6834 - val_loss: 1.9424\n",
      "Epoch 638/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.8757e-08 - val_accuracy: 0.6834 - val_loss: 1.9427\n",
      "Epoch 639/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 7.8482e-08 - val_accuracy: 0.6834 - val_loss: 1.9430\n",
      "Epoch 640/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 7.8210e-08 - val_accuracy: 0.6834 - val_loss: 1.9433\n",
      "Epoch 641/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 7.7941e-08 - val_accuracy: 0.6834 - val_loss: 1.9436\n",
      "Epoch 642/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.7673e-08 - val_accuracy: 0.6834 - val_loss: 1.9439\n",
      "Epoch 643/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 7.7408e-08 - val_accuracy: 0.6834 - val_loss: 1.9442\n",
      "Epoch 644/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.7148e-08 - val_accuracy: 0.6834 - val_loss: 1.9445\n",
      "Epoch 645/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.6889e-08 - val_accuracy: 0.6834 - val_loss: 1.9448\n",
      "Epoch 646/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.6630e-08 - val_accuracy: 0.6834 - val_loss: 1.9451\n",
      "Epoch 647/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 7.6370e-08 - val_accuracy: 0.6834 - val_loss: 1.9454\n",
      "Epoch 648/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.6116e-08 - val_accuracy: 0.6834 - val_loss: 1.9457\n",
      "Epoch 649/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.5864e-08 - val_accuracy: 0.6834 - val_loss: 1.9460\n",
      "Epoch 650/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.5613e-08 - val_accuracy: 0.6838 - val_loss: 1.9463\n",
      "Epoch 651/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.5362e-08 - val_accuracy: 0.6838 - val_loss: 1.9466\n",
      "Epoch 652/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.5114e-08 - val_accuracy: 0.6838 - val_loss: 1.9469\n",
      "Epoch 653/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.4866e-08 - val_accuracy: 0.6838 - val_loss: 1.9471\n",
      "Epoch 654/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.4617e-08 - val_accuracy: 0.6838 - val_loss: 1.9474\n",
      "Epoch 655/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 7.4373e-08 - val_accuracy: 0.6838 - val_loss: 1.9477\n",
      "Epoch 656/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.4132e-08 - val_accuracy: 0.6838 - val_loss: 1.9480\n",
      "Epoch 657/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.3888e-08 - val_accuracy: 0.6838 - val_loss: 1.9483\n",
      "Epoch 658/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.3645e-08 - val_accuracy: 0.6838 - val_loss: 1.9486\n",
      "Epoch 659/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.3405e-08 - val_accuracy: 0.6838 - val_loss: 1.9488\n",
      "Epoch 660/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.3165e-08 - val_accuracy: 0.6838 - val_loss: 1.9491\n",
      "Epoch 661/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.2926e-08 - val_accuracy: 0.6838 - val_loss: 1.9494\n",
      "Epoch 662/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.2688e-08 - val_accuracy: 0.6838 - val_loss: 1.9497\n",
      "Epoch 663/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.2452e-08 - val_accuracy: 0.6838 - val_loss: 1.9500\n",
      "Epoch 664/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 7.2217e-08 - val_accuracy: 0.6838 - val_loss: 1.9503\n",
      "Epoch 665/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 7.1983e-08 - val_accuracy: 0.6838 - val_loss: 1.9506\n",
      "Epoch 666/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.1749e-08 - val_accuracy: 0.6838 - val_loss: 1.9509\n",
      "Epoch 667/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.1516e-08 - val_accuracy: 0.6838 - val_loss: 1.9511\n",
      "Epoch 668/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.1283e-08 - val_accuracy: 0.6838 - val_loss: 1.9514\n",
      "Epoch 669/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.1055e-08 - val_accuracy: 0.6838 - val_loss: 1.9517\n",
      "Epoch 670/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.0825e-08 - val_accuracy: 0.6838 - val_loss: 1.9520\n",
      "Epoch 671/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.0593e-08 - val_accuracy: 0.6838 - val_loss: 1.9523\n",
      "Epoch 672/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 7.0364e-08 - val_accuracy: 0.6838 - val_loss: 1.9526\n",
      "Epoch 673/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.0136e-08 - val_accuracy: 0.6838 - val_loss: 1.9529\n",
      "Epoch 674/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.9909e-08 - val_accuracy: 0.6838 - val_loss: 1.9532\n",
      "Epoch 675/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 6.9684e-08 - val_accuracy: 0.6838 - val_loss: 1.9535\n",
      "Epoch 676/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 6.9457e-08 - val_accuracy: 0.6838 - val_loss: 1.9538\n",
      "Epoch 677/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.9232e-08 - val_accuracy: 0.6838 - val_loss: 1.9541\n",
      "Epoch 678/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.9008e-08 - val_accuracy: 0.6838 - val_loss: 1.9543\n",
      "Epoch 679/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.8785e-08 - val_accuracy: 0.6838 - val_loss: 1.9547\n",
      "Epoch 680/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.8561e-08 - val_accuracy: 0.6838 - val_loss: 1.9550\n",
      "Epoch 681/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.8339e-08 - val_accuracy: 0.6838 - val_loss: 1.9553\n",
      "Epoch 682/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.8118e-08 - val_accuracy: 0.6838 - val_loss: 1.9556\n",
      "Epoch 683/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.7896e-08 - val_accuracy: 0.6838 - val_loss: 1.9559\n",
      "Epoch 684/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.7678e-08 - val_accuracy: 0.6838 - val_loss: 1.9562\n",
      "Epoch 685/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 6.7458e-08 - val_accuracy: 0.6838 - val_loss: 1.9565\n",
      "Epoch 686/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.7238e-08 - val_accuracy: 0.6838 - val_loss: 1.9568\n",
      "Epoch 687/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.7020e-08 - val_accuracy: 0.6838 - val_loss: 1.9571\n",
      "Epoch 688/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.6803e-08 - val_accuracy: 0.6838 - val_loss: 1.9574\n",
      "Epoch 689/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.6587e-08 - val_accuracy: 0.6838 - val_loss: 1.9577\n",
      "Epoch 690/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.6370e-08 - val_accuracy: 0.6838 - val_loss: 1.9580\n",
      "Epoch 691/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.6155e-08 - val_accuracy: 0.6838 - val_loss: 1.9583\n",
      "Epoch 692/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.5940e-08 - val_accuracy: 0.6838 - val_loss: 1.9586\n",
      "Epoch 693/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 6.5727e-08 - val_accuracy: 0.6838 - val_loss: 1.9589\n",
      "Epoch 694/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.5513e-08 - val_accuracy: 0.6838 - val_loss: 1.9592\n",
      "Epoch 695/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.5301e-08 - val_accuracy: 0.6838 - val_loss: 1.9595\n",
      "Epoch 696/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.5088e-08 - val_accuracy: 0.6838 - val_loss: 1.9598\n",
      "Epoch 697/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.4879e-08 - val_accuracy: 0.6838 - val_loss: 1.9601\n",
      "Epoch 698/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.4668e-08 - val_accuracy: 0.6838 - val_loss: 1.9605\n",
      "Epoch 699/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.4459e-08 - val_accuracy: 0.6838 - val_loss: 1.9608\n",
      "Epoch 700/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.4250e-08 - val_accuracy: 0.6838 - val_loss: 1.9611\n",
      "Epoch 701/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.4041e-08 - val_accuracy: 0.6838 - val_loss: 1.9614\n",
      "Epoch 702/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.3833e-08 - val_accuracy: 0.6838 - val_loss: 1.9618\n",
      "Epoch 703/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.3625e-08 - val_accuracy: 0.6838 - val_loss: 1.9621\n",
      "Epoch 704/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 6.3417e-08 - val_accuracy: 0.6838 - val_loss: 1.9624\n",
      "Epoch 705/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.3211e-08 - val_accuracy: 0.6838 - val_loss: 1.9627\n",
      "Epoch 706/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.3005e-08 - val_accuracy: 0.6838 - val_loss: 1.9630\n",
      "Epoch 707/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.2801e-08 - val_accuracy: 0.6838 - val_loss: 1.9633\n",
      "Epoch 708/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 6.2597e-08 - val_accuracy: 0.6838 - val_loss: 1.9637\n",
      "Epoch 709/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 6.2393e-08 - val_accuracy: 0.6838 - val_loss: 1.9640\n",
      "Epoch 710/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.2190e-08 - val_accuracy: 0.6838 - val_loss: 1.9643\n",
      "Epoch 711/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.1990e-08 - val_accuracy: 0.6838 - val_loss: 1.9646\n",
      "Epoch 712/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 6.1793e-08 - val_accuracy: 0.6838 - val_loss: 1.9649\n",
      "Epoch 713/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.1599e-08 - val_accuracy: 0.6838 - val_loss: 1.9652\n",
      "Epoch 714/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.1406e-08 - val_accuracy: 0.6838 - val_loss: 1.9655\n",
      "Epoch 715/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.1216e-08 - val_accuracy: 0.6838 - val_loss: 1.9658\n",
      "Epoch 716/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.1027e-08 - val_accuracy: 0.6838 - val_loss: 1.9661\n",
      "Epoch 717/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.0841e-08 - val_accuracy: 0.6838 - val_loss: 1.9663\n",
      "Epoch 718/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 6.0655e-08 - val_accuracy: 0.6838 - val_loss: 1.9666\n",
      "Epoch 719/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 6.0471e-08 - val_accuracy: 0.6838 - val_loss: 1.9669\n",
      "Epoch 720/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 6.0290e-08 - val_accuracy: 0.6838 - val_loss: 1.9672\n",
      "Epoch 721/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.0109e-08 - val_accuracy: 0.6838 - val_loss: 1.9675\n",
      "Epoch 722/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.9930e-08 - val_accuracy: 0.6838 - val_loss: 1.9677\n",
      "Epoch 723/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.9751e-08 - val_accuracy: 0.6838 - val_loss: 1.9680\n",
      "Epoch 724/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.9576e-08 - val_accuracy: 0.6842 - val_loss: 1.9683\n",
      "Epoch 725/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 5.9401e-08 - val_accuracy: 0.6842 - val_loss: 1.9685\n",
      "Epoch 726/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.9224e-08 - val_accuracy: 0.6842 - val_loss: 1.9688\n",
      "Epoch 727/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 5.9050e-08 - val_accuracy: 0.6842 - val_loss: 1.9691\n",
      "Epoch 728/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 5.8877e-08 - val_accuracy: 0.6842 - val_loss: 1.9694\n",
      "Epoch 729/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.8704e-08 - val_accuracy: 0.6842 - val_loss: 1.9697\n",
      "Epoch 730/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 5.8533e-08 - val_accuracy: 0.6842 - val_loss: 1.9699\n",
      "Epoch 731/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 5.8361e-08 - val_accuracy: 0.6842 - val_loss: 1.9702\n",
      "Epoch 732/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 5.8191e-08 - val_accuracy: 0.6842 - val_loss: 1.9704\n",
      "Epoch 733/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.8021e-08 - val_accuracy: 0.6842 - val_loss: 1.9707\n",
      "Epoch 734/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.7855e-08 - val_accuracy: 0.6842 - val_loss: 1.9709\n",
      "Epoch 735/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.7688e-08 - val_accuracy: 0.6842 - val_loss: 1.9712\n",
      "Epoch 736/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.7519e-08 - val_accuracy: 0.6842 - val_loss: 1.9715\n",
      "Epoch 737/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.7350e-08 - val_accuracy: 0.6842 - val_loss: 1.9718\n",
      "Epoch 738/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.7184e-08 - val_accuracy: 0.6842 - val_loss: 1.9721\n",
      "Epoch 739/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 5.7019e-08 - val_accuracy: 0.6842 - val_loss: 1.9723\n",
      "Epoch 740/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 5.6854e-08 - val_accuracy: 0.6842 - val_loss: 1.9726\n",
      "Epoch 741/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 5.6688e-08 - val_accuracy: 0.6842 - val_loss: 1.9729\n",
      "Epoch 742/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 5.6524e-08 - val_accuracy: 0.6842 - val_loss: 1.9731\n",
      "Epoch 743/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.6363e-08 - val_accuracy: 0.6842 - val_loss: 1.9734\n",
      "Epoch 744/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.6200e-08 - val_accuracy: 0.6842 - val_loss: 1.9737\n",
      "Epoch 745/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.6036e-08 - val_accuracy: 0.6842 - val_loss: 1.9740\n",
      "Epoch 746/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.5874e-08 - val_accuracy: 0.6842 - val_loss: 1.9743\n",
      "Epoch 747/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.5714e-08 - val_accuracy: 0.6842 - val_loss: 1.9745\n",
      "Epoch 748/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 5.5553e-08 - val_accuracy: 0.6842 - val_loss: 1.9748\n",
      "Epoch 749/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.5393e-08 - val_accuracy: 0.6842 - val_loss: 1.9751\n",
      "Epoch 750/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.5233e-08 - val_accuracy: 0.6842 - val_loss: 1.9753\n",
      "Epoch 751/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 5.5073e-08 - val_accuracy: 0.6842 - val_loss: 1.9756\n",
      "Epoch 752/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 5.4914e-08 - val_accuracy: 0.6842 - val_loss: 1.9759\n",
      "Epoch 753/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 5.4754e-08 - val_accuracy: 0.6842 - val_loss: 1.9761\n",
      "Epoch 754/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 5.4596e-08 - val_accuracy: 0.6842 - val_loss: 1.9764\n",
      "Epoch 755/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.4440e-08 - val_accuracy: 0.6842 - val_loss: 1.9767\n",
      "Epoch 756/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.4281e-08 - val_accuracy: 0.6842 - val_loss: 1.9770\n",
      "Epoch 757/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.4124e-08 - val_accuracy: 0.6842 - val_loss: 1.9773\n",
      "Epoch 758/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.3969e-08 - val_accuracy: 0.6842 - val_loss: 1.9776\n",
      "Epoch 759/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.3814e-08 - val_accuracy: 0.6842 - val_loss: 1.9778\n",
      "Epoch 760/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.3658e-08 - val_accuracy: 0.6842 - val_loss: 1.9781\n",
      "Epoch 761/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.3503e-08 - val_accuracy: 0.6842 - val_loss: 1.9784\n",
      "Epoch 762/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 5.3348e-08 - val_accuracy: 0.6842 - val_loss: 1.9787\n",
      "Epoch 763/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 5.3193e-08 - val_accuracy: 0.6842 - val_loss: 1.9789\n",
      "Epoch 764/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 5.3039e-08 - val_accuracy: 0.6847 - val_loss: 1.9792\n",
      "Epoch 765/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.2888e-08 - val_accuracy: 0.6847 - val_loss: 1.9795\n",
      "Epoch 766/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.2736e-08 - val_accuracy: 0.6847 - val_loss: 1.9798\n",
      "Epoch 767/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.2582e-08 - val_accuracy: 0.6847 - val_loss: 1.9801\n",
      "Epoch 768/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.2429e-08 - val_accuracy: 0.6847 - val_loss: 1.9804\n",
      "Epoch 769/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.2278e-08 - val_accuracy: 0.6847 - val_loss: 1.9807\n",
      "Epoch 770/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.2127e-08 - val_accuracy: 0.6847 - val_loss: 1.9810\n",
      "Epoch 771/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.1977e-08 - val_accuracy: 0.6847 - val_loss: 1.9812\n",
      "Epoch 772/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.1826e-08 - val_accuracy: 0.6847 - val_loss: 1.9815\n",
      "Epoch 773/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.1676e-08 - val_accuracy: 0.6847 - val_loss: 1.9818\n",
      "Epoch 774/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.1526e-08 - val_accuracy: 0.6847 - val_loss: 1.9821\n",
      "Epoch 775/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 5.1376e-08 - val_accuracy: 0.6847 - val_loss: 1.9824\n",
      "Epoch 776/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.1228e-08 - val_accuracy: 0.6847 - val_loss: 1.9827\n",
      "Epoch 777/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 5.1080e-08 - val_accuracy: 0.6847 - val_loss: 1.9830\n",
      "Epoch 778/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 5.0931e-08 - val_accuracy: 0.6847 - val_loss: 1.9833\n",
      "Epoch 779/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.0784e-08 - val_accuracy: 0.6847 - val_loss: 1.9835\n",
      "Epoch 780/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.0637e-08 - val_accuracy: 0.6847 - val_loss: 1.9838\n",
      "Epoch 781/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 5.0489e-08 - val_accuracy: 0.6847 - val_loss: 1.9841\n",
      "Epoch 782/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.0343e-08 - val_accuracy: 0.6847 - val_loss: 1.9844\n",
      "Epoch 783/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 5.0198e-08 - val_accuracy: 0.6847 - val_loss: 1.9847\n",
      "Epoch 784/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 5.0051e-08 - val_accuracy: 0.6847 - val_loss: 1.9850\n",
      "Epoch 785/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.9907e-08 - val_accuracy: 0.6847 - val_loss: 1.9853\n",
      "Epoch 786/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.9763e-08 - val_accuracy: 0.6847 - val_loss: 1.9856\n",
      "Epoch 787/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.9618e-08 - val_accuracy: 0.6847 - val_loss: 1.9859\n",
      "Epoch 788/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.9475e-08 - val_accuracy: 0.6847 - val_loss: 1.9862\n",
      "Epoch 789/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.9331e-08 - val_accuracy: 0.6847 - val_loss: 1.9865\n",
      "Epoch 790/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.9187e-08 - val_accuracy: 0.6847 - val_loss: 1.9868\n",
      "Epoch 791/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.9044e-08 - val_accuracy: 0.6847 - val_loss: 1.9871\n",
      "Epoch 792/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.8901e-08 - val_accuracy: 0.6847 - val_loss: 1.9874\n",
      "Epoch 793/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.8761e-08 - val_accuracy: 0.6847 - val_loss: 1.9877\n",
      "Epoch 794/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 4.8620e-08 - val_accuracy: 0.6847 - val_loss: 1.9880\n",
      "Epoch 795/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.8476e-08 - val_accuracy: 0.6851 - val_loss: 1.9883\n",
      "Epoch 796/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.8336e-08 - val_accuracy: 0.6851 - val_loss: 1.9886\n",
      "Epoch 797/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.8196e-08 - val_accuracy: 0.6851 - val_loss: 1.9890\n",
      "Epoch 798/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.8057e-08 - val_accuracy: 0.6851 - val_loss: 1.9893\n",
      "Epoch 799/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.7917e-08 - val_accuracy: 0.6851 - val_loss: 1.9896\n",
      "Epoch 800/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.7778e-08 - val_accuracy: 0.6851 - val_loss: 1.9899\n",
      "Epoch 801/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.7639e-08 - val_accuracy: 0.6851 - val_loss: 1.9902\n",
      "Epoch 802/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.7500e-08 - val_accuracy: 0.6851 - val_loss: 1.9905\n",
      "Epoch 803/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.7361e-08 - val_accuracy: 0.6851 - val_loss: 1.9908\n",
      "Epoch 804/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.7222e-08 - val_accuracy: 0.6851 - val_loss: 1.9911\n",
      "Epoch 805/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.7084e-08 - val_accuracy: 0.6851 - val_loss: 1.9914\n",
      "Epoch 806/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.6946e-08 - val_accuracy: 0.6851 - val_loss: 1.9917\n",
      "Epoch 807/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.6809e-08 - val_accuracy: 0.6851 - val_loss: 1.9920\n",
      "Epoch 808/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 4.6673e-08 - val_accuracy: 0.6851 - val_loss: 1.9923\n",
      "Epoch 809/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.6536e-08 - val_accuracy: 0.6851 - val_loss: 1.9926\n",
      "Epoch 810/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.6401e-08 - val_accuracy: 0.6851 - val_loss: 1.9929\n",
      "Epoch 811/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.6265e-08 - val_accuracy: 0.6851 - val_loss: 1.9933\n",
      "Epoch 812/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.6130e-08 - val_accuracy: 0.6851 - val_loss: 1.9936\n",
      "Epoch 813/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.5995e-08 - val_accuracy: 0.6851 - val_loss: 1.9939\n",
      "Epoch 814/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.5861e-08 - val_accuracy: 0.6851 - val_loss: 1.9942\n",
      "Epoch 815/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 4.5727e-08 - val_accuracy: 0.6851 - val_loss: 1.9945\n",
      "Epoch 816/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.5594e-08 - val_accuracy: 0.6851 - val_loss: 1.9948\n",
      "Epoch 817/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.5461e-08 - val_accuracy: 0.6851 - val_loss: 1.9952\n",
      "Epoch 818/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 4.5327e-08 - val_accuracy: 0.6851 - val_loss: 1.9955\n",
      "Epoch 819/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 4.5195e-08 - val_accuracy: 0.6847 - val_loss: 1.9958\n",
      "Epoch 820/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.5062e-08 - val_accuracy: 0.6847 - val_loss: 1.9961\n",
      "Epoch 821/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 4.4933e-08 - val_accuracy: 0.6847 - val_loss: 1.9964\n",
      "Epoch 822/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.4805e-08 - val_accuracy: 0.6847 - val_loss: 1.9967\n",
      "Epoch 823/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.4679e-08 - val_accuracy: 0.6847 - val_loss: 1.9970\n",
      "Epoch 824/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.4555e-08 - val_accuracy: 0.6847 - val_loss: 1.9973\n",
      "Epoch 825/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 4.4432e-08 - val_accuracy: 0.6847 - val_loss: 1.9976\n",
      "Epoch 826/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.4312e-08 - val_accuracy: 0.6847 - val_loss: 1.9978\n",
      "Epoch 827/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 4.4193e-08 - val_accuracy: 0.6847 - val_loss: 1.9981\n",
      "Epoch 828/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 4.4074e-08 - val_accuracy: 0.6847 - val_loss: 1.9984\n",
      "Epoch 829/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.3957e-08 - val_accuracy: 0.6847 - val_loss: 1.9986\n",
      "Epoch 830/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 4.3840e-08 - val_accuracy: 0.6847 - val_loss: 1.9989\n",
      "Epoch 831/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 4.3725e-08 - val_accuracy: 0.6847 - val_loss: 1.9992\n",
      "Epoch 832/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.3611e-08 - val_accuracy: 0.6851 - val_loss: 1.9994\n",
      "Epoch 833/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 4.3497e-08 - val_accuracy: 0.6851 - val_loss: 1.9997\n",
      "Epoch 834/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.3385e-08 - val_accuracy: 0.6851 - val_loss: 1.9999\n",
      "Epoch 835/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.3273e-08 - val_accuracy: 0.6851 - val_loss: 2.0002\n",
      "Epoch 836/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 4.3161e-08 - val_accuracy: 0.6851 - val_loss: 2.0004\n",
      "Epoch 837/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.3051e-08 - val_accuracy: 0.6851 - val_loss: 2.0007\n",
      "Epoch 838/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.2941e-08 - val_accuracy: 0.6851 - val_loss: 2.0009\n",
      "Epoch 839/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.2831e-08 - val_accuracy: 0.6851 - val_loss: 2.0012\n",
      "Epoch 840/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.2722e-08 - val_accuracy: 0.6851 - val_loss: 2.0014\n",
      "Epoch 841/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.2613e-08 - val_accuracy: 0.6851 - val_loss: 2.0017\n",
      "Epoch 842/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.2506e-08 - val_accuracy: 0.6851 - val_loss: 2.0019\n",
      "Epoch 843/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.2398e-08 - val_accuracy: 0.6851 - val_loss: 2.0022\n",
      "Epoch 844/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.2292e-08 - val_accuracy: 0.6851 - val_loss: 2.0025\n",
      "Epoch 845/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.2186e-08 - val_accuracy: 0.6851 - val_loss: 2.0027\n",
      "Epoch 846/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.2079e-08 - val_accuracy: 0.6851 - val_loss: 2.0030\n",
      "Epoch 847/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.1974e-08 - val_accuracy: 0.6851 - val_loss: 2.0032\n",
      "Epoch 848/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.1868e-08 - val_accuracy: 0.6851 - val_loss: 2.0035\n",
      "Epoch 849/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.1762e-08 - val_accuracy: 0.6851 - val_loss: 2.0037\n",
      "Epoch 850/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.1658e-08 - val_accuracy: 0.6847 - val_loss: 2.0039\n",
      "Epoch 851/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 4.1553e-08 - val_accuracy: 0.6847 - val_loss: 2.0042\n",
      "Epoch 852/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.1449e-08 - val_accuracy: 0.6847 - val_loss: 2.0044\n",
      "Epoch 853/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.1346e-08 - val_accuracy: 0.6847 - val_loss: 2.0047\n",
      "Epoch 854/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 4.1242e-08 - val_accuracy: 0.6847 - val_loss: 2.0049\n",
      "Epoch 855/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 4.1139e-08 - val_accuracy: 0.6847 - val_loss: 2.0051\n",
      "Epoch 856/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.1036e-08 - val_accuracy: 0.6847 - val_loss: 2.0054\n",
      "Epoch 857/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.0933e-08 - val_accuracy: 0.6847 - val_loss: 2.0056\n",
      "Epoch 858/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 4.0830e-08 - val_accuracy: 0.6847 - val_loss: 2.0059\n",
      "Epoch 859/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.0728e-08 - val_accuracy: 0.6847 - val_loss: 2.0061\n",
      "Epoch 860/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.0627e-08 - val_accuracy: 0.6847 - val_loss: 2.0064\n",
      "Epoch 861/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 4.0526e-08 - val_accuracy: 0.6847 - val_loss: 2.0066\n",
      "Epoch 862/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.0425e-08 - val_accuracy: 0.6847 - val_loss: 2.0069\n",
      "Epoch 863/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 4.0324e-08 - val_accuracy: 0.6847 - val_loss: 2.0071\n",
      "Epoch 864/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 4.0223e-08 - val_accuracy: 0.6847 - val_loss: 2.0073\n",
      "Epoch 865/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 4.0122e-08 - val_accuracy: 0.6847 - val_loss: 2.0076\n",
      "Epoch 866/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.0022e-08 - val_accuracy: 0.6847 - val_loss: 2.0078\n",
      "Epoch 867/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 3.9923e-08 - val_accuracy: 0.6847 - val_loss: 2.0081\n",
      "Epoch 868/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 3.9823e-08 - val_accuracy: 0.6847 - val_loss: 2.0083\n",
      "Epoch 869/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.9723e-08 - val_accuracy: 0.6847 - val_loss: 2.0086\n",
      "Epoch 870/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 3.9624e-08 - val_accuracy: 0.6847 - val_loss: 2.0088\n",
      "Epoch 871/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.9526e-08 - val_accuracy: 0.6847 - val_loss: 2.0091\n",
      "Epoch 872/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.9427e-08 - val_accuracy: 0.6847 - val_loss: 2.0093\n",
      "Epoch 873/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.9327e-08 - val_accuracy: 0.6847 - val_loss: 2.0095\n",
      "Epoch 874/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.9230e-08 - val_accuracy: 0.6847 - val_loss: 2.0098\n",
      "Epoch 875/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.9133e-08 - val_accuracy: 0.6847 - val_loss: 2.0100\n",
      "Epoch 876/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 3.9034e-08 - val_accuracy: 0.6847 - val_loss: 2.0103\n",
      "Epoch 877/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.8937e-08 - val_accuracy: 0.6847 - val_loss: 2.0106\n",
      "Epoch 878/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.8840e-08 - val_accuracy: 0.6847 - val_loss: 2.0108\n",
      "Epoch 879/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.8744e-08 - val_accuracy: 0.6847 - val_loss: 2.0111\n",
      "Epoch 880/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.8647e-08 - val_accuracy: 0.6847 - val_loss: 2.0113\n",
      "Epoch 881/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.8551e-08 - val_accuracy: 0.6847 - val_loss: 2.0116\n",
      "Epoch 882/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.8454e-08 - val_accuracy: 0.6847 - val_loss: 2.0118\n",
      "Epoch 883/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.8357e-08 - val_accuracy: 0.6847 - val_loss: 2.0120\n",
      "Epoch 884/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.8260e-08 - val_accuracy: 0.6847 - val_loss: 2.0123\n",
      "Epoch 885/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.8165e-08 - val_accuracy: 0.6847 - val_loss: 2.0125\n",
      "Epoch 886/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 3.8070e-08 - val_accuracy: 0.6847 - val_loss: 2.0128\n",
      "Epoch 887/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 3.7975e-08 - val_accuracy: 0.6847 - val_loss: 2.0131\n",
      "Epoch 888/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.7881e-08 - val_accuracy: 0.6847 - val_loss: 2.0133\n",
      "Epoch 889/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.7787e-08 - val_accuracy: 0.6847 - val_loss: 2.0136\n",
      "Epoch 890/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.7692e-08 - val_accuracy: 0.6847 - val_loss: 2.0138\n",
      "Epoch 891/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.7597e-08 - val_accuracy: 0.6847 - val_loss: 2.0141\n",
      "Epoch 892/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.7502e-08 - val_accuracy: 0.6847 - val_loss: 2.0143\n",
      "Epoch 893/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.7408e-08 - val_accuracy: 0.6847 - val_loss: 2.0146\n",
      "Epoch 894/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.7315e-08 - val_accuracy: 0.6847 - val_loss: 2.0148\n",
      "Epoch 895/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 3.7221e-08 - val_accuracy: 0.6847 - val_loss: 2.0151\n",
      "Epoch 896/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.7127e-08 - val_accuracy: 0.6847 - val_loss: 2.0154\n",
      "Epoch 897/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.7035e-08 - val_accuracy: 0.6847 - val_loss: 2.0156\n",
      "Epoch 898/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.6942e-08 - val_accuracy: 0.6847 - val_loss: 2.0159\n",
      "Epoch 899/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 3.6849e-08 - val_accuracy: 0.6847 - val_loss: 2.0161\n",
      "Epoch 900/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.6757e-08 - val_accuracy: 0.6847 - val_loss: 2.0164\n",
      "Epoch 901/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.6665e-08 - val_accuracy: 0.6847 - val_loss: 2.0166\n",
      "Epoch 902/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.6573e-08 - val_accuracy: 0.6847 - val_loss: 2.0169\n",
      "Epoch 903/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.6481e-08 - val_accuracy: 0.6847 - val_loss: 2.0172\n",
      "Epoch 904/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.6390e-08 - val_accuracy: 0.6847 - val_loss: 2.0174\n",
      "Epoch 905/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.6299e-08 - val_accuracy: 0.6847 - val_loss: 2.0177\n",
      "Epoch 906/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.6208e-08 - val_accuracy: 0.6847 - val_loss: 2.0179\n",
      "Epoch 907/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.6117e-08 - val_accuracy: 0.6847 - val_loss: 2.0182\n",
      "Epoch 908/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.6026e-08 - val_accuracy: 0.6847 - val_loss: 2.0185\n",
      "Epoch 909/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.5937e-08 - val_accuracy: 0.6847 - val_loss: 2.0187\n",
      "Epoch 910/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.5846e-08 - val_accuracy: 0.6847 - val_loss: 2.0190\n",
      "Epoch 911/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.5756e-08 - val_accuracy: 0.6847 - val_loss: 2.0192\n",
      "Epoch 912/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.5666e-08 - val_accuracy: 0.6847 - val_loss: 2.0195\n",
      "Epoch 913/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.5576e-08 - val_accuracy: 0.6847 - val_loss: 2.0198\n",
      "Epoch 914/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.5488e-08 - val_accuracy: 0.6847 - val_loss: 2.0201\n",
      "Epoch 915/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 3.5398e-08 - val_accuracy: 0.6847 - val_loss: 2.0203\n",
      "Epoch 916/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.5309e-08 - val_accuracy: 0.6847 - val_loss: 2.0206\n",
      "Epoch 917/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.5220e-08 - val_accuracy: 0.6847 - val_loss: 2.0208\n",
      "Epoch 918/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.5132e-08 - val_accuracy: 0.6847 - val_loss: 2.0211\n",
      "Epoch 919/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.5044e-08 - val_accuracy: 0.6847 - val_loss: 2.0214\n",
      "Epoch 920/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4955e-08 - val_accuracy: 0.6847 - val_loss: 2.0217\n",
      "Epoch 921/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 3.4868e-08 - val_accuracy: 0.6847 - val_loss: 2.0219\n",
      "Epoch 922/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.4781e-08 - val_accuracy: 0.6847 - val_loss: 2.0222\n",
      "Epoch 923/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.4693e-08 - val_accuracy: 0.6847 - val_loss: 2.0225\n",
      "Epoch 924/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.4605e-08 - val_accuracy: 0.6847 - val_loss: 2.0227\n",
      "Epoch 925/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.4518e-08 - val_accuracy: 0.6847 - val_loss: 2.0230\n",
      "Epoch 926/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4432e-08 - val_accuracy: 0.6847 - val_loss: 2.0232\n",
      "Epoch 927/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4345e-08 - val_accuracy: 0.6847 - val_loss: 2.0235\n",
      "Epoch 928/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 3.4258e-08 - val_accuracy: 0.6847 - val_loss: 2.0238\n",
      "Epoch 929/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.4172e-08 - val_accuracy: 0.6847 - val_loss: 2.0241\n",
      "Epoch 930/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4087e-08 - val_accuracy: 0.6847 - val_loss: 2.0244\n",
      "Epoch 931/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.4002e-08 - val_accuracy: 0.6847 - val_loss: 2.0246\n",
      "Epoch 932/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.3916e-08 - val_accuracy: 0.6847 - val_loss: 2.0249\n",
      "Epoch 933/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.3830e-08 - val_accuracy: 0.6847 - val_loss: 2.0252\n",
      "Epoch 934/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 3.3743e-08 - val_accuracy: 0.6847 - val_loss: 2.0254\n",
      "Epoch 935/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.3658e-08 - val_accuracy: 0.6847 - val_loss: 2.0257\n",
      "Epoch 936/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.3574e-08 - val_accuracy: 0.6847 - val_loss: 2.0259\n",
      "Epoch 937/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.3490e-08 - val_accuracy: 0.6847 - val_loss: 2.0262\n",
      "Epoch 938/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.3404e-08 - val_accuracy: 0.6847 - val_loss: 2.0265\n",
      "Epoch 939/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.3320e-08 - val_accuracy: 0.6847 - val_loss: 2.0268\n",
      "Epoch 940/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.3236e-08 - val_accuracy: 0.6847 - val_loss: 2.0271\n",
      "Epoch 941/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.3154e-08 - val_accuracy: 0.6847 - val_loss: 2.0274\n",
      "Epoch 942/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 3.3070e-08 - val_accuracy: 0.6847 - val_loss: 2.0276\n",
      "Epoch 943/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.2986e-08 - val_accuracy: 0.6847 - val_loss: 2.0279\n",
      "Epoch 944/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.2903e-08 - val_accuracy: 0.6847 - val_loss: 2.0282\n",
      "Epoch 945/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.2819e-08 - val_accuracy: 0.6847 - val_loss: 2.0285\n",
      "Epoch 946/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.2735e-08 - val_accuracy: 0.6847 - val_loss: 2.0288\n",
      "Epoch 947/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 3.2654e-08 - val_accuracy: 0.6847 - val_loss: 2.0290\n",
      "Epoch 948/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.2572e-08 - val_accuracy: 0.6847 - val_loss: 2.0293\n",
      "Epoch 949/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.2489e-08 - val_accuracy: 0.6847 - val_loss: 2.0296\n",
      "Epoch 950/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.2406e-08 - val_accuracy: 0.6847 - val_loss: 2.0299\n",
      "Epoch 951/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.2324e-08 - val_accuracy: 0.6847 - val_loss: 2.0301\n",
      "Epoch 952/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.2242e-08 - val_accuracy: 0.6847 - val_loss: 2.0304\n",
      "Epoch 953/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.2162e-08 - val_accuracy: 0.6847 - val_loss: 2.0307\n",
      "Epoch 954/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 3.2080e-08 - val_accuracy: 0.6847 - val_loss: 2.0310\n",
      "Epoch 955/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.1998e-08 - val_accuracy: 0.6847 - val_loss: 2.0313\n",
      "Epoch 956/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 3.1917e-08 - val_accuracy: 0.6847 - val_loss: 2.0316\n",
      "Epoch 957/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.1836e-08 - val_accuracy: 0.6847 - val_loss: 2.0319\n",
      "Epoch 958/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.1756e-08 - val_accuracy: 0.6847 - val_loss: 2.0321\n",
      "Epoch 959/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.1676e-08 - val_accuracy: 0.6847 - val_loss: 2.0324\n",
      "Epoch 960/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.1595e-08 - val_accuracy: 0.6847 - val_loss: 2.0327\n",
      "Epoch 961/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.1514e-08 - val_accuracy: 0.6847 - val_loss: 2.0330\n",
      "Epoch 962/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.1435e-08 - val_accuracy: 0.6847 - val_loss: 2.0333\n",
      "Epoch 963/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.1356e-08 - val_accuracy: 0.6847 - val_loss: 2.0335\n",
      "Epoch 964/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 3.1276e-08 - val_accuracy: 0.6842 - val_loss: 2.0339\n",
      "Epoch 965/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.1196e-08 - val_accuracy: 0.6842 - val_loss: 2.0341\n",
      "Epoch 966/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 3.1117e-08 - val_accuracy: 0.6842 - val_loss: 2.0344\n",
      "Epoch 967/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.1038e-08 - val_accuracy: 0.6842 - val_loss: 2.0347\n",
      "Epoch 968/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.0959e-08 - val_accuracy: 0.6842 - val_loss: 2.0350\n",
      "Epoch 969/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.0880e-08 - val_accuracy: 0.6838 - val_loss: 2.0353\n",
      "Epoch 970/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.0802e-08 - val_accuracy: 0.6838 - val_loss: 2.0356\n",
      "Epoch 971/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.0724e-08 - val_accuracy: 0.6838 - val_loss: 2.0359\n",
      "Epoch 972/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.0646e-08 - val_accuracy: 0.6838 - val_loss: 2.0362\n",
      "Epoch 973/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 3.0567e-08 - val_accuracy: 0.6838 - val_loss: 2.0365\n",
      "Epoch 974/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.0489e-08 - val_accuracy: 0.6838 - val_loss: 2.0367\n",
      "Epoch 975/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 3.0412e-08 - val_accuracy: 0.6838 - val_loss: 2.0370\n",
      "Epoch 976/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 3.0335e-08 - val_accuracy: 0.6838 - val_loss: 2.0373\n",
      "Epoch 977/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 3.0258e-08 - val_accuracy: 0.6838 - val_loss: 2.0376\n",
      "Epoch 978/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.0181e-08 - val_accuracy: 0.6838 - val_loss: 2.0379\n",
      "Epoch 979/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 3.0104e-08 - val_accuracy: 0.6838 - val_loss: 2.0382\n",
      "Epoch 980/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 3.0026e-08 - val_accuracy: 0.6838 - val_loss: 2.0385\n",
      "Epoch 981/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.9950e-08 - val_accuracy: 0.6838 - val_loss: 2.0388\n",
      "Epoch 982/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.9874e-08 - val_accuracy: 0.6838 - val_loss: 2.0391\n",
      "Epoch 983/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.9798e-08 - val_accuracy: 0.6838 - val_loss: 2.0394\n",
      "Epoch 984/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 2.9722e-08 - val_accuracy: 0.6838 - val_loss: 2.0397\n",
      "Epoch 985/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 2.9646e-08 - val_accuracy: 0.6838 - val_loss: 2.0400\n",
      "Epoch 986/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.9571e-08 - val_accuracy: 0.6838 - val_loss: 2.0403\n",
      "Epoch 987/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 2.9495e-08 - val_accuracy: 0.6838 - val_loss: 2.0406\n",
      "Epoch 988/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.9420e-08 - val_accuracy: 0.6838 - val_loss: 2.0409\n",
      "Epoch 989/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 2.9345e-08 - val_accuracy: 0.6838 - val_loss: 2.0412\n",
      "Epoch 990/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 2.9270e-08 - val_accuracy: 0.6834 - val_loss: 2.0415\n",
      "Epoch 991/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.9196e-08 - val_accuracy: 0.6834 - val_loss: 2.0418\n",
      "Epoch 992/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.9122e-08 - val_accuracy: 0.6834 - val_loss: 2.0421\n",
      "Epoch 993/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.9049e-08 - val_accuracy: 0.6834 - val_loss: 2.0424\n",
      "Epoch 994/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.8976e-08 - val_accuracy: 0.6834 - val_loss: 2.0427\n",
      "Epoch 995/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 2.8904e-08 - val_accuracy: 0.6834 - val_loss: 2.0429\n",
      "Epoch 996/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.8833e-08 - val_accuracy: 0.6834 - val_loss: 2.0431\n",
      "Epoch 997/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.8764e-08 - val_accuracy: 0.6834 - val_loss: 2.0434\n",
      "Epoch 998/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.8696e-08 - val_accuracy: 0.6834 - val_loss: 2.0436\n",
      "Epoch 999/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 2.8630e-08 - val_accuracy: 0.6834 - val_loss: 2.0438\n",
      "Epoch 1000/1000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 2.8564e-08 - val_accuracy: 0.6834 - val_loss: 2.0441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3c35edea0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model49.fit( ... (NB try epochs=1000)\n",
    "model49.fit(\n",
    "    x_train_49, \n",
    "    y_train_49, \n",
    "    epochs=1000, \n",
    "    batch_size=512, \n",
    "    validation_data=(x_val_49, y_val_49), \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step\n",
      "Validation Accuracy: 0.68337617823479\n",
      "Validation F1 Score: 0.7197572999620782\n"
     ]
    }
   ],
   "source": [
    "# accuracy_score...\n",
    "# f1_score...\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = model49.predict(x_val_49)\n",
    "val_predictions = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Compute accuracy\n",
    "val_accuracy = accuracy_score(y_val_49, val_predictions)\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "# Compute F1 score\n",
    "val_f1_score = f1_score(y_val_49, val_predictions)\n",
    "print(f'Validation F1 Score: {val_f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Transfer Learning:\n",
    " - ### Make an identical model to part 2, but take the weights learned from the original model on the rest of the data.\n",
    " - ### NB: the `Dense` layer takes a `weights=` keyword argument\n",
    " - ### Try making the layers static or trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ digit_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ digit_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m200,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,009</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m267,009\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m257\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,752</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m266,752\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit_input_transfer = Input(shape=(x_train_49.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model_transfer = Model(...\n",
    "#model_transfer.compile(...\n",
    "\n",
    "hidden_1_transfer = Dense(256, activation='relu', trainable=False)(digit_input_transfer)\n",
    "hidden_2_transfer = Dense(256, activation='relu', trainable=False)(hidden_1_transfer)\n",
    "output_transfer = Dense(1, activation='sigmoid')(hidden_2_transfer)\n",
    "\n",
    "model_transfer = Model(digit_input_transfer, output_transfer)\n",
    "model_transfer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set weights after creating the layers\n",
    "model_transfer.layers[1].set_weights(model_rest.layers[1].get_weights())\n",
    "model_transfer.layers[2].set_weights(model_rest.layers[2].get_weights())\n",
    "\n",
    "model_transfer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.6000 - loss: 1.1759 - val_accuracy: 0.4910 - val_loss: 1.7161\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6000 - loss: 1.1225 - val_accuracy: 0.4910 - val_loss: 1.6250\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6000 - loss: 1.0732 - val_accuracy: 0.4910 - val_loss: 1.5383\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6000 - loss: 1.0283 - val_accuracy: 0.4910 - val_loss: 1.4570\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6000 - loss: 0.9880 - val_accuracy: 0.4901 - val_loss: 1.3818\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6000 - loss: 0.9520 - val_accuracy: 0.4893 - val_loss: 1.3130\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6000 - loss: 0.9200 - val_accuracy: 0.4863 - val_loss: 1.2509\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6000 - loss: 0.8914 - val_accuracy: 0.4846 - val_loss: 1.1952\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6000 - loss: 0.8656 - val_accuracy: 0.4803 - val_loss: 1.1456\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5000 - loss: 0.8419 - val_accuracy: 0.4777 - val_loss: 1.1018\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4000 - loss: 0.8199 - val_accuracy: 0.4751 - val_loss: 1.0633\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4000 - loss: 0.7990 - val_accuracy: 0.4751 - val_loss: 1.0295\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4000 - loss: 0.7787 - val_accuracy: 0.4713 - val_loss: 0.9999\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.4000 - loss: 0.7587 - val_accuracy: 0.4683 - val_loss: 0.9740\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4000 - loss: 0.7388 - val_accuracy: 0.4704 - val_loss: 0.9512\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6000 - loss: 0.7190 - val_accuracy: 0.4726 - val_loss: 0.9311\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6000 - loss: 0.6991 - val_accuracy: 0.4734 - val_loss: 0.9134\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6000 - loss: 0.6792 - val_accuracy: 0.4743 - val_loss: 0.8976\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7000 - loss: 0.6594 - val_accuracy: 0.4799 - val_loss: 0.8836\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7000 - loss: 0.6399 - val_accuracy: 0.4820 - val_loss: 0.8710\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7000 - loss: 0.6209 - val_accuracy: 0.4846 - val_loss: 0.8596\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7000 - loss: 0.6023 - val_accuracy: 0.4867 - val_loss: 0.8492\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7000 - loss: 0.5845 - val_accuracy: 0.4893 - val_loss: 0.8395\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7000 - loss: 0.5673 - val_accuracy: 0.4901 - val_loss: 0.8303\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7000 - loss: 0.5509 - val_accuracy: 0.4906 - val_loss: 0.8215\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7000 - loss: 0.5353 - val_accuracy: 0.4940 - val_loss: 0.8127\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7000 - loss: 0.5205 - val_accuracy: 0.4974 - val_loss: 0.8039\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7000 - loss: 0.5063 - val_accuracy: 0.5009 - val_loss: 0.7949\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7000 - loss: 0.4928 - val_accuracy: 0.5034 - val_loss: 0.7855\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7000 - loss: 0.4799 - val_accuracy: 0.5073 - val_loss: 0.7758\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7000 - loss: 0.4675 - val_accuracy: 0.5090 - val_loss: 0.7657\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7000 - loss: 0.4556 - val_accuracy: 0.5124 - val_loss: 0.7552\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7000 - loss: 0.4441 - val_accuracy: 0.5176 - val_loss: 0.7443\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8000 - loss: 0.4330 - val_accuracy: 0.5227 - val_loss: 0.7331\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8000 - loss: 0.4222 - val_accuracy: 0.5291 - val_loss: 0.7217\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8000 - loss: 0.4119 - val_accuracy: 0.5377 - val_loss: 0.7102\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8000 - loss: 0.4019 - val_accuracy: 0.5424 - val_loss: 0.6987\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9000 - loss: 0.3922 - val_accuracy: 0.5536 - val_loss: 0.6874\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9000 - loss: 0.3830 - val_accuracy: 0.5613 - val_loss: 0.6762\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9000 - loss: 0.3741 - val_accuracy: 0.5728 - val_loss: 0.6654\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9000 - loss: 0.3656 - val_accuracy: 0.5848 - val_loss: 0.6550\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9000 - loss: 0.3574 - val_accuracy: 0.5977 - val_loss: 0.6451\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9000 - loss: 0.3496 - val_accuracy: 0.6105 - val_loss: 0.6356\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9000 - loss: 0.3422 - val_accuracy: 0.6204 - val_loss: 0.6267\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.3350 - val_accuracy: 0.6272 - val_loss: 0.6184\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.3282 - val_accuracy: 0.6358 - val_loss: 0.6105\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.3216 - val_accuracy: 0.6440 - val_loss: 0.6032\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.3153 - val_accuracy: 0.6508 - val_loss: 0.5964\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.3092 - val_accuracy: 0.6564 - val_loss: 0.5901\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.3034 - val_accuracy: 0.6615 - val_loss: 0.5842\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2978 - val_accuracy: 0.6684 - val_loss: 0.5787\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2924 - val_accuracy: 0.6735 - val_loss: 0.5736\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2871 - val_accuracy: 0.6787 - val_loss: 0.5688\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2821 - val_accuracy: 0.6864 - val_loss: 0.5643\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2772 - val_accuracy: 0.6932 - val_loss: 0.5601\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2725 - val_accuracy: 0.6962 - val_loss: 0.5561\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2679 - val_accuracy: 0.7005 - val_loss: 0.5523\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2635 - val_accuracy: 0.7031 - val_loss: 0.5487\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2593 - val_accuracy: 0.7082 - val_loss: 0.5453\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2552 - val_accuracy: 0.7112 - val_loss: 0.5419\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2512 - val_accuracy: 0.7172 - val_loss: 0.5387\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2474 - val_accuracy: 0.7224 - val_loss: 0.5356\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2436 - val_accuracy: 0.7232 - val_loss: 0.5326\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2400 - val_accuracy: 0.7279 - val_loss: 0.5297\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2365 - val_accuracy: 0.7301 - val_loss: 0.5268\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2332 - val_accuracy: 0.7326 - val_loss: 0.5239\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2299 - val_accuracy: 0.7352 - val_loss: 0.5212\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2267 - val_accuracy: 0.7395 - val_loss: 0.5184\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2236 - val_accuracy: 0.7421 - val_loss: 0.5158\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2205 - val_accuracy: 0.7438 - val_loss: 0.5131\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9000 - loss: 0.2176 - val_accuracy: 0.7455 - val_loss: 0.5105\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2147 - val_accuracy: 0.7494 - val_loss: 0.5080\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.2119 - val_accuracy: 0.7506 - val_loss: 0.5055\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2092 - val_accuracy: 0.7532 - val_loss: 0.5031\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2066 - val_accuracy: 0.7571 - val_loss: 0.5008\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.2040 - val_accuracy: 0.7584 - val_loss: 0.4985\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9000 - loss: 0.2015 - val_accuracy: 0.7609 - val_loss: 0.4963\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9000 - loss: 0.1990 - val_accuracy: 0.7644 - val_loss: 0.4941\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9000 - loss: 0.1966 - val_accuracy: 0.7665 - val_loss: 0.4920\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1942 - val_accuracy: 0.7669 - val_loss: 0.4900\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.1919 - val_accuracy: 0.7695 - val_loss: 0.4880\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1897 - val_accuracy: 0.7725 - val_loss: 0.4861\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1875 - val_accuracy: 0.7742 - val_loss: 0.4843\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1854 - val_accuracy: 0.7768 - val_loss: 0.4825\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1833 - val_accuracy: 0.7781 - val_loss: 0.4808\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1812 - val_accuracy: 0.7815 - val_loss: 0.4792\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1792 - val_accuracy: 0.7811 - val_loss: 0.4776\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1772 - val_accuracy: 0.7828 - val_loss: 0.4760\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1753 - val_accuracy: 0.7849 - val_loss: 0.4745\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1734 - val_accuracy: 0.7853 - val_loss: 0.4731\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1715 - val_accuracy: 0.7866 - val_loss: 0.4717\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1697 - val_accuracy: 0.7875 - val_loss: 0.4703\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1679 - val_accuracy: 0.7879 - val_loss: 0.4690\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1662 - val_accuracy: 0.7905 - val_loss: 0.4677\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1644 - val_accuracy: 0.7909 - val_loss: 0.4665\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1628 - val_accuracy: 0.7922 - val_loss: 0.4653\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1611 - val_accuracy: 0.7939 - val_loss: 0.4641\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1595 - val_accuracy: 0.7952 - val_loss: 0.4629\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1579 - val_accuracy: 0.7965 - val_loss: 0.4618\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1563 - val_accuracy: 0.7982 - val_loss: 0.4607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3ce929e70>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_transfer.fit(...    epochs=100, \n",
    "# accuracy_score...\n",
    "# f1_score...\n",
    "\n",
    "# Fit the model\n",
    "model_transfer.fit(\n",
    "    x_train_49, \n",
    "    y_train_49, \n",
    "    epochs=100, \n",
    "    batch_size=512, \n",
    "    validation_data=(x_val_49, y_val_49), \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step\n",
      "Validation Accuracy: 0.7982005141388174\n",
      "Validation F1 Score: 0.7902004454342985\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "val_predictions_transfer = model_transfer.predict(x_val_49)\n",
    "val_predictions_transfer = (val_predictions_transfer > 0.5).astype(int)\n",
    "\n",
    "# Compute accuracy\n",
    "val_accuracy_transfer = accuracy_score(y_val_49, val_predictions_transfer)\n",
    "print(f'Validation Accuracy: {val_accuracy_transfer}')\n",
    "\n",
    "# Compute F1 score\n",
    "val_f1_score_transfer = f1_score(y_val_49, val_predictions_transfer)\n",
    "print(f'Validation F1 Score: {val_f1_score_transfer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis:\n",
    " - We only transferred the first two layers and not the last one. Why?\n",
    " - Write the answer in a markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**ANSWER**\n",
    "\n",
    "We did not transfer the last layer (output layer) because the number of required outputs are different between the two datasets. \n",
    "\n",
    "- The original (first) model has an output layer of 10, because the model is predicting a probability across all digits. \n",
    "    - The output layer has 10 instead of 8 because we are retaining placeholders for 4 and 9; however, all of the y_train_rest_ohe values for the extra 2 columns would be zero, as no 4 or 9 exists in the \"_rest\" data. \n",
    "    - As mentioned in the instructions, this original model could also be made with 8 output nodes.\n",
    "\n",
    "- The transfer model has an output layer consisting of a single node because we are predicting two digits (4 and 9). Therefore, we can use a binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Data Augmentation (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to prevent overfitting is to augment the data.\n",
    "More data is always better, but sometimes we can't easily collect more data. \n",
    "A set of techniques to turn our current data set into a bigger one are called `data augmentation`. \n",
    "\n",
    "Data augmentation can take many forms, and are specific to the data and problem being solved. \n",
    "For example, in an image recognition problem, it is very common to rotate, crop, and zoom\n",
    "images to generate new ones. We can think of this as a form of regularization, since we are, \n",
    "in some sense, forcing a penalty if the model does not have rotation /scale invariance. \n",
    "In speech recognition, this can take the form of distorting an audio clip to have higher pitches\n",
    "(e.g. speeding it up), which should \"teach\" a model that it should be pitch invariant. \n",
    "\n",
    "In text classification problems, it typically a little more difficult to augment data. \n",
    "One common method is known as back-translation: if an automated machine translation model is \n",
    "available, we can translate our text into one language (e.g. english to french) and then back\n",
    "to the original language again (french to english). This typically yields a very similar \n",
    "piece of text to the original, but with different words. \n",
    "\n",
    "Here we'll try a simpler approach. In a low-data setting, we do not want the model to be too sensitive\n",
    "to any given word. Accordingly, we can augment our data by creating additional examples which are \n",
    "identical to our current example, but with some words set to unknown words.\n",
    "\n",
    "This problem is more opened ended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load data\n",
    "\n",
    "Load and process the IMDB sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(imdb_data_dir, \"train\")\n",
    "    test_dir = os.path.join(imdb_data_dir, \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename) as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create models\n",
    "\n",
    "Train two identical models. \n",
    "\n",
    "In one of them, try randomly removing some fraction of the words (this is equivalent to having the model pretend that it is seeing some fraction of unknown words, since unknown words are skipped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly remove words from documents\n",
    "def randomly_remove_words(docs, fraction=0.1):\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        num_words_to_remove = int(len(words) * fraction)\n",
    "        indices_to_remove = random.sample(range(len(words)), num_words_to_remove)\n",
    "        new_doc = ' '.join([word for i, word in enumerate(words) if i not in indices_to_remove])\n",
    "        new_docs.append(new_doc)\n",
    "    return new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "vectorizer = CountVectorizer(max_features=50000)\n",
    "X_train = vectorizer.fit_transform(train_docs).toarray()\n",
    "X_test = vectorizer.transform(test_docs).toarray()\n",
    "\n",
    "# Create a copy of the training data with some words removed\n",
    "train_docs_with_removed_words = randomly_remove_words(train_docs, fraction=0.1)\n",
    "X_train_with_removed_words = vectorizer.transform(train_docs_with_removed_words).toarray()\n",
    "\n",
    "# Create augmented training set by combining original and word-removed versions\n",
    "X_train_augmented = np.vstack([X_train, X_train_with_removed_words])\n",
    "y_train_augmented = np.concatenate([y_train, y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X_test=X_test, y_test=y_test):\n",
    "    \"\"\"Evaluate model performance using accuracy and log loss\"\"\"\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    loss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Log Loss: {loss:.4f}\")\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def create_model(input_dim):\n",
    "    K.clear_session()\n",
    "    text_input = Input(shape=(input_dim,))\n",
    "    hidden_state = Dense(\n",
    "        32,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=l2(1e-4),\n",
    "    )(text_input)\n",
    "    hidden_state = Dropout(0.2)(hidden_state)\n",
    "    hidden_state = Dense(\n",
    "        16,\n",
    "        activation=\"relu\",\n",
    "    )(hidden_state)\n",
    "    hidden_state = Dropout(0.2)(hidden_state)\n",
    "    output = Dense(1, activation=\"sigmoid\")(hidden_state)\n",
    "    model = Model(text_input, output)\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,032</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │     \u001b[38;5;34m1,600,032\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 226ms/step - acc: 0.6786 - loss: 0.6070 - val_acc: 0.8757 - val_loss: 0.3706\n",
      "Epoch 2/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 139ms/step - acc: 0.8959 - loss: 0.3207 - val_acc: 0.8860 - val_loss: 0.3195\n",
      "Epoch 3/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - acc: 0.9417 - loss: 0.2091 - val_acc: 0.8762 - val_loss: 0.3622\n",
      "Epoch 4/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - acc: 0.9595 - loss: 0.1589 - val_acc: 0.8722 - val_loss: 0.3962\n",
      "Epoch 5/5\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - acc: 0.9766 - loss: 0.1103 - val_acc: 0.8808 - val_loss: 0.3872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3554dffa0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the first model on the original data\n",
    "model1 = create_model(X_train.shape[1])\n",
    "model1.fit(X_train, \n",
    "           y_train, \n",
    "           epochs=5, \n",
    "           batch_size=512, \n",
    "           validation_data=(X_test, y_test), \n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Test Accuracy: 0.8808\n",
      "Test Log Loss: 0.3663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8808, 0.366286880864604)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy and log loss for the first model\n",
    "eval_model(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,032</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │     \u001b[38;5;34m1,600,032\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,600,577</span> (6.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,600,577\u001b[0m (6.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 146ms/step - acc: 0.7713 - loss: 0.5102 - val_acc: 0.8773 - val_loss: 0.3352\n",
      "Epoch 2/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 111ms/step - acc: 0.9500 - loss: 0.1837 - val_acc: 0.8810 - val_loss: 0.3573\n",
      "Epoch 3/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 113ms/step - acc: 0.9781 - loss: 0.1014 - val_acc: 0.8744 - val_loss: 0.4493\n",
      "Epoch 4/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step - acc: 0.9893 - loss: 0.0677 - val_acc: 0.8666 - val_loss: 0.5548\n",
      "Epoch 5/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 111ms/step - acc: 0.9931 - loss: 0.0534 - val_acc: 0.8672 - val_loss: 0.6005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x35569fb20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the second model on the data with removed words\n",
    "model2 = create_model(X_train.shape[1]) # keep the same input dimension as we are creating identical models\n",
    "model2.fit(X_train_augmented,  # Combined original and augmented data\n",
    "           y_train_augmented,  # Combined labels\n",
    "           epochs=5, \n",
    "           batch_size=512, \n",
    "           validation_data=(X_test, y_test), \n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "Test Accuracy: 0.8672\n",
      "Test Log Loss: 0.5684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8672, 0.5684250428894968)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy and log loss for the second model\n",
    "eval_model(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results. \n",
    "   - What is the result of dropping words.\n",
    "   - How does it compare to the image / audio methods described here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** \n",
    "\n",
    "- The result of dropping words at 10% did not result in increased model performance, and actually increased the model's tendency to overfit. This is perhaps because the drop rate of 10% was too close to the original data, but is also likely due to how the sentences with dropped words lose their original meaning.\n",
    "- The image / audio methods described above are ways to introduce known, real-world invariances into the model. For example, upside down or rotated images may actually appear in training data, and it is beneficial for the model to be able to identify them regardless of positioning. Dropping random words from text isn't as valid of an approach, because it is unlikely the altered sentences with missing words would actually be seen in real-world data. Other methods such as back translation are likely to have better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
