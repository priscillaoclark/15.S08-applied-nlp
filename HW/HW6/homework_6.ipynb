{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0: Other activation functions (10%)\n",
    "\n",
    "### The leaky Relu is defined as $max(0.1x, x)$.\n",
    " - What is its derivative? (Please express in \"easy\" format\")\n",
    " - Is it suitable for back propagation?\n",
    "\n",
    "### $tanh$ is defined as $\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    " - What is its derivative? (Please express in \"easy\" format\")\n",
    " - Is it suitable for back propagation?\n",
    " - How is it different from the sigmoid activation\n",
    " - What is an example of when to use it? When should you not use it?\n",
    " \n",
    "Please put answers as text below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: The Deep Learning Recipe (40%)\n",
    "\n",
    "In this problem, we'll follow the \"deep learning recipe\" covered in class on the IMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from helpers import load_imdb_data_text\n",
    "# or copy the loading function from the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text('../../data/aclImdb/')\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps \n",
    " - be one with data\n",
    " - set up e2e harness + get dumb baselines\n",
    " - overfit\n",
    " - regualarize\n",
    " - tune\n",
    " - squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: be one with the data\n",
    " - make some histograms\n",
    " - calculate some summary statistics\n",
    " - read a bunch of training examples and discuss any oddities you find\n",
    " - finally, turn the data into count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# make some plots, calculate some summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# print out some documents, find some anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_features=50000, lowercase=True)\n",
    "# vec.fit(...\n",
    "# x_train = ...\n",
    "# x_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: set up test harness and get baselines\n",
    " - state your baseline models and calculate the log loss and accuracy\n",
    "   - what is the best constant guess?\n",
    "   - what about a rules-based model? (e.g. checking if one of a few known words is present)\n",
    " - make a function that calculates model performance on the test set\n",
    "   - `def eval_model(your_model):`\n",
    " - make a keras model\n",
    "   - try to initialize the last layer appropriately (see [here](https://keras.io/api/layers/initializers/))\n",
    "     - `bias_initializer=Constant(some_constant)`\n",
    "   - evaluate the model with your function BEFORE training\n",
    " - examine data exactly as it is presented to the network\n",
    " - make sure you can memorize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# calculate the accuracy and log loss for a constant guess\n",
    "# calculate the accuracy and log loss for a rules based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(m):\n",
    "    # your code here\n",
    "    # print or return the accuracy and log loss on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other keras imports\n",
    "import keras.backend as K\n",
    "from keras.initializers import Constant # for last layer initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: what value of X do I need for $\\sigma(x)$ to be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a model\n",
    "# inpt = Input(shape=...)\n",
    "# hidden = ... (inpt)\n",
    "# hidden = ...(hidden)\n",
    "# ...\n",
    "# model = ...\n",
    "# model.compile... # don't forget to compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model before training it\n",
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine data as it is presented to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# print out a few training examples\n",
    "# they should be vectors of counts.\n",
    "# turn them back into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you can \"memorize\" or complete fit a small batch of data\n",
    "# try the first 100 training examples\n",
    "# the loss should go to near 0 pretty quickly\n",
    "#model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model)\n",
    "# at this point, the model is probably over fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Overfit\n",
    " - make the network large, and convince yourself you can overfit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "#model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularize\n",
    " - use regularizers, dropout, network size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers ...\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model code here\n",
    "# just like you did in the previous part\n",
    "# add dropout, regularization, maybe remove a Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - 6: Tune and Squeeze\n",
    "It will take a long time to tune the number of units in the Dense layers, so we will skip the tune phase. \n",
    "\n",
    "### Todo\n",
    " - Retrain the model\n",
    " - Make sure let it train enough\n",
    " - use callbacks to make sure the network stops before overfitting too much \n",
    " - use callbacks to reduce the learning rate appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# add these callbacks just like we did in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model)\n",
    "# you should be able to get > 88% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Transfer Learning (30 %)\n",
    "In this problem we will explore a technique called transfer learning. Often, we don't have very much labeled data for the problem at hand (we call it __data-poor__), but we can find labeled data for a similar problem (which we call ___data-rich__). \n",
    "\n",
    "In transfer learning, we use the __data-rich problem__ to train an network with good performance. We then make a similar network for the __data-poor problem__ but use the weights learned from the first problem in this network. This greatly reduces the amount of data needed to train the data-poor problem. You can think of this as reducing the number of free parameters. \n",
    "\n",
    "Here, we will use the mnist digit recognition problem. We will pretend that we are interested in telling the difference between the digits `4` and `9`, but we only have 10 labeled examples. We will pretend that we have tons of labeled examples of all of the other digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some imports\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "\n",
    "## Part 0: Subset the data into two datasets\n",
    " 1. One part will have `x_train_49`, `y_train_49`, etc. which has only `4`s and `9`s. \n",
    " 2. The second part will have variables `x_train_rest` etc, which will have the rest of the data and none of the digits `4` and `9`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocess_training_data(data):\n",
    "    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "    data = data.astype('float32') / 255\n",
    "    return data\n",
    "\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "\n",
    "def subset_to_9_and_4(x, y):  # this is a new function\n",
    "    mask = (y == 9) | (y == 4)\n",
    "    new_x = x[mask]\n",
    "    new_y = (y[mask] == 4).astype('int64')\n",
    "    return new_x, new_y\n",
    "\n",
    "def subset_to_rest(x, y):  # this is a new function\n",
    "    mask = ~((y == 9) | (y == 4))\n",
    "    new_x = x[mask]\n",
    "    new_y = y[mask]\n",
    "    return new_x, new_y\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_training_data(x_train)\n",
    "x_test = preprocess_training_data(x_test)\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "y_train_ohe = preprocess_targets(y_train, num_classes)\n",
    "y_test_ohe = preprocess_targets(y_test, num_classes)\n",
    "\n",
    "train_frac = 0.8\n",
    "cutoff = int(x_train.shape[0] * train_frac)\n",
    "x_train, x_val = x_train[:cutoff], x_train[cutoff:]\n",
    "y_train, y_val = y_train[:cutoff], y_train[cutoff:]\n",
    "y_train_ohe, y_val_ohe = y_train_ohe[:cutoff], y_train_ohe[cutoff:]\n",
    "\n",
    "x_train_49, y_train_49 = subset_to_9_and_4(x_train, y_train)\n",
    "x_val_49, y_val_49 = subset_to_9_and_4(x_val, y_val)\n",
    "x_test_49, y_test_49 = subset_to_9_and_4(x_test, y_test)\n",
    "\n",
    "print(x_train_49.shape)\n",
    "\n",
    "x_train_rest, y_train_rest = subset_to_rest(x_train, y_train)\n",
    "x_test_rest, y_test_rest = subset_to_rest(x_test, y_test)\n",
    "\n",
    "y_train_rest_ohe = to_categorical(y_train_rest, num_classes)\n",
    "y_test_rest_ohe = to_categorical(y_test_rest, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "## Now we will throw away most of the training data for the 4-9 problem\n",
    " - we will keep only 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 10\n",
    "x_train_49, y_train_49 = x_train_49[:num_points], y_train_49[:num_points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "\n",
    "## Part 1: Build a neural network to fit the `rest` data.\n",
    " - ### Include 2 densely connected hidden layers with 256 neurons each.\n",
    " - The output dimension should be either 8 or 10, depending on how you do the problem\n",
    " - ### Compute the accuracy score for this model\n",
    "\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "num_hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_input = Input(shape=(x_train_rest.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model_rest = ...\n",
    "#model_rest.compile( ... # to be removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model for 10 epochs and compute the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_rest.fit(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_score(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $ \n",
    "## Part 2: Fit a model on the `4`-`9` data\n",
    " - ### Use the same 2 densely-connected layers with 256 hidden units\n",
    " - ### Here the output layer could have 1 or two units, depending on how you set up the problem\n",
    " - ### NB: DO NOT use `K.clear_session()` because we need stuff for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "digit_input_49 = Input(shape=(x_train_49.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model49 = Model(...\n",
    "model49.compile( ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model49.fit( ... (NB try epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score...\n",
    "# f1_score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $ \n",
    "## Part 3: Transfer Learning:\n",
    " - ### Make an identical model to part 2, but take the weights learned from the original model on the rest of the data.\n",
    " - ### NB: the `Dense` layer takes a `weights=` keyword argument\n",
    " - ### Try making the layers static or trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "digit_input_transfer = Input(shape=(x_train_49.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model_transfer = Model(...\n",
    "#model_transfer.compile(...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_transfer.fit(...    epochs=100, \n",
    "# accuracy_score...\n",
    "# f1_score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis:\n",
    " - We only transferred the first two layers and not the last one. Why?\n",
    " - Write the answer in a markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Data Augmentation (20%)\n",
    "Another way to prevent overfitting is to augment the data.\n",
    "More data is always better, but sometimes we can't easily collect more data. \n",
    "A set of techniques to turn our current data set into a bigger one are called `data augmentation`. \n",
    "\n",
    "Data augmentation can take many forms, and are specific to the data and problem being solve. \n",
    "For example, in an image recognition problem, it is very common to rotate, crop, and zoom\n",
    "images to generate new ones. We can think of this as a form of regularization, since we are, \n",
    "in some sense, forcing a pentalty if the model does not have rotation /scale invariance. \n",
    "In speech recognition, this can take the form of distoring an audio clip to have higher pitches\n",
    "(e.g. speeding it up), which should \"teach\" a model that it should be pitch invariant. \n",
    "\n",
    "In text classification problems, it typcially a little more difficult to augment data. \n",
    "One common method is known as back-translation: if an autmated machine translation model is \n",
    "available, we can translate our text into one language (e.g. english to french) and then back\n",
    "to the original language again (french to english). This typically yields a very similar \n",
    "piece of text to the original, but with different words. \n",
    "\n",
    "Here we'll try a simpler approach. In a low-data setting, we do not want the model to be too sensitive\n",
    "to any given word. Accordingly, we can augment our data by creating additional examples which are \n",
    "identical to our current example, but with some words set to unknown words.\n",
    "\n",
    "This problem is more opened ended.\n",
    "TODO:\n",
    " - Load and process the IMDB sentiment data\n",
    " - train two identical models. In one of them, try randomly removing some fraction of the words (this is equivalent to having the model pretend that it is seeing some fraction of unknown words, since unknown words are skipped).\n",
    " - Discuss the results. \n",
    "   - What is the result of dropping words.\n",
    "   - How does it compare to the image / audio methods described here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
