{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0: Other activation functions (10%)\n",
    "\n",
    "### The leaky Relu is defined as $max(0.1x, x)$.\n",
    " - What is its derivative? (Please express in \"easy\" format\")\n",
    " - Is it suitable for back propagation?\n",
    "\n",
    "1)The derivative of the leaky ReLU function can be expressed as:\n",
    "\n",
    "   f′(x)=0.1x if x＜0\n",
    "\n",
    "   f′(x)=x if x＞0\n",
    "\n",
    "2)Yes, leaky ReLU is suitable for backpropagation. Its derivative is simple and computationally efficient, allowing for effective gradient flow \n",
    "during training, especially in deep networks where traditional ReLU might fail due to the dying ReLU issue.\n",
    "\n",
    "### $tanh$ is defined as $\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
    " - What is its derivative? (Please express in \"easy\" format\")\n",
    " - Is it suitable for back propagation?\n",
    " - How is it different from the sigmoid activation\n",
    " - What is an example of when to use it? When should you not use it?\n",
    "\n",
    "1)The derivative of the tanh function can be derived and simplified to:\n",
    " \n",
    "   f′(x)=1−tanh^2(x)\n",
    "\n",
    "2)Yes, the tanh function is suitable for backpropagation. Its derivative is straightforward to compute, and the function itself is differentiable everywhere, which is crucial for gradient-based optimization methods.\n",
    "\n",
    "3)Difference from Sigmoid Activation:\n",
    "Range: The tanh function outputs values between -1 and 1, making it zero-centered. In contrast, the sigmoid function outputs values between 0 and 1.\n",
    "Gradient: Tanh has a stronger gradient for most inputs compared to the sigmoid function, which can be beneficial for learning but also makes it more prone to the vanishing gradient problem in very deep networks.\n",
    "\n",
    "4)Use tanh in hidden layers of neural networks where you need a non-linear activation function that is zero-centered, which can help in stabilizing the learning process.\n",
    "Avoid using tanh in very deep networks or when dealing with problems where the vanishing gradient problem might be a significant issue. In such cases, ReLU or its variants (like leaky ReLU) might be more appropriate. Additionally, for binary classification problems where the output needs to be interpreted as a probability, the sigmoid function is typically preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: The Deep Learning Recipe (40%)\n",
    "\n",
    "In this problem, we'll follow the \"deep learning recipe\" covered in class on the IMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "%pylab inline\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "def load_imdb_data_text(imdb_data_dir, random_seed=1234):\n",
    "    \"\"\"Provided helper function to load data\"\"\"\n",
    "    train_dir = os.path.join(\"D:/data/aclImdb/\", \"train\")\n",
    "    test_dir = os.path.join(\"D:/data/aclImdb/\", \"test\")\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(train_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename, encoding=\"utf-8\") as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    train_docs = texts\n",
    "    y_train = np.array(targets)\n",
    "\n",
    "    texts = []\n",
    "    targets = []\n",
    "    for label in (\"pos\", \"neg\"):\n",
    "        data_dir = os.path.join(test_dir, label)\n",
    "        files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "        for filename in files:\n",
    "            with open(filename, encoding=\"utf-8\") as fi:\n",
    "                text = fi.read()\n",
    "            target = label == \"pos\"\n",
    "            texts.append(text)\n",
    "            targets.append(target)\n",
    "\n",
    "    test_docs = texts\n",
    "    y_test = np.array(targets)\n",
    "\n",
    "    inds = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    train_docs = [train_docs[i] for i in inds]\n",
    "    y_train = y_train[inds]\n",
    "\n",
    "    return (train_docs, y_train), (test_docs, y_test)\n",
    "\n",
    "# or copy the loading function from the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 25000 train docs and 25000 test docs\n"
     ]
    }
   ],
   "source": [
    "(train_docs, y_train), (test_docs, y_test) = load_imdb_data_text(\"D:/data/aclImdb/\")\n",
    "print('found {} train docs and {} test docs'.format(len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps \n",
    " - be one with data\n",
    " - set up e2e harness + get dumb baselines\n",
    " - overfit\n",
    " - regualarize\n",
    " - tune\n",
    " - squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: be one with the data\n",
    " - make some histograms\n",
    " - calculate some summary statistics\n",
    " - read a bunch of training examples and discuss any oddities you find\n",
    " - finally, turn the data into count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train document length: 233.7872 words\n",
      "Average test document length: 228.52668 words\n",
      "Number of training samples: 25000\n",
      "Number of test samples: 25000\n",
      "Positive labels in train set: 12500\n",
      "Negative labels in train set: 12500\n",
      "Positive labels in test set: 12500\n",
      "Negative labels in test set: 12500\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Document lengths\n",
    "train_doc_lengths = [len(doc.split()) for doc in train_docs]\n",
    "test_doc_lengths = [len(doc.split()) for doc in test_docs]\n",
    "\n",
    "# Plot histograms of document lengths\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(train_doc_lengths, bins=50, alpha=0.7, label='Train Docs')\n",
    "plt.hist(test_doc_lengths, bins=50, alpha=0.7, label='Test Docs')\n",
    "plt.xlabel('Document Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(f\"Average train document length: {np.mean(train_doc_lengths)} words\")\n",
    "print(f\"Average test document length: {np.mean(test_doc_lengths)} words\")\n",
    "print(f\"Number of training samples: {len(train_docs)}\")\n",
    "print(f\"Number of test samples: {len(test_docs)}\")\n",
    "print(f\"Positive labels in train set: {np.sum(y_train)}\")\n",
    "print(f\"Negative labels in train set: {len(y_train) - np.sum(y_train)}\")\n",
    "print(f\"Positive labels in test set: {np.sum(y_test)}\")\n",
    "print(f\"Negative labels in test set: {len(y_test) - np.sum(y_test)}\")\n",
    "\n",
    "# make some plots, calculate some summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 (Label: Positive):\n",
      "\n",
      "Wrestlemania 14 is not often looked as one of the great Wrestlemania's but I would personally put it, in my top 5, if not the top 3. It has so many great things, and it truly signified the birth of The Attitude Era, which was WWE's best era, in my opinion. HBK has the heart of a lion, and him putting over Austin like he did, on his way out, was pure class on his part. It has one of the hottest crowds you will ever see, and it has J.R and The King at their announcing best!. <br /><br />Matches.<b\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2 (Label: Negative):\n",
      "\n",
      "OK, the movie is good but I give it a 1 because the idea of a computer virus becoming an organic virus is pure fairy tale. This kind of crap just adds to those uncomputer savvy moron's paranoid delusions that a computer virus is exactly like an organic virus. First of all, strings of code and dozens of 1s and 0s add up to computer virus. An organic virus is much more complex, even though it's way tinier. Though, it's considered one of the simplest forms in the universe, organic virus's attach bu\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3 (Label: Positive):\n",
      "\n",
      "Michael Winner is probably best known for his revenge-themed films, such as \"Death Wish\" and \"Chato's Land\", but he is equally gifted as a director of occult Horror cinema, as \"The Sentinel\" of 1977 proves. \"The Sentinel\", which is based on a novel by John Konvitz, who also wrote the screenplay, is a clever and immensely creepy religious chiller that no lover of occult Horror should consider missing. The film is obviously inspired by successful occult classics such as \"Rosemary's Baby\", \"The Exo\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 4 (Label: Negative):\n",
      "\n",
      "Considering John Doe apparently inspired Kyle XY's creator I was expecting its pilot to be quite interesting. However I probably had too high expectations because I was quite disappointed by it. First they turned the protagonist into a freak who had the crazy idea of showing off his amazing knowledge in front of an audience, in a public area. So after that scene I began to worry that it was just entertainment. But the problem is that it got worse as none of the other characters were properly int\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 5 (Label: Negative):\n",
      "\n",
      "For such a great classic tale, the setting (location), Grendel was disappointing. As a writer, I blame the script which completely lacked dramatic tension. The rubric of the club story is useful and would have provided a new take on the literary classic. For some weird reason that rubric was dropped early on. To know this was shot in 21 days says to me, \"rushed\" and it unfortunately shows. Now we'll have to wait for the Hollywood version on the big screen. I word on FX, I can tolerate really cra\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print out some documents, find some anomalies\n",
    "# Display a few training documents to observe the text structure\n",
    "for i in range(5):\n",
    "    print(f\"\\nDocument {i+1} (Label: {'Positive' if y_train[i] else 'Negative'}):\\n\")\n",
    "    print(train_docs[i][:500])  # Print first 500 characters of the document for readability\n",
    "    print(\"\\n\" + \"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 50000)\n",
      "x_test shape: (25000, 50000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_features=50000, lowercase=True)\n",
    "\n",
    "# Fit the vectorizer on the training data and transform both train and test data\n",
    "x_train = vec.fit_transform(train_docs)\n",
    "x_test = vec.transform(test_docs)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "# vec.fit(...\n",
    "# x_train = ...\n",
    "# x_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: set up test harness and get baselines\n",
    " - state your baseline models and calculate the log loss and accuracy\n",
    "   - what is the best constant guess?\n",
    "   - what about a rules-based model? (e.g. checking if one of a few known words is present)\n",
    " - make a function that calculates model performance on the test set\n",
    "   - `def eval_model(your_model):`\n",
    " - make a keras model\n",
    "   - try to initialize the last layer appropriately (see [here](https://keras.io/api/layers/initializers/))\n",
    "     - `bias_initializer=Constant(some_constant)`\n",
    "   - evaluate the model with your function BEFORE training\n",
    " - examine data exactly as it is presented to the network\n",
    " - make sure you can memorize a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant Guess - Accuracy: 0.5000, Log Loss: 0.6931\n",
      "Rules-Based Model - Accuracy: 0.5000, Log Loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Baseline Models and Performance Metrics\n",
    "# calculate the accuracy and log loss for a constant guess\n",
    "# calculate the accuracy and log loss for a rules based approach\n",
    "\n",
    "\n",
    "# Constant Guess Model - always predicts the most frequent class (for example, \"positive\" if balanced)\n",
    "constant_guess = np.ones(len(y_test))  # Assume \"positive\" as a constant guess\n",
    "constant_accuracy = accuracy_score(y_test, constant_guess)\n",
    "constant_log_loss = log_loss(y_test, constant_guess * 0.5)  # Multiply by 0.5 to simulate balanced prediction\n",
    "\n",
    "print(f\"Constant Guess - Accuracy: {constant_accuracy:.4f}, Log Loss: {constant_log_loss:.4f}\")\n",
    "\n",
    "# Rules-Based Model - predict positive if certain words are present\n",
    "positive_words = [\"great\", \"excellent\", \"amazing\", \"good\", \"wonderful\"]\n",
    "\n",
    "def rules_based_predict(documents):\n",
    "    predictions = []\n",
    "    for doc in documents:\n",
    "        if any(word in doc for word in positive_words):\n",
    "            predictions.append(1)  # Positive sentiment\n",
    "        else:\n",
    "            predictions.append(0)  # Negative sentiment\n",
    "    return np.array(predictions)\n",
    "\n",
    "rules_based_predictions = rules_based_predict(test_docs)\n",
    "rules_based_accuracy = accuracy_score(y_test, rules_based_predictions)\n",
    "rules_based_log_loss = log_loss(y_test, rules_based_predictions * 0.5)\n",
    "\n",
    "print(f\"Rules-Based Model - Accuracy: {rules_based_accuracy:.4f}, Log Loss: {rules_based_log_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - Accuracy: 0.4994, Log Loss: 7.9808\n",
      "Test Accuracy: 0.4994, Test Log Loss: 7.980758067064219\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Model Evaluation Function\n",
    "# Define a function to evaluate the model’s performance on accuracy and log loss.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "input_shape = x_train.shape[1]  # 特征数量\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = Sequential([\n",
    "    Input(shape=(input_shape,)),  # Input layer using the feature dimension\n",
    "    Dense(64, activation='relu'),   # Hidden layer with ReLU activation\n",
    "    Dense(1, activation='sigmoid') # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model using the evaluation function defined earlier\n",
    "test_accuracy, test_log_loss = eval_model(model)\n",
    "print(f\"Test Accuracy: {test_accuracy}, Test Log Loss: {test_log_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model before training:\n",
      "Model - Accuracy: 0.4999, Log Loss: 7.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.49992, 7.97246802684671)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3: Build the Neural Network with Keras\n",
    "\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Prepare count vectorizer features\n",
    "vec = CountVectorizer(max_features=5000, lowercase=True)\n",
    "x_train = vec.fit_transform(train_docs).toarray()\n",
    "x_test = vec.transform(test_docs).toarray()\n",
    "\n",
    "# Neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid', bias_initializer=Constant(0.5))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model before training\n",
    "print(\"Evaluating model before training:\")\n",
    "eval_model(model)\n",
    "\n",
    "\n",
    "    # your code here\n",
    "    # print or return the accuracy and log loss on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other keras imports\n",
    "import keras.backend as K\n",
    "from keras.initializers import Constant # for last layer initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hint: what value of X do I need for $\\sigma(x)$ to be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a model\n",
    "# inpt = Input(shape=...)\n",
    "# hidden = ... (inpt)\n",
    "# hidden = ...(hidden)\n",
    "# ...\n",
    "# model = ...\n",
    "# model.compile... # don't forget to compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - Accuracy: 0.4999, Log Loss: 7.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.49992, 7.97246802684671)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model before training it\n",
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine data as it is presented to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data in vector form:\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Converting vectors back to words:\n",
      "Document 1 words: ['10', '14', '15', '16', '1st', '20', '3rd', 'actually', 'after', 'age', 'all', 'almost', 'amazing', 'among', 'and', 'another', 'antics', 'anybody', 'ape', 'are', 'as', 'at', 'attitude', 'average', 'away', 'awesome', 'back', 'bad', 'battle', 'be', 'because', 'been', 'before', 'best', 'better', 'big', 'biggest', 'birth', 'bits', 'bottom', 'br', 'breath', 'but', 'by', 'can', 'championship', 'class', 'constant', 'crowd', 'dead', 'decision', 'defeat', 'definitely', 'did', 'driver', 'eating', 'end', 'ending', 'entertaining', 'era', 'european', 'even', 'events', 'ever', 'every', 'everything', 'exciting', 'fan', 'fast', 'favorites', 'finally', 'flying', 'for', 'from', 'gets', 'gives', 'go', 'goes', 'going', 'gonna', 'good', 'great', 'greatest', 'guest', 'had', 'hardcore', 'hart', 'has', 'have', 'he', 'heard', 'heart', 'heat', 'helps', 'her', 'here', 'high', 'him', 'his', 'history', 'hits', 'hot', 'if', 'in', 'interview', 'is', 'it', 'jack', 'jennifer', 'just', 'kane', 'keeps', 'ken', 'kick', 'kinda', 'king', 'last', 'light', 'like', 'line', 'lion', 'looked', 'looking', 'looks', 'loud', 'main', 'making', 'man', 'many', 'match', 'matches', 'me', 'mike', 'minutes', 'mixed', 'more', 'much', 'my', 'new', 'nice', 'not', 'note', 'nuts', 'obvious', 'of', 'off', 'often', 'on', 'one', 'opinion', 'or', 'out', 'over', 'owen', 'part', 'passed', 'personally', 'pop', 'pops', 'potential', 'predictable', 'pure', 'put', 'puts', 'putting', 'ranks', 'rarely', 'reaction', 'real', 'really', 'return', 'review', 'right', 'ring', 'rock', 'royal', 'said', 'saw', 'see', 'sh', 'she', 'short', 'showed', 'sick', 'slaughter', 'smart', 'so', 'some', 'somewhat', 'special', 'started', 'steve', 'strong', 'stuff', 'stupid', 'supposed', 'surely', 'surprising', 'table', 'tag', 'taking', 'team', 'tell', 'terry', 'than', 'that', 'the', 'their', 'them', 'there', 'these', 'they', 'things', 'this', 'though', 'thought', 'through', 'time', 'title', 'titles', 'to', 'together', 'too', 'top', 'truly', 'two', 'underrated', 'up', 'very', 'vs', 'want', 'was', 'way', 'well', 'went', 'were', 'what', 'when', 'which', 'will', 'win', 'wins', 'with', 'woman', 'wonder', 'would', 'yet', 'you']\n",
      "Document 2 words: ['add', 'adds', 'all', 'an', 'and', 'be', 'because', 'becoming', 'but', 'change', 'code', 'complex', 'computer', 'considered', 'could', 'crap', 'didn', 'dozens', 'effect', 'even', 'exactly', 'explain', 'fairy', 'first', 'forms', 'from', 'give', 'good', 'has', 'how', 'idea', 'in', 'into', 'is', 'it', 'just', 'kind', 'like', 'maybe', 'me', 'more', 'movie', 'much', 'obviously', 'of', 'ok', 'on', 'one', 'own', 'pure', 'see', 'some', 'something', 'tale', 'that', 'the', 'themselves', 'then', 'these', 'think', 'this', 'those', 'though', 'to', 'turns', 'universe', 'up', 'user', 'virus', 'way', 'writer', 'your']\n",
      "Document 3 words: ['above', 'acted', 'adequate', 'alice', 'all', 'along', 'also', 'am', 'amazingly', 'an', 'and', 'another', 'any', 'apartment', 'as', 'atmosphere', 'atmospheric', 'baby', 'based', 'be', 'beautiful', 'become', 'becoming', 'been', 'before', 'best', 'big', 'bizarre', 'boyfriend', 'br', 'brooklyn', 'bunch', 'but', 'by', 'carradine', 'cast', 'certainly', 'characters', 'chris', 'christopher', 'cinema', 'classics', 'clever', 'comes', 'concerned', 'consider', 'constantly', 'creepy', 'cynical', 'death', 'description', 'detective', 'director', 'easily', 'eccentric', 'eerie', 'effective', 'equally', 'even', 'events', 'fact', 'famous', 'fan', 'fans', 'fantastic', 'far', 'favorite', 'features', 'film', 'films', 'finds', 'for', 'from', 'further', 'genuine', 'gifted', 'give', 'good', 'great', 'had', 'hard', 'hate', 'he', 'her', 'here', 'highly', 'his', 'horror', 'however', 'icon', 'immensely', 'in', 'includes', 'incredibly', 'influence', 'inspired', 'intelligent', 'is', 'it', 'its', 'jeff', 'jerry', 'john', 'kind', 'known', 'land', 'lawyer', 'like', 'little', 'long', 'losing', 'lovable', 'lover', 'man', 'mansion', 'many', 'marry', 'master', 'may', 'mentioned', 'michael', 'mind', 'missing', 'model', 'moments', 'more', 'mostly', 'my', 'names', 'neighbors', 'new', 'nice', 'no', 'not', 'novel', 'obscure', 'obviously', 'of', 'often', 'old', 'omen', 'on', 'one', 'only', 'or', 'other', 'outcome', 'parker', 'past', 'personal', 'photographed', 'plot', 'probably', 'producers', 'proof', 'proves', 'really', 'recommended', 'religious', 'revenge', 'role', 'say', 'scares', 'screenplay', 'search', 'setting', 'several', 'shock', 'should', 'sinister', 'so', 'soon', 'spoil', 'stars', 'strange', 'successful', 'such', 'superb', 'supporting', 'synopsis', 'than', 'that', 'the', 'these', 'this', 'time', 'to', 'tom', 'unsettling', 've', 'very', 'walken', 'well', 'what', 'which', 'who', 'widely', 'will', 'willing', 'winner', 'wish', 'with', 'would', 'writer', 'wrote', 'yet', 'york', 'young']\n",
      "Document 4 words: ['after', 'already', 'also', 'amazing', 'an', 'and', 'anyone', 'apparently', 'area', 'as', 'at', 'audience', 'bad', 'be', 'because', 'began', 'believable', 'better', 'br', 'bunch', 'but', 'by', 'casting', 'characters', 'club', 'coming', 'considering', 'correct', 'could', 'crazy', 'creator', 'developing', 'didn', 'disappointed', 'don', 'entertainment', 'episode', 'expectations', 'expecting', 'far', 'felt', 'find', 'first', 'focused', 'format', 'found', 'freak', 'front', 'good', 'got', 'had', 'have', 'he', 'help', 'high', 'his', 'how', 'however', 'idea', 'identity', 'if', 'impression', 'in', 'inspired', 'interesting', 'into', 'intriguing', 'introduced', 'is', 'issue', 'it', 'its', 'john', 'just', 'know', 'knowledge', 'kyle', 'leads', 'less', 'like', 'll', 'made', 'make', 'met', 'missed', 'much', 'never', 'next', 'none', 'not', 'now', 'of', 'off', 'on', 'one', 'other', 'overall', 'own', 'people', 'performance', 'pilot', 'plan', 'pleasant', 'police', 'potential', 'probably', 'problem', 'production', 'properly', 'protagonist', 'public', 'quality', 'quite', 'really', 'recommend', 'sad', 'scene', 'see', 'show', 'showing', 'slightly', 'so', 'some', 'story', 'target', 'that', 'the', 'there', 'they', 'to', 'too', 'turned', 'understand', 'us', 'very', 'was', 'wasn', 'watch', 'way', 'were', 'which', 'who', 'why', 'worry', 'worse', 'worst']\n",
      "Document 5 words: ['acting', 'actors', 'and', 'as', 'big', 'blame', 'but', 'can', 'cgi', 'classic', 'club', 'completely', 'crappy', 'days', 'disappointing', 'drama', 'dramatic', 'dropped', 'early', 'for', 'fx', 'great', 'has', 'have', 'hollywood', 'in', 'is', 'it', 'just', 'know', 'lacked', 'lacking', 'll', 'location', 'me', 'more', 'my', 'new', 'now', 'of', 'on', 'one', 'pick', 'provided', 'really', 'reason', 'rock', 'rushed', 'says', 'screen', 'script', 'setting', 'shot', 'shows', 'since', 'slow', 'some', 'story', 'such', 'take', 'tale', 'tension', 'than', 'that', 'the', 'this', 'to', 'too', 'unfortunately', 'useful', 'version', 'wait', 'was', 'we', 'weird', 'which', 'word', 'would', 'writer', 'writing']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# print out a few training examples\n",
    "# they should be vectors of counts.\n",
    "# turn them back into words\n",
    "# Display the first few training examples as vectors of counts\n",
    "# Step 4: Examine Training Data as Presented to the Network\n",
    "print(\"Sample training data in vector form:\")\n",
    "print(x_train[:5])\n",
    "\n",
    "# Convert these vectors back to words to confirm the vectorization is correct\n",
    "print(\"\\nConverting vectors back to words:\")\n",
    "for i in range(5):\n",
    "    words = [vec.get_feature_names_out()[idx] for idx, count in enumerate(x_train[i]) if count > 0]\n",
    "    print(f\"Document {i+1} words: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on a small batch to ensure memorization:\n",
      "Epoch 1/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3375 - loss: 0.8096\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5764 - loss: 0.6569 \n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8351 - loss: 0.5968\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8975 - loss: 0.5248\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9101 - loss: 0.4857\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9474 - loss: 0.4382\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.3749\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.3433\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.3086\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2716\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2554\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2536\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2123\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.2320   \n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1892\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1994\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1714\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1589\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1363\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1235\n",
      "Model - Accuracy: 0.5703, Log Loss: 6.8508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.57028, 6.85076181587773)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Ensure Model Can Memorize a Small Batch\n",
    "# try the first 100 training examples\n",
    "# the loss should go to near 0 pretty quickly\n",
    "#model.fit(...)\n",
    "\n",
    "# Fit the model on a small batch of data\n",
    "print(\"Training on a small batch to ensure memorization:\")\n",
    "model.fit(x_train[:100], y_train[:100], epochs=20, batch_size=16, verbose=1)\n",
    "\n",
    "# Evaluate the model on the same batch to confirm it has memorized it\n",
    "eval_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, the model is probably over fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Overfit\n",
    " - make the network large, and convince yourself you can overfit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4855 - loss: 0.7124\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7480 - loss: 0.4993\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.1385\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0227\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0025\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 3.3052e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.5921e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.2162e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.6857e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.5819e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.6795e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6147e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6385e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1267e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.9590e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9588e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.4622e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.6928e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.7415e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7065e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.3390e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.0437e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4617e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1015e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1980e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1359e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0063e-06\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.4083e-06\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.9557e-06\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.3431e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.8669e-06\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.0030e-06\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.0269e-06\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.7956e-06\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.4061e-06\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.3781e-06\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1171e-06\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9313e-06\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.0701e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.9212e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.6408e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.3646e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.8323e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.1384e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.6790e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.0893e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.8547e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4070e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.4614e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6822e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3087e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 9.9183e-07\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7953e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.6815e-07 \n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.3932e-07\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7509e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4128e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.5429e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4199e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4489e-06\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.2468e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0685e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.8863e-07\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1402e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0830e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.6146e-07\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1223e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.7844e-07\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.3316e-07\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.4761e-07\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.8610e-07\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.3207e-07\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.4739e-07\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3161e-06\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.0996e-07\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.6431e-07\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.5480e-07\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.1139e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.3124e-07\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.4922e-07\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0746e-07\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.3662e-07\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.0597e-07\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.4834e-07\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.4341e-07\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.8252e-07\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.6851e-07\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.4560e-07\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.8709e-07\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9002e-07\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3430e-07\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.6499e-07\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.6623e-07\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.8498e-07\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.8590e-07\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.7846e-07\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.3783e-07\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.3602e-07\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1425e-07\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.9496e-07\n",
      "Training Accuracy: 1.0000, Training Loss: 0.0000\n",
      "Test Accuracy: 0.6979, Test Loss: 1.7268\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Check the input shape from the training data\n",
    "input_shape = x_train.shape[1]  # Number of features in x_train\n",
    "\n",
    "# Define a larger model to encourage overfitting\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(input_shape,)))  # Specify the input shape here\n",
    "\n",
    "# Add multiple layers with a large number of neurons\n",
    "model.add(Dense(512, activation='relu'))  # First hidden layer with 512 neurons\n",
    "model.add(Dense(256, activation='relu'))  # Second hidden layer with 256 neurons\n",
    "model.add(Dense(128, activation='relu'))  # Third hidden layer with 128 neurons\n",
    "model.add(Dense(64, activation='relu'))   # Fourth hidden layer with 64 neurons\n",
    "model.add(Dense(32, activation='relu'))   # Fifth hidden layer with 32 neurons\n",
    "\n",
    "# Output layer with 1 neuron for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Overfit the model on a small subset of training data (e.g., the first 100 samples)\n",
    "x_train_subset = x_train[:100]\n",
    "y_train_subset = y_train[:100]\n",
    "\n",
    "# Train the model on this subset and monitor the training loss\n",
    "history = model.fit(x_train_subset, y_train_subset, epochs=100, batch_size=10, verbose=1)\n",
    "\n",
    "# Evaluate the model to see the training vs test performance\n",
    "train_loss, train_accuracy = model.evaluate(x_train_subset, y_train_subset, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}, Training Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularize\n",
    " - use regularizers, dropout, network size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.7252 - loss: 1.1159 - val_accuracy: 0.8610 - val_loss: 0.6522\n",
      "Epoch 2/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.8793 - loss: 0.5910 - val_accuracy: 0.8564 - val_loss: 0.5866\n",
      "Epoch 3/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8891 - loss: 0.5261 - val_accuracy: 0.8724 - val_loss: 0.5393\n",
      "Epoch 4/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.8965 - loss: 0.4856 - val_accuracy: 0.8718 - val_loss: 0.5441\n",
      "Epoch 5/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9077 - loss: 0.4674 - val_accuracy: 0.8698 - val_loss: 0.5323\n",
      "Epoch 6/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9138 - loss: 0.4449 - val_accuracy: 0.8706 - val_loss: 0.5313\n",
      "Epoch 7/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9085 - loss: 0.4487 - val_accuracy: 0.8718 - val_loss: 0.5244\n",
      "Epoch 8/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9121 - loss: 0.4377 - val_accuracy: 0.8670 - val_loss: 0.5348\n",
      "Epoch 9/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9209 - loss: 0.4224 - val_accuracy: 0.8674 - val_loss: 0.5388\n",
      "Epoch 10/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9203 - loss: 0.4211 - val_accuracy: 0.8642 - val_loss: 0.5569\n",
      "Epoch 11/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9197 - loss: 0.4206 - val_accuracy: 0.8628 - val_loss: 0.5497\n",
      "Epoch 12/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9235 - loss: 0.4136 - val_accuracy: 0.8714 - val_loss: 0.5383\n",
      "Epoch 13/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9217 - loss: 0.4156 - val_accuracy: 0.8776 - val_loss: 0.5272\n",
      "Epoch 14/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9239 - loss: 0.4102 - val_accuracy: 0.8714 - val_loss: 0.5323\n",
      "Epoch 15/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9246 - loss: 0.4072 - val_accuracy: 0.8726 - val_loss: 0.5370\n",
      "Epoch 16/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9236 - loss: 0.4025 - val_accuracy: 0.8692 - val_loss: 0.5252\n",
      "Epoch 17/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9246 - loss: 0.4025 - val_accuracy: 0.8730 - val_loss: 0.5281\n",
      "Epoch 18/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9251 - loss: 0.3984 - val_accuracy: 0.8712 - val_loss: 0.5201\n",
      "Epoch 19/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9259 - loss: 0.3908 - val_accuracy: 0.8700 - val_loss: 0.5280\n",
      "Epoch 20/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9278 - loss: 0.3925 - val_accuracy: 0.8702 - val_loss: 0.5351\n",
      "Epoch 21/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9247 - loss: 0.3961 - val_accuracy: 0.8700 - val_loss: 0.5369\n",
      "Epoch 22/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9259 - loss: 0.3919 - val_accuracy: 0.8630 - val_loss: 0.5447\n",
      "Epoch 23/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9241 - loss: 0.4034 - val_accuracy: 0.8672 - val_loss: 0.5402\n",
      "Epoch 24/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9265 - loss: 0.3832 - val_accuracy: 0.8586 - val_loss: 0.5677\n",
      "Epoch 25/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9247 - loss: 0.3946 - val_accuracy: 0.8696 - val_loss: 0.5684\n",
      "Epoch 26/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9242 - loss: 0.3904 - val_accuracy: 0.8658 - val_loss: 0.5456\n",
      "Epoch 27/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9276 - loss: 0.3864 - val_accuracy: 0.8682 - val_loss: 0.5440\n",
      "Epoch 28/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9262 - loss: 0.3848 - val_accuracy: 0.8668 - val_loss: 0.5448\n",
      "Epoch 29/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9261 - loss: 0.3839 - val_accuracy: 0.8660 - val_loss: 0.5398\n",
      "Epoch 30/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9216 - loss: 0.3980 - val_accuracy: 0.8690 - val_loss: 0.5346\n",
      "Epoch 31/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9242 - loss: 0.3824 - val_accuracy: 0.8568 - val_loss: 0.5653\n",
      "Epoch 32/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9262 - loss: 0.3864 - val_accuracy: 0.8706 - val_loss: 0.5333\n",
      "Epoch 33/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9264 - loss: 0.3904 - val_accuracy: 0.8690 - val_loss: 0.5284\n",
      "Epoch 34/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9237 - loss: 0.3880 - val_accuracy: 0.8658 - val_loss: 0.5410\n",
      "Epoch 35/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9230 - loss: 0.3871 - val_accuracy: 0.8690 - val_loss: 0.5363\n",
      "Epoch 36/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9244 - loss: 0.3906 - val_accuracy: 0.8664 - val_loss: 0.5365\n",
      "Epoch 37/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9181 - loss: 0.3997 - val_accuracy: 0.8734 - val_loss: 0.5199\n",
      "Epoch 38/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9296 - loss: 0.3757 - val_accuracy: 0.8584 - val_loss: 0.5491\n",
      "Epoch 39/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9221 - loss: 0.3897 - val_accuracy: 0.8572 - val_loss: 0.5438\n",
      "Epoch 40/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9221 - loss: 0.3887 - val_accuracy: 0.8666 - val_loss: 0.5436\n",
      "Epoch 41/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9282 - loss: 0.3802 - val_accuracy: 0.8640 - val_loss: 0.5641\n",
      "Epoch 42/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9200 - loss: 0.3996 - val_accuracy: 0.8650 - val_loss: 0.5555\n",
      "Epoch 43/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9248 - loss: 0.3893 - val_accuracy: 0.8682 - val_loss: 0.5258\n",
      "Epoch 44/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9263 - loss: 0.3866 - val_accuracy: 0.8668 - val_loss: 0.5351\n",
      "Epoch 45/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9276 - loss: 0.3783 - val_accuracy: 0.8698 - val_loss: 0.5430\n",
      "Epoch 46/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9256 - loss: 0.3808 - val_accuracy: 0.8624 - val_loss: 0.5406\n",
      "Epoch 47/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9226 - loss: 0.3892 - val_accuracy: 0.8630 - val_loss: 0.5618\n",
      "Epoch 48/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9245 - loss: 0.3820 - val_accuracy: 0.8652 - val_loss: 0.5330\n",
      "Epoch 49/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9218 - loss: 0.3833 - val_accuracy: 0.8640 - val_loss: 0.5310\n",
      "Epoch 50/50\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.9272 - loss: 0.3766 - val_accuracy: 0.8612 - val_loss: 0.5293\n",
      "Test Accuracy: 0.8656, Test Loss: 0.5255\n"
     ]
    }
   ],
   "source": [
    "# model code here\n",
    "# just like you did in the previous part\n",
    "# add dropout, regularization, maybe remove a Dense layer\n",
    "\n",
    "# Set input shape based on feature count in x_train\n",
    "input_shape = x_train.shape[1]\n",
    "\n",
    "# Define a neural network model with regularization\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(input_shape,)))\n",
    "\n",
    "# Add hidden layers with L2 regularization and Dropout\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))  # First layer with L2 regularization\n",
    "model.add(Dropout(0.5))  # Dropout to reduce overfitting\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))  # Second layer with L2 regularization\n",
    "model.add(Dropout(0.5))  # Dropout\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))   # Third layer with L2 regularization\n",
    "model.add(Dropout(0.3))  # Dropout\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the full training set and monitor validation loss\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - 6: Tune and Squeeze\n",
    "It will take a long time to tune the number of units in the Dense layers, so we will skip the tune phase. \n",
    "\n",
    "### Todo\n",
    " - Retrain the model\n",
    " - Make sure let it train enough\n",
    " - use callbacks to make sure the network stops before overfitting too much \n",
    " - use callbacks to reduce the learning rate appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.7644 - loss: 1.4591 - val_accuracy: 0.8323 - val_loss: 0.7232 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.8849 - loss: 0.5344 - val_accuracy: 0.8772 - val_loss: 0.4296 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.9011 - loss: 0.3791 - val_accuracy: 0.8816 - val_loss: 0.3820 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.9069 - loss: 0.3334 - val_accuracy: 0.8824 - val_loss: 0.3744 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.9148 - loss: 0.3030 - val_accuracy: 0.8747 - val_loss: 0.3894 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.9210 - loss: 0.2913 - val_accuracy: 0.8767 - val_loss: 0.3939 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 29ms/step - accuracy: 0.9294 - loss: 0.2738 - val_accuracy: 0.8737 - val_loss: 0.3921 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.9360 - loss: 0.2540 - val_accuracy: 0.8716 - val_loss: 0.3978 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9425 - loss: 0.2372 - val_accuracy: 0.8656 - val_loss: 0.4042 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9661 - loss: 0.1780 - val_accuracy: 0.8737 - val_loss: 0.4358 - learning_rate: 1.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9741 - loss: 0.1488 - val_accuracy: 0.8680 - val_loss: 0.4591 - learning_rate: 1.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9762 - loss: 0.1341 - val_accuracy: 0.8536 - val_loss: 0.5391 - learning_rate: 1.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9824 - loss: 0.1156 - val_accuracy: 0.8690 - val_loss: 0.4739 - learning_rate: 1.5000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.9827 - loss: 0.1088 - val_accuracy: 0.8666 - val_loss: 0.4979 - learning_rate: 1.5000e-04\n",
      "Model - Accuracy: 0.8824, Log Loss: 0.2901\n",
      "Test Accuracy: 0.88244, Test Log Loss: 0.2900983059985259\n"
     ]
    }
   ],
   "source": [
    "# add these callbacks just like we did in class\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "# Define input shape based on x_train's feature count\n",
    "input_shape = x_train.shape[1]\n",
    "\n",
    "# Define the neural network model with more units and adjusted regularization\n",
    "model = Sequential([\n",
    "    Input(shape=(input_shape,)),\n",
    "    Dense(1024, activation='relu', kernel_regularizer=l2(0.0005)),  # Increased units, reduced L2 regularization\n",
    "    Dropout(0.3),  # Reduced dropout rate for first layer\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with a reduced learning rate and RMSprop optimizer\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model with the modified parameters\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy, test_log_loss = eval_model(model)\n",
    "print(f\"Test Accuracy: {test_accuracy}, Test Log Loss: {test_log_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should be able to get > 88% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Transfer Learning (30 %)\n",
    "In this problem we will explore a technique called transfer learning. Often, we don't have very much labeled data for the problem at hand (we call it __data-poor__), but we can find labeled data for a similar problem (which we call ___data-rich__). \n",
    "\n",
    "In transfer learning, we use the __data-rich problem__ to train an network with good performance. We then make a similar network for the __data-poor problem__ but use the weights learned from the first problem in this network. This greatly reduces the amount of data needed to train the data-poor problem. You can think of this as reducing the number of free parameters. \n",
    "\n",
    "Here, we will use the mnist digit recognition problem. We will pretend that we are interested in telling the difference between the digits `4` and `9`, but we only have 10 labeled examples. We will pretend that we have tons of labeled examples of all of the other digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some imports\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "\n",
    "## Part 0: Subset the data into two datasets\n",
    " 1. One part will have `x_train_49`, `y_train_49`, etc. which has only `4`s and `9`s. \n",
    " 2. The second part will have variables `x_train_rest` etc, which will have the rest of the data and none of the digits `4` and `9`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocess_training_data(data):\n",
    "    data = data.reshape(data.shape[0], data.shape[1] * data.shape[2])\n",
    "    data = data.astype('float32') / 255\n",
    "    return data\n",
    "\n",
    "def preprocess_targets(target, num_classes):\n",
    "    return to_categorical(target, num_classes)\n",
    "\n",
    "\n",
    "def subset_to_9_and_4(x, y):  # this is a new function\n",
    "    mask = (y == 9) | (y == 4)\n",
    "    new_x = x[mask]\n",
    "    new_y = (y[mask] == 4).astype('int64')\n",
    "    return new_x, new_y\n",
    "\n",
    "def subset_to_rest(x, y):  # this is a new function\n",
    "    mask = ~((y == 9) | (y == 4))\n",
    "    new_x = x[mask]\n",
    "    new_y = y[mask]\n",
    "    return new_x, new_y\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_training_data(x_train)\n",
    "x_test = preprocess_training_data(x_test)\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "y_train_ohe = preprocess_targets(y_train, num_classes)\n",
    "y_test_ohe = preprocess_targets(y_test, num_classes)\n",
    "\n",
    "train_frac = 0.8\n",
    "cutoff = int(x_train.shape[0] * train_frac)\n",
    "x_train, x_val = x_train[:cutoff], x_train[cutoff:]\n",
    "y_train, y_val = y_train[:cutoff], y_train[cutoff:]\n",
    "y_train_ohe, y_val_ohe = y_train_ohe[:cutoff], y_train_ohe[cutoff:]\n",
    "\n",
    "x_train_49, y_train_49 = subset_to_9_and_4(x_train, y_train)\n",
    "x_val_49, y_val_49 = subset_to_9_and_4(x_val, y_val)\n",
    "x_test_49, y_test_49 = subset_to_9_and_4(x_test, y_test)\n",
    "\n",
    "print(x_train_49.shape)\n",
    "\n",
    "x_train_rest, y_train_rest = subset_to_rest(x_train, y_train)\n",
    "x_test_rest, y_test_rest = subset_to_rest(x_test, y_test)\n",
    "\n",
    "y_train_rest_ohe = to_categorical(y_train_rest, num_classes)\n",
    "y_test_rest_ohe = to_categorical(y_test_rest, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "## Now we will throw away most of the training data for the 4-9 problem\n",
    " - we will keep only 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 10\n",
    "x_train_49, y_train_49 = x_train_49[:num_points], y_train_49[:num_points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $\n",
    "\n",
    "## Part 1: Build a neural network to fit the `rest` data.\n",
    " - ### Include 2 densely connected hidden layers with 256 neurons each.\n",
    " - The output dimension should be either 8 or 10, depending on how you do the problem\n",
    " - ### Compute the accuracy score for this model\n",
    "\n",
    "# $ \\\\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "num_hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_input = Input(shape=(x_train_rest.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model_rest = ...\n",
    "#model_rest.compile( ... # to be removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model for 10 epochs and compute the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_rest.fit(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_score(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $ \n",
    "## Part 2: Fit a model on the `4`-`9` data\n",
    " - ### Use the same 2 densely-connected layers with 256 hidden units\n",
    " - ### Here the output layer could have 1 or two units, depending on how you set up the problem\n",
    " - ### NB: DO NOT use `K.clear_session()` because we need stuff for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "digit_input_49 = Input(shape=(x_train_49.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model49 = Model(...\n",
    "model49.compile( ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model49.fit( ... (NB try epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score...\n",
    "# f1_score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $ \\\\ $ \n",
    "## Part 3: Transfer Learning:\n",
    " - ### Make an identical model to part 2, but take the weights learned from the original model on the rest of the data.\n",
    " - ### NB: the `Dense` layer takes a `weights=` keyword argument\n",
    " - ### Try making the layers static or trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_input_transfer = Input(shape=(x_train_49.shape[1],), name='digit_input')\n",
    "# add code here\n",
    "#model_transfer = Model(...\n",
    "#model_transfer.compile(...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_transfer.fit(...    epochs=100, \n",
    "# accuracy_score...\n",
    "# f1_score..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis:\n",
    " - We only transferred the first two layers and not the last one. Why?\n",
    " - Write the answer in a markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Data Augmentation (20%)\n",
    "Another way to prevent overfitting is to augment the data.\n",
    "More data is always better, but sometimes we can't easily collect more data. \n",
    "A set of techniques to turn our current data set into a bigger one are called `data augmentation`. \n",
    "\n",
    "Data augmentation can take many forms, and are specific to the data and problem being solve. \n",
    "For example, in an image recognition problem, it is very common to rotate, crop, and zoom\n",
    "images to generate new ones. We can think of this as a form of regularization, since we are, \n",
    "in some sense, forcing a pentalty if the model does not have rotation /scale invariance. \n",
    "In speech recognition, this can take the form of distoring an audio clip to have higher pitches\n",
    "(e.g. speeding it up), which should \"teach\" a model that it should be pitch invariant. \n",
    "\n",
    "In text classification problems, it typcially a little more difficult to augment data. \n",
    "One common method is known as back-translation: if an autmated machine translation model is \n",
    "available, we can translate our text into one language (e.g. english to french) and then back\n",
    "to the original language again (french to english). This typically yields a very similar \n",
    "piece of text to the original, but with different words. \n",
    "\n",
    "Here we'll try a simpler approach. In a low-data setting, we do not want the model to be too sensitive\n",
    "to any given word. Accordingly, we can augment our data by creating additional examples which are \n",
    "identical to our current example, but with some words set to unknown words.\n",
    "\n",
    "This problem is more opened ended.\n",
    "TODO:\n",
    " - Load and process the IMDB sentiment data\n",
    " - train two identical models. In one of them, try randomly removing some fraction of the words (this is equivalent to having the model pretend that it is seeing some fraction of unknown words, since unknown words are skipped).\n",
    " - Discuss the results. \n",
    "   - What is the result of dropping words.\n",
    "   - How does it compare to the image / audio methods described here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
